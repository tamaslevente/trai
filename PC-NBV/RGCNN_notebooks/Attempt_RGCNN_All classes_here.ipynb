{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f67684ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "880fa9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetGraph(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GetGraph, self).__init__()\n",
    "\n",
    "    def forward(self, point_cloud):\n",
    "        point_cloud_transpose = point_cloud.permute(0, 2, 1)\n",
    "        point_cloud_inner = torch.matmul(point_cloud, point_cloud_transpose)\n",
    "        point_cloud_inner = -2 * point_cloud_inner\n",
    "        point_cloud_square = torch.sum(torch.mul(point_cloud, point_cloud), dim=2, keepdim=True)\n",
    "        point_cloud_square_tranpose = point_cloud_square.permute(0, 2, 1)\n",
    "        adj_matrix = point_cloud_square + point_cloud_inner + point_cloud_square_tranpose\n",
    "        adj_matrix = torch.exp(-adj_matrix)\n",
    "        return adj_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "019f889c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetLaplacian(nn.Module):\n",
    "    def __init__(self, normalize=True):\n",
    "        super(GetLaplacian, self).__init__()\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def diag(self, mat):\n",
    "        # input is batch x vertices\n",
    "        d = []\n",
    "        for vec in mat:\n",
    "            d.append(torch.diag(vec))\n",
    "        return torch.stack(d)\n",
    "\n",
    "    def forward(self, adj_matrix):\n",
    "        if self.normalize:\n",
    "            D = torch.sum(adj_matrix, dim=2)\n",
    "            eye = torch.ones_like(D)\n",
    "            eye = self.diag(eye)\n",
    "            D = 1 / torch.sqrt(D)\n",
    "            D = self.diag(D)\n",
    "            L = eye - torch.matmul(torch.matmul(D, adj_matrix), D)\n",
    "        else:\n",
    "            D = torch.sum(adj_matrix, dim=1)\n",
    "            D = torch.matrix_diag(D)\n",
    "            L = D - adj_matrix\n",
    "        return L\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "020a6c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetFilter(nn.Module):\n",
    "    def __init__(self, Fin, K, Fout):\n",
    "        super(GetFilter, self).__init__()\n",
    "        self.Fin = Fin\n",
    "        self.Fout = Fout\n",
    "        self.K = K\n",
    "        self.W = nn.Parameter(torch.Tensor(self.K * self.Fin, self.Fout))\n",
    "        nn.init.normal_(self.W, mean=0, std=0.2)\n",
    "        self.B = nn.Parameter(torch.Tensor(self.Fout))\n",
    "        nn.init.normal_(self.B, mean=0, std=0.2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    # def reset_parameters(self):\n",
    "\n",
    "    def forward(self, x, L):\n",
    "        N, M, Fin = list(x.size())\n",
    "        K = self.K\n",
    "        x0 = x.clone()\n",
    "        x = x0.unsqueeze(0)\n",
    "\n",
    "        #         x = x.expand(-1,-1,-1,1)\n",
    "        def concat(x, x_):\n",
    "            x_ = x_.unsqueeze(0)\n",
    "            #             x_ = x.expand(1,-1,-1)\n",
    "            return torch.cat((x, x_), dim=0)\n",
    "\n",
    "        if K > 1:\n",
    "            x1 = torch.matmul(L, x0)\n",
    "            x = concat(x, x1)\n",
    "        for k in range(2, K):\n",
    "            x2 = 2 * torch.matmul(L, x1) - x0\n",
    "            x = concat(x, x2)\n",
    "            x0, x1 = x1, x2\n",
    "        x = x.permute(1, 2, 3, 0)\n",
    "        x = x.reshape(N * M, Fin * K)\n",
    "        x = torch.matmul(x, self.W)\n",
    "        x = torch.add(x, self.B)\n",
    "        x = self.relu(x)\n",
    "        return x.reshape(N, M, self.Fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60cc549e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RGCNN_Seg(nn.Module):\n",
    "    def __init__(self, vertice, F, K, M, regularization=0, dropout=0, batch_size=100, eval_frequency=200,\n",
    "                 dir_name=''):\n",
    "\n",
    "        # Verify the consistency w.r.t. the number of layers.\n",
    "        assert len(F) == len(K)\n",
    "\n",
    "        super(RGCNN_Seg, self).__init__()\n",
    "        # Keep the useful Laplacians only. May be zero.\n",
    "        self.vertice = vertice\n",
    "        # Print information about NN architecture.\n",
    "        Ngconv = len(F)\n",
    "        Nfc = len(M)\n",
    "        print('NN architecture')\n",
    "        print('  input: M_0 = {}'.format(vertice))\n",
    "        for i in range(Ngconv):\n",
    "            print('  layer {0}: gconv{0}'.format(i + 1))\n",
    "            print('    representation: M_{0} * F_{1}= {2} * {3} = {4}'.format(\n",
    "                i, i + 1, vertice, F[i], vertice * F[i]))\n",
    "            F_last = F[i - 1] if i > 0 else 1\n",
    "            print('    weights: F_{0} * F_{1} * K_{1} = {2} * {3} * {4} = {5}'.format(\n",
    "                i, i + 1, F_last, F[i], K[i], F_last * F[i] * K[i]))\n",
    "            print('    biases: F_{} = {}'.format(i + 1, F[i]))\n",
    "        for i in range(Nfc):\n",
    "            name = 'fc{}'.format(i + 1)\n",
    "            print('  layer {}: {}'.format(Ngconv + i + 1, name))\n",
    "            print('    representation: M_{} = {}'.format(Ngconv + i + 1, M[i]))\n",
    "            M_last = M[i - 1] if i > 0 else vertice if Ngconv == 0 else vertice * F[-1]\n",
    "            print('    weights: M_{} * M_{} = {} * {} = {}'.format(\n",
    "                Ngconv + i, Ngconv + i + 1, M_last, M[i], M_last * M[i]))\n",
    "            print('    biases: M_{} = {}'.format(Ngconv + i + 1, M[i]))\n",
    "\n",
    "        # Operations\n",
    "        self.getGraph = GetGraph()\n",
    "        self.getLaplacian = GetLaplacian(normalize=True)\n",
    "        # Store attributes and bind operations.\n",
    "        self.F, self.K, self.M = F, K, M\n",
    "        self.regularization, self.dropout = regularization, dropout\n",
    "        self.batch_size, self.eval_frequency = batch_size, eval_frequency\n",
    "        self.dir_name = dir_name\n",
    "        self.regularizer = []\n",
    "        for i in range(len(F)):\n",
    "            if i == 0:\n",
    "                layer = GetFilter(Fin=6, K=K[i], Fout=F[i])\n",
    "            else:\n",
    "                layer = GetFilter(Fin=F[i - 1], K=K[i], Fout=F[i])\n",
    "            setattr(self, 'gcn%d' % i, layer)\n",
    "\n",
    "    def forward(self, x, cat):\n",
    "        L = self.getGraph(x)\n",
    "        L = self.getLaplacian(L)\n",
    "        #         cat = torch.unsqueeze(cat,1)\n",
    "        #         cat = torch.zeros(self.batch_size, self.class_size).scatter_(1, cat, 1)\n",
    "        #         cat = torch.unsqueeze(cat,1)\n",
    "        #         cat = cat.expand(-1,self.vertice,-1).double()\n",
    "        #         x = torch.cat((x,cat),dim=2)\n",
    "        for i in range(len(self.F)):\n",
    "            x = getattr(self, 'gcn%d' % i)(x, L)\n",
    "            self.regularizer.append(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8dc5b999",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating data\n",
    "# def genData(cls,limit=None):\n",
    "#     assert type(cls) is str\n",
    "\n",
    "#     seg_classes = {'Earphone': [16, 17, 18], 'Motorbike': [30, 31, 32, 33, 34, 35], 'Rocket': [41, 42, 43],\n",
    "#                    'Car': [8, 9, 10, 11], 'Laptop': [28, 29], 'Cap': [6, 7], 'Skateboard': [44, 45, 46],\n",
    "#                    'Mug': [36, 37], 'Guitar': [19, 20, 21], 'Bag': [4, 5], 'Lamp': [24, 25, 26, 27],\n",
    "#                    'Table': [47, 48, 49], 'Airplane': [0, 1, 2, 3], 'Pistol': [38, 39, 40], 'Chair': [12, 13, 14, 15],\n",
    "#                    'Knife': [22, 23]}\n",
    "\n",
    "#     data = np.load(\"/home/tegs/RGCNN/data_%s.npy\" % cls)\n",
    "#     label = np.load(\"/home/tegs/RGCNN/label_%s.npy\" % cls)\n",
    "\n",
    "#     data = data[:limit]\n",
    "#     label = label[:limit]\n",
    "\n",
    "#     seg = {}\n",
    "#     name = {}\n",
    "#     i = 0\n",
    "#     for k,v in sorted(seg_classes.items()):\n",
    "#         for value in v:\n",
    "#             seg[value] = i\n",
    "#             name[value] = k\n",
    "#         i += 1\n",
    "#     cnt = data.shape[0]\n",
    "#     cat = np.zeros((cnt))\n",
    "#     for i in range(cnt):\n",
    "#         cat[i] = seg[label[i][0]]\n",
    "#     return data,label,cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ec7c150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "#     train_data, train_label, train_cat = genData('train')\n",
    "#     val_data, val_label, val_cat = genData('val')\n",
    "#     test_data, test_label, test_cat = genData('test')\n",
    "    params = dict()\n",
    "    params['dir_name'] = 'model'\n",
    "    params['num_epochs'] = 50\n",
    "    params['batch_size'] = 26\n",
    "    params['eval_frequency'] = 30\n",
    "\n",
    "    # Building blocks.\n",
    "    params['filter'] = 'chebyshev5'\n",
    "    params['brelu'] = 'b1relu'\n",
    "    params['pool'] = 'apool1'\n",
    "\n",
    "    # Number of classes.\n",
    "    # C = y.max() + 1\n",
    "    # assert C == np.unique(y) .size\n",
    "\n",
    "    # Architecture.\n",
    "    params['F'] = [128, 512, 1024, 512, 128, 50]  # Number of graph convolutional filters.\n",
    "    params['K'] = [6, 5, 3, 1, 1, 1]  # Polynomial orders.\n",
    "    params['M'] = [384, 16, 1]  # Output dimensionality of fully connected layers.\n",
    "\n",
    "    # Optimization.\n",
    "    params['regularization'] = 1e-9\n",
    "    params['dropout'] = 1\n",
    "    params['learning_rate'] = 1e-3\n",
    "    params['decay_rate'] = 0.95\n",
    "    params['momentum'] = 0\n",
    "    params['decay_steps'] = train_data.shape[0] / params['batch_size']\n",
    "\n",
    "    model = seg_model.rgcnn(2048, **params)\n",
    "    accuracy, loss, t_step = model.fit(train_data, train_cat, train_label, val_data, val_cat, val_label,\n",
    "                                       is_continue=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f6af98a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'base_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-71db80189b07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mrgcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \"\"\"\n\u001b[1;32m      3\u001b[0m     \u001b[0mGraph\u001b[0m \u001b[0mCNN\u001b[0m \u001b[0mwhich\u001b[0m \u001b[0muses\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mChebyshev\u001b[0m \u001b[0mapproximation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mThe\u001b[0m \u001b[0mfollowing\u001b[0m \u001b[0mare\u001b[0m \u001b[0mhyper\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mparameters\u001b[0m \u001b[0mof\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0mconvolutional\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mThey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mlists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhich\u001b[0m \u001b[0mlength\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mequal\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0mof\u001b[0m \u001b[0mgconv\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'base_model' is not defined"
     ]
    }
   ],
   "source": [
    "class rgcnn(base_model):\n",
    "    \"\"\"\n",
    "    Graph CNN which uses the Chebyshev approximation.\n",
    "    The following are hyper-parameters of graph convolutional layers.\n",
    "    They are lists, which length is equal to the number of gconv layers.\n",
    "        F: Number of features.\n",
    "        K: List of polynomial orders, i.e. filter sizes or number of hopes.\n",
    "    The following are hyper-parameters of fully connected layers.\n",
    "    They are lists, which length is equal to the number of fc layers.\n",
    "        M: Number of features per sample, i.e. number of hidden neurons.\n",
    "           The last layer is the softmax, i.e. M[-1] is the number of classes.\n",
    "    Training parameters:\n",
    "        num_epochs:    Number of training epochs.\n",
    "        learning_rate: Initial learning rate.\n",
    "        decay_rate:    Base of exponential decay. No decay with 1.\n",
    "        decay_steps:   Number of steps after which the learning rate decays.\n",
    "        momentum:      Momentum. 0 indicates no momentum.\n",
    "    Regularization parameters:\n",
    "        regularization: L2 regularizations of weights and biases.\n",
    "        dropout:        Dropout (fc layers): probability to keep hidden neurons. No dropout with 1.\n",
    "        batch_size:     Batch size. Must divide evenly into the dataset sizes.\n",
    "        eval_frequency: Number of steps between evaluations.\n",
    "    Directories:\n",
    "        dir_name: Name for directories (summaries and model parameters).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vertice, F, K, M, filter='chebyshev5', brelu='b1relu', pool='mpool1',\n",
    "                 num_epochs=20, learning_rate=0.1, decay_rate=0.95, decay_steps=None, momentum=0.9,\n",
    "                 regularization=0, dropout=0, batch_size=100, eval_frequency=200,\n",
    "                 dir_name=''):\n",
    "        super().__init__()\n",
    "\n",
    "        # Verify the consistency w.r.t. the number of layers.\n",
    "        assert len(F) == len(K)\n",
    "\n",
    "        # Keep the useful Laplacians only. May be zero.\n",
    "        M_0 = vertice\n",
    "        # Print information about NN architecture.\n",
    "        Ngconv = len(F)\n",
    "        Nfc = len(M)\n",
    "        print('NN architecture')\n",
    "        print('  input: M_0 = {}'.format(vertice))\n",
    "        for i in range(Ngconv):\n",
    "            print('  layer {0}: gconv{0}'.format(i + 1))\n",
    "            print('    representation: M_{0} * F_{1}= {2} * {3} = {4}'.format(\n",
    "                i, i + 1, vertice, F[i], vertice * F[i]))\n",
    "            F_last = F[i - 1] if i > 0 else 1\n",
    "            print('    weights: F_{0} * F_{1} * K_{1} = {2} * {3} * {4} = {5}'.format(\n",
    "                i, i + 1, F_last, F[i], K[i], F_last * F[i] * K[i]))\n",
    "            print('    biases: F_{} = {}'.format(i + 1, F[i]))\n",
    "        for i in range(Nfc):\n",
    "            name = 'fc{}'.format(i + 1)\n",
    "            print('  layer {}: {}'.format(Ngconv + i + 1, name))\n",
    "            print('    representation: M_{} = {}'.format(Ngconv + i + 1, M[i]))\n",
    "            M_last = M[i - 1] if i > 0 else vertice if Ngconv == 0 else vertice * F[-1]\n",
    "            print('    weights: M_{} * M_{} = {} * {} = {}'.format(\n",
    "                Ngconv + i, Ngconv + i + 1, M_last, M[i], M_last * M[i]))\n",
    "            print('    biases: M_{} = {}'.format(Ngconv + i + 1, M[i]))\n",
    "\n",
    "        # Store attributes and bind operations.\n",
    "        self.F, self.K, self.M = F, K, M\n",
    "        self.num_epochs, self.learning_rate = num_epochs, learning_rate\n",
    "        self.decay_rate, self.decay_steps, self.momentum = decay_rate, decay_steps, momentum\n",
    "        self.regularization, self.dropout = regularization, dropout\n",
    "        self.batch_size, self.eval_frequency = batch_size, eval_frequency\n",
    "        self.dir_name = dir_name\n",
    "        self.filter = getattr(self, filter)\n",
    "        self.brelu = getattr(self, brelu)\n",
    "\n",
    "        # Build the computational graph.\n",
    "        self.build_graph(M_0)\n",
    "\n",
    "#     def chebyshev5(self, x, L, Fout, K):\n",
    "#         # If K == 1 it is equivalent to fc layer\n",
    "#         N, M, Fin = x.get_shape()\n",
    "#         N, M, Fin = int(N), int(M), int(Fin)\n",
    "#         x0 = x  # N x M x Fin\n",
    "#         x = tf.expand_dims(x0, 0)\n",
    "\n",
    "#         def concat(x, x_):\n",
    "#             x_ = tf.expand_dims(x_, 0)  # 1 x M x Fin*N\n",
    "#             return tf.concat([x, x_], axis=0)  # K x M x Fin*N\n",
    "\n",
    "#         if K > 1:\n",
    "#             x1 = tf.matmul(L, x0)\n",
    "#             x = concat(x, x1)\n",
    "#         for k in range(2, K):\n",
    "#             x2 = 2 * tf.matmul(L, x1) - x0\n",
    "#             x = concat(x, x2)\n",
    "#             x0, x1 = x1, x2\n",
    "#         # K x N x M x Fin\n",
    "#         x = tf.transpose(x, perm=[1, 2, 3, 0])  # N x M x Fin x K\n",
    "#         x = tf.reshape(x, [N * M, Fin * K])  # N*M x Fin*K\n",
    "#         # Filter: Fin*Fout filters of order K, i.e. one filterbank per feature pair.\n",
    "#         W = self._weight_variable([Fin * K, Fout], regularization=False)\n",
    "#         x = tf.matmul(x, W)  # N*M x Fout\n",
    "#         return tf.reshape(x, [N, M, Fout])  # N x M x Fout\n",
    "\n",
    "    def b1relu(self, x):\n",
    "        \"\"\"Bias and ReLU. One bias per filter.\"\"\"\n",
    "        N, M, F = x.get_shape()\n",
    "        b = self._bias_variable([1, 1, int(F)], regularization=False)\n",
    "        return tf.nn.relu(x + b)\n",
    "\n",
    "    def b2relu(self, x):\n",
    "        \"\"\"Bias and ReLU. One bias per vertex per filter.\"\"\"\n",
    "        N, M, F = x.get_shape()\n",
    "        b = self._bias_variable([1, int(M), int(F)], regularization=False)\n",
    "        return tf.nn.relu(x + b)\n",
    "\n",
    "    def fc(self, x, Mout, relu=True):\n",
    "        \"\"\"Fully connected layer with Mout features.\"\"\"\n",
    "        N, Min = x.get_shape()\n",
    "        W = self._weight_variable([int(Min), Mout], regularization=True)\n",
    "        b = self._bias_variable([Mout], regularization=True)\n",
    "        x = tf.matmul(x, W) + b\n",
    "        return tf.nn.relu(x) if relu else x\n",
    "\n",
    "    def pairwise_distance(self, point_cloud):\n",
    "        \"\"\"Compute pairwise distance of a point cloud.\n",
    "        Args:\n",
    "            point_cloud: tensor (batch_size, num_points, num_dims)\n",
    "        Returns:\n",
    "            pairwise distance: (batch_size, num_points, num_points)\n",
    "        \"\"\"\n",
    "\n",
    "        og_batch_size = point_cloud.get_shape().as_list()[0]\n",
    "        point_cloud = tf.squeeze(point_cloud)\n",
    "        if og_batch_size == 1:\n",
    "            point_cloud = tf.expand_dims(point_cloud, 0)\n",
    "\n",
    "        point_cloud_transpose = tf.transpose(point_cloud, perm=[0, 2, 1])\n",
    "        point_cloud_inner = tf.matmul(point_cloud, point_cloud_transpose)\n",
    "        point_cloud_inner = -2 * point_cloud_inner\n",
    "        point_cloud_square = tf.reduce_sum(tf.square(point_cloud), axis=-1, keep_dims=True)\n",
    "        point_cloud_square_tranpose = tf.transpose(point_cloud_square, perm=[0, 2, 1])\n",
    "        adj_matrix = point_cloud_square + point_cloud_inner + point_cloud_square_tranpose\n",
    "        adj_matrix = tf.exp(-adj_matrix)\n",
    "        return adj_matrix\n",
    "\n",
    "    def knn(self, adj_matrix, k=30):\n",
    "        \"\"\"Get KNN based on the pairwise distance.\n",
    "        Args:\n",
    "            pairwise distance: (batch_size, num_points, num_points)\n",
    "            k: int\n",
    "        Returns:\n",
    "            nearest neighbors: (batch_size, num_points, k)\n",
    "        \"\"\"\n",
    "        # neg_adj = -adj_matrix\n",
    "        _, nn_idx = tf.nn.top_k(adj_matrix, k=k)\n",
    "        return nn_idx\n",
    "\n",
    "    def get_laplacian(self, adj_matrix, normalize=True):\n",
    "        \"\"\"Compute pairwise distance of a point cloud.\n",
    "        Args:\n",
    "            pairwise distance: tensor (batch_size, num_points, num_points)\n",
    "        Returns:\n",
    "            pairwise distance: (batch_size, num_points, num_points)\n",
    "        \"\"\"\n",
    "        if normalize:\n",
    "            D = tf.reduce_sum(adj_matrix, axis=1)  # (batch_size,num_points)\n",
    "            eye = tf.ones_like(D)\n",
    "            eye = tf.matrix_diag(eye)\n",
    "            D = 1 / tf.sqrt(D)\n",
    "            D = tf.matrix_diag(D)\n",
    "            L = eye - tf.matmul(tf.matmul(D, adj_matrix), D)\n",
    "        else:\n",
    "            D = tf.reduce_sum(adj_matrix, axis=1)  # (batch_size,num_points)\n",
    "            # eye = tf.ones_like(D)\n",
    "            # eye = tf.matrix_diag(eye)\n",
    "            # D = 1 / tf.sqrt(D)\n",
    "            D = tf.matrix_diag(D)\n",
    "            L = D - adj_matrix\n",
    "        return L\n",
    "\n",
    "    def get_one_matrix_knn(self, matrix, k):\n",
    "        values, indices = tf.nn.top_k(matrix, k,\n",
    "                                      sorted=False)  # indices will be [[0, 1], [1, 2]], values will be [[6., 2.], [4., 5.]]\n",
    "\n",
    "        my_range = tf.expand_dims(tf.range(0, indices.get_shape()[0]), 1)  # will be [[0], [1]]\n",
    "        my_range_repeated = tf.tile(my_range, [1, k])  # will be [[0, 0], [1, 1]]\n",
    "\n",
    "        # change shapes to [N, k, 1] and [N, k, 1], to concatenate into [N, k, 2]\n",
    "        full_indices = tf.concat([tf.expand_dims(my_range_repeated, 2), tf.expand_dims(indices, 2)], axis=2)\n",
    "        full_indices = tf.reshape(full_indices, [-1, 2])\n",
    "\n",
    "        to_substract = tf.sparse_to_dense(full_indices, matrix.get_shape(), tf.reshape(values, [-1]), default_value=0.,\n",
    "                                          validate_indices=False)\n",
    "\n",
    "        # res = matrix - to_substract  # res should be all 0.\n",
    "        return to_substract\n",
    "\n",
    "    def _inference(self, x, cat, dropout):\n",
    "        L = self.pairwise_distance(x)\n",
    "        # L_ =self.get_laplacian(L,normalize=False)\n",
    "        # L = tf.stack([self.get_one_matrix_knn(matrix = L[o],k = 30) for o in range(L.get_shape()[0])])\n",
    "        L = self.get_laplacian(L)\n",
    "        cat = tf.expand_dims(cat, axis=1)\n",
    "        cat = tf.one_hot(cat, 16, axis=-1)\n",
    "        cat = tf.tile(cat, [1, 2048, 1])\n",
    "        x = tf.concat([x, cat], axis=2)\n",
    "\n",
    "        x1 = 0 #cache for layer1\n",
    "        for i in range(len(self.F)):\n",
    "            with tf.variable_scope('conv{}'.format(i)):\n",
    "                with tf.name_scope('filter'):\n",
    "                    if i == 4:\n",
    "                        x = tf.concat([x, x1], axis=2)\n",
    "                    x = self.filter(x, L, self.F[i], self.K[i])\n",
    "                    if i == 1:\n",
    "                        x1 = x\n",
    "                    self.regularizers.append(tf.nn.l2_loss(tf.matmul(tf.matmul(tf.transpose(x, perm=[0, 2, 1]), L), x)))\n",
    "                with tf.name_scope('bias_relu'):\n",
    "                    x = self.brelu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a2cbb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
