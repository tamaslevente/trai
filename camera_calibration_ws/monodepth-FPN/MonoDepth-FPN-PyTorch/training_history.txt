%%%%%%%%
%Rules:%
%%%%%%%%
    ~ I will only add things that I changed with respect to the previous training session.
    ~ A "+" means a new configuration added.
    ~ A "-" means a removed configuration.
#############
###Default###
#############
Loss: RMSE_log; 
Optimizer: SGD, momentum=0.9, lr=1e-3, lr_decay_step=5, lr_decay_gamma=0.1;
Training parameters: epochs=10, bs=1 (batch_size), num_workers=1;
Other training parameters: gamma_sup=1. (factor of supervised loss), gamma_unsup=1. (factor of unsupervised learning), gamma_reg=10. (factor of regularization loss);
#########     #
    #       # #
    #         #
    #       # # # 
Loss:
Optimizer:
Training parameters:
Other training parameters:
Others: 
Results: After 6 epochs no major improvements
[epoch  6][iter  590] loss: 4.9486 RMSElog: 4.9486
#########   ###
    #      #   #
    #         #
    #       # # # 
Loss:
Optimizer:
Training parameters:
Other training parameters:
Others: Changed the mode of scaling the network output (multiplying by 65535)
Results: Again, as expected, not much of a difference
[epoch  6][iter  590] loss: 5.3301 RMSElog: 5.3301
#########     # # #
    #         ###
    #            #
    #       # # # 
Loss:
Optimizer:
Training parameters: epochs=15;
Other training parameters:
Others: Multiplying loss with 10
Results: Much faster drop in loss, so... we'll keep this change, but there is still a problem in scaling the image as it is identifying some very distant points
#########     #
    #       # #
    #      #####
    #         # 
Loss:
Optimizer:
Training parameters:
Other training parameters:
Others: I'll try scaling the output with he max value from gt depth image
Results: As expected the scaling goes well. 
[epoch 14][iter  580] loss: 37.7501 RMSElog: 3.7750
[epoch 14][iter  590] loss: 43.2417 RMSElog: 4.3242
#########     #####
    #        #### 
    #            #
    #       # ##  
Loss:
Optimizer:
Training parameters:
Other training parameters:
Others: new loss format (making the mean over an epoch)
Results:  not a good loss, because I made a wrong mean
#########     ###
    #        ### 
    #       #   #
    #        ###  
Loss:
Optimizer:
Training parameters:
Other training parameters:
Others: changed from using PIL to OpenCV for reading images (also now is no normalization)
Results: Ok, I would say.
Epoch: 14 	Training Loss: 41.911617 	Validation Loss: 562.930368
#########   ####  
    #         #
    #        #
    #       # 
Loss:
Optimizer:
Training parameters:
Other training parameters:
Others: 6000 is the max depth for output after scaling, plus only some error corrections for OpenCV when saving images
Results: 
Epoch: 14 	Training Loss: 41.909513 	Validation Loss: 561.895508
#########   / \
    #       \ / 
    #       / \
    #       \_/ 
Loss:
Optimizer:
Training parameters:
Other training parameters:
Others: -applied normalization on inputs (basically dividing images with the max depth value (6571) from the dataset, and also restoring the predicted image by multiplying the output with the same value)   
Results: not much of an improvement
Epoch: 14 	Training Loss: 41.883721 	Validation Loss: 284.111349
#########   ###
    #       ###
    #         #
    #       ###  
Loss: Change RMSE_log with L1 
Optimizer:
Training parameters:
Other training parameters:
Others: fixed some errors when calculating the validation loss
Results: bad ideea :))
Epoch: 4 	Training Loss: 349168.011146 	Validation Loss: 196926.170417
#########     #    ####
    #       # #    #  #
    #         #    #  #
    #       # # #  ####
Loss: RMSElog + Gradient loss
Optimizer:
Training parameters:
Other training parameters:
Others: loss = 10*depth_loss + 0.01*grad_loss
Results: Some changes, in the fact that the range has dropped from 0~6000 to 0~1400 (not ok)
Epoch: 14 	Training Loss: 201.239118 	Validation Loss: 118.499354
#########     #       #
    #       # #     # #
    #         #       #
    #       # # #   # # #
Loss: 
Optimizer:
Training parameters:
Other training parameters:
Others: loss = 10*depth_loss + 0.001*grad_loss
Results: Now range is from 0~2000
Epoch: 14 	Training Loss: 58.986532 	Validation Loss: 121.968695
#########     #     ###
    #       # #    #  #
    #         #      #
    #       # # #   #### 
Loss:
Optimizer:
Training parameters:
Other training parameters:
Others: loss = 10*(depth_loss + 0.001*grad_loss) with gradient loss from epoch zero
Results: Not ok.
Epoch: 14 	Training Loss: 201.117745 	Validation Loss: 118.385841
#########     #    ###
    #       # #     __#
    #         #       #
    #       # # #  ###
Loss:
Optimizer:
Training parameters:
Other training parameters:
Others: loss = 10*(depth_loss + 0.001*grad_loss) from epoch five
Results: 
Epoch: 12 	Training Loss: 202.250685 	Validation Loss: 118.536363
#########     #      #
    #       # #    # #
    #         #   ######
    #       # # #    #
Loss:
Optimizer:
Training parameters:
Other training parameters:
Others: loss = 10*(depth_loss + 0.0001*grad_loss) from epoch five
Results: Again, not good.
Epoch: 14 	Training Loss: 58.996475 	Validation Loss: 38.082026
#########     #       ####
    #       # #      ###
    #         #         #
    #       # # #   ###
Loss:
Optimizer:
Training parameters:
Other training parameters:
Others: loss = 10*(depth_loss + 0.0001*grad_loss) from epoch 9
Results: Still, not good.
Epoch: 14 	Training Loss: 59.500130 	Validation Loss: 38.195396
#########     #         ###
    #       # #        ##
    #         #       #  #
    #       # # #     ### 
Loss:
Optimizer:
Training parameters:
Other training parameters:
Others: loss = 10*(depth_loss + 0.00001*grad_loss) from epoch 6
Results: Seems like no matter how small I make the gradient, it'll still downscale the maximum depth. (Pretty strange)
Epoch: 14 	Training Loss: 43.861073 	Validation Loss: 29.600205
#########     #    #####
    #       # #       #
    #         #      #
    #       # # #   #
Loss:
Optimizer:
Training parameters:
Other training parameters:
Others:  loss = 10*(depth_loss + 0.001*grad_loss) from epoch 6. I'm rescaling images only for representation. 
Results: So, by not calculaing loss on rescaled images, there seems to be some small improvement in this configuration, especially because now gradient loss has values much smaller (~5)
Epoch: 14 	Training Loss: 26.806246 	Validation Loss: 213.137326
#########     #    ####
    #       # #    #__#
    #         #    #  # 
    #       # # #  ####
Loss:
Optimizer:
Training parameters:
Other training parameters:
Others: loss = 10*(depth_loss + 1*grad_loss) from fourth epoch
Results: We need a smaller impact from gradient loss.
Epoch: 10 	Training Loss: 52.656817 	Validation Loss: 88086.864388
#########     #    ####
    #       # #    #__#
    #         #       # 
    #       # # #  ####
Loss:
Optimizer:
Training parameters:
Other training parameters:
Others: loss = 10*(depth_loss + 0.1*grad_loss) from fourth epoch
Results: Doesn't seem very good, we'll try smaller. It's still making a small rescaling (from max 6000 to max 5000)
Epoch: 14 	Training Loss: 29.714553 	Validation Loss: 19.944820
#########   ###     ####
    #      #   #    #  #
    #         #     #  #  
    #       # # #   ####
Loss:
Optimizer:
Training parameters:
Other training parameters:
Others: loss = 10*(depth_loss + 0.01*grad_loss) from fourth epoch
Results: Results ar interesting. I will say that for the moment it is ok like this, in future tests I might try to raise it, maybe just a little bit.
#########   ###       #
    #      #   #    # #
    #         #       #
    #       # # #   # # #
Loss: 
Optimizer:
Training parameters:
Other training parameters:
Others: Changed the training dataset with RGB images made only from depth
Results: Mostly, not that much of an improvement.
Epoch: 14 	Training Loss: 27.093058 	Validation Loss: 18.416263
#########   ###      ###
    #      #   #    #   #
    #         #        #
    #       # # #    # # #
Loss:
Optimizer:
Training parameters:
Other training parameters:
Others: resized images to 360x640 (this is the camera format) because otherwise I couldn't reconstruct the point cloud.
Results: Yeah... Seems a little worse now. :/ 
Epoch: 14 	Training Loss: 45.351826 	Validation Loss: 26.970308
#########   ###      ###
    #      #   #     ___# 
    #         #         #
    #       # # #    ### 
Loss:
Optimizer:
Training parameters:
Other training parameters:
Others: loss = 20*(depth_loss + 0.01*grad_loss) and on 30 epochs.
Results: Not much of an improvement after epoch 7... :/
Epoch: 7 	Training Loss: 89.957028 	Validation Loss: 53.369854
Epoch: 29 	Training Loss: 89.056271 	Validation Loss: 53.559971
#########   ###         #
    #      #   #      # #
    #         #      #####
    #       # # #      #
Loss: + Normal Loss diff
Optimizer:
Training parameters:
Other training parameters:
Others: loss = 10*(depth_loss + 0.01*grad_loss) + normals_diff_loss on 15 epochs
Results: Pretty bad, I mean, it's not much of an improvement from the last try, and honestly it's not that good since I changed sizes. :(
Epoch: 14 	Training Loss: 102.961625 	Validation Loss: 59.245287
#########   ###       ####
    #      #   #     #__
    #         #         #
    #       # # #   ####
Loss: - Depth loss and grad_loss
Optimizer:
Training parameters:
Other training parameters:
Others: loss_val = depth_loss_eval + normals_diff_loss_eval - depth_loss_eval
Results: Not even worth it, of course it does nothing.
Epoch: 1 	Training Loss: 35.740943 	Validation Loss: 20.163141
#########   ###      ###
    #      #   #    #__
    #         #     #  #
    #       # # #   #### 
Loss: + Depth and grad loss
Optimizer:  
Training parameters:
Other training parameters:
Others: loss = 10*(depth_loss + 0.01*grad_loss) + normals_diff_loss, but normal only from the 8th epoch
Results: Not great, not great at all
Epoch: 14 	Training Loss: 102.577704 	Validation Loss: 59.296663
#########   ###     #####
    #      #   #       #
    #         #       #
    #       # # #    #
Loss:
Optimizer:
Training parameters:
Other training parameters:
Others: loss = depth_loss + grad_loss + normals_diff_loss
Results: Just bad...
Epoch: 14 	Training Loss: 68.308816 	Validation Loss: 38.464952
#########   ###     ####
    #      #   #    #__#
    #         #     #  #
    #       # # #   ####
Loss: - depth loss and gradient loss and normal loss + DDDDepthLoss
Optimizer:
Training parameters:
Other training parameters:
Others: loss = depth_loss + 10*dddDepth_loss - depth_loss + FINALLY FIXED THE VALIDATION LOSS, NOW IT IS BIGGER THAN TRAINING LOSS, AS IT SHOULD BE!!!
Results: Of! Not very good results... And I mean, only a uniform distribution.:))
Epoch: 14 	Training Loss: 565.429661 	Validation Loss: 568.003488
#########   ###      ####
    #      #   #     #__#
    #         #         #
    #       # # #     ###
Loss: depth_loss, grad_loss, normal_loss
Optimizer:
Training parameters:
Other training parameters:
Others: original three: loss = 10*(depth_loss + grad_loss + normal_loss)
Results: It is somehow ok, but the scale is totally wrong.
Epoch: 14 	Training Loss: 77.451734 	Validation Loss: 80.114268
#########      # # #   #####
    #         ____#   #   #
    #            #   #   #
    #       # # #   #####
Loss:
Optimizer:
Training parameters: 
Other training parameters:
Others: Addingsome log in DDDDepthLoss function
Results: Pretty bad... Extremely bad
Epoch: 0 	Training Loss: 28.474500 	Validation Loss: 28.636637
Epoch: 19 	Training Loss: 28.474500 	Validation Loss: 28.637274
#########      # # #      #
    #         ____#    # # 
    #            #      #
    #       # # #    # # # 
Loss:
Optimizer:
Training parameters: 
Other training parameters:
Others: Normalized pointclouds in DDDDepthLoss between 1e-7 and 1 with formula exactly like in RMSE_log function for calculating loss.
Results: Disappointing...
Epoch: 7 	Training Loss: 778.920775 	Validation Loss: 779.173272
#########      # # #     ##
    #         ____#    #  #
    #            #      #
    #       # # #     # # # 
Loss: 
Optimizer:
Training parameters: 
Other training parameters:
Others: loss_val = depth_loss_eval + 10*dddDepth_loss_eval - depth_loss_eval with modifications in DDDDepthLoss function
Results:Absolutely nothing good!
#########      # # #  # # # 
    #         ____#  ____#
    #            #      #
    #       # # #  # # #
Loss:
Optimizer:
Training parameters: 
Other training parameters:
Others: Removed point cloud normalization and kept only the shift on x anf y axes of point clouds in DDDDepthLoss (and now, this is correct)!!!!
Results:
#########      # # #       #
    #         ____#     # #
    #            #   # # # #  
    #       # # #       #
Loss: Only DDDDepthLoss
Optimizer:
Training parameters: 
Other training parameters:
Others: loss = 10*dddDepth_loss, A correct DDDDepthLoss function, which gradients are computed by pytorch
Results: Not bad, not great, as it is exactly as depth_loss... BUT IT WORKS!!!
Epoch: 14 	Training Loss: 45.133220 	Validation Loss: 83.153221
#########      # # #    ####
    #         ____#    #__
    #            #        #
    #       # # #     ####
Loss:
Optimizer:
Training parameters: 
Other training parameters:
Others: DDDDepthLoss is calculated not only between z axis, but on all three (x,y,z) (Even though it is no difference between the two, as numbers!)
Results:
#########      # # #    ##
    #         ____#   #__ 
    #            #   #  #
    #       # # #    ###
Loss:
Optimizer:
Training parameters: 
Other training parameters:
Others: 
Results:
#########      # # #  #####  
    #         ____#      #
    #            #     #
    #       # # #    #
Loss:
Optimizer:
Training parameters: 
Other training parameters:
Others: 
Results:
#########      # # #   #####
    #         ____#   #___#
    #            #   #   #
    #       # # #   #####
Loss:
Optimizer:
Training parameters: 
Other training parameters:
Others: 
Results:
#########      # # #  #####
    #         ____#  #___# 
    #            #      #
    #       # # #   ####
Loss:
Optimizer:
Training parameters: 
Other training parameters:
Others: 
Results: