#bs=1 epochs =10#
#################
[epoch  0][iter    0] loss: 102.1421 RMSElog: 10.2142 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter   10] loss: 96.3584 RMSElog: 9.6358 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter   20] loss: 104.1300 RMSElog: 10.4130 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter   30] loss: 105.0377 RMSElog: 10.5038 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter   40] loss: 101.4625 RMSElog: 10.1463 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter   50] loss: 101.8135 RMSElog: 10.1814 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter   60] loss: 100.5578 RMSElog: 10.0558 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter   70] loss: 105.2577 RMSElog: 10.5258 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter   80] loss: 99.1979 RMSElog: 9.9198 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter   90] loss: 101.8199 RMSElog: 10.1820 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  100] loss: 99.9605 RMSElog: 9.9961 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  110] loss: 97.8561 RMSElog: 9.7856 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  120] loss: 97.0205 RMSElog: 9.7020 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  130] loss: 98.7426 RMSElog: 9.8743 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  140] loss: 93.4997 RMSElog: 9.3500 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  150] loss: 99.7364 RMSElog: 9.9736 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  160] loss: 92.8675 RMSElog: 9.2868 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  170] loss: 98.7566 RMSElog: 9.8757 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  180] loss: 99.8795 RMSElog: 9.9879 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  190] loss: 98.3744 RMSElog: 9.8374 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  200] loss: 100.5708 RMSElog: 10.0571 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  210] loss: 97.9316 RMSElog: 9.7932 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  220] loss: 102.7599 RMSElog: 10.2760 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  230] loss: 97.3038 RMSElog: 9.7304 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  240] loss: 99.2101 RMSElog: 9.9210 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  250] loss: 93.4858 RMSElog: 9.3486 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  260] loss: 100.2585 RMSElog: 10.0258 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  270] loss: 93.0985 RMSElog: 9.3098 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  280] loss: 99.4858 RMSElog: 9.9486 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  290] loss: 95.2008 RMSElog: 9.5201 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  300] loss: 96.6946 RMSElog: 9.6695 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  310] loss: 98.7955 RMSElog: 9.8795 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  320] loss: 98.5869 RMSElog: 9.8587 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  330] loss: 97.7763 RMSElog: 9.7776 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  340] loss: 101.1741 RMSElog: 10.1174 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  350] loss: 102.2461 RMSElog: 10.2246 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  360] loss: 91.8394 RMSElog: 9.1839 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  370] loss: 98.1989 RMSElog: 9.8199 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  380] loss: 101.8889 RMSElog: 10.1889 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  390] loss: 96.6095 RMSElog: 9.6610 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  400] loss: 97.7234 RMSElog: 9.7723 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  410] loss: 99.2947 RMSElog: 9.9295 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  420] loss: 97.4834 RMSElog: 9.7483 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  430] loss: 101.1590 RMSElog: 10.1159 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  440] loss: 101.0272 RMSElog: 10.1027 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  450] loss: 100.3411 RMSElog: 10.0341 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  460] loss: 97.1710 RMSElog: 9.7171 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  470] loss: 100.6386 RMSElog: 10.0639 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  480] loss: 98.2169 RMSElog: 9.8217 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  490] loss: 97.0805 RMSElog: 9.7080 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  500] loss: 101.8557 RMSElog: 10.1856 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  510] loss: 98.2158 RMSElog: 9.8216 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  520] loss: 97.9008 RMSElog: 9.7901 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  530] loss: 100.0756 RMSElog: 10.0076 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  540] loss: 96.3880 RMSElog: 9.6388 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  550] loss: 99.6100 RMSElog: 9.9610 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  560] loss: 101.5150 RMSElog: 10.1515 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  570] loss: 96.7024 RMSElog: 9.6702 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  580] loss: 101.7094 RMSElog: 10.1709 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  590] loss: 102.5575 RMSElog: 10.2558 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter    0] loss: 100.5090 RMSElog: 10.0509 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter   10] loss: 100.2923 RMSElog: 10.0292 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter   20] loss: 102.5085 RMSElog: 10.2509 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter   30] loss: 101.3287 RMSElog: 10.1329 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter   40] loss: 101.6553 RMSElog: 10.1655 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter   50] loss: 99.3804 RMSElog: 9.9380 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter   60] loss: 100.4031 RMSElog: 10.0403 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter   70] loss: 100.6337 RMSElog: 10.0634 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter   80] loss: 101.7533 RMSElog: 10.1753 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter   90] loss: 103.5933 RMSElog: 10.3593 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  100] loss: 99.4352 RMSElog: 9.9435 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  110] loss: 100.6191 RMSElog: 10.0619 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  120] loss: 99.5148 RMSElog: 9.9515 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  130] loss: 105.0238 RMSElog: 10.5024 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  140] loss: 99.7965 RMSElog: 9.9796 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  150] loss: 102.6984 RMSElog: 10.2698 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  160] loss: 98.7715 RMSElog: 9.8771 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  170] loss: 101.0798 RMSElog: 10.1080 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  180] loss: 102.7606 RMSElog: 10.2761 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  190] loss: 98.3694 RMSElog: 9.8369 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  200] loss: 99.9074 RMSElog: 9.9907 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  210] loss: 101.0709 RMSElog: 10.1071 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  220] loss: 97.5017 RMSElog: 9.7502 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  230] loss: 101.4707 RMSElog: 10.1471 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  240] loss: 99.0244 RMSElog: 9.9024 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  250] loss: 102.6012 RMSElog: 10.2601 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  260] loss: 99.5738 RMSElog: 9.9574 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  270] loss: 99.1009 RMSElog: 9.9101 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  280] loss: 99.9587 RMSElog: 9.9959 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  290] loss: 101.8856 RMSElog: 10.1886 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  300] loss: 99.8862 RMSElog: 9.9886 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  310] loss: 98.2803 RMSElog: 9.8280 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  320] loss: 99.2914 RMSElog: 9.9291 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  330] loss: 98.0161 RMSElog: 9.8016 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  340] loss: 97.7657 RMSElog: 9.7766 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  350] loss: 101.2011 RMSElog: 10.1201 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  360] loss: 98.9546 RMSElog: 9.8955 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  370] loss: 99.0484 RMSElog: 9.9048 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  380] loss: 97.2485 RMSElog: 9.7248 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  390] loss: 97.6532 RMSElog: 9.7653 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  400] loss: 96.1403 RMSElog: 9.6140 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  410] loss: 99.6342 RMSElog: 9.9634 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  420] loss: 99.1566 RMSElog: 9.9157 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  430] loss: 97.2715 RMSElog: 9.7271 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  440] loss: 98.8290 RMSElog: 9.8829 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  450] loss: 100.5371 RMSElog: 10.0537 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  460] loss: 99.8478 RMSElog: 9.9848 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  470] loss: 97.4681 RMSElog: 9.7468 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  480] loss: 101.8611 RMSElog: 10.1861 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  490] loss: 98.2878 RMSElog: 9.8288 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  500] loss: 101.2390 RMSElog: 10.1239 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  510] loss: 96.3522 RMSElog: 9.6352 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  520] loss: 100.1657 RMSElog: 10.0166 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  530] loss: 99.1563 RMSElog: 9.9156 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  540] loss: 101.1949 RMSElog: 10.1195 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  550] loss: 97.3790 RMSElog: 9.7379 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  560] loss: 102.8131 RMSElog: 10.2813 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  570] loss: 99.7468 RMSElog: 9.9747 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  580] loss: 95.9734 RMSElog: 9.5973 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  590] loss: 99.0568 RMSElog: 9.9057 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter    0] loss: 99.1787 RMSElog: 9.9179 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter   10] loss: 104.0516 RMSElog: 10.4052 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter   20] loss: 102.2149 RMSElog: 10.2215 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter   30] loss: 96.3605 RMSElog: 9.6360 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter   40] loss: 102.7560 RMSElog: 10.2756 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter   50] loss: 104.3340 RMSElog: 10.4334 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter   60] loss: 97.2722 RMSElog: 9.7272 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter   70] loss: 97.4476 RMSElog: 9.7448 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter   80] loss: 100.8418 RMSElog: 10.0842 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter   90] loss: 103.9722 RMSElog: 10.3972 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  100] loss: 101.8656 RMSElog: 10.1866 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  110] loss: 100.9193 RMSElog: 10.0919 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  120] loss: 99.4392 RMSElog: 9.9439 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  130] loss: 98.7344 RMSElog: 9.8734 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  140] loss: 99.3294 RMSElog: 9.9329 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  150] loss: 99.3518 RMSElog: 9.9352 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  160] loss: 96.9428 RMSElog: 9.6943 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  170] loss: 102.2683 RMSElog: 10.2268 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  180] loss: 101.3649 RMSElog: 10.1365 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  190] loss: 101.0437 RMSElog: 10.1044 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  200] loss: 102.1660 RMSElog: 10.2166 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  210] loss: 92.4589 RMSElog: 9.2459 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  220] loss: 98.2720 RMSElog: 9.8272 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  230] loss: 96.5457 RMSElog: 9.6546 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  240] loss: 97.3902 RMSElog: 9.7390 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  250] loss: 102.0426 RMSElog: 10.2043 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  260] loss: 100.1281 RMSElog: 10.0128 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  270] loss: 96.8970 RMSElog: 9.6897 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  280] loss: 100.2199 RMSElog: 10.0220 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  290] loss: 96.7864 RMSElog: 9.6786 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  300] loss: 96.8515 RMSElog: 9.6852 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  310] loss: 95.8229 RMSElog: 9.5823 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  320] loss: 99.8591 RMSElog: 9.9859 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  330] loss: 97.8472 RMSElog: 9.7847 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  340] loss: 101.3932 RMSElog: 10.1393 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  350] loss: 100.6269 RMSElog: 10.0627 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  360] loss: 96.1386 RMSElog: 9.6139 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  370] loss: 91.6469 RMSElog: 9.1647 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  380] loss: 102.3405 RMSElog: 10.2340 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  390] loss: 97.4674 RMSElog: 9.7467 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  400] loss: 98.4685 RMSElog: 9.8468 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  410] loss: 101.7356 RMSElog: 10.1736 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  420] loss: 91.7421 RMSElog: 9.1742 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  430] loss: 96.4265 RMSElog: 9.6426 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  440] loss: 97.4832 RMSElog: 9.7483 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  450] loss: 100.1030 RMSElog: 10.0103 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  460] loss: 101.5312 RMSElog: 10.1531 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  470] loss: 96.9678 RMSElog: 9.6968 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  480] loss: 101.8908 RMSElog: 10.1891 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  490] loss: 99.2180 RMSElog: 9.9218 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  500] loss: 97.0612 RMSElog: 9.7061 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  510] loss: 98.2056 RMSElog: 9.8206 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  520] loss: 97.3096 RMSElog: 9.7310 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  530] loss: 98.8531 RMSElog: 9.8853 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  540] loss: 101.5333 RMSElog: 10.1533 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  550] loss: 95.2373 RMSElog: 9.5237 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  560] loss: 98.0236 RMSElog: 9.8024 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  570] loss: 102.2773 RMSElog: 10.2277 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  580] loss: 97.0092 RMSElog: 9.7009 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  590] loss: 102.8699 RMSElog: 10.2870 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter    0] loss: 92.3709 RMSElog: 9.2371 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter   10] loss: 96.6144 RMSElog: 9.6614 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter   20] loss: 99.1571 RMSElog: 9.9157 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter   30] loss: 99.3604 RMSElog: 9.9360 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter   40] loss: 101.8034 RMSElog: 10.1803 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter   50] loss: 101.3601 RMSElog: 10.1360 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter   60] loss: 98.0963 RMSElog: 9.8096 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter   70] loss: 96.4959 RMSElog: 9.6496 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter   80] loss: 96.5002 RMSElog: 9.6500 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter   90] loss: 98.6832 RMSElog: 9.8683 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  100] loss: 100.8598 RMSElog: 10.0860 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  110] loss: 91.4366 RMSElog: 9.1437 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  120] loss: 101.0208 RMSElog: 10.1021 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  130] loss: 98.7139 RMSElog: 9.8714 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  140] loss: 103.6537 RMSElog: 10.3654 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  150] loss: 96.8231 RMSElog: 9.6823 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  160] loss: 95.2558 RMSElog: 9.5256 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  170] loss: 97.8300 RMSElog: 9.7830 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  180] loss: 97.0577 RMSElog: 9.7058 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  190] loss: 100.6576 RMSElog: 10.0658 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  200] loss: 99.5950 RMSElog: 9.9595 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  210] loss: 100.4435 RMSElog: 10.0444 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  220] loss: 97.6109 RMSElog: 9.7611 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  230] loss: 99.9337 RMSElog: 9.9934 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  240] loss: 98.7297 RMSElog: 9.8730 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  250] loss: 99.3110 RMSElog: 9.9311 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  260] loss: 101.4295 RMSElog: 10.1429 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  270] loss: 100.2532 RMSElog: 10.0253 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  280] loss: 96.6303 RMSElog: 9.6630 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  290] loss: 98.6449 RMSElog: 9.8645 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  300] loss: 98.9759 RMSElog: 9.8976 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  310] loss: 96.3693 RMSElog: 9.6369 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  320] loss: 96.1703 RMSElog: 9.6170 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  330] loss: 97.1042 RMSElog: 9.7104 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  340] loss: 95.7028 RMSElog: 9.5703 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  350] loss: 94.6251 RMSElog: 9.4625 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  360] loss: 93.9232 RMSElog: 9.3923 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  370] loss: 92.6488 RMSElog: 9.2649 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  380] loss: 92.8952 RMSElog: 9.2895 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  390] loss: 88.9606 RMSElog: 8.8961 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  400] loss: 91.1640 RMSElog: 9.1164 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  410] loss: 93.1509 RMSElog: 9.3151 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  420] loss: 92.9060 RMSElog: 9.2906 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  430] loss: 94.7969 RMSElog: 9.4797 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  440] loss: 88.1078 RMSElog: 8.8108 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  450] loss: 92.7157 RMSElog: 9.2716 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  460] loss: 91.5506 RMSElog: 9.1551 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  470] loss: 87.1415 RMSElog: 8.7142 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  480] loss: 93.5750 RMSElog: 9.3575 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  490] loss: 94.4056 RMSElog: 9.4406 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  500] loss: 88.1006 RMSElog: 8.8101 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  510] loss: 91.8658 RMSElog: 9.1866 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  520] loss: 91.1690 RMSElog: 9.1169 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  530] loss: 91.3387 RMSElog: 9.1339 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  540] loss: 92.6735 RMSElog: 9.2673 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  550] loss: 91.2723 RMSElog: 9.1272 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  560] loss: 88.9476 RMSElog: 8.8948 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  570] loss: 87.0694 RMSElog: 8.7069 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  580] loss: 88.5292 RMSElog: 8.8529 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  590] loss: 91.2549 RMSElog: 9.1255 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter    0] loss: 90.0797 RMSElog: 9.0080 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter   10] loss: 89.4799 RMSElog: 8.9480 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter   20] loss: 91.9332 RMSElog: 9.1933 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter   30] loss: 90.5235 RMSElog: 9.0524 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter   40] loss: 89.8740 RMSElog: 8.9874 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter   50] loss: 85.6950 RMSElog: 8.5695 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter   60] loss: 90.3671 RMSElog: 9.0367 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter   70] loss: 89.2255 RMSElog: 8.9226 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter   80] loss: 92.5477 RMSElog: 9.2548 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter   90] loss: 90.0457 RMSElog: 9.0046 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  100] loss: 87.4792 RMSElog: 8.7479 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  110] loss: 85.4520 RMSElog: 8.5452 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  120] loss: 92.7508 RMSElog: 9.2751 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  130] loss: 87.3126 RMSElog: 8.7313 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  140] loss: 88.9372 RMSElog: 8.8937 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  150] loss: 90.1298 RMSElog: 9.0130 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  160] loss: 84.7207 RMSElog: 8.4721 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  170] loss: 88.7673 RMSElog: 8.8767 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  180] loss: 89.4053 RMSElog: 8.9405 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  190] loss: 87.3518 RMSElog: 8.7352 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  200] loss: 92.3858 RMSElog: 9.2386 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  210] loss: 90.7151 RMSElog: 9.0715 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  220] loss: 88.3685 RMSElog: 8.8369 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  230] loss: 88.4568 RMSElog: 8.8457 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  240] loss: 89.9225 RMSElog: 8.9922 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  250] loss: 85.8093 RMSElog: 8.5809 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  260] loss: 87.4048 RMSElog: 8.7405 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  270] loss: 86.2579 RMSElog: 8.6258 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  280] loss: 89.5657 RMSElog: 8.9566 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  290] loss: 89.5245 RMSElog: 8.9525 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  300] loss: 86.9326 RMSElog: 8.6933 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  310] loss: 86.3346 RMSElog: 8.6335 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  320] loss: 91.8873 RMSElog: 9.1887 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  330] loss: 89.5144 RMSElog: 8.9514 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  340] loss: 87.9131 RMSElog: 8.7913 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  350] loss: 89.5166 RMSElog: 8.9517 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  360] loss: 90.1156 RMSElog: 9.0116 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  370] loss: 87.7917 RMSElog: 8.7792 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  380] loss: 88.2038 RMSElog: 8.8204 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  390] loss: 85.3098 RMSElog: 8.5310 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  400] loss: 94.7720 RMSElog: 9.4772 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  410] loss: 86.0613 RMSElog: 8.6061 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  420] loss: 89.9756 RMSElog: 8.9976 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  430] loss: 88.2651 RMSElog: 8.8265 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  440] loss: 84.7308 RMSElog: 8.4731 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  450] loss: 90.3827 RMSElog: 9.0383 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  460] loss: 90.0400 RMSElog: 9.0040 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  470] loss: 89.9435 RMSElog: 8.9943 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  480] loss: 87.8253 RMSElog: 8.7825 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  490] loss: 88.1923 RMSElog: 8.8192 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  500] loss: 90.1905 RMSElog: 9.0191 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  510] loss: 86.9910 RMSElog: 8.6991 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  520] loss: 87.5684 RMSElog: 8.7568 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  530] loss: 89.6922 RMSElog: 8.9692 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  540] loss: 83.7076 RMSElog: 8.3708 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  550] loss: 84.5805 RMSElog: 8.4580 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  560] loss: 89.3384 RMSElog: 8.9338 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  570] loss: 85.8901 RMSElog: 8.5890 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  580] loss: 86.5720 RMSElog: 8.6572 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  590] loss: 86.8443 RMSElog: 8.6844 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  4][iter    0] loss: 112832.0469 RMSElog: 8.6062 grad_loss: 11274.5986 normal_loss: 0.0000
[epoch  4][iter   10] loss: 174979.6562 RMSElog: 8.8228 grad_loss: 17489.1426 normal_loss: 0.0000
[epoch  4][iter   20] loss: 114803.6016 RMSElog: 8.5750 grad_loss: 11471.7852 normal_loss: 0.0000
[epoch  4][iter   30] loss: 174354.0312 RMSElog: 9.1117 grad_loss: 17426.2910 normal_loss: 0.0000
[epoch  4][iter   40] loss: 114132.6172 RMSElog: 8.6990 grad_loss: 11404.5625 normal_loss: 0.0000
[epoch  4][iter   50] loss: 127050.1094 RMSElog: 8.8402 grad_loss: 12696.1709 normal_loss: 0.0000
[epoch  4][iter   60] loss: 198011.0156 RMSElog: 8.9762 grad_loss: 19792.1250 normal_loss: 0.0000
[epoch  4][iter   70] loss: 143416.2656 RMSElog: 8.5652 grad_loss: 14333.0605 normal_loss: 0.0000
[epoch  4][iter   80] loss: 176932.5156 RMSElog: 9.0789 grad_loss: 17684.1738 normal_loss: 0.0000
[epoch  4][iter   90] loss: 145367.5000 RMSElog: 8.9006 grad_loss: 14527.8496 normal_loss: 0.0000
[epoch  4][iter  100] loss: 157787.1562 RMSElog: 8.7949 grad_loss: 15769.9209 normal_loss: 0.0000
[epoch  4][iter  110] loss: 128297.2500 RMSElog: 8.6898 grad_loss: 12821.0352 normal_loss: 0.0000
[epoch  4][iter  120] loss: 213391.3125 RMSElog: 9.0535 grad_loss: 21330.0781 normal_loss: 0.0000
[epoch  4][iter  130] loss: 185878.8750 RMSElog: 8.8579 grad_loss: 18579.0293 normal_loss: 0.0000
[epoch  4][iter  140] loss: 166470.7500 RMSElog: 8.8950 grad_loss: 16638.1797 normal_loss: 0.0000
[epoch  4][iter  150] loss: 115639.4922 RMSElog: 8.5447 grad_loss: 11555.4043 normal_loss: 0.0000
[epoch  4][iter  160] loss: 141144.4688 RMSElog: 8.4747 grad_loss: 14105.9717 normal_loss: 0.0000
[epoch  4][iter  170] loss: 141205.1719 RMSElog: 8.8459 grad_loss: 14111.6709 normal_loss: 0.0000
[epoch  4][iter  180] loss: 123271.8359 RMSElog: 8.8046 grad_loss: 12318.3789 normal_loss: 0.0000
[epoch  4][iter  190] loss: 172661.9375 RMSElog: 8.9609 grad_loss: 17257.2324 normal_loss: 0.0000
[epoch  4][iter  200] loss: 160644.2812 RMSElog: 8.9320 grad_loss: 16055.4971 normal_loss: 0.0000
[epoch  4][iter  210] loss: 141420.7656 RMSElog: 8.7386 grad_loss: 14133.3379 normal_loss: 0.0000
[epoch  4][iter  220] loss: 143818.2344 RMSElog: 8.8507 grad_loss: 14372.9727 normal_loss: 0.0000
[epoch  4][iter  230] loss: 203536.2812 RMSElog: 8.8077 grad_loss: 20344.8203 normal_loss: 0.0000
[epoch  4][iter  240] loss: 146190.5000 RMSElog: 8.7688 grad_loss: 14610.2812 normal_loss: 0.0000
[epoch  4][iter  250] loss: 208744.0469 RMSElog: 8.8531 grad_loss: 20865.5508 normal_loss: 0.0000
[epoch  4][iter  260] loss: 176194.6250 RMSElog: 9.0510 grad_loss: 17610.4121 normal_loss: 0.0000
[epoch  4][iter  270] loss: 231672.5938 RMSElog: 9.1850 grad_loss: 23158.0742 normal_loss: 0.0000
[epoch  4][iter  280] loss: 196894.3438 RMSElog: 8.9988 grad_loss: 19680.4355 normal_loss: 0.0000
[epoch  4][iter  290] loss: 161261.6406 RMSElog: 8.8117 grad_loss: 16117.3525 normal_loss: 0.0000
[epoch  4][iter  300] loss: 182291.2812 RMSElog: 9.0114 grad_loss: 18220.1172 normal_loss: 0.0000
[epoch  4][iter  310] loss: 110207.5391 RMSElog: 8.2751 grad_loss: 11012.4785 normal_loss: 0.0000
[epoch  4][iter  320] loss: 103930.4219 RMSElog: 8.4112 grad_loss: 10384.6309 normal_loss: 0.0000
[epoch  4][iter  330] loss: 217888.1250 RMSElog: 8.9597 grad_loss: 21779.8535 normal_loss: 0.0000
[epoch  4][iter  340] loss: 102205.1172 RMSElog: 8.7475 grad_loss: 10211.7646 normal_loss: 0.0000
[epoch  4][iter  350] loss: 105714.1406 RMSElog: 8.2520 grad_loss: 10563.1621 normal_loss: 0.0000
[epoch  4][iter  360] loss: 116047.8281 RMSElog: 8.3903 grad_loss: 11596.3926 normal_loss: 0.0000
[epoch  4][iter  370] loss: 158902.4531 RMSElog: 8.8299 grad_loss: 15881.4150 normal_loss: 0.0000
[epoch  4][iter  380] loss: 206375.4531 RMSElog: 9.0571 grad_loss: 20628.4883 normal_loss: 0.0000
[epoch  4][iter  390] loss: 143508.7344 RMSElog: 8.5245 grad_loss: 14342.3486 normal_loss: 0.0000
[epoch  4][iter  400] loss: 120717.9609 RMSElog: 8.3390 grad_loss: 12063.4570 normal_loss: 0.0000
[epoch  4][iter  410] loss: 149362.0625 RMSElog: 8.7833 grad_loss: 14927.4229 normal_loss: 0.0000
[epoch  4][iter  420] loss: 109234.0781 RMSElog: 8.4019 grad_loss: 10915.0059 normal_loss: 0.0000
[epoch  4][iter  430] loss: 139039.5156 RMSElog: 8.6627 grad_loss: 13895.2891 normal_loss: 0.0000
[epoch  4][iter  440] loss: 198267.6250 RMSElog: 8.8907 grad_loss: 19817.8711 normal_loss: 0.0000
[epoch  4][iter  450] loss: 167708.9375 RMSElog: 9.0305 grad_loss: 16761.8633 normal_loss: 0.0000
[epoch  4][iter  460] loss: 232956.2188 RMSElog: 9.0031 grad_loss: 23286.6172 normal_loss: 0.0000
[epoch  4][iter  470] loss: 174607.0156 RMSElog: 8.8944 grad_loss: 17451.8066 normal_loss: 0.0000
[epoch  4][iter  480] loss: 179878.8750 RMSElog: 8.8451 grad_loss: 17979.0410 normal_loss: 0.0000
[epoch  4][iter  490] loss: 151345.0312 RMSElog: 8.6972 grad_loss: 15125.8066 normal_loss: 0.0000
[epoch  4][iter  500] loss: 167771.5625 RMSElog: 8.8815 grad_loss: 16768.2754 normal_loss: 0.0000
[epoch  4][iter  510] loss: 156232.3281 RMSElog: 8.8516 grad_loss: 15614.3809 normal_loss: 0.0000
[epoch  4][iter  520] loss: 148030.6094 RMSElog: 8.7013 grad_loss: 14794.3594 normal_loss: 0.0000
[epoch  4][iter  530] loss: 144634.5000 RMSElog: 8.8454 grad_loss: 14454.6045 normal_loss: 0.0000
[epoch  4][iter  540] loss: 105767.5391 RMSElog: 8.3471 grad_loss: 10568.4072 normal_loss: 0.0000
[epoch  4][iter  550] loss: 110575.5000 RMSElog: 8.3469 grad_loss: 11049.2031 normal_loss: 0.0000
[epoch  4][iter  560] loss: 214465.9062 RMSElog: 8.9711 grad_loss: 21437.6191 normal_loss: 0.0000
[epoch  4][iter  570] loss: 170667.7500 RMSElog: 8.9259 grad_loss: 17057.8496 normal_loss: 0.0000
[epoch  4][iter  580] loss: 123068.4453 RMSElog: 8.3733 grad_loss: 12298.4717 normal_loss: 0.0000
[epoch  4][iter  590] loss: 168522.7500 RMSElog: 8.7287 grad_loss: 16843.5469 normal_loss: 0.0000
[epoch  5][iter    0] loss: 221063.8125 RMSElog: 9.1051 grad_loss: 22097.2754 normal_loss: 0.0000
[epoch  5][iter   10] loss: 114290.2500 RMSElog: 8.5231 grad_loss: 11420.5020 normal_loss: 0.0000
[epoch  5][iter   20] loss: 115508.4297 RMSElog: 8.4094 grad_loss: 11542.4336 normal_loss: 0.0000
[epoch  5][iter   30] loss: 137237.8594 RMSElog: 8.9763 grad_loss: 13714.8096 normal_loss: 0.0000
[epoch  5][iter   40] loss: 189480.7812 RMSElog: 8.9724 grad_loss: 18939.1055 normal_loss: 0.0000
[epoch  5][iter   50] loss: 138035.0312 RMSElog: 8.5328 grad_loss: 13794.9697 normal_loss: 0.0000
[epoch  5][iter   60] loss: 170034.4375 RMSElog: 8.8197 grad_loss: 16994.6230 normal_loss: 0.0000
[epoch  5][iter   70] loss: 144963.4688 RMSElog: 8.8026 grad_loss: 14487.5439 normal_loss: 0.0000
[epoch  5][iter   80] loss: 176194.0781 RMSElog: 9.0201 grad_loss: 17610.3887 normal_loss: 0.0000
[epoch  5][iter   90] loss: 180401.4688 RMSElog: 8.9593 grad_loss: 18031.1875 normal_loss: 0.0000
[epoch  5][iter  100] loss: 100723.2891 RMSElog: 8.6550 grad_loss: 10063.6738 normal_loss: 0.0000
[epoch  5][iter  110] loss: 139073.3594 RMSElog: 8.6922 grad_loss: 13898.6436 normal_loss: 0.0000
[epoch  5][iter  120] loss: 122306.4219 RMSElog: 8.6095 grad_loss: 12222.0332 normal_loss: 0.0000
[epoch  5][iter  130] loss: 193952.2812 RMSElog: 9.0456 grad_loss: 19386.1836 normal_loss: 0.0000
[epoch  5][iter  140] loss: 195645.0938 RMSElog: 8.7978 grad_loss: 19555.7129 normal_loss: 0.0000
[epoch  5][iter  150] loss: 160570.0312 RMSElog: 8.8218 grad_loss: 16048.1826 normal_loss: 0.0000
[epoch  5][iter  160] loss: 191946.1719 RMSElog: 8.8862 grad_loss: 19185.7305 normal_loss: 0.0000
[epoch  5][iter  170] loss: 187210.1406 RMSElog: 8.8264 grad_loss: 18712.1875 normal_loss: 0.0000
[epoch  5][iter  180] loss: 163763.0938 RMSElog: 8.7752 grad_loss: 16367.5342 normal_loss: 0.0000
[epoch  5][iter  190] loss: 141419.6250 RMSElog: 8.6829 grad_loss: 14133.2793 normal_loss: 0.0000
[epoch  5][iter  200] loss: 100087.8906 RMSElog: 8.3704 grad_loss: 10000.4189 normal_loss: 0.0000
[epoch  5][iter  210] loss: 196650.5625 RMSElog: 8.7403 grad_loss: 19656.3164 normal_loss: 0.0000
[epoch  5][iter  220] loss: 119445.3125 RMSElog: 8.4024 grad_loss: 11936.1289 normal_loss: 0.0000
[epoch  5][iter  230] loss: 155457.9844 RMSElog: 8.9716 grad_loss: 15536.8262 normal_loss: 0.0000
[epoch  5][iter  240] loss: 165429.9219 RMSElog: 8.9614 grad_loss: 16534.0312 normal_loss: 0.0000
[epoch  5][iter  250] loss: 181440.1250 RMSElog: 8.9028 grad_loss: 18135.1094 normal_loss: 0.0000
[epoch  5][iter  260] loss: 185748.2656 RMSElog: 8.9046 grad_loss: 18565.9219 normal_loss: 0.0000
[epoch  5][iter  270] loss: 104428.3438 RMSElog: 8.4470 grad_loss: 10434.3867 normal_loss: 0.0000
[epoch  5][iter  280] loss: 153074.3125 RMSElog: 8.6072 grad_loss: 15298.8242 normal_loss: 0.0000
[epoch  5][iter  290] loss: 179276.3750 RMSElog: 8.7467 grad_loss: 17918.8906 normal_loss: 0.0000
[epoch  5][iter  300] loss: 190911.2500 RMSElog: 9.0574 grad_loss: 19082.0684 normal_loss: 0.0000
[epoch  5][iter  310] loss: 137230.5312 RMSElog: 8.9996 grad_loss: 13714.0527 normal_loss: 0.0000
[epoch  5][iter  320] loss: 162811.9375 RMSElog: 8.8806 grad_loss: 16272.3135 normal_loss: 0.0000
[epoch  5][iter  330] loss: 152514.9375 RMSElog: 8.7245 grad_loss: 15242.7686 normal_loss: 0.0000
[epoch  5][iter  340] loss: 148766.8750 RMSElog: 8.7756 grad_loss: 14867.9121 normal_loss: 0.0000
[epoch  5][iter  350] loss: 217147.2500 RMSElog: 8.9224 grad_loss: 21705.8027 normal_loss: 0.0000
[epoch  5][iter  360] loss: 189557.6875 RMSElog: 8.8243 grad_loss: 18946.9453 normal_loss: 0.0000
[epoch  5][iter  370] loss: 176931.3125 RMSElog: 9.0319 grad_loss: 17684.0996 normal_loss: 0.0000
[epoch  5][iter  380] loss: 133261.0781 RMSElog: 8.6235 grad_loss: 13317.4844 normal_loss: 0.0000
[epoch  5][iter  390] loss: 140912.3125 RMSElog: 8.4603 grad_loss: 14082.7715 normal_loss: 0.0000
[epoch  5][iter  400] loss: 196409.1875 RMSElog: 8.7731 grad_loss: 19632.1445 normal_loss: 0.0000
[epoch  5][iter  410] loss: 117668.4375 RMSElog: 8.4300 grad_loss: 11758.4141 normal_loss: 0.0000
[epoch  5][iter  420] loss: 154284.0938 RMSElog: 8.6662 grad_loss: 15419.7441 normal_loss: 0.0000
[epoch  5][iter  430] loss: 172732.6719 RMSElog: 8.9840 grad_loss: 17264.2832 normal_loss: 0.0000
[epoch  5][iter  440] loss: 182543.2500 RMSElog: 8.9871 grad_loss: 18245.3379 normal_loss: 0.0000
[epoch  5][iter  450] loss: 177172.7188 RMSElog: 9.1305 grad_loss: 17708.1406 normal_loss: 0.0000
[epoch  5][iter  460] loss: 161608.2969 RMSElog: 8.9154 grad_loss: 16151.9150 normal_loss: 0.0000
[epoch  5][iter  470] loss: 125668.4297 RMSElog: 8.3968 grad_loss: 12558.4463 normal_loss: 0.0000
[epoch  5][iter  480] loss: 162692.7969 RMSElog: 8.9497 grad_loss: 16260.3301 normal_loss: 0.0000
[epoch  5][iter  490] loss: 102204.9141 RMSElog: 8.7506 grad_loss: 10211.7402 normal_loss: 0.0000
[epoch  5][iter  500] loss: 204622.8125 RMSElog: 8.9180 grad_loss: 20453.3633 normal_loss: 0.0000
[epoch  5][iter  510] loss: 195749.6875 RMSElog: 8.8476 grad_loss: 19566.1211 normal_loss: 0.0000
[epoch  5][iter  520] loss: 135443.6406 RMSElog: 8.9267 grad_loss: 13535.4375 normal_loss: 0.0000
[epoch  5][iter  530] loss: 148216.3125 RMSElog: 8.8068 grad_loss: 14812.8242 normal_loss: 0.0000
[epoch  5][iter  540] loss: 175271.5625 RMSElog: 8.7938 grad_loss: 17518.3633 normal_loss: 0.0000
[epoch  5][iter  550] loss: 178446.2344 RMSElog: 8.9093 grad_loss: 17835.7129 normal_loss: 0.0000
[epoch  5][iter  560] loss: 150271.4219 RMSElog: 8.6706 grad_loss: 15018.4707 normal_loss: 0.0000
[epoch  5][iter  570] loss: 108392.3359 RMSElog: 8.6125 grad_loss: 10830.6211 normal_loss: 0.0000
[epoch  5][iter  580] loss: 172723.9688 RMSElog: 8.9409 grad_loss: 17263.4551 normal_loss: 0.0000
[epoch  5][iter  590] loss: 188475.6250 RMSElog: 8.9266 grad_loss: 18838.6367 normal_loss: 0.0000
[epoch  6][iter    0] loss: 115709.8828 RMSElog: 8.1998 grad_loss: 11562.7881 normal_loss: 0.0000
[epoch  6][iter   10] loss: 130500.6953 RMSElog: 8.5758 grad_loss: 13041.4932 normal_loss: 0.0000
[epoch  6][iter   20] loss: 159399.4531 RMSElog: 8.6966 grad_loss: 15931.2490 normal_loss: 0.0000
[epoch  6][iter   30] loss: 177172.4688 RMSElog: 9.1248 grad_loss: 17708.1211 normal_loss: 0.0000
[epoch  6][iter   40] loss: 104060.3750 RMSElog: 8.5137 grad_loss: 10397.5234 normal_loss: 0.0000
[epoch  6][iter   50] loss: 211611.9062 RMSElog: 8.8879 grad_loss: 21152.3027 normal_loss: 0.0000
[epoch  6][iter   60] loss: 108801.7109 RMSElog: 8.1826 grad_loss: 10871.9883 normal_loss: 0.0000
[epoch  6][iter   70] loss: 103918.6797 RMSElog: 8.1512 grad_loss: 10383.7168 normal_loss: 0.0000
[epoch  6][iter   80] loss: 174983.2500 RMSElog: 8.7793 grad_loss: 17489.5449 normal_loss: 0.0000
[epoch  6][iter   90] loss: 120740.8906 RMSElog: 8.4930 grad_loss: 12065.5957 normal_loss: 0.0000
[epoch  6][iter  100] loss: 135180.9531 RMSElog: 8.4637 grad_loss: 13509.6309 normal_loss: 0.0000
[epoch  6][iter  110] loss: 141143.6719 RMSElog: 8.4481 grad_loss: 14105.9189 normal_loss: 0.0000
[epoch  6][iter  120] loss: 121161.1328 RMSElog: 8.7490 grad_loss: 12107.3643 normal_loss: 0.0000
[epoch  6][iter  130] loss: 158901.5000 RMSElog: 8.7879 grad_loss: 15881.3613 normal_loss: 0.0000
[epoch  6][iter  140] loss: 154832.5469 RMSElog: 8.7975 grad_loss: 15474.4570 normal_loss: 0.0000
[epoch  6][iter  150] loss: 132017.0938 RMSElog: 8.8390 grad_loss: 13192.8701 normal_loss: 0.0000
[epoch  6][iter  160] loss: 121809.8594 RMSElog: 8.2894 grad_loss: 12172.6973 normal_loss: 0.0000
[epoch  6][iter  170] loss: 121842.3125 RMSElog: 8.5494 grad_loss: 12175.6816 normal_loss: 0.0000
[epoch  6][iter  180] loss: 204622.5000 RMSElog: 8.8904 grad_loss: 20453.3594 normal_loss: 0.0000
[epoch  6][iter  190] loss: 140029.5938 RMSElog: 8.6495 grad_loss: 13994.3105 normal_loss: 0.0000
[epoch  6][iter  200] loss: 212211.1562 RMSElog: 8.9053 grad_loss: 21212.2109 normal_loss: 0.0000
[epoch  6][iter  210] loss: 189556.3906 RMSElog: 8.7670 grad_loss: 18946.8711 normal_loss: 0.0000
[epoch  6][iter  220] loss: 166047.4219 RMSElog: 8.7559 grad_loss: 16595.9863 normal_loss: 0.0000
[epoch  6][iter  230] loss: 106151.7969 RMSElog: 8.0710 grad_loss: 10607.1084 normal_loss: 0.0000
[epoch  6][iter  240] loss: 223848.7656 RMSElog: 9.0977 grad_loss: 22375.7793 normal_loss: 0.0000
[epoch  6][iter  250] loss: 100722.6875 RMSElog: 8.6375 grad_loss: 10063.6309 normal_loss: 0.0000
[epoch  6][iter  260] loss: 115506.8750 RMSElog: 8.3193 grad_loss: 11542.3682 normal_loss: 0.0000
[epoch  6][iter  270] loss: 170033.2656 RMSElog: 8.7637 grad_loss: 16994.5625 normal_loss: 0.0000
[epoch  6][iter  280] loss: 152567.7656 RMSElog: 8.5820 grad_loss: 15248.1943 normal_loss: 0.0000
[epoch  6][iter  290] loss: 125668.2891 RMSElog: 8.3906 grad_loss: 12558.4385 normal_loss: 0.0000
[epoch  6][iter  300] loss: 114480.6562 RMSElog: 8.2610 grad_loss: 11439.8047 normal_loss: 0.0000
[epoch  6][iter  310] loss: 162349.0312 RMSElog: 8.8281 grad_loss: 16226.0742 normal_loss: 0.0000
[epoch  6][iter  320] loss: 161260.1719 RMSElog: 8.7361 grad_loss: 16117.2812 normal_loss: 0.0000
[epoch  6][iter  330] loss: 105597.9219 RMSElog: 8.3555 grad_loss: 10551.4365 normal_loss: 0.0000
[epoch  6][iter  340] loss: 206374.7188 RMSElog: 9.0253 grad_loss: 20628.4473 normal_loss: 0.0000
[epoch  6][iter  350] loss: 195749.3750 RMSElog: 8.8239 grad_loss: 19566.1133 normal_loss: 0.0000
[epoch  6][iter  360] loss: 107451.9141 RMSElog: 8.2584 grad_loss: 10736.9326 normal_loss: 0.0000
[epoch  6][iter  370] loss: 159800.1719 RMSElog: 8.8047 grad_loss: 15971.2129 normal_loss: 0.0000
[epoch  6][iter  380] loss: 161059.2812 RMSElog: 8.8623 grad_loss: 16097.0664 normal_loss: 0.0000
[epoch  6][iter  390] loss: 149107.7031 RMSElog: 8.7872 grad_loss: 14901.9834 normal_loss: 0.0000
[epoch  6][iter  400] loss: 160764.9062 RMSElog: 8.7613 grad_loss: 16067.7285 normal_loss: 0.0000
[epoch  6][iter  410] loss: 152536.7656 RMSElog: 8.6487 grad_loss: 15245.0283 normal_loss: 0.0000
[epoch  6][iter  420] loss: 178570.3906 RMSElog: 8.8889 grad_loss: 17848.1504 normal_loss: 0.0000
[epoch  6][iter  430] loss: 167320.5312 RMSElog: 8.7806 grad_loss: 16723.2715 normal_loss: 0.0000
[epoch  6][iter  440] loss: 167930.9219 RMSElog: 8.6526 grad_loss: 16784.4395 normal_loss: 0.0000
[epoch  6][iter  450] loss: 215515.9531 RMSElog: 9.1246 grad_loss: 21542.4707 normal_loss: 0.0000
[epoch  6][iter  460] loss: 160026.8438 RMSElog: 8.7720 grad_loss: 15993.9121 normal_loss: 0.0000
[epoch  6][iter  470] loss: 178445.7969 RMSElog: 8.8793 grad_loss: 17835.7012 normal_loss: 0.0000
[epoch  6][iter  480] loss: 157401.8438 RMSElog: 8.8871 grad_loss: 15731.2979 normal_loss: 0.0000
[epoch  6][iter  490] loss: 143878.5156 RMSElog: 8.7154 grad_loss: 14379.1357 normal_loss: 0.0000
[epoch  6][iter  500] loss: 182453.8281 RMSElog: 8.9647 grad_loss: 18236.4180 normal_loss: 0.0000
[epoch  6][iter  510] loss: 174738.8281 RMSElog: 8.7643 grad_loss: 17465.1191 normal_loss: 0.0000
[epoch  6][iter  520] loss: 136480.0312 RMSElog: 8.6790 grad_loss: 13639.3242 normal_loss: 0.0000
[epoch  6][iter  530] loss: 133440.9531 RMSElog: 8.5991 grad_loss: 13335.4961 normal_loss: 0.0000
[epoch  6][iter  540] loss: 114356.5547 RMSElog: 8.5958 grad_loss: 11427.0596 normal_loss: 0.0000
[epoch  6][iter  550] loss: 140912.2812 RMSElog: 8.4466 grad_loss: 14082.7812 normal_loss: 0.0000
[epoch  6][iter  560] loss: 151697.8281 RMSElog: 8.7606 grad_loss: 15161.0215 normal_loss: 0.0000
[epoch  6][iter  570] loss: 172660.6875 RMSElog: 8.9192 grad_loss: 17257.1484 normal_loss: 0.0000
[epoch  6][iter  580] loss: 143436.0625 RMSElog: 8.6456 grad_loss: 14334.9609 normal_loss: 0.0000
[epoch  6][iter  590] loss: 213389.5469 RMSElog: 8.9758 grad_loss: 21329.9785 normal_loss: 0.0000
[epoch  7][iter    0] loss: 133020.1719 RMSElog: 8.6490 grad_loss: 13293.3672 normal_loss: 0.0000
[epoch  7][iter   10] loss: 175974.6875 RMSElog: 9.1671 grad_loss: 17588.3008 normal_loss: 0.0000
[epoch  7][iter   20] loss: 152514.3594 RMSElog: 8.6908 grad_loss: 15242.7461 normal_loss: 0.0000
[epoch  7][iter   30] loss: 160856.6250 RMSElog: 8.8403 grad_loss: 16076.8223 normal_loss: 0.0000
[epoch  7][iter   40] loss: 100952.7344 RMSElog: 8.1731 grad_loss: 10087.1006 normal_loss: 0.0000
[epoch  7][iter   50] loss: 158901.3906 RMSElog: 8.7829 grad_loss: 15881.3564 normal_loss: 0.0000
[epoch  7][iter   60] loss: 104413.8672 RMSElog: 8.2792 grad_loss: 10433.1074 normal_loss: 0.0000
[epoch  7][iter   70] loss: 142669.7969 RMSElog: 8.6338 grad_loss: 14258.3457 normal_loss: 0.0000
[epoch  7][iter   80] loss: 146757.5000 RMSElog: 8.7599 grad_loss: 14666.9902 normal_loss: 0.0000
[epoch  7][iter   90] loss: 153568.7031 RMSElog: 8.6537 grad_loss: 15348.2168 normal_loss: 0.0000
[epoch  7][iter  100] loss: 132016.2188 RMSElog: 8.7821 grad_loss: 13192.8398 normal_loss: 0.0000
[epoch  7][iter  110] loss: 230493.1250 RMSElog: 9.0377 grad_loss: 23040.2754 normal_loss: 0.0000
[epoch  7][iter  120] loss: 129682.7422 RMSElog: 8.4872 grad_loss: 12959.7871 normal_loss: 0.0000
[epoch  7][iter  130] loss: 204009.6719 RMSElog: 8.7814 grad_loss: 20392.1855 normal_loss: 0.0000
[epoch  7][iter  140] loss: 165551.6250 RMSElog: 8.8355 grad_loss: 16546.3262 normal_loss: 0.0000
[epoch  7][iter  150] loss: 146933.5312 RMSElog: 8.6116 grad_loss: 14684.7422 normal_loss: 0.0000
[epoch  7][iter  160] loss: 182295.5625 RMSElog: 9.0260 grad_loss: 18220.5312 normal_loss: 0.0000
[epoch  7][iter  170] loss: 142111.8125 RMSElog: 8.4758 grad_loss: 14202.7051 normal_loss: 0.0000
[epoch  7][iter  180] loss: 160642.7031 RMSElog: 8.8659 grad_loss: 16055.4043 normal_loss: 0.0000
[epoch  7][iter  190] loss: 104950.9453 RMSElog: 8.5890 grad_loss: 10486.5059 normal_loss: 0.0000
[epoch  7][iter  200] loss: 202090.5938 RMSElog: 8.7316 grad_loss: 20200.3262 normal_loss: 0.0000
[epoch  7][iter  210] loss: 185877.2031 RMSElog: 8.7837 grad_loss: 18578.9375 normal_loss: 0.0000
[epoch  7][iter  220] loss: 138909.1875 RMSElog: 8.6465 grad_loss: 13882.2715 normal_loss: 0.0000
[epoch  7][iter  230] loss: 104104.0000 RMSElog: 8.5583 grad_loss: 10401.8418 normal_loss: 0.0000
[epoch  7][iter  240] loss: 179715.7188 RMSElog: 9.0135 grad_loss: 17962.5586 normal_loss: 0.0000
[epoch  7][iter  250] loss: 142516.8906 RMSElog: 8.8361 grad_loss: 14242.8525 normal_loss: 0.0000
[epoch  7][iter  260] loss: 167656.8438 RMSElog: 8.9614 grad_loss: 16756.7227 normal_loss: 0.0000
[epoch  7][iter  270] loss: 206374.6250 RMSElog: 9.0254 grad_loss: 20628.4375 normal_loss: 0.0000
[epoch  7][iter  280] loss: 215926.6562 RMSElog: 8.8753 grad_loss: 21583.7910 normal_loss: 0.0000
[epoch  7][iter  290] loss: 125657.7500 RMSElog: 8.5766 grad_loss: 12557.1992 normal_loss: 0.0000
[epoch  7][iter  300] loss: 122331.0000 RMSElog: 8.3147 grad_loss: 12224.7852 normal_loss: 0.0000
[epoch  7][iter  310] loss: 140310.4375 RMSElog: 8.8277 grad_loss: 14022.2148 normal_loss: 0.0000
[epoch  7][iter  320] loss: 144632.3125 RMSElog: 8.7245 grad_loss: 14454.5059 normal_loss: 0.0000
[epoch  7][iter  330] loss: 183988.7500 RMSElog: 8.5899 grad_loss: 18390.2852 normal_loss: 0.0000
[epoch  7][iter  340] loss: 133440.8750 RMSElog: 8.6082 grad_loss: 13335.4795 normal_loss: 0.0000
[epoch  7][iter  350] loss: 214780.9531 RMSElog: 8.8385 grad_loss: 21469.2578 normal_loss: 0.0000
[epoch  7][iter  360] loss: 179623.1250 RMSElog: 8.9311 grad_loss: 17953.3809 normal_loss: 0.0000
[epoch  7][iter  370] loss: 155738.6875 RMSElog: 8.7667 grad_loss: 15565.1025 normal_loss: 0.0000
[epoch  7][iter  380] loss: 157680.5625 RMSElog: 8.8991 grad_loss: 15759.1562 normal_loss: 0.0000
[epoch  7][iter  390] loss: 102069.3047 RMSElog: 8.3872 grad_loss: 10198.5430 normal_loss: 0.0000
[epoch  7][iter  400] loss: 220668.4219 RMSElog: 8.8372 grad_loss: 22058.0039 normal_loss: 0.0000
[epoch  7][iter  410] loss: 167060.8594 RMSElog: 8.8282 grad_loss: 16697.2578 normal_loss: 0.0000
[epoch  7][iter  420] loss: 136773.5312 RMSElog: 8.6553 grad_loss: 13668.6973 normal_loss: 0.0000
[epoch  7][iter  430] loss: 203947.1875 RMSElog: 9.1650 grad_loss: 20385.5547 normal_loss: 0.0000
[epoch  7][iter  440] loss: 162534.4688 RMSElog: 9.0110 grad_loss: 16244.4355 normal_loss: 0.0000
[epoch  7][iter  450] loss: 140439.1875 RMSElog: 8.8134 grad_loss: 14035.1055 normal_loss: 0.0000
[epoch  7][iter  460] loss: 221186.5938 RMSElog: 8.8827 grad_loss: 22109.7773 normal_loss: 0.0000
[epoch  7][iter  470] loss: 108392.2188 RMSElog: 8.6057 grad_loss: 10830.6162 normal_loss: 0.0000
[epoch  7][iter  480] loss: 164157.1562 RMSElog: 8.8813 grad_loss: 16406.8340 normal_loss: 0.0000
[epoch  7][iter  490] loss: 145366.6250 RMSElog: 8.8645 grad_loss: 14527.7979 normal_loss: 0.0000
[epoch  7][iter  500] loss: 179877.4062 RMSElog: 8.7775 grad_loss: 17978.9629 normal_loss: 0.0000
[epoch  7][iter  510] loss: 159316.3906 RMSElog: 8.8898 grad_loss: 15922.7490 normal_loss: 0.0000
[epoch  7][iter  520] loss: 150276.0781 RMSElog: 8.6697 grad_loss: 15018.9375 normal_loss: 0.0000
[epoch  7][iter  530] loss: 177641.9688 RMSElog: 8.7926 grad_loss: 17755.4043 normal_loss: 0.0000
[epoch  7][iter  540] loss: 161607.8438 RMSElog: 8.8917 grad_loss: 16151.8936 normal_loss: 0.0000
[epoch  7][iter  550] loss: 224917.3438 RMSElog: 8.8131 grad_loss: 22482.9219 normal_loss: 0.0000
[epoch  7][iter  560] loss: 201347.2188 RMSElog: 9.0783 grad_loss: 20125.6445 normal_loss: 0.0000
[epoch  7][iter  570] loss: 115467.1562 RMSElog: 8.2157 grad_loss: 11538.5000 normal_loss: 0.0000
[epoch  7][iter  580] loss: 141419.6719 RMSElog: 8.6869 grad_loss: 14133.2812 normal_loss: 0.0000
[epoch  7][iter  590] loss: 202602.7344 RMSElog: 8.7332 grad_loss: 20251.5410 normal_loss: 0.0000
[epoch  8][iter    0] loss: 177919.9375 RMSElog: 8.9644 grad_loss: 17782.2832 normal_loss: 0.7466
[epoch  8][iter   10] loss: 166989.8438 RMSElog: 8.8269 grad_loss: 16689.4590 normal_loss: 0.6990
[epoch  8][iter   20] loss: 173325.7500 RMSElog: 8.8981 grad_loss: 17322.9219 normal_loss: 0.7547
[epoch  8][iter   30] loss: 170525.0000 RMSElog: 8.6524 grad_loss: 17043.1094 normal_loss: 0.7390
[epoch  8][iter   40] loss: 147441.4844 RMSElog: 8.5193 grad_loss: 14734.9209 normal_loss: 0.7078
[epoch  8][iter   50] loss: 129689.1016 RMSElog: 8.7523 grad_loss: 12959.4863 normal_loss: 0.6717
[epoch  8][iter   60] loss: 205341.3906 RMSElog: 8.8053 grad_loss: 20524.6641 normal_loss: 0.6697
[epoch  8][iter   70] loss: 100729.2656 RMSElog: 8.6538 grad_loss: 10063.6191 normal_loss: 0.6539
[epoch  8][iter   80] loss: 108399.4844 RMSElog: 8.6019 grad_loss: 10830.6074 normal_loss: 0.7390
[epoch  8][iter   90] loss: 159268.8906 RMSElog: 8.6360 grad_loss: 15917.5254 normal_loss: 0.7274
[epoch  8][iter  100] loss: 105683.1328 RMSElog: 8.1041 grad_loss: 10559.5449 normal_loss: 0.6641
[epoch  8][iter  110] loss: 178453.2500 RMSElog: 8.8958 grad_loss: 17835.7012 normal_loss: 0.7272
[epoch  8][iter  120] loss: 200934.0312 RMSElog: 8.8824 grad_loss: 20083.8301 normal_loss: 0.6891
[epoch  8][iter  130] loss: 206505.2812 RMSElog: 8.8251 grad_loss: 20641.0469 normal_loss: 0.6553
[epoch  8][iter  140] loss: 107925.6250 RMSElog: 8.1348 grad_loss: 10783.7441 normal_loss: 0.6833
[epoch  8][iter  150] loss: 150277.5156 RMSElog: 8.6576 grad_loss: 15018.4277 normal_loss: 0.6655
[epoch  8][iter  160] loss: 233368.8281 RMSElog: 9.0076 grad_loss: 23327.1992 normal_loss: 0.6756
[epoch  8][iter  170] loss: 153575.2188 RMSElog: 8.6737 grad_loss: 15348.2021 normal_loss: 0.6461
[epoch  8][iter  180] loss: 122337.6172 RMSElog: 8.3195 grad_loss: 12224.7754 normal_loss: 0.6667
[epoch  8][iter  190] loss: 138453.5781 RMSElog: 8.4975 grad_loss: 13836.1426 normal_loss: 0.7174
[epoch  8][iter  200] loss: 115442.6484 RMSElog: 8.6586 grad_loss: 11534.8613 normal_loss: 0.7453
[epoch  8][iter  210] loss: 165290.0938 RMSElog: 8.8728 grad_loss: 16519.4297 normal_loss: 0.7063
[epoch  8][iter  220] loss: 140317.7969 RMSElog: 8.8465 grad_loss: 14022.2227 normal_loss: 0.7109
[epoch  8][iter  230] loss: 159323.4062 RMSElog: 8.9043 grad_loss: 15922.7383 normal_loss: 0.6978
[epoch  8][iter  240] loss: 164163.4688 RMSElog: 8.8770 grad_loss: 16406.7988 normal_loss: 0.6716
[epoch  8][iter  250] loss: 146927.3594 RMSElog: 8.5245 grad_loss: 14683.4961 normal_loss: 0.7158
[epoch  8][iter  260] loss: 127483.0000 RMSElog: 8.3047 grad_loss: 12739.3057 normal_loss: 0.6898
[epoch  8][iter  270] loss: 232752.0000 RMSElog: 9.0408 grad_loss: 23265.4883 normal_loss: 0.6708
[epoch  8][iter  280] loss: 114903.5000 RMSElog: 8.2801 grad_loss: 11481.3965 normal_loss: 0.6726
[epoch  8][iter  290] loss: 165309.4531 RMSElog: 8.8404 grad_loss: 16521.4043 normal_loss: 0.7016
[epoch  8][iter  300] loss: 103924.9141 RMSElog: 8.1147 grad_loss: 10383.7021 normal_loss: 0.6751
[epoch  8][iter  310] loss: 104456.0859 RMSElog: 8.0728 grad_loss: 10436.8926 normal_loss: 0.6422
[epoch  8][iter  320] loss: 133267.2031 RMSElog: 8.6168 grad_loss: 13317.4502 normal_loss: 0.6528
[epoch  8][iter  330] loss: 144639.6719 RMSElog: 8.7777 grad_loss: 14454.5332 normal_loss: 0.6560
[epoch  8][iter  340] loss: 113885.5625 RMSElog: 8.6679 grad_loss: 11379.1416 normal_loss: 0.7471
[epoch  8][iter  350] loss: 207660.0312 RMSElog: 8.9370 grad_loss: 20756.4160 normal_loss: 0.6495
[epoch  8][iter  360] loss: 159808.1875 RMSElog: 8.8634 grad_loss: 15971.2520 normal_loss: 0.7031
[epoch  8][iter  370] loss: 178766.7812 RMSElog: 8.9135 grad_loss: 17867.0664 normal_loss: 0.6967
[epoch  8][iter  380] loss: 211516.3438 RMSElog: 8.8045 grad_loss: 21142.1719 normal_loss: 0.6581
[epoch  8][iter  390] loss: 154606.8438 RMSElog: 8.7310 grad_loss: 15451.2891 normal_loss: 0.6636
[epoch  8][iter  400] loss: 100625.9453 RMSElog: 8.0646 grad_loss: 10053.8838 normal_loss: 0.6463
[epoch  8][iter  410] loss: 133026.0781 RMSElog: 8.6348 grad_loss: 13293.3125 normal_loss: 0.6614
[epoch  8][iter  420] loss: 233376.7812 RMSElog: 9.0047 grad_loss: 23327.9883 normal_loss: 0.6856
[epoch  8][iter  430] loss: 180408.3594 RMSElog: 8.9562 grad_loss: 18031.1680 normal_loss: 0.7101
[epoch  8][iter  440] loss: 182296.7812 RMSElog: 8.9477 grad_loss: 18220.0508 normal_loss: 0.6796
[epoch  8][iter  450] loss: 140447.3906 RMSElog: 8.7950 grad_loss: 14035.1055 normal_loss: 0.8389
[epoch  8][iter  460] loss: 204629.0781 RMSElog: 8.9151 grad_loss: 20453.3379 normal_loss: 0.6535
[epoch  8][iter  470] loss: 155952.0312 RMSElog: 8.6991 grad_loss: 15585.7539 normal_loss: 0.7496
[epoch  8][iter  480] loss: 164751.0000 RMSElog: 8.8619 grad_loss: 16465.5195 normal_loss: 0.7178
[epoch  8][iter  490] loss: 136780.7969 RMSElog: 8.6646 grad_loss: 13668.6904 normal_loss: 0.7243
[epoch  8][iter  500] loss: 176244.7031 RMSElog: 9.0364 grad_loss: 17614.7305 normal_loss: 0.7031
[epoch  8][iter  510] loss: 108807.8516 RMSElog: 8.1704 grad_loss: 10871.9531 normal_loss: 0.6622
[epoch  8][iter  520] loss: 127054.7656 RMSElog: 8.7554 grad_loss: 12696.0654 normal_loss: 0.6553
[epoch  8][iter  530] loss: 162356.2500 RMSElog: 8.8313 grad_loss: 16226.0723 normal_loss: 0.7217
[epoch  8][iter  540] loss: 121816.5469 RMSElog: 8.2960 grad_loss: 12172.6846 normal_loss: 0.6734
[epoch  8][iter  550] loss: 142119.2656 RMSElog: 8.5093 grad_loss: 14202.7031 normal_loss: 0.7139
[epoch  8][iter  560] loss: 179284.0938 RMSElog: 8.7772 grad_loss: 17918.9004 normal_loss: 0.7320
[epoch  8][iter  570] loss: 177648.7344 RMSElog: 8.7840 grad_loss: 17755.3828 normal_loss: 0.7072
[epoch  8][iter  580] loss: 129149.8125 RMSElog: 8.9505 grad_loss: 12905.3311 normal_loss: 0.6999
[epoch  8][iter  590] loss: 195393.5000 RMSElog: 8.9314 grad_loss: 19529.7676 normal_loss: 0.6507
[epoch  9][iter    0] loss: 150304.6562 RMSElog: 8.8919 grad_loss: 15020.8672 normal_loss: 0.7062
[epoch  9][iter   10] loss: 160056.5781 RMSElog: 8.7079 grad_loss: 15996.2832 normal_loss: 0.6666
[epoch  9][iter   20] loss: 114296.8906 RMSElog: 8.4996 grad_loss: 11420.4629 normal_loss: 0.7268
[epoch  9][iter   30] loss: 121849.5391 RMSElog: 8.5464 grad_loss: 12175.6719 normal_loss: 0.7355
[epoch  9][iter   40] loss: 176938.1875 RMSElog: 9.0316 grad_loss: 17684.0820 normal_loss: 0.7044
[epoch  9][iter   50] loss: 188623.1562 RMSElog: 8.7577 grad_loss: 18852.8828 normal_loss: 0.6762
[epoch  9][iter   60] loss: 176477.2031 RMSElog: 8.7842 grad_loss: 17638.2188 normal_loss: 0.7162
[epoch  9][iter   70] loss: 170524.5938 RMSElog: 8.6538 grad_loss: 17043.0918 normal_loss: 0.7123
[epoch  9][iter   80] loss: 125296.7031 RMSElog: 8.6047 grad_loss: 12520.4316 normal_loss: 0.6333
[epoch  9][iter   90] loss: 102210.4766 RMSElog: 8.6776 grad_loss: 10211.6934 normal_loss: 0.6767
[epoch  9][iter  100] loss: 117674.5938 RMSElog: 8.4105 grad_loss: 11758.3809 normal_loss: 0.6675
[epoch  9][iter  110] loss: 135449.6250 RMSElog: 8.8562 grad_loss: 13535.4141 normal_loss: 0.6923
[epoch  9][iter  120] loss: 129148.6328 RMSElog: 8.8855 grad_loss: 12905.2939 normal_loss: 0.6836
[epoch  9][iter  130] loss: 87678.0078 RMSElog: 8.6605 grad_loss: 8758.4756 normal_loss: 0.6651
[epoch  9][iter  140] loss: 159269.3906 RMSElog: 8.6603 grad_loss: 15917.5469 normal_loss: 0.7324
[epoch  9][iter  150] loss: 167723.7656 RMSElog: 8.8673 grad_loss: 16762.7969 normal_loss: 0.7124
[epoch  9][iter  160] loss: 207864.8594 RMSElog: 9.0791 grad_loss: 20776.7305 normal_loss: 0.6765
[epoch  9][iter  170] loss: 95986.3672 RMSElog: 8.3244 grad_loss: 9589.6797 normal_loss: 0.6329
[epoch  9][iter  180] loss: 122429.9219 RMSElog: 8.2787 grad_loss: 12234.0371 normal_loss: 0.6764
[epoch  9][iter  190] loss: 218526.7812 RMSElog: 9.1074 grad_loss: 21842.8926 normal_loss: 0.6774
[epoch  9][iter  200] loss: 180408.2656 RMSElog: 8.9555 grad_loss: 18031.1641 normal_loss: 0.7065
[epoch  9][iter  210] loss: 133267.1094 RMSElog: 8.6159 grad_loss: 13317.4473 normal_loss: 0.6474
[epoch  9][iter  220] loss: 105718.1406 RMSElog: 8.0368 grad_loss: 10563.1016 normal_loss: 0.6761
[epoch  9][iter  230] loss: 185755.0938 RMSElog: 8.9121 grad_loss: 18565.8926 normal_loss: 0.7043
[epoch  9][iter  240] loss: 160863.3438 RMSElog: 8.8577 grad_loss: 16076.7979 normal_loss: 0.6790
[epoch  9][iter  250] loss: 191673.3438 RMSElog: 8.9500 grad_loss: 19157.7031 normal_loss: 0.6822
[epoch  9][iter  260] loss: 200447.8125 RMSElog: 8.7775 grad_loss: 20035.3613 normal_loss: 0.6425
[epoch  9][iter  270] loss: 221193.2656 RMSElog: 8.9015 grad_loss: 22109.7637 normal_loss: 0.6601
[epoch  9][iter  280] loss: 160768.1406 RMSElog: 8.8228 grad_loss: 16067.2646 normal_loss: 0.7267
[epoch  9][iter  290] loss: 119637.3438 RMSElog: 8.5627 grad_loss: 11954.5430 normal_loss: 0.6291
[epoch  9][iter  300] loss: 134614.9531 RMSElog: 8.7362 grad_loss: 13452.0781 normal_loss: 0.6809
[epoch  9][iter  310] loss: 198424.0469 RMSElog: 8.9099 grad_loss: 19832.8262 normal_loss: 0.6671
[epoch  9][iter  320] loss: 158908.4219 RMSElog: 8.7962 grad_loss: 15881.3516 normal_loss: 0.6952
[epoch  9][iter  330] loss: 164750.8125 RMSElog: 8.8552 grad_loss: 16465.5117 normal_loss: 0.7157
[epoch  9][iter  340] loss: 156980.8125 RMSElog: 9.2362 grad_loss: 15687.9707 normal_loss: 0.8750
[epoch  9][iter  350] loss: 181925.9531 RMSElog: 8.8900 grad_loss: 18182.9844 normal_loss: 0.7209
[epoch  9][iter  360] loss: 153703.9688 RMSElog: 8.7589 grad_loss: 15360.9766 normal_loss: 0.6612
[epoch  9][iter  370] loss: 148162.7656 RMSElog: 8.6949 grad_loss: 14806.9238 normal_loss: 0.6577
[epoch  9][iter  380] loss: 175282.9844 RMSElog: 8.6629 grad_loss: 17518.9473 normal_loss: 0.6888
[epoch  9][iter  390] loss: 156690.9531 RMSElog: 8.7536 grad_loss: 15659.6865 normal_loss: 0.6543
[epoch  9][iter  400] loss: 154839.0000 RMSElog: 8.7814 grad_loss: 15474.4248 normal_loss: 0.6935
[epoch  9][iter  410] loss: 163158.8281 RMSElog: 9.0230 grad_loss: 16306.1025 normal_loss: 0.7564
[epoch  9][iter  420] loss: 120723.3750 RMSElog: 8.2833 grad_loss: 12063.3809 normal_loss: 0.6738
[epoch  9][iter  430] loss: 172739.3750 RMSElog: 8.9670 grad_loss: 17264.2598 normal_loss: 0.7104
[epoch  9][iter  440] loss: 141426.7031 RMSElog: 8.6933 grad_loss: 14133.2676 normal_loss: 0.7101
[epoch  9][iter  450] loss: 215522.8125 RMSElog: 9.1431 grad_loss: 21542.4590 normal_loss: 0.6799
[epoch  9][iter  460] loss: 220567.6875 RMSElog: 9.0083 grad_loss: 22047.0879 normal_loss: 0.6738
[epoch  9][iter  470] loss: 119451.1719 RMSElog: 8.3478 grad_loss: 11936.0850 normal_loss: 0.6841
[epoch  9][iter  480] loss: 146196.1094 RMSElog: 8.7356 grad_loss: 14610.1855 normal_loss: 0.6909
[epoch  9][iter  490] loss: 165290.1406 RMSElog: 8.8766 grad_loss: 16519.4316 normal_loss: 0.7055
[epoch  9][iter  500] loss: 126949.8359 RMSElog: 8.4549 grad_loss: 12685.8594 normal_loss: 0.6689
[epoch  9][iter  510] loss: 145391.9844 RMSElog: 8.7185 grad_loss: 14529.7871 normal_loss: 0.6925
[epoch  9][iter  520] loss: 78496.6875 RMSElog: 8.6444 grad_loss: 7840.3667 normal_loss: 0.6579
[epoch  9][iter  530] loss: 122367.4609 RMSElog: 8.5778 grad_loss: 12227.5117 normal_loss: 0.6565
[epoch  9][iter  540] loss: 138589.5625 RMSElog: 8.5829 grad_loss: 13849.6895 normal_loss: 0.6844
[epoch  9][iter  550] loss: 167068.2969 RMSElog: 8.8649 grad_loss: 16697.2578 normal_loss: 0.7075
[epoch  9][iter  560] loss: 196899.7812 RMSElog: 8.9626 grad_loss: 19680.3438 normal_loss: 0.6712
[epoch  9][iter  570] loss: 184355.4531 RMSElog: 9.0667 grad_loss: 18425.7930 normal_loss: 0.6856
[epoch  9][iter  580] loss: 132996.7188 RMSElog: 8.5416 grad_loss: 13290.4502 normal_loss: 0.6795
[epoch  9][iter  590] loss: 179283.4375 RMSElog: 8.7510 grad_loss: 17918.8750 normal_loss: 0.7161
###########
#epochs 10#
###########
[epoch  0][iter    0] loss: 102.3135 RMSElog: 10.2314 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter   10] loss: 101.2238 RMSElog: 10.1224 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter   20] loss: 100.2499 RMSElog: 10.0250 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter   30] loss: 102.8188 RMSElog: 10.2819 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter   40] loss: 103.2318 RMSElog: 10.3232 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter   50] loss: 101.0677 RMSElog: 10.1068 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter   60] loss: 103.4272 RMSElog: 10.3427 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter   70] loss: 103.4556 RMSElog: 10.3456 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter   80] loss: 100.1805 RMSElog: 10.0181 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter   90] loss: 99.2800 RMSElog: 9.9280 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  100] loss: 102.7809 RMSElog: 10.2781 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  110] loss: 103.4670 RMSElog: 10.3467 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  120] loss: 103.6301 RMSElog: 10.3630 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  130] loss: 102.5509 RMSElog: 10.2551 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  140] loss: 100.1063 RMSElog: 10.0106 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  150] loss: 98.9271 RMSElog: 9.8927 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  160] loss: 99.5770 RMSElog: 9.9577 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  170] loss: 98.4237 RMSElog: 9.8424 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  180] loss: 99.0515 RMSElog: 9.9052 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  190] loss: 94.9963 RMSElog: 9.4996 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  200] loss: 91.8876 RMSElog: 9.1888 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  210] loss: 99.9594 RMSElog: 9.9959 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  220] loss: 102.4397 RMSElog: 10.2440 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  230] loss: 100.9161 RMSElog: 10.0916 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  240] loss: 96.3701 RMSElog: 9.6370 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  250] loss: 98.3699 RMSElog: 9.8370 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  260] loss: 97.4739 RMSElog: 9.7474 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  270] loss: 98.2204 RMSElog: 9.8220 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  280] loss: 95.3719 RMSElog: 9.5372 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  290] loss: 97.8597 RMSElog: 9.7860 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  300] loss: 99.1950 RMSElog: 9.9195 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  310] loss: 98.4628 RMSElog: 9.8463 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  320] loss: 100.4167 RMSElog: 10.0417 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  330] loss: 101.2096 RMSElog: 10.1210 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  340] loss: 102.2834 RMSElog: 10.2283 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  350] loss: 96.2097 RMSElog: 9.6210 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  360] loss: 94.8568 RMSElog: 9.4857 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  370] loss: 98.0656 RMSElog: 9.8066 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  380] loss: 102.3742 RMSElog: 10.2374 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  390] loss: 98.1896 RMSElog: 9.8190 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  400] loss: 100.5797 RMSElog: 10.0580 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  410] loss: 99.0790 RMSElog: 9.9079 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  420] loss: 98.2229 RMSElog: 9.8223 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  430] loss: 98.1488 RMSElog: 9.8149 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  440] loss: 98.0889 RMSElog: 9.8089 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  450] loss: 99.2174 RMSElog: 9.9217 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  460] loss: 96.7827 RMSElog: 9.6783 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  470] loss: 102.7953 RMSElog: 10.2795 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  480] loss: 99.8606 RMSElog: 9.9861 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  490] loss: 101.9306 RMSElog: 10.1931 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  500] loss: 97.7419 RMSElog: 9.7742 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  510] loss: 103.7546 RMSElog: 10.3755 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  520] loss: 98.2798 RMSElog: 9.8280 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  530] loss: 95.7752 RMSElog: 9.5775 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  540] loss: 104.2948 RMSElog: 10.4295 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  550] loss: 98.3848 RMSElog: 9.8385 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  560] loss: 100.4348 RMSElog: 10.0435 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  570] loss: 100.8203 RMSElog: 10.0820 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  580] loss: 95.9753 RMSElog: 9.5975 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  590] loss: 101.1711 RMSElog: 10.1171 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter    0] loss: 92.2445 RMSElog: 9.2245 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter   10] loss: 101.4293 RMSElog: 10.1429 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter   20] loss: 101.4273 RMSElog: 10.1427 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter   30] loss: 100.5521 RMSElog: 10.0552 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter   40] loss: 103.3625 RMSElog: 10.3362 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter   50] loss: 102.4334 RMSElog: 10.2433 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter   60] loss: 98.3647 RMSElog: 9.8365 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter   70] loss: 97.9115 RMSElog: 9.7912 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter   80] loss: 97.3725 RMSElog: 9.7372 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter   90] loss: 92.5885 RMSElog: 9.2589 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  100] loss: 100.8677 RMSElog: 10.0868 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  110] loss: 98.3780 RMSElog: 9.8378 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  120] loss: 96.6522 RMSElog: 9.6652 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  130] loss: 99.0329 RMSElog: 9.9033 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  140] loss: 101.8461 RMSElog: 10.1846 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  150] loss: 99.8561 RMSElog: 9.9856 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  160] loss: 101.0109 RMSElog: 10.1011 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  170] loss: 98.7588 RMSElog: 9.8759 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  180] loss: 97.6419 RMSElog: 9.7642 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  190] loss: 99.1366 RMSElog: 9.9137 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  200] loss: 101.4443 RMSElog: 10.1444 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  210] loss: 103.1233 RMSElog: 10.3123 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  220] loss: 95.0796 RMSElog: 9.5080 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  230] loss: 99.2238 RMSElog: 9.9224 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  240] loss: 100.5102 RMSElog: 10.0510 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  250] loss: 92.9547 RMSElog: 9.2955 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  260] loss: 96.8232 RMSElog: 9.6823 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  270] loss: 102.7209 RMSElog: 10.2721 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  280] loss: 93.2600 RMSElog: 9.3260 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  290] loss: 96.9437 RMSElog: 9.6944 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  300] loss: 99.5952 RMSElog: 9.9595 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  310] loss: 95.2370 RMSElog: 9.5237 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  320] loss: 96.9538 RMSElog: 9.6954 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  330] loss: 95.9639 RMSElog: 9.5964 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  340] loss: 102.0149 RMSElog: 10.2015 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  350] loss: 98.5336 RMSElog: 9.8534 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  360] loss: 95.2266 RMSElog: 9.5227 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  370] loss: 98.5303 RMSElog: 9.8530 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  380] loss: 97.4835 RMSElog: 9.7484 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  390] loss: 97.4220 RMSElog: 9.7422 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  400] loss: 99.6502 RMSElog: 9.9650 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  410] loss: 99.0681 RMSElog: 9.9068 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  420] loss: 97.8610 RMSElog: 9.7861 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  430] loss: 100.4440 RMSElog: 10.0444 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  440] loss: 98.1270 RMSElog: 9.8127 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  450] loss: 101.5931 RMSElog: 10.1593 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  460] loss: 98.3123 RMSElog: 9.8312 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  470] loss: 97.9899 RMSElog: 9.7990 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  480] loss: 99.8031 RMSElog: 9.9803 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  490] loss: 97.1366 RMSElog: 9.7137 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  500] loss: 100.3281 RMSElog: 10.0328 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  510] loss: 99.7421 RMSElog: 9.9742 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  520] loss: 97.1281 RMSElog: 9.7128 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  530] loss: 97.6969 RMSElog: 9.7697 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  540] loss: 96.6283 RMSElog: 9.6628 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  550] loss: 96.9431 RMSElog: 9.6943 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  560] loss: 96.6998 RMSElog: 9.6700 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  570] loss: 97.9865 RMSElog: 9.7987 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  580] loss: 91.5707 RMSElog: 9.1571 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  590] loss: 96.5924 RMSElog: 9.6592 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter    0] loss: 102.3307 RMSElog: 10.2331 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter   10] loss: 92.1732 RMSElog: 9.2173 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter   20] loss: 99.4173 RMSElog: 9.9417 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter   30] loss: 103.2587 RMSElog: 10.3259 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter   40] loss: 102.1108 RMSElog: 10.2111 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter   50] loss: 99.9919 RMSElog: 9.9992 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter   60] loss: 95.7525 RMSElog: 9.5752 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter   70] loss: 98.5728 RMSElog: 9.8573 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter   80] loss: 98.4721 RMSElog: 9.8472 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter   90] loss: 98.4451 RMSElog: 9.8445 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  100] loss: 95.6860 RMSElog: 9.5686 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  110] loss: 99.0106 RMSElog: 9.9011 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  120] loss: 98.5889 RMSElog: 9.8589 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  130] loss: 101.5617 RMSElog: 10.1562 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  140] loss: 101.7556 RMSElog: 10.1756 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  150] loss: 96.8680 RMSElog: 9.6868 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  160] loss: 98.1861 RMSElog: 9.8186 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  170] loss: 101.6066 RMSElog: 10.1607 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  180] loss: 100.9788 RMSElog: 10.0979 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  190] loss: 96.1681 RMSElog: 9.6168 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  200] loss: 98.8453 RMSElog: 9.8845 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  210] loss: 95.3754 RMSElog: 9.5375 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  220] loss: 101.5263 RMSElog: 10.1526 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  230] loss: 96.3107 RMSElog: 9.6311 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  240] loss: 96.4644 RMSElog: 9.6464 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  250] loss: 99.9685 RMSElog: 9.9968 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  260] loss: 97.3688 RMSElog: 9.7369 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  270] loss: 96.5108 RMSElog: 9.6511 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  280] loss: 97.4221 RMSElog: 9.7422 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  290] loss: 100.5795 RMSElog: 10.0579 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  300] loss: 95.1995 RMSElog: 9.5199 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  310] loss: 91.5498 RMSElog: 9.1550 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  320] loss: 92.7554 RMSElog: 9.2755 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  330] loss: 93.5930 RMSElog: 9.3593 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  340] loss: 90.9774 RMSElog: 9.0977 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  350] loss: 96.6990 RMSElog: 9.6699 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  360] loss: 92.0260 RMSElog: 9.2026 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  370] loss: 85.6260 RMSElog: 8.5626 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  380] loss: 88.2072 RMSElog: 8.8207 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  390] loss: 91.4054 RMSElog: 9.1405 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  400] loss: 92.5896 RMSElog: 9.2590 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  410] loss: 94.8857 RMSElog: 9.4886 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  420] loss: 94.9565 RMSElog: 9.4957 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  430] loss: 92.5218 RMSElog: 9.2522 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  440] loss: 89.3740 RMSElog: 8.9374 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  450] loss: 90.4455 RMSElog: 9.0446 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  460] loss: 87.7536 RMSElog: 8.7754 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  470] loss: 90.1201 RMSElog: 9.0120 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  480] loss: 86.9116 RMSElog: 8.6912 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  490] loss: 88.2012 RMSElog: 8.8201 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  500] loss: 85.9688 RMSElog: 8.5969 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  510] loss: 89.5909 RMSElog: 8.9591 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  520] loss: 86.8693 RMSElog: 8.6869 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  530] loss: 88.8110 RMSElog: 8.8811 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  540] loss: 85.3311 RMSElog: 8.5331 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  550] loss: 90.5315 RMSElog: 9.0531 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  560] loss: 85.9529 RMSElog: 8.5953 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  570] loss: 84.7536 RMSElog: 8.4754 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  580] loss: 91.9952 RMSElog: 9.1995 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  590] loss: 91.2866 RMSElog: 9.1287 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter    0] loss: 87.9772 RMSElog: 8.7977 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter   10] loss: 87.6433 RMSElog: 8.7643 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter   20] loss: 90.0178 RMSElog: 9.0018 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter   30] loss: 89.7253 RMSElog: 8.9725 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter   40] loss: 89.3613 RMSElog: 8.9361 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter   50] loss: 89.8821 RMSElog: 8.9882 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter   60] loss: 87.0892 RMSElog: 8.7089 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter   70] loss: 90.5604 RMSElog: 9.0560 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter   80] loss: 87.5623 RMSElog: 8.7562 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter   90] loss: 89.7400 RMSElog: 8.9740 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  100] loss: 91.1057 RMSElog: 9.1106 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  110] loss: 84.5432 RMSElog: 8.4543 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  120] loss: 88.8597 RMSElog: 8.8860 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  130] loss: 85.9379 RMSElog: 8.5938 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  140] loss: 89.8266 RMSElog: 8.9827 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  150] loss: 86.7147 RMSElog: 8.6715 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  160] loss: 84.4211 RMSElog: 8.4421 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  170] loss: 90.2267 RMSElog: 9.0227 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  180] loss: 89.4997 RMSElog: 8.9500 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  190] loss: 89.3549 RMSElog: 8.9355 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  200] loss: 89.5807 RMSElog: 8.9581 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  210] loss: 88.5077 RMSElog: 8.8508 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  220] loss: 90.3906 RMSElog: 9.0391 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  230] loss: 86.5177 RMSElog: 8.6518 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  240] loss: 89.9490 RMSElog: 8.9949 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  250] loss: 89.9695 RMSElog: 8.9969 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  260] loss: 88.5719 RMSElog: 8.8572 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  270] loss: 91.5446 RMSElog: 9.1545 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  280] loss: 86.9644 RMSElog: 8.6964 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  290] loss: 91.2576 RMSElog: 9.1258 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  300] loss: 83.9051 RMSElog: 8.3905 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  310] loss: 84.2646 RMSElog: 8.4265 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  320] loss: 86.1038 RMSElog: 8.6104 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  330] loss: 92.8254 RMSElog: 9.2825 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  340] loss: 89.5055 RMSElog: 8.9505 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  350] loss: 89.3037 RMSElog: 8.9304 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  360] loss: 88.6293 RMSElog: 8.8629 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  370] loss: 90.8041 RMSElog: 9.0804 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  380] loss: 84.2528 RMSElog: 8.4253 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  390] loss: 87.3390 RMSElog: 8.7339 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  400] loss: 86.5133 RMSElog: 8.6513 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  410] loss: 92.5960 RMSElog: 9.2596 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  420] loss: 89.3856 RMSElog: 8.9386 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  430] loss: 90.8305 RMSElog: 9.0830 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  440] loss: 88.8537 RMSElog: 8.8854 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  450] loss: 83.8644 RMSElog: 8.3864 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  460] loss: 88.6369 RMSElog: 8.8637 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  470] loss: 88.0027 RMSElog: 8.8003 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  480] loss: 89.6295 RMSElog: 8.9630 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  490] loss: 88.4372 RMSElog: 8.8437 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  500] loss: 89.3345 RMSElog: 8.9335 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  510] loss: 83.6774 RMSElog: 8.3677 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  520] loss: 89.2551 RMSElog: 8.9255 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  530] loss: 88.9273 RMSElog: 8.8927 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  540] loss: 88.5864 RMSElog: 8.8586 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  550] loss: 86.6584 RMSElog: 8.6658 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  560] loss: 88.8530 RMSElog: 8.8853 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  570] loss: 84.1069 RMSElog: 8.4107 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  580] loss: 82.9083 RMSElog: 8.2908 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  590] loss: 84.3208 RMSElog: 8.4321 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  4][iter    0] loss: 172094.5156 RMSElog: 8.8997 grad_loss: 17200.5508 normal_loss: 0.0000
[epoch  4][iter   10] loss: 110575.8594 RMSElog: 8.3300 grad_loss: 11049.2559 normal_loss: 0.0000
[epoch  4][iter   20] loss: 148159.0312 RMSElog: 8.8321 grad_loss: 14807.0713 normal_loss: 0.0000
[epoch  4][iter   30] loss: 196251.0938 RMSElog: 8.8575 grad_loss: 19616.2520 normal_loss: 0.0000
[epoch  4][iter   40] loss: 210496.1094 RMSElog: 8.9871 grad_loss: 21040.6250 normal_loss: 0.0000
[epoch  4][iter   50] loss: 105872.5781 RMSElog: 8.5087 grad_loss: 10578.7490 normal_loss: 0.0000
[epoch  4][iter   60] loss: 164744.7031 RMSElog: 8.8077 grad_loss: 16465.6621 normal_loss: 0.0000
[epoch  4][iter   70] loss: 139410.3750 RMSElog: 8.6875 grad_loss: 13932.3496 normal_loss: 0.0000
[epoch  4][iter   80] loss: 195481.3125 RMSElog: 8.8273 grad_loss: 19539.3027 normal_loss: 0.0000
[epoch  4][iter   90] loss: 218522.0000 RMSElog: 9.1757 grad_loss: 21843.0234 normal_loss: 0.0000
[epoch  4][iter  100] loss: 144957.2344 RMSElog: 8.9877 grad_loss: 14486.7363 normal_loss: 0.0000
[epoch  4][iter  110] loss: 160619.7812 RMSElog: 8.9033 grad_loss: 16053.0752 normal_loss: 0.0000
[epoch  4][iter  120] loss: 198419.0938 RMSElog: 8.9595 grad_loss: 19832.9512 normal_loss: 0.0000
[epoch  4][iter  130] loss: 200444.0469 RMSElog: 8.8685 grad_loss: 20035.5352 normal_loss: 0.0000
[epoch  4][iter  140] loss: 161064.5156 RMSElog: 8.7861 grad_loss: 16097.6660 normal_loss: 0.0000
[epoch  4][iter  150] loss: 191181.2812 RMSElog: 8.8805 grad_loss: 19109.2480 normal_loss: 0.0000
[epoch  4][iter  160] loss: 189717.9062 RMSElog: 8.9085 grad_loss: 18962.8828 normal_loss: 0.0000
[epoch  4][iter  170] loss: 106742.6719 RMSElog: 8.4036 grad_loss: 10665.8643 normal_loss: 0.0000
[epoch  4][iter  180] loss: 90461.4688 RMSElog: 8.4012 grad_loss: 9037.7451 normal_loss: 0.0000
[epoch  4][iter  190] loss: 162341.7969 RMSElog: 8.8258 grad_loss: 16225.3535 normal_loss: 0.0000
[epoch  4][iter  200] loss: 183458.0312 RMSElog: 8.9693 grad_loss: 18336.8340 normal_loss: 0.0000
[epoch  4][iter  210] loss: 148216.5625 RMSElog: 8.8169 grad_loss: 14812.8398 normal_loss: 0.0000
[epoch  4][iter  220] loss: 162349.9688 RMSElog: 8.8565 grad_loss: 16226.1406 normal_loss: 0.0000
[epoch  4][iter  230] loss: 105072.1562 RMSElog: 8.3675 grad_loss: 10498.8486 normal_loss: 0.0000
[epoch  4][iter  240] loss: 172229.9375 RMSElog: 8.8615 grad_loss: 17214.1328 normal_loss: 0.0000
[epoch  4][iter  250] loss: 100089.3828 RMSElog: 8.4363 grad_loss: 10000.5020 normal_loss: 0.0000
[epoch  4][iter  260] loss: 126000.0156 RMSElog: 8.5197 grad_loss: 12591.4824 normal_loss: 0.0000
[epoch  4][iter  270] loss: 140618.3125 RMSElog: 8.7097 grad_loss: 14053.1211 normal_loss: 0.0000
[epoch  4][iter  280] loss: 177642.6250 RMSElog: 8.8406 grad_loss: 17755.4219 normal_loss: 0.0000
[epoch  4][iter  290] loss: 144633.8906 RMSElog: 8.7950 grad_loss: 14454.5938 normal_loss: 0.0000
[epoch  4][iter  300] loss: 148518.2656 RMSElog: 8.8975 grad_loss: 14842.9297 normal_loss: 0.0000
[epoch  4][iter  310] loss: 159801.4688 RMSElog: 8.8495 grad_loss: 15971.2969 normal_loss: 0.0000
[epoch  4][iter  320] loss: 182296.1250 RMSElog: 9.0452 grad_loss: 18220.5684 normal_loss: 0.0000
[epoch  4][iter  330] loss: 163370.1562 RMSElog: 8.9824 grad_loss: 16328.0332 normal_loss: 0.0000
[epoch  4][iter  340] loss: 178055.9062 RMSElog: 8.9427 grad_loss: 17796.6465 normal_loss: 0.0000
[epoch  4][iter  350] loss: 165283.7188 RMSElog: 8.8713 grad_loss: 16519.5000 normal_loss: 0.0000
[epoch  4][iter  360] loss: 147740.7188 RMSElog: 8.6904 grad_loss: 14765.3818 normal_loss: 0.0000
[epoch  4][iter  370] loss: 103328.8281 RMSElog: 8.3203 grad_loss: 10324.5625 normal_loss: 0.0000
[epoch  4][iter  380] loss: 109119.0000 RMSElog: 8.4359 grad_loss: 10903.4648 normal_loss: 0.0000
[epoch  4][iter  390] loss: 138687.6875 RMSElog: 8.5945 grad_loss: 13860.1738 normal_loss: 0.0000
[epoch  4][iter  400] loss: 224918.1875 RMSElog: 8.8484 grad_loss: 22482.9707 normal_loss: 0.0000
[epoch  4][iter  410] loss: 126848.6562 RMSElog: 8.5279 grad_loss: 12676.3369 normal_loss: 0.0000
[epoch  4][iter  420] loss: 167760.1250 RMSElog: 8.8325 grad_loss: 16767.1797 normal_loss: 0.0000
[epoch  4][iter  430] loss: 129718.7578 RMSElog: 8.7351 grad_loss: 12963.1406 normal_loss: 0.0000
[epoch  4][iter  440] loss: 117669.1484 RMSElog: 8.4597 grad_loss: 11758.4551 normal_loss: 0.0000
[epoch  4][iter  450] loss: 102204.7734 RMSElog: 8.7373 grad_loss: 10211.7402 normal_loss: 0.0000
[epoch  4][iter  460] loss: 113400.1875 RMSElog: 8.3733 grad_loss: 11331.6455 normal_loss: 0.0000
[epoch  4][iter  470] loss: 175522.2500 RMSElog: 8.8420 grad_loss: 17543.3828 normal_loss: 0.0000
[epoch  4][iter  480] loss: 163569.4375 RMSElog: 8.7985 grad_loss: 16348.1455 normal_loss: 0.0000
[epoch  4][iter  490] loss: 80075.8281 RMSElog: 8.2635 grad_loss: 7999.3188 normal_loss: 0.0000
[epoch  4][iter  500] loss: 138924.6562 RMSElog: 8.7151 grad_loss: 13883.7500 normal_loss: 0.0000
[epoch  4][iter  510] loss: 117084.3594 RMSElog: 8.3002 grad_loss: 11700.1357 normal_loss: 0.0000
[epoch  4][iter  520] loss: 143415.4219 RMSElog: 8.5290 grad_loss: 14333.0127 normal_loss: 0.0000
[epoch  4][iter  530] loss: 159399.8281 RMSElog: 8.7125 grad_loss: 15931.2705 normal_loss: 0.0000
[epoch  4][iter  540] loss: 199581.0781 RMSElog: 8.8327 grad_loss: 19949.2754 normal_loss: 0.0000
[epoch  4][iter  550] loss: 141420.6562 RMSElog: 8.7317 grad_loss: 14133.3340 normal_loss: 0.0000
[epoch  4][iter  560] loss: 152537.8281 RMSElog: 8.6853 grad_loss: 15245.0977 normal_loss: 0.0000
[epoch  4][iter  570] loss: 158009.2031 RMSElog: 8.8102 grad_loss: 15792.1094 normal_loss: 0.0000
[epoch  4][iter  580] loss: 148768.1875 RMSElog: 8.8558 grad_loss: 14867.9639 normal_loss: 0.0000
[epoch  4][iter  590] loss: 181267.9531 RMSElog: 8.7896 grad_loss: 18118.0059 normal_loss: 0.0000
[epoch  5][iter    0] loss: 149109.1562 RMSElog: 8.8451 grad_loss: 14902.0703 normal_loss: 0.0000
[epoch  5][iter   10] loss: 163763.3125 RMSElog: 8.7742 grad_loss: 16367.5566 normal_loss: 0.0000
[epoch  5][iter   20] loss: 147183.9219 RMSElog: 8.8448 grad_loss: 14709.5469 normal_loss: 0.0000
[epoch  5][iter   30] loss: 220668.8906 RMSElog: 8.8515 grad_loss: 22058.0371 normal_loss: 0.0000
[epoch  5][iter   40] loss: 144168.2031 RMSElog: 8.6136 grad_loss: 14408.2070 normal_loss: 0.0000
[epoch  5][iter   50] loss: 160720.5625 RMSElog: 8.9580 grad_loss: 16063.0977 normal_loss: 0.0000
[epoch  5][iter   60] loss: 147519.0312 RMSElog: 8.7349 grad_loss: 14743.1689 normal_loss: 0.0000
[epoch  5][iter   70] loss: 127439.1328 RMSElog: 8.4032 grad_loss: 12735.5098 normal_loss: 0.0000
[epoch  5][iter   80] loss: 146439.2500 RMSElog: 8.7332 grad_loss: 14635.1914 normal_loss: 0.0000
[epoch  5][iter   90] loss: 158855.9844 RMSElog: 8.8295 grad_loss: 15876.7695 normal_loss: 0.0000
[epoch  5][iter  100] loss: 233371.7344 RMSElog: 9.0804 grad_loss: 23328.0938 normal_loss: 0.0000
[epoch  5][iter  110] loss: 125461.3359 RMSElog: 8.3995 grad_loss: 12537.7344 normal_loss: 0.0000
[epoch  5][iter  120] loss: 143879.0312 RMSElog: 8.7642 grad_loss: 14379.1387 normal_loss: 0.0000
[epoch  5][iter  130] loss: 165430.6875 RMSElog: 9.0156 grad_loss: 16534.0527 normal_loss: 0.0000
[epoch  5][iter  140] loss: 187211.5781 RMSElog: 8.9013 grad_loss: 18712.2578 normal_loss: 0.0000
[epoch  5][iter  150] loss: 114154.4141 RMSElog: 8.5370 grad_loss: 11406.9043 normal_loss: 0.0000
[epoch  5][iter  160] loss: 167759.3750 RMSElog: 8.7876 grad_loss: 16767.1504 normal_loss: 0.0000
[epoch  5][iter  170] loss: 147740.3906 RMSElog: 8.6625 grad_loss: 14765.3770 normal_loss: 0.0000
[epoch  5][iter  180] loss: 191343.0000 RMSElog: 8.7506 grad_loss: 19125.5508 normal_loss: 0.0000
[epoch  5][iter  190] loss: 167393.1406 RMSElog: 8.9217 grad_loss: 16730.3926 normal_loss: 0.0000
[epoch  5][iter  200] loss: 170668.0312 RMSElog: 8.9555 grad_loss: 17057.8477 normal_loss: 0.0000
[epoch  5][iter  210] loss: 121937.1172 RMSElog: 8.7462 grad_loss: 12184.9658 normal_loss: 0.0000
[epoch  5][iter  220] loss: 164743.8125 RMSElog: 8.7797 grad_loss: 16465.6016 normal_loss: 0.0000
[epoch  5][iter  230] loss: 138909.5156 RMSElog: 8.6588 grad_loss: 13882.2930 normal_loss: 0.0000
[epoch  5][iter  240] loss: 160003.2500 RMSElog: 8.8348 grad_loss: 15991.4902 normal_loss: 0.0000
[epoch  5][iter  250] loss: 103507.7344 RMSElog: 8.3892 grad_loss: 10342.3848 normal_loss: 0.0000
[epoch  5][iter  260] loss: 194550.5469 RMSElog: 8.7881 grad_loss: 19446.2676 normal_loss: 0.0000
[epoch  5][iter  270] loss: 112687.4062 RMSElog: 8.1938 grad_loss: 11260.5469 normal_loss: 0.0000
[epoch  5][iter  280] loss: 230493.6719 RMSElog: 9.0857 grad_loss: 23040.2812 normal_loss: 0.0000
[epoch  5][iter  290] loss: 123507.9062 RMSElog: 8.7328 grad_loss: 12342.0586 normal_loss: 0.0000
[epoch  5][iter  300] loss: 239426.8125 RMSElog: 9.0768 grad_loss: 23933.6055 normal_loss: 0.0000
[epoch  5][iter  310] loss: 174606.0938 RMSElog: 8.8156 grad_loss: 17451.7930 normal_loss: 0.0000
[epoch  5][iter  320] loss: 154282.9688 RMSElog: 8.6370 grad_loss: 15419.6602 normal_loss: 0.0000
[epoch  5][iter  330] loss: 149011.9375 RMSElog: 8.6589 grad_loss: 14892.5352 normal_loss: 0.0000
[epoch  5][iter  340] loss: 227298.0469 RMSElog: 9.1557 grad_loss: 22720.6484 normal_loss: 0.0000
[epoch  5][iter  350] loss: 109117.3438 RMSElog: 8.3241 grad_loss: 10903.4102 normal_loss: 0.0000
[epoch  5][iter  360] loss: 198266.7969 RMSElog: 8.8530 grad_loss: 19817.8262 normal_loss: 0.0000
[epoch  5][iter  370] loss: 132017.6719 RMSElog: 8.8836 grad_loss: 13192.8828 normal_loss: 0.0000
[epoch  5][iter  380] loss: 138994.0781 RMSElog: 8.6477 grad_loss: 13890.7607 normal_loss: 0.0000
[epoch  5][iter  390] loss: 190911.5312 RMSElog: 9.0589 grad_loss: 19082.0938 normal_loss: 0.0000
[epoch  5][iter  400] loss: 199046.0000 RMSElog: 8.8871 grad_loss: 19895.7129 normal_loss: 0.0000
[epoch  5][iter  410] loss: 146189.5469 RMSElog: 8.7545 grad_loss: 14610.2002 normal_loss: 0.0000
[epoch  5][iter  420] loss: 154005.0781 RMSElog: 8.7068 grad_loss: 15391.8008 normal_loss: 0.0000
[epoch  5][iter  430] loss: 226923.5000 RMSElog: 8.9726 grad_loss: 22683.3770 normal_loss: 0.0000
[epoch  5][iter  440] loss: 167708.0312 RMSElog: 8.9890 grad_loss: 16761.8145 normal_loss: 0.0000
[epoch  5][iter  450] loss: 182838.3438 RMSElog: 9.0015 grad_loss: 18274.8320 normal_loss: 0.0000
[epoch  5][iter  460] loss: 170034.2500 RMSElog: 8.8061 grad_loss: 16994.6191 normal_loss: 0.0000
[epoch  5][iter  470] loss: 141142.8281 RMSElog: 8.3860 grad_loss: 14105.8965 normal_loss: 0.0000
[epoch  5][iter  480] loss: 183066.7188 RMSElog: 8.6078 grad_loss: 18298.0645 normal_loss: 0.0000
[epoch  5][iter  490] loss: 177790.1406 RMSElog: 9.0123 grad_loss: 17770.0020 normal_loss: 0.0000
[epoch  5][iter  500] loss: 154265.1875 RMSElog: 8.6818 grad_loss: 15417.8369 normal_loss: 0.0000
[epoch  5][iter  510] loss: 105866.9922 RMSElog: 8.1459 grad_loss: 10578.5537 normal_loss: 0.0000
[epoch  5][iter  520] loss: 152747.5938 RMSElog: 8.7091 grad_loss: 15266.0498 normal_loss: 0.0000
[epoch  5][iter  530] loss: 108392.2188 RMSElog: 8.6123 grad_loss: 10830.6094 normal_loss: 0.0000
[epoch  5][iter  540] loss: 110857.4062 RMSElog: 8.6816 grad_loss: 11077.0586 normal_loss: 0.0000
[epoch  5][iter  550] loss: 190946.6562 RMSElog: 8.8860 grad_loss: 19085.7793 normal_loss: 0.0000
[epoch  5][iter  560] loss: 177641.9531 RMSElog: 8.7910 grad_loss: 17755.4043 normal_loss: 0.0000
[epoch  5][iter  570] loss: 117084.1250 RMSElog: 8.2907 grad_loss: 11700.1211 normal_loss: 0.0000
[epoch  5][iter  580] loss: 114897.6953 RMSElog: 8.3066 grad_loss: 11481.4629 normal_loss: 0.0000
[epoch  5][iter  590] loss: 110779.2578 RMSElog: 8.1622 grad_loss: 11069.7637 normal_loss: 0.0000
[epoch  6][iter    0] loss: 189480.8125 RMSElog: 8.9698 grad_loss: 18939.1113 normal_loss: 0.0000
[epoch  6][iter   10] loss: 152308.1719 RMSElog: 8.7185 grad_loss: 15222.0986 normal_loss: 0.0000
[epoch  6][iter   20] loss: 114896.9219 RMSElog: 8.2777 grad_loss: 11481.4150 normal_loss: 0.0000
[epoch  6][iter   30] loss: 201137.9375 RMSElog: 8.8493 grad_loss: 20104.9434 normal_loss: 0.0000
[epoch  6][iter   40] loss: 147584.3125 RMSElog: 8.6154 grad_loss: 14749.8154 normal_loss: 0.0000
[epoch  6][iter   50] loss: 183847.7344 RMSElog: 9.0423 grad_loss: 18375.7305 normal_loss: 0.0000
[epoch  6][iter   60] loss: 140310.7188 RMSElog: 8.8451 grad_loss: 14022.2275 normal_loss: 0.0000
[epoch  6][iter   70] loss: 72267.1953 RMSElog: 8.4859 grad_loss: 7218.2334 normal_loss: 0.0000
[epoch  6][iter   80] loss: 116045.8438 RMSElog: 8.2861 grad_loss: 11596.2979 normal_loss: 0.0000
[epoch  6][iter   90] loss: 145296.3906 RMSElog: 8.5958 grad_loss: 14521.0439 normal_loss: 0.0000
[epoch  6][iter  100] loss: 135944.7500 RMSElog: 8.7386 grad_loss: 13585.7363 normal_loss: 0.0000
[epoch  6][iter  110] loss: 115638.8906 RMSElog: 8.5076 grad_loss: 11555.3809 normal_loss: 0.0000
[epoch  6][iter  120] loss: 125258.5469 RMSElog: 8.4539 grad_loss: 12517.4004 normal_loss: 0.0000
[epoch  6][iter  130] loss: 222992.4375 RMSElog: 9.0680 grad_loss: 22290.1758 normal_loss: 0.0000
[epoch  6][iter  140] loss: 174739.1250 RMSElog: 8.7741 grad_loss: 17465.1387 normal_loss: 0.0000
[epoch  6][iter  150] loss: 127036.7266 RMSElog: 8.4142 grad_loss: 12695.2588 normal_loss: 0.0000
[epoch  6][iter  160] loss: 109411.0625 RMSElog: 8.0647 grad_loss: 10933.0420 normal_loss: 0.0000
[epoch  6][iter  170] loss: 199046.1094 RMSElog: 8.8866 grad_loss: 19895.7246 normal_loss: 0.0000
[epoch  6][iter  180] loss: 163368.6250 RMSElog: 8.8567 grad_loss: 16328.0068 normal_loss: 0.0000
[epoch  6][iter  190] loss: 154832.4062 RMSElog: 8.7957 grad_loss: 15474.4453 normal_loss: 0.0000
[epoch  6][iter  200] loss: 206375.1719 RMSElog: 9.0316 grad_loss: 20628.4863 normal_loss: 0.0000
[epoch  6][iter  210] loss: 170465.3281 RMSElog: 8.7808 grad_loss: 17037.7520 normal_loss: 0.0000
[epoch  6][iter  220] loss: 152522.8906 RMSElog: 8.7462 grad_loss: 15243.5430 normal_loss: 0.0000
[epoch  6][iter  230] loss: 138446.3438 RMSElog: 8.4985 grad_loss: 13836.1367 normal_loss: 0.0000
[epoch  6][iter  240] loss: 115507.3516 RMSElog: 8.3382 grad_loss: 11542.3975 normal_loss: 0.0000
[epoch  6][iter  250] loss: 148767.0938 RMSElog: 8.7817 grad_loss: 14867.9287 normal_loss: 0.0000
[epoch  6][iter  260] loss: 129142.6719 RMSElog: 8.9247 grad_loss: 12905.3428 normal_loss: 0.0000
[epoch  6][iter  270] loss: 190910.9688 RMSElog: 9.0376 grad_loss: 19082.0605 normal_loss: 0.0000
[epoch  6][iter  280] loss: 180956.1875 RMSElog: 8.9731 grad_loss: 18086.6465 normal_loss: 0.0000
[epoch  6][iter  290] loss: 162868.7031 RMSElog: 8.8133 grad_loss: 16278.0566 normal_loss: 0.0000
[epoch  6][iter  300] loss: 153972.7344 RMSElog: 8.6104 grad_loss: 15388.6631 normal_loss: 0.0000
[epoch  6][iter  310] loss: 153940.2031 RMSElog: 8.9761 grad_loss: 15385.0439 normal_loss: 0.0000
[epoch  6][iter  320] loss: 147887.1875 RMSElog: 8.6187 grad_loss: 14780.0996 normal_loss: 0.0000
[epoch  6][iter  330] loss: 154600.4688 RMSElog: 8.7331 grad_loss: 15451.3135 normal_loss: 0.0000
[epoch  6][iter  340] loss: 166047.5938 RMSElog: 8.7657 grad_loss: 16595.9941 normal_loss: 0.0000
[epoch  6][iter  350] loss: 215926.8438 RMSElog: 8.8737 grad_loss: 21583.8105 normal_loss: 0.0000
[epoch  6][iter  360] loss: 100639.8281 RMSElog: 8.7737 grad_loss: 10055.2090 normal_loss: 0.0000
[epoch  6][iter  370] loss: 181959.2188 RMSElog: 8.9503 grad_loss: 18186.9707 normal_loss: 0.0000
[epoch  6][iter  380] loss: 173139.3750 RMSElog: 8.6262 grad_loss: 17305.3105 normal_loss: 0.0000
[epoch  6][iter  390] loss: 148517.0938 RMSElog: 8.8206 grad_loss: 14842.8887 normal_loss: 0.0000
[epoch  6][iter  400] loss: 100619.5781 RMSElog: 8.0538 grad_loss: 10053.9043 normal_loss: 0.0000
[epoch  6][iter  410] loss: 104310.0859 RMSElog: 8.0500 grad_loss: 10422.9590 normal_loss: 0.0000
[epoch  6][iter  420] loss: 125999.3750 RMSElog: 8.4972 grad_loss: 12591.4404 normal_loss: 0.0000
[epoch  6][iter  430] loss: 220561.1562 RMSElog: 9.0035 grad_loss: 22047.1113 normal_loss: 0.0000
[epoch  6][iter  440] loss: 141419.5625 RMSElog: 8.6851 grad_loss: 14133.2715 normal_loss: 0.0000
[epoch  6][iter  450] loss: 135181.0469 RMSElog: 8.4705 grad_loss: 13509.6338 normal_loss: 0.0000
[epoch  6][iter  460] loss: 102204.4844 RMSElog: 8.7017 grad_loss: 10211.7461 normal_loss: 0.0000
[epoch  6][iter  470] loss: 173941.9531 RMSElog: 8.8422 grad_loss: 17385.3535 normal_loss: 0.0000
[epoch  6][iter  480] loss: 147574.5156 RMSElog: 8.8071 grad_loss: 14748.6445 normal_loss: 0.0000
[epoch  6][iter  490] loss: 152494.0781 RMSElog: 8.7545 grad_loss: 15240.6523 normal_loss: 0.0000
[epoch  6][iter  500] loss: 185471.9688 RMSElog: 8.8847 grad_loss: 18538.3125 normal_loss: 0.0000
[epoch  6][iter  510] loss: 132016.7344 RMSElog: 8.8132 grad_loss: 13192.8594 normal_loss: 0.0000
[epoch  6][iter  520] loss: 90460.4531 RMSElog: 8.3470 grad_loss: 9037.6982 normal_loss: 0.0000
[epoch  6][iter  530] loss: 231671.2656 RMSElog: 9.1308 grad_loss: 23157.9961 normal_loss: 0.0000
[epoch  6][iter  540] loss: 156956.7031 RMSElog: 8.8264 grad_loss: 15686.8447 normal_loss: 0.0000
[epoch  6][iter  550] loss: 198417.8125 RMSElog: 8.9116 grad_loss: 19832.8691 normal_loss: 0.0000
[epoch  6][iter  560] loss: 106486.8359 RMSElog: 8.0394 grad_loss: 10640.6445 normal_loss: 0.0000
[epoch  6][iter  570] loss: 152514.3906 RMSElog: 8.7014 grad_loss: 15242.7383 normal_loss: 0.0000
[epoch  6][iter  580] loss: 116557.0703 RMSElog: 8.5157 grad_loss: 11647.1914 normal_loss: 0.0000
[epoch  6][iter  590] loss: 208158.4375 RMSElog: 9.1278 grad_loss: 20806.7168 normal_loss: 0.0000
[epoch  7][iter    0] loss: 194550.2188 RMSElog: 8.7881 grad_loss: 19446.2344 normal_loss: 0.0000
[epoch  7][iter   10] loss: 173139.3594 RMSElog: 8.6267 grad_loss: 17305.3086 normal_loss: 0.0000
[epoch  7][iter   20] loss: 162533.4688 RMSElog: 8.9302 grad_loss: 16244.4160 normal_loss: 0.0000
[epoch  7][iter   30] loss: 196892.9531 RMSElog: 8.9452 grad_loss: 19680.3496 normal_loss: 0.0000
[epoch  7][iter   40] loss: 195415.2500 RMSElog: 8.5905 grad_loss: 19532.9355 normal_loss: 0.0000
[epoch  7][iter   50] loss: 163703.2969 RMSElog: 8.6213 grad_loss: 16361.7080 normal_loss: 0.0000
[epoch  7][iter   60] loss: 206374.5156 RMSElog: 9.0098 grad_loss: 20628.4414 normal_loss: 0.0000
[epoch  7][iter   70] loss: 186026.1562 RMSElog: 8.8647 grad_loss: 18593.7500 normal_loss: 0.0000
[epoch  7][iter   80] loss: 159317.1406 RMSElog: 8.9283 grad_loss: 15922.7852 normal_loss: 0.0000
[epoch  7][iter   90] loss: 166982.7188 RMSElog: 8.8057 grad_loss: 16689.4648 normal_loss: 0.0000
[epoch  7][iter  100] loss: 182542.4844 RMSElog: 8.9302 grad_loss: 18245.3184 normal_loss: 0.0000
[epoch  7][iter  110] loss: 159262.1562 RMSElog: 8.6516 grad_loss: 15917.5635 normal_loss: 0.0000
[epoch  7][iter  120] loss: 148111.5625 RMSElog: 8.7533 grad_loss: 14802.4033 normal_loss: 0.0000
[epoch  7][iter  130] loss: 122653.2734 RMSElog: 8.7044 grad_loss: 12256.6230 normal_loss: 0.0000
[epoch  7][iter  140] loss: 100619.9609 RMSElog: 8.0668 grad_loss: 10053.9297 normal_loss: 0.0000
[epoch  7][iter  150] loss: 141839.5469 RMSElog: 8.4384 grad_loss: 14175.5156 normal_loss: 0.0000
[epoch  7][iter  160] loss: 166184.1875 RMSElog: 8.8485 grad_loss: 16609.5703 normal_loss: 0.0000
[epoch  7][iter  170] loss: 191946.1875 RMSElog: 8.8857 grad_loss: 19185.7344 normal_loss: 0.0000
[epoch  7][iter  180] loss: 176237.8906 RMSElog: 9.0383 grad_loss: 17614.7500 normal_loss: 0.0000
[epoch  7][iter  190] loss: 104310.0781 RMSElog: 8.0537 grad_loss: 10422.9541 normal_loss: 0.0000
[epoch  7][iter  200] loss: 226923.6094 RMSElog: 8.9631 grad_loss: 22683.3984 normal_loss: 0.0000
[epoch  7][iter  210] loss: 126943.3906 RMSElog: 8.4551 grad_loss: 12685.8838 normal_loss: 0.0000
[epoch  7][iter  220] loss: 125258.4688 RMSElog: 8.4480 grad_loss: 12517.3984 normal_loss: 0.0000
[epoch  7][iter  230] loss: 208794.5312 RMSElog: 8.8114 grad_loss: 20870.6426 normal_loss: 0.0000
[epoch  7][iter  240] loss: 132399.3125 RMSElog: 8.7331 grad_loss: 13231.1973 normal_loss: 0.0000
[epoch  7][iter  250] loss: 174983.9219 RMSElog: 8.8092 grad_loss: 17489.5840 normal_loss: 0.0000
[epoch  7][iter  260] loss: 185044.9375 RMSElog: 8.7426 grad_loss: 18495.7520 normal_loss: 0.0000
[epoch  7][iter  270] loss: 170033.3750 RMSElog: 8.7695 grad_loss: 16994.5684 normal_loss: 0.0000
[epoch  7][iter  280] loss: 107451.2891 RMSElog: 8.2074 grad_loss: 10736.9219 normal_loss: 0.0000
[epoch  7][iter  290] loss: 147456.3750 RMSElog: 8.7375 grad_loss: 14736.9004 normal_loss: 0.0000
[epoch  7][iter  300] loss: 207858.0625 RMSElog: 9.0574 grad_loss: 20776.7500 normal_loss: 0.0000
[epoch  7][iter  310] loss: 106740.8750 RMSElog: 8.3015 grad_loss: 10665.7861 normal_loss: 0.0000
[epoch  7][iter  320] loss: 106486.8359 RMSElog: 8.0410 grad_loss: 10640.6426 normal_loss: 0.0000
[epoch  7][iter  330] loss: 230493.1094 RMSElog: 9.0455 grad_loss: 23040.2656 normal_loss: 0.0000
[epoch  7][iter  340] loss: 146438.7656 RMSElog: 8.6975 grad_loss: 14635.1797 normal_loss: 0.0000
[epoch  7][iter  350] loss: 214041.0781 RMSElog: 8.8020 grad_loss: 21395.3047 normal_loss: 0.0000
[epoch  7][iter  360] loss: 110572.9766 RMSElog: 8.1418 grad_loss: 11049.1562 normal_loss: 0.0000
[epoch  7][iter  370] loss: 181439.3594 RMSElog: 8.8685 grad_loss: 18135.0664 normal_loss: 0.0000
[epoch  7][iter  380] loss: 174605.9062 RMSElog: 8.8096 grad_loss: 17451.7793 normal_loss: 0.0000
[epoch  7][iter  390] loss: 108770.2188 RMSElog: 8.1131 grad_loss: 10868.9082 normal_loss: 0.0000
[epoch  7][iter  400] loss: 177909.4688 RMSElog: 8.7598 grad_loss: 17782.1875 normal_loss: 0.0000
[epoch  7][iter  410] loss: 114289.4922 RMSElog: 8.4828 grad_loss: 11420.4668 normal_loss: 0.0000
[epoch  7][iter  420] loss: 197688.9219 RMSElog: 9.1550 grad_loss: 19759.7383 normal_loss: 0.0000
[epoch  7][iter  430] loss: 173548.5625 RMSElog: 8.9982 grad_loss: 17345.8574 normal_loss: 0.0000
[epoch  7][iter  440] loss: 157401.8594 RMSElog: 8.9014 grad_loss: 15731.2852 normal_loss: 0.0000
[epoch  7][iter  450] loss: 148220.2188 RMSElog: 8.7176 grad_loss: 14813.3047 normal_loss: 0.0000
[epoch  7][iter  460] loss: 167931.4844 RMSElog: 8.6907 grad_loss: 16784.4570 normal_loss: 0.0000
[epoch  7][iter  470] loss: 146933.4219 RMSElog: 8.6062 grad_loss: 14684.7363 normal_loss: 0.0000
[epoch  7][iter  480] loss: 208157.9375 RMSElog: 9.1125 grad_loss: 20806.6797 normal_loss: 0.0000
[epoch  7][iter  490] loss: 160413.3125 RMSElog: 8.8757 grad_loss: 16032.4551 normal_loss: 0.0000
[epoch  7][iter  500] loss: 154264.8594 RMSElog: 8.6691 grad_loss: 15417.8174 normal_loss: 0.0000
[epoch  7][iter  510] loss: 145366.3125 RMSElog: 8.8527 grad_loss: 14527.7793 normal_loss: 0.0000
[epoch  7][iter  520] loss: 176200.7969 RMSElog: 8.9960 grad_loss: 17611.0840 normal_loss: 0.0000
[epoch  7][iter  530] loss: 113178.2891 RMSElog: 8.5698 grad_loss: 11309.2598 normal_loss: 0.0000
[epoch  7][iter  540] loss: 204622.5156 RMSElog: 8.8920 grad_loss: 20453.3594 normal_loss: 0.0000
[epoch  7][iter  550] loss: 139073.3438 RMSElog: 8.6948 grad_loss: 13898.6406 normal_loss: 0.0000
[epoch  7][iter  560] loss: 175333.5938 RMSElog: 8.9568 grad_loss: 17524.4023 normal_loss: 0.0000
[epoch  7][iter  570] loss: 143878.2656 RMSElog: 8.7153 grad_loss: 14379.1123 normal_loss: 0.0000
[epoch  7][iter  580] loss: 150341.4219 RMSElog: 8.8975 grad_loss: 15025.2441 normal_loss: 0.0000
[epoch  7][iter  590] loss: 194964.4062 RMSElog: 8.8365 grad_loss: 19487.6055 normal_loss: 0.0000
[epoch  8][iter    0] loss: 227060.7812 RMSElog: 9.0871 grad_loss: 22696.2773 normal_loss: 0.7125
[epoch  8][iter   10] loss: 213383.7500 RMSElog: 8.9105 grad_loss: 21328.7598 normal_loss: 0.7058
[epoch  8][iter   20] loss: 177648.9219 RMSElog: 8.7871 grad_loss: 17755.3887 normal_loss: 0.7161
[epoch  8][iter   30] loss: 172099.3750 RMSElog: 8.7964 grad_loss: 17200.4277 normal_loss: 0.7123
[epoch  8][iter   40] loss: 135188.3438 RMSElog: 8.4755 grad_loss: 13509.6367 normal_loss: 0.7213
[epoch  8][iter   50] loss: 139857.6250 RMSElog: 8.2324 grad_loss: 13976.7686 normal_loss: 0.7615
[epoch  8][iter   60] loss: 109237.0938 RMSElog: 8.1125 grad_loss: 10914.9238 normal_loss: 0.6733
[epoch  8][iter   70] loss: 185051.9375 RMSElog: 8.7561 grad_loss: 18495.7305 normal_loss: 0.7064
[epoch  8][iter   80] loss: 111901.4453 RMSElog: 8.4328 grad_loss: 11180.9668 normal_loss: 0.7450
[epoch  8][iter   90] loss: 119411.2188 RMSElog: 8.5415 grad_loss: 11931.9287 normal_loss: 0.6524
[epoch  8][iter  100] loss: 179723.1094 RMSElog: 9.0340 grad_loss: 17962.5879 normal_loss: 0.6894
[epoch  8][iter  110] loss: 160034.5312 RMSElog: 8.7904 grad_loss: 15993.9346 normal_loss: 0.7289
[epoch  8][iter  120] loss: 156980.5156 RMSElog: 9.2099 grad_loss: 15687.9678 normal_loss: 0.8743
[epoch  8][iter  130] loss: 226845.3438 RMSElog: 9.0405 grad_loss: 22674.8203 normal_loss: 0.6730
[epoch  8][iter  140] loss: 133026.8438 RMSElog: 8.6565 grad_loss: 13293.3535 normal_loss: 0.6753
[epoch  8][iter  150] loss: 101203.6016 RMSElog: 8.6456 grad_loss: 10110.9941 normal_loss: 0.7211
[epoch  8][iter  160] loss: 217892.7344 RMSElog: 8.8913 grad_loss: 21779.7227 normal_loss: 0.6598
[epoch  8][iter  170] loss: 208165.4844 RMSElog: 9.1335 grad_loss: 20806.7031 normal_loss: 0.7129
[epoch  8][iter  180] loss: 154839.2344 RMSElog: 8.7962 grad_loss: 15474.4297 normal_loss: 0.6974
[epoch  8][iter  190] loss: 138453.6562 RMSElog: 8.5112 grad_loss: 13836.1377 normal_loss: 0.7171
[epoch  8][iter  200] loss: 143443.1406 RMSElog: 8.6663 grad_loss: 14334.9570 normal_loss: 0.6900
[epoch  8][iter  210] loss: 215933.5781 RMSElog: 8.8994 grad_loss: 21583.8047 normal_loss: 0.6543
[epoch  8][iter  220] loss: 148222.3750 RMSElog: 8.7894 grad_loss: 14812.7773 normal_loss: 0.6718
[epoch  8][iter  230] loss: 191349.0312 RMSElog: 8.7253 grad_loss: 19125.5215 normal_loss: 0.6554
[epoch  8][iter  240] loss: 231678.5312 RMSElog: 9.1265 grad_loss: 23158.0098 normal_loss: 0.7164
[epoch  8][iter  250] loss: 241953.2031 RMSElog: 9.0349 grad_loss: 24185.5781 normal_loss: 0.7066
[epoch  8][iter  260] loss: 165957.9688 RMSElog: 8.7859 grad_loss: 16586.3438 normal_loss: 0.6670
[epoch  8][iter  270] loss: 183462.5938 RMSElog: 8.8723 grad_loss: 18336.7031 normal_loss: 0.6837
[epoch  8][iter  280] loss: 198272.6875 RMSElog: 8.8241 grad_loss: 19817.7695 normal_loss: 0.6760
[epoch  8][iter  290] loss: 127482.7266 RMSElog: 8.2842 grad_loss: 12739.3057 normal_loss: 0.6831
[epoch  8][iter  300] loss: 174613.1406 RMSElog: 8.8241 grad_loss: 17451.7676 normal_loss: 0.7218
[epoch  8][iter  310] loss: 110784.6016 RMSElog: 8.1056 grad_loss: 11069.6924 normal_loss: 0.6624
[epoch  8][iter  320] loss: 188096.5469 RMSElog: 8.9468 grad_loss: 18800.0430 normal_loss: 0.6638
[epoch  8][iter  330] loss: 107883.3203 RMSElog: 8.0758 grad_loss: 10779.5898 normal_loss: 0.6656
[epoch  8][iter  340] loss: 174548.8438 RMSElog: 8.7465 grad_loss: 17445.4219 normal_loss: 0.7171
[epoch  8][iter  350] loss: 177272.9062 RMSElog: 8.8978 grad_loss: 17717.6934 normal_loss: 0.6999
[epoch  8][iter  360] loss: 194387.2031 RMSElog: 8.8273 grad_loss: 19429.2070 normal_loss: 0.6846
[epoch  8][iter  370] loss: 125265.9375 RMSElog: 8.4530 grad_loss: 12517.3984 normal_loss: 0.7424
[epoch  8][iter  380] loss: 178062.2188 RMSElog: 8.8989 grad_loss: 17796.5859 normal_loss: 0.7378
[epoch  8][iter  390] loss: 182549.2188 RMSElog: 8.9479 grad_loss: 18245.2832 normal_loss: 0.6914
[epoch  8][iter  400] loss: 115326.3594 RMSElog: 8.2980 grad_loss: 11523.6709 normal_loss: 0.6666
[epoch  8][iter  410] loss: 146940.9062 RMSElog: 8.6163 grad_loss: 14684.7373 normal_loss: 0.7375
[epoch  8][iter  420] loss: 114904.0312 RMSElog: 8.2873 grad_loss: 11481.4258 normal_loss: 0.6903
[epoch  8][iter  430] loss: 128864.4922 RMSElog: 8.6651 grad_loss: 12877.1084 normal_loss: 0.6755
[epoch  8][iter  440] loss: 186939.0781 RMSElog: 8.8957 grad_loss: 18684.3477 normal_loss: 0.6647
[epoch  8][iter  450] loss: 113898.8438 RMSElog: 8.1197 grad_loss: 11381.0850 normal_loss: 0.6798
[epoch  8][iter  460] loss: 114488.1328 RMSElog: 8.2838 grad_loss: 11439.8281 normal_loss: 0.7010
[epoch  8][iter  470] loss: 107458.1875 RMSElog: 8.2133 grad_loss: 10736.9092 normal_loss: 0.6961
[epoch  8][iter  480] loss: 146195.6562 RMSElog: 8.7227 grad_loss: 14610.1670 normal_loss: 0.6757
[epoch  8][iter  490] loss: 191673.4688 RMSElog: 8.9501 grad_loss: 19157.7109 normal_loss: 0.6875
[epoch  8][iter  500] loss: 150282.6406 RMSElog: 8.6756 grad_loss: 15018.9297 normal_loss: 0.6591
[epoch  8][iter  510] loss: 114363.7969 RMSElog: 8.6003 grad_loss: 11427.0547 normal_loss: 0.7242
[epoch  8][iter  520] loss: 114883.2500 RMSElog: 8.6594 grad_loss: 11478.9336 normal_loss: 0.7321
[epoch  8][iter  530] loss: 100967.3906 RMSElog: 8.1343 grad_loss: 10087.9385 normal_loss: 0.6663
[epoch  8][iter  540] loss: 147581.3438 RMSElog: 8.8031 grad_loss: 14748.6426 normal_loss: 0.6896
[epoch  8][iter  550] loss: 163682.0781 RMSElog: 8.7972 grad_loss: 16358.6348 normal_loss: 0.7762
[epoch  8][iter  560] loss: 214787.8750 RMSElog: 8.8688 grad_loss: 21469.2578 normal_loss: 0.6593
[epoch  8][iter  570] loss: 158014.5938 RMSElog: 8.7762 grad_loss: 15792.0273 normal_loss: 0.6566
[epoch  8][iter  580] loss: 183074.1250 RMSElog: 8.6298 grad_loss: 18298.0645 normal_loss: 0.7196
[epoch  8][iter  590] loss: 228033.8438 RMSElog: 8.9295 grad_loss: 22793.7500 normal_loss: 0.7052
[epoch  9][iter    0] loss: 122661.9453 RMSElog: 8.7229 grad_loss: 12256.6191 normal_loss: 0.8529
[epoch  9][iter   10] loss: 206060.9375 RMSElog: 8.7639 grad_loss: 20596.6680 normal_loss: 0.6626
[epoch  9][iter   20] loss: 103332.1172 RMSElog: 8.0700 grad_loss: 10324.4844 normal_loss: 0.6569
[epoch  9][iter   30] loss: 146142.0469 RMSElog: 8.5881 grad_loss: 14604.8818 normal_loss: 0.7353
[epoch  9][iter   40] loss: 167723.4531 RMSElog: 8.8568 grad_loss: 16762.7930 normal_loss: 0.6963
[epoch  9][iter   50] loss: 160623.6562 RMSElog: 8.8013 grad_loss: 16052.9111 normal_loss: 0.6536
[epoch  9][iter   60] loss: 196255.0938 RMSElog: 8.7733 grad_loss: 19616.0781 normal_loss: 0.6575
[epoch  9][iter   70] loss: 127445.3750 RMSElog: 8.3974 grad_loss: 12735.4736 normal_loss: 0.6657
[epoch  9][iter   80] loss: 135450.0312 RMSElog: 8.8597 grad_loss: 13535.4336 normal_loss: 0.7111
[epoch  9][iter   90] loss: 78496.3594 RMSElog: 8.6155 grad_loss: 7840.3652 normal_loss: 0.6557
[epoch  9][iter  100] loss: 217893.2031 RMSElog: 8.8985 grad_loss: 21779.7422 normal_loss: 0.6801
[epoch  9][iter  110] loss: 185127.6250 RMSElog: 8.9112 grad_loss: 18503.0801 normal_loss: 0.7700
[epoch  9][iter  120] loss: 177648.8906 RMSElog: 8.7896 grad_loss: 17755.3887 normal_loss: 0.7111
[epoch  9][iter  130] loss: 138453.5000 RMSElog: 8.5058 grad_loss: 13836.1318 normal_loss: 0.7120
[epoch  9][iter  140] loss: 157101.8750 RMSElog: 8.8202 grad_loss: 15700.6924 normal_loss: 0.6749
[epoch  9][iter  150] loss: 105878.6719 RMSElog: 8.4898 grad_loss: 10578.6729 normal_loss: 0.7042
[epoch  9][iter  160] loss: 130124.5000 RMSElog: 8.3024 grad_loss: 13003.4678 normal_loss: 0.6795
[epoch  9][iter  170] loss: 121894.4531 RMSElog: 8.2877 grad_loss: 12180.4629 normal_loss: 0.6940
[epoch  9][iter  180] loss: 148227.1250 RMSElog: 8.7249 grad_loss: 14813.2998 normal_loss: 0.6876
[epoch  9][iter  190] loss: 176892.4844 RMSElog: 8.9760 grad_loss: 17679.6191 normal_loss: 0.6532
[epoch  9][iter  200] loss: 199053.4688 RMSElog: 8.8974 grad_loss: 19895.7148 normal_loss: 0.7366
[epoch  9][iter  210] loss: 161627.8438 RMSElog: 8.9568 grad_loss: 16153.1660 normal_loss: 0.6616
[epoch  9][iter  220] loss: 144174.7969 RMSElog: 8.6167 grad_loss: 14408.1797 normal_loss: 0.6831
[epoch  9][iter  230] loss: 161793.8594 RMSElog: 8.8565 grad_loss: 16169.8770 normal_loss: 0.6524
[epoch  9][iter  240] loss: 235192.0469 RMSElog: 9.1072 grad_loss: 23509.4102 normal_loss: 0.6876
[epoch  9][iter  250] loss: 132024.0781 RMSElog: 8.8307 grad_loss: 13192.8672 normal_loss: 0.7091
[epoch  9][iter  260] loss: 194556.2188 RMSElog: 8.7911 grad_loss: 19446.1777 normal_loss: 0.6517
[epoch  9][iter  270] loss: 190917.5000 RMSElog: 9.0470 grad_loss: 19082.0391 normal_loss: 0.6649
[epoch  9][iter  280] loss: 164750.3281 RMSElog: 8.8205 grad_loss: 16465.5020 normal_loss: 0.7112
[epoch  9][iter  290] loss: 157043.2656 RMSElog: 8.8769 grad_loss: 15694.7939 normal_loss: 0.6556
[epoch  9][iter  300] loss: 151704.3906 RMSElog: 8.7894 grad_loss: 15161.0049 normal_loss: 0.6450
[epoch  9][iter  310] loss: 143441.8594 RMSElog: 8.6194 grad_loss: 14334.9111 normal_loss: 0.6567
[epoch  9][iter  320] loss: 142575.4844 RMSElog: 8.5828 grad_loss: 14248.3105 normal_loss: 0.6551
[epoch  9][iter  330] loss: 163236.0625 RMSElog: 8.8294 grad_loss: 16314.0859 normal_loss: 0.6905
[epoch  9][iter  340] loss: 150304.7656 RMSElog: 8.8926 grad_loss: 15020.8711 normal_loss: 0.7125
[epoch  9][iter  350] loss: 104316.7891 RMSElog: 8.0778 grad_loss: 10422.9434 normal_loss: 0.6577
[epoch  9][iter  360] loss: 135797.7812 RMSElog: 8.4565 grad_loss: 13570.6143 normal_loss: 0.7078
[epoch  9][iter  370] loss: 148118.6875 RMSElog: 8.7699 grad_loss: 14802.4023 normal_loss: 0.6968
[epoch  9][iter  380] loss: 160649.5000 RMSElog: 8.8861 grad_loss: 16055.3994 normal_loss: 0.6652
[epoch  9][iter  390] loss: 182296.2188 RMSElog: 8.9552 grad_loss: 18220.0195 normal_loss: 0.6467
[epoch  9][iter  400] loss: 160056.8281 RMSElog: 8.7123 grad_loss: 15996.3047 normal_loss: 0.6660
[epoch  9][iter  410] loss: 206381.3906 RMSElog: 9.0296 grad_loss: 20628.4277 normal_loss: 0.6822
[epoch  9][iter  420] loss: 154290.6094 RMSElog: 8.6591 grad_loss: 15419.7227 normal_loss: 0.6793
[epoch  9][iter  430] loss: 154594.2344 RMSElog: 8.7605 grad_loss: 15449.9268 normal_loss: 0.7358
[epoch  9][iter  440] loss: 133267.2812 RMSElog: 8.6368 grad_loss: 13317.4502 normal_loss: 0.6413
[epoch  9][iter  450] loss: 214047.8125 RMSElog: 8.8188 grad_loss: 21395.2910 normal_loss: 0.6710
[epoch  9][iter  460] loss: 127757.1484 RMSElog: 8.6942 grad_loss: 12766.3193 normal_loss: 0.7014
[epoch  9][iter  470] loss: 157408.5469 RMSElog: 8.8851 grad_loss: 15731.2754 normal_loss: 0.6940
[epoch  9][iter  480] loss: 215522.4844 RMSElog: 9.1298 grad_loss: 21542.4512 normal_loss: 0.6687
[epoch  9][iter  490] loss: 155952.0469 RMSElog: 8.6956 grad_loss: 15585.7598 normal_loss: 0.7489
[epoch  9][iter  500] loss: 188998.5938 RMSElog: 8.9219 grad_loss: 18890.2383 normal_loss: 0.6986
[epoch  9][iter  510] loss: 117386.7344 RMSElog: 8.4470 grad_loss: 11729.5625 normal_loss: 0.6644
[epoch  9][iter  520] loss: 165957.5156 RMSElog: 8.7814 grad_loss: 16586.3203 normal_loss: 0.6502
[epoch  9][iter  530] loss: 100625.7891 RMSElog: 8.0561 grad_loss: 10053.8809 normal_loss: 0.6429
[epoch  9][iter  540] loss: 115326.1094 RMSElog: 8.2812 grad_loss: 11523.6611 normal_loss: 0.6693
[epoch  9][iter  550] loss: 137308.5625 RMSElog: 8.5518 grad_loss: 13721.6221 normal_loss: 0.6824
[epoch  9][iter  560] loss: 112298.8125 RMSElog: 8.3380 grad_loss: 11220.8691 normal_loss: 0.6738
[epoch  9][iter  570] loss: 133026.3906 RMSElog: 8.6637 grad_loss: 13293.3213 normal_loss: 0.6535
[epoch  9][iter  580] loss: 109124.1328 RMSElog: 8.3081 grad_loss: 10903.4141 normal_loss: 0.6919
[epoch  9][iter  590] loss: 239433.2812 RMSElog: 9.0530 grad_loss: 23933.5781 normal_loss: 0.6978
###########
#epochs 30#
###########
[epoch  0][iter    0] loss: 102.9274 RMSElog: 10.2927 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter   10] loss: 104.2282 RMSElog: 10.4228 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter   20] loss: 103.1048 RMSElog: 10.3105 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter   30] loss: 102.0529 RMSElog: 10.2053 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter   40] loss: 102.8384 RMSElog: 10.2838 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter   50] loss: 106.0400 RMSElog: 10.6040 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter   60] loss: 105.2535 RMSElog: 10.5254 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter   70] loss: 101.1415 RMSElog: 10.1142 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter   80] loss: 106.6197 RMSElog: 10.6620 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter   90] loss: 100.6064 RMSElog: 10.0606 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  100] loss: 100.0012 RMSElog: 10.0001 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  110] loss: 102.8872 RMSElog: 10.2887 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  120] loss: 101.5198 RMSElog: 10.1520 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  130] loss: 103.1917 RMSElog: 10.3192 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  140] loss: 97.6046 RMSElog: 9.7605 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  150] loss: 99.8302 RMSElog: 9.9830 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  160] loss: 99.3478 RMSElog: 9.9348 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  170] loss: 101.8391 RMSElog: 10.1839 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  180] loss: 95.1691 RMSElog: 9.5169 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  190] loss: 98.5007 RMSElog: 9.8501 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  200] loss: 97.0059 RMSElog: 9.7006 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  210] loss: 103.3939 RMSElog: 10.3394 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  220] loss: 97.3976 RMSElog: 9.7398 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  230] loss: 101.4414 RMSElog: 10.1441 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  240] loss: 101.4538 RMSElog: 10.1454 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  250] loss: 101.7213 RMSElog: 10.1721 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  260] loss: 99.4729 RMSElog: 9.9473 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  270] loss: 97.2313 RMSElog: 9.7231 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  280] loss: 97.6442 RMSElog: 9.7644 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  290] loss: 100.8997 RMSElog: 10.0900 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  300] loss: 99.0118 RMSElog: 9.9012 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  310] loss: 93.2950 RMSElog: 9.3295 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  320] loss: 99.5392 RMSElog: 9.9539 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  330] loss: 98.7549 RMSElog: 9.8755 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  340] loss: 97.8144 RMSElog: 9.7814 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  350] loss: 95.7263 RMSElog: 9.5726 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  360] loss: 98.8081 RMSElog: 9.8808 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  370] loss: 99.2044 RMSElog: 9.9204 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  380] loss: 96.4834 RMSElog: 9.6483 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  390] loss: 98.1925 RMSElog: 9.8192 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  400] loss: 98.3895 RMSElog: 9.8389 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  410] loss: 101.1052 RMSElog: 10.1105 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  420] loss: 98.9526 RMSElog: 9.8953 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  430] loss: 98.7744 RMSElog: 9.8774 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  440] loss: 98.9576 RMSElog: 9.8958 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  450] loss: 96.7298 RMSElog: 9.6730 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  460] loss: 92.3266 RMSElog: 9.2327 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  470] loss: 96.6792 RMSElog: 9.6679 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  480] loss: 97.4122 RMSElog: 9.7412 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  490] loss: 102.5421 RMSElog: 10.2542 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  500] loss: 97.6466 RMSElog: 9.7647 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  510] loss: 99.1555 RMSElog: 9.9155 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  520] loss: 101.1795 RMSElog: 10.1179 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  530] loss: 97.4311 RMSElog: 9.7431 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  540] loss: 104.4048 RMSElog: 10.4405 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  550] loss: 96.9290 RMSElog: 9.6929 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  560] loss: 102.3190 RMSElog: 10.2319 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  570] loss: 102.4920 RMSElog: 10.2492 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  580] loss: 100.0338 RMSElog: 10.0034 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  590] loss: 95.7702 RMSElog: 9.5770 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter    0] loss: 97.3725 RMSElog: 9.7373 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter   10] loss: 98.8242 RMSElog: 9.8824 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter   20] loss: 97.6610 RMSElog: 9.7661 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter   30] loss: 99.1316 RMSElog: 9.9132 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter   40] loss: 99.7010 RMSElog: 9.9701 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter   50] loss: 100.3693 RMSElog: 10.0369 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter   60] loss: 101.0962 RMSElog: 10.1096 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter   70] loss: 92.4874 RMSElog: 9.2487 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter   80] loss: 98.5828 RMSElog: 9.8583 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter   90] loss: 99.5540 RMSElog: 9.9554 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  100] loss: 100.4506 RMSElog: 10.0451 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  110] loss: 96.1339 RMSElog: 9.6134 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  120] loss: 98.2967 RMSElog: 9.8297 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  130] loss: 97.2617 RMSElog: 9.7262 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  140] loss: 97.3629 RMSElog: 9.7363 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  150] loss: 97.6852 RMSElog: 9.7685 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  160] loss: 101.0806 RMSElog: 10.1081 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  170] loss: 100.7399 RMSElog: 10.0740 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  180] loss: 98.8132 RMSElog: 9.8813 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  190] loss: 101.4369 RMSElog: 10.1437 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  200] loss: 97.4427 RMSElog: 9.7443 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  210] loss: 102.9301 RMSElog: 10.2930 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  220] loss: 100.7814 RMSElog: 10.0781 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  230] loss: 98.2514 RMSElog: 9.8251 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  240] loss: 104.3231 RMSElog: 10.4323 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  250] loss: 100.5506 RMSElog: 10.0551 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  260] loss: 100.0973 RMSElog: 10.0097 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  270] loss: 101.1678 RMSElog: 10.1168 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  280] loss: 94.9582 RMSElog: 9.4958 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  290] loss: 97.2725 RMSElog: 9.7272 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  300] loss: 98.9250 RMSElog: 9.8925 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  310] loss: 98.1928 RMSElog: 9.8193 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  320] loss: 97.1649 RMSElog: 9.7165 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  330] loss: 97.8473 RMSElog: 9.7847 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  340] loss: 99.7426 RMSElog: 9.9743 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  350] loss: 97.0594 RMSElog: 9.7059 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  360] loss: 95.8221 RMSElog: 9.5822 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  370] loss: 99.5053 RMSElog: 9.9505 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  380] loss: 91.8323 RMSElog: 9.1832 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  390] loss: 98.7456 RMSElog: 9.8746 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  400] loss: 99.0790 RMSElog: 9.9079 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  410] loss: 99.9198 RMSElog: 9.9920 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  420] loss: 100.7322 RMSElog: 10.0732 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  430] loss: 102.8158 RMSElog: 10.2816 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  440] loss: 94.0102 RMSElog: 9.4010 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  450] loss: 101.4893 RMSElog: 10.1489 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  460] loss: 98.9213 RMSElog: 9.8921 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  470] loss: 97.1615 RMSElog: 9.7162 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  480] loss: 96.4670 RMSElog: 9.6467 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  490] loss: 98.7244 RMSElog: 9.8724 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  500] loss: 98.5405 RMSElog: 9.8540 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  510] loss: 102.3569 RMSElog: 10.2357 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  520] loss: 98.1238 RMSElog: 9.8124 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  530] loss: 99.5332 RMSElog: 9.9533 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  540] loss: 101.8391 RMSElog: 10.1839 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  550] loss: 101.0468 RMSElog: 10.1047 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  560] loss: 98.9291 RMSElog: 9.8929 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  570] loss: 95.4197 RMSElog: 9.5420 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  580] loss: 97.8103 RMSElog: 9.7810 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  590] loss: 100.2282 RMSElog: 10.0228 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter    0] loss: 102.4243 RMSElog: 10.2424 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter   10] loss: 91.9529 RMSElog: 9.1953 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter   20] loss: 103.6160 RMSElog: 10.3616 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter   30] loss: 100.7729 RMSElog: 10.0773 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter   40] loss: 97.0063 RMSElog: 9.7006 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter   50] loss: 93.2310 RMSElog: 9.3231 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter   60] loss: 96.1709 RMSElog: 9.6171 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter   70] loss: 100.2436 RMSElog: 10.0244 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter   80] loss: 103.2854 RMSElog: 10.3285 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter   90] loss: 97.3095 RMSElog: 9.7309 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  100] loss: 96.5947 RMSElog: 9.6595 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  110] loss: 98.9987 RMSElog: 9.8999 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  120] loss: 102.8692 RMSElog: 10.2869 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  130] loss: 96.0178 RMSElog: 9.6018 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  140] loss: 96.4718 RMSElog: 9.6472 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  150] loss: 97.3049 RMSElog: 9.7305 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  160] loss: 92.2618 RMSElog: 9.2262 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  170] loss: 99.6160 RMSElog: 9.9616 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  180] loss: 97.1106 RMSElog: 9.7111 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  190] loss: 99.2809 RMSElog: 9.9281 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  200] loss: 100.1553 RMSElog: 10.0155 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  210] loss: 96.4818 RMSElog: 9.6482 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  220] loss: 97.5051 RMSElog: 9.7505 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  230] loss: 98.8645 RMSElog: 9.8865 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  240] loss: 98.5694 RMSElog: 9.8569 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  250] loss: 99.0997 RMSElog: 9.9100 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  260] loss: 96.5244 RMSElog: 9.6524 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  270] loss: 95.8025 RMSElog: 9.5802 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  280] loss: 101.9858 RMSElog: 10.1986 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  290] loss: 98.6376 RMSElog: 9.8638 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  300] loss: 97.0698 RMSElog: 9.7070 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  310] loss: 103.6373 RMSElog: 10.3637 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  320] loss: 96.8528 RMSElog: 9.6853 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  330] loss: 97.9277 RMSElog: 9.7928 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  340] loss: 98.3067 RMSElog: 9.8307 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  350] loss: 97.4117 RMSElog: 9.7412 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  360] loss: 98.1268 RMSElog: 9.8127 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  370] loss: 100.2740 RMSElog: 10.0274 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  380] loss: 99.3855 RMSElog: 9.9385 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  390] loss: 95.9084 RMSElog: 9.5908 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  400] loss: 96.3361 RMSElog: 9.6336 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  410] loss: 99.1921 RMSElog: 9.9192 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  420] loss: 98.5782 RMSElog: 9.8578 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  430] loss: 99.4141 RMSElog: 9.9414 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  440] loss: 96.3519 RMSElog: 9.6352 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  450] loss: 97.8152 RMSElog: 9.7815 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  460] loss: 98.8887 RMSElog: 9.8889 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  470] loss: 89.4831 RMSElog: 8.9483 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  480] loss: 91.6247 RMSElog: 9.1625 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  490] loss: 96.2682 RMSElog: 9.6268 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  500] loss: 90.6325 RMSElog: 9.0632 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  510] loss: 89.5761 RMSElog: 8.9576 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  520] loss: 95.0711 RMSElog: 9.5071 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  530] loss: 88.7353 RMSElog: 8.8735 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  540] loss: 92.7259 RMSElog: 9.2726 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  550] loss: 92.5380 RMSElog: 9.2538 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  560] loss: 90.8221 RMSElog: 9.0822 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  570] loss: 94.7823 RMSElog: 9.4782 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  580] loss: 93.8251 RMSElog: 9.3825 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  590] loss: 91.7924 RMSElog: 9.1792 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter    0] loss: 92.4858 RMSElog: 9.2486 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter   10] loss: 91.5291 RMSElog: 9.1529 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter   20] loss: 91.7691 RMSElog: 9.1769 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter   30] loss: 89.6363 RMSElog: 8.9636 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter   40] loss: 89.6503 RMSElog: 8.9650 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter   50] loss: 90.1623 RMSElog: 9.0162 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter   60] loss: 91.0928 RMSElog: 9.1093 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter   70] loss: 88.1729 RMSElog: 8.8173 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter   80] loss: 89.9531 RMSElog: 8.9953 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter   90] loss: 88.0089 RMSElog: 8.8009 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  100] loss: 91.6993 RMSElog: 9.1699 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  110] loss: 85.9397 RMSElog: 8.5940 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  120] loss: 89.9050 RMSElog: 8.9905 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  130] loss: 85.6513 RMSElog: 8.5651 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  140] loss: 91.2722 RMSElog: 9.1272 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  150] loss: 85.5241 RMSElog: 8.5524 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  160] loss: 92.8644 RMSElog: 9.2864 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  170] loss: 91.3951 RMSElog: 9.1395 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  180] loss: 86.9529 RMSElog: 8.6953 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  190] loss: 84.1512 RMSElog: 8.4151 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  200] loss: 88.0086 RMSElog: 8.8009 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  210] loss: 85.0126 RMSElog: 8.5013 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  220] loss: 89.5374 RMSElog: 8.9537 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  230] loss: 91.4635 RMSElog: 9.1464 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  240] loss: 91.8199 RMSElog: 9.1820 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  250] loss: 84.8887 RMSElog: 8.4889 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  260] loss: 88.4178 RMSElog: 8.8418 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  270] loss: 91.9764 RMSElog: 9.1976 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  280] loss: 88.8448 RMSElog: 8.8845 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  290] loss: 84.4858 RMSElog: 8.4486 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  300] loss: 89.8231 RMSElog: 8.9823 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  310] loss: 88.0931 RMSElog: 8.8093 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  320] loss: 91.4240 RMSElog: 9.1424 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  330] loss: 89.1586 RMSElog: 8.9159 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  340] loss: 87.5430 RMSElog: 8.7543 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  350] loss: 88.3712 RMSElog: 8.8371 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  360] loss: 87.3347 RMSElog: 8.7335 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  370] loss: 91.4483 RMSElog: 9.1448 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  380] loss: 89.4424 RMSElog: 8.9442 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  390] loss: 90.0716 RMSElog: 9.0072 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  400] loss: 86.1600 RMSElog: 8.6160 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  410] loss: 89.2689 RMSElog: 8.9269 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  420] loss: 88.4586 RMSElog: 8.8459 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  430] loss: 86.2510 RMSElog: 8.6251 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  440] loss: 87.4250 RMSElog: 8.7425 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  450] loss: 88.6049 RMSElog: 8.8605 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  460] loss: 84.3502 RMSElog: 8.4350 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  470] loss: 84.6590 RMSElog: 8.4659 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  480] loss: 89.9231 RMSElog: 8.9923 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  490] loss: 89.3376 RMSElog: 8.9338 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  500] loss: 88.2880 RMSElog: 8.8288 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  510] loss: 90.2789 RMSElog: 9.0279 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  520] loss: 86.0258 RMSElog: 8.6026 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  530] loss: 84.8863 RMSElog: 8.4886 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  540] loss: 84.2082 RMSElog: 8.4208 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  550] loss: 83.0370 RMSElog: 8.3037 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  560] loss: 88.8945 RMSElog: 8.8894 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  570] loss: 87.6894 RMSElog: 8.7689 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  580] loss: 90.2877 RMSElog: 9.0288 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  590] loss: 90.8958 RMSElog: 9.0896 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  4][iter    0] loss: 202996.2656 RMSElog: 8.8611 grad_loss: 20290.7656 normal_loss: 0.0000
[epoch  4][iter   10] loss: 180957.8750 RMSElog: 9.0215 grad_loss: 18086.7656 normal_loss: 0.0000
[epoch  4][iter   20] loss: 201477.9688 RMSElog: 9.0620 grad_loss: 20138.7344 normal_loss: 0.0000
[epoch  4][iter   30] loss: 154834.4531 RMSElog: 8.8968 grad_loss: 15474.5488 normal_loss: 0.0000
[epoch  4][iter   40] loss: 119900.8438 RMSElog: 8.6018 grad_loss: 11981.4824 normal_loss: 0.0000
[epoch  4][iter   50] loss: 166522.0000 RMSElog: 9.0440 grad_loss: 16643.1543 normal_loss: 0.0000
[epoch  4][iter   60] loss: 128536.6328 RMSElog: 8.7229 grad_loss: 12844.9404 normal_loss: 0.0000
[epoch  4][iter   70] loss: 165628.0625 RMSElog: 8.8691 grad_loss: 16553.9375 normal_loss: 0.0000
[epoch  4][iter   80] loss: 144562.6719 RMSElog: 8.1684 grad_loss: 14448.0996 normal_loss: 0.0000
[epoch  4][iter   90] loss: 178056.1719 RMSElog: 8.9499 grad_loss: 17796.6680 normal_loss: 0.0000
[epoch  4][iter  100] loss: 142113.9375 RMSElog: 8.5920 grad_loss: 14202.8027 normal_loss: 0.0000
[epoch  4][iter  110] loss: 114290.2188 RMSElog: 8.5255 grad_loss: 11420.4961 normal_loss: 0.0000
[epoch  4][iter  120] loss: 100954.6094 RMSElog: 8.2601 grad_loss: 10087.2012 normal_loss: 0.0000
[epoch  4][iter  130] loss: 148113.1406 RMSElog: 8.8302 grad_loss: 14802.4844 normal_loss: 0.0000
[epoch  4][iter  140] loss: 127090.5547 RMSElog: 8.6961 grad_loss: 12700.3594 normal_loss: 0.0000
[epoch  4][iter  150] loss: 185046.4688 RMSElog: 8.8162 grad_loss: 18495.8301 normal_loss: 0.0000
[epoch  4][iter  160] loss: 138995.3438 RMSElog: 8.7027 grad_loss: 13890.8311 normal_loss: 0.0000
[epoch  4][iter  170] loss: 106742.8828 RMSElog: 8.4192 grad_loss: 10665.8691 normal_loss: 0.0000
[epoch  4][iter  180] loss: 153941.0781 RMSElog: 9.0099 grad_loss: 15385.0986 normal_loss: 0.0000
[epoch  4][iter  190] loss: 154602.0312 RMSElog: 8.8024 grad_loss: 15451.4004 normal_loss: 0.0000
[epoch  4][iter  200] loss: 154266.8125 RMSElog: 8.7289 grad_loss: 15417.9531 normal_loss: 0.0000
[epoch  4][iter  210] loss: 100089.1094 RMSElog: 8.4238 grad_loss: 10000.4873 normal_loss: 0.0000
[epoch  4][iter  220] loss: 176932.7344 RMSElog: 9.0977 grad_loss: 17684.1758 normal_loss: 0.0000
[epoch  4][iter  230] loss: 174982.1875 RMSElog: 9.1416 grad_loss: 17489.0781 normal_loss: 0.0000
[epoch  4][iter  240] loss: 130406.1562 RMSElog: 8.6674 grad_loss: 13031.9482 normal_loss: 0.0000
[epoch  4][iter  250] loss: 104430.4531 RMSElog: 8.5740 grad_loss: 10434.4707 normal_loss: 0.0000
[epoch  4][iter  260] loss: 241948.4062 RMSElog: 9.1377 grad_loss: 24185.7031 normal_loss: 0.0000
[epoch  4][iter  270] loss: 150298.6562 RMSElog: 8.8616 grad_loss: 15021.0049 normal_loss: 0.0000
[epoch  4][iter  280] loss: 211614.8125 RMSElog: 9.0474 grad_loss: 21152.4336 normal_loss: 0.0000
[epoch  4][iter  290] loss: 158902.8906 RMSElog: 8.8596 grad_loss: 15881.4297 normal_loss: 0.0000
[epoch  4][iter  300] loss: 104105.4297 RMSElog: 8.6425 grad_loss: 10401.9004 normal_loss: 0.0000
[epoch  4][iter  310] loss: 129684.0156 RMSElog: 8.5471 grad_loss: 12959.8545 normal_loss: 0.0000
[epoch  4][iter  320] loss: 115474.5703 RMSElog: 8.6005 grad_loss: 11538.8564 normal_loss: 0.0000
[epoch  4][iter  330] loss: 127750.5312 RMSElog: 8.7010 grad_loss: 12766.3516 normal_loss: 0.0000
[epoch  4][iter  340] loss: 167771.8906 RMSElog: 8.8928 grad_loss: 16768.2969 normal_loss: 0.0000
[epoch  4][iter  350] loss: 108773.5938 RMSElog: 8.3597 grad_loss: 10869.0000 normal_loss: 0.0000
[epoch  4][iter  360] loss: 146934.2344 RMSElog: 8.6673 grad_loss: 14684.7568 normal_loss: 0.0000
[epoch  4][iter  370] loss: 112689.6016 RMSElog: 8.3297 grad_loss: 11260.6299 normal_loss: 0.0000
[epoch  4][iter  380] loss: 165988.0312 RMSElog: 8.9564 grad_loss: 16589.8457 normal_loss: 0.0000
[epoch  4][iter  390] loss: 181441.3438 RMSElog: 8.9513 grad_loss: 18135.1836 normal_loss: 0.0000
[epoch  4][iter  400] loss: 163704.3906 RMSElog: 8.6910 grad_loss: 16361.7471 normal_loss: 0.0000
[epoch  4][iter  410] loss: 223849.9844 RMSElog: 9.1459 grad_loss: 22375.8516 normal_loss: 0.0000
[epoch  4][iter  420] loss: 111494.2344 RMSElog: 8.4179 grad_loss: 11141.0059 normal_loss: 0.0000
[epoch  4][iter  430] loss: 166184.8281 RMSElog: 8.8702 grad_loss: 16609.6113 normal_loss: 0.0000
[epoch  4][iter  440] loss: 105072.0938 RMSElog: 8.3710 grad_loss: 10498.8379 normal_loss: 0.0000
[epoch  4][iter  450] loss: 136846.6562 RMSElog: 8.4544 grad_loss: 13676.2119 normal_loss: 0.0000
[epoch  4][iter  460] loss: 211900.4688 RMSElog: 8.9138 grad_loss: 21181.1328 normal_loss: 0.0000
[epoch  4][iter  470] loss: 104952.2969 RMSElog: 8.6553 grad_loss: 10486.5742 normal_loss: 0.0000
[epoch  4][iter  480] loss: 116558.1172 RMSElog: 8.5809 grad_loss: 11647.2305 normal_loss: 0.0000
[epoch  4][iter  490] loss: 150127.7969 RMSElog: 8.8380 grad_loss: 15003.9414 normal_loss: 0.0000
[epoch  4][iter  500] loss: 160051.3750 RMSElog: 8.7501 grad_loss: 15996.3877 normal_loss: 0.0000
[epoch  4][iter  510] loss: 195389.6719 RMSElog: 9.0479 grad_loss: 19529.9180 normal_loss: 0.0000
[epoch  4][iter  520] loss: 125483.9141 RMSElog: 8.8270 grad_loss: 12539.5645 normal_loss: 0.0000
[epoch  4][iter  530] loss: 176239.0938 RMSElog: 9.0978 grad_loss: 17614.8125 normal_loss: 0.0000
[epoch  4][iter  540] loss: 165283.7188 RMSElog: 8.8903 grad_loss: 16519.4805 normal_loss: 0.0000
[epoch  4][iter  550] loss: 135187.8750 RMSElog: 8.6036 grad_loss: 13510.1836 normal_loss: 0.0000
[epoch  4][iter  560] loss: 136481.1094 RMSElog: 8.7399 grad_loss: 13639.3711 normal_loss: 0.0000
[epoch  4][iter  570] loss: 207892.1562 RMSElog: 8.8414 grad_loss: 20780.3730 normal_loss: 0.0000
[epoch  4][iter  580] loss: 188477.0625 RMSElog: 8.9865 grad_loss: 18838.7207 normal_loss: 0.0000
[epoch  4][iter  590] loss: 201138.4688 RMSElog: 8.8662 grad_loss: 20104.9824 normal_loss: 0.0000
[epoch  5][iter    0] loss: 200928.0000 RMSElog: 8.9016 grad_loss: 20083.8984 normal_loss: 0.0000
[epoch  5][iter   10] loss: 119877.5781 RMSElog: 8.6931 grad_loss: 11979.0645 normal_loss: 0.0000
[epoch  5][iter   20] loss: 109412.8828 RMSElog: 8.2129 grad_loss: 10933.0752 normal_loss: 0.0000
[epoch  5][iter   30] loss: 158010.0156 RMSElog: 8.8622 grad_loss: 15792.1396 normal_loss: 0.0000
[epoch  5][iter   40] loss: 241947.1250 RMSElog: 9.0783 grad_loss: 24185.6348 normal_loss: 0.0000
[epoch  5][iter   50] loss: 160858.2500 RMSElog: 8.9415 grad_loss: 16076.8828 normal_loss: 0.0000
[epoch  5][iter   60] loss: 155887.2812 RMSElog: 8.6949 grad_loss: 15580.0332 normal_loss: 0.0000
[epoch  5][iter   70] loss: 123271.2891 RMSElog: 8.7807 grad_loss: 12318.3486 normal_loss: 0.0000
[epoch  5][iter   80] loss: 217886.8125 RMSElog: 8.9192 grad_loss: 21779.7617 normal_loss: 0.0000
[epoch  5][iter   90] loss: 135182.0000 RMSElog: 8.5160 grad_loss: 13509.6836 normal_loss: 0.0000
[epoch  5][iter  100] loss: 178760.5625 RMSElog: 8.9350 grad_loss: 17867.1211 normal_loss: 0.0000
[epoch  5][iter  110] loss: 153424.7969 RMSElog: 8.8578 grad_loss: 15333.6221 normal_loss: 0.0000
[epoch  5][iter  120] loss: 179877.7812 RMSElog: 8.7892 grad_loss: 17978.9883 normal_loss: 0.0000
[epoch  5][iter  130] loss: 196249.8594 RMSElog: 8.8165 grad_loss: 19616.1699 normal_loss: 0.0000
[epoch  5][iter  140] loss: 191534.0469 RMSElog: 8.7289 grad_loss: 19144.6758 normal_loss: 0.0000
[epoch  5][iter  150] loss: 149345.0625 RMSElog: 8.7470 grad_loss: 14925.7598 normal_loss: 0.0000
[epoch  5][iter  160] loss: 105871.8125 RMSElog: 8.4857 grad_loss: 10578.6963 normal_loss: 0.0000
[epoch  5][iter  170] loss: 165430.1719 RMSElog: 8.9773 grad_loss: 16534.0410 normal_loss: 0.0000
[epoch  5][iter  180] loss: 176238.2500 RMSElog: 9.0513 grad_loss: 17614.7734 normal_loss: 0.0000
[epoch  5][iter  190] loss: 116046.1016 RMSElog: 8.3178 grad_loss: 11596.2930 normal_loss: 0.0000
[epoch  5][iter  200] loss: 198718.0312 RMSElog: 8.9270 grad_loss: 19862.8750 normal_loss: 0.0000
[epoch  5][iter  210] loss: 170333.7344 RMSElog: 8.9375 grad_loss: 17024.4355 normal_loss: 0.0000
[epoch  5][iter  220] loss: 159775.5938 RMSElog: 8.7398 grad_loss: 15968.8184 normal_loss: 0.0000
[epoch  5][iter  230] loss: 198418.2812 RMSElog: 8.9344 grad_loss: 19832.8945 normal_loss: 0.0000
[epoch  5][iter  240] loss: 140255.8594 RMSElog: 8.6391 grad_loss: 14016.9473 normal_loss: 0.0000
[epoch  5][iter  250] loss: 140312.1562 RMSElog: 8.9512 grad_loss: 14022.2646 normal_loss: 0.0000
[epoch  5][iter  260] loss: 146921.0625 RMSElog: 8.5627 grad_loss: 14683.5430 normal_loss: 0.0000
[epoch  5][iter  270] loss: 158902.0938 RMSElog: 8.8217 grad_loss: 15881.3877 normal_loss: 0.0000
[epoch  5][iter  280] loss: 122424.6797 RMSElog: 8.3586 grad_loss: 12234.1094 normal_loss: 0.0000
[epoch  5][iter  290] loss: 144168.0625 RMSElog: 8.6080 grad_loss: 14408.1973 normal_loss: 0.0000
[epoch  5][iter  300] loss: 167320.7656 RMSElog: 8.7931 grad_loss: 16723.2832 normal_loss: 0.0000
[epoch  5][iter  310] loss: 157786.0781 RMSElog: 8.7556 grad_loss: 15769.8525 normal_loss: 0.0000
[epoch  5][iter  320] loss: 174980.5312 RMSElog: 9.0231 grad_loss: 17489.0293 normal_loss: 0.0000
[epoch  5][iter  330] loss: 114356.9531 RMSElog: 8.6170 grad_loss: 11427.0781 normal_loss: 0.0000
[epoch  5][iter  340] loss: 211510.4844 RMSElog: 8.8152 grad_loss: 21142.2344 normal_loss: 0.0000
[epoch  5][iter  350] loss: 173696.7969 RMSElog: 8.9229 grad_loss: 17360.7559 normal_loss: 0.0000
[epoch  5][iter  360] loss: 162409.7969 RMSElog: 8.8565 grad_loss: 16232.1230 normal_loss: 0.0000
[epoch  5][iter  370] loss: 119405.2266 RMSElog: 8.5578 grad_loss: 11931.9648 normal_loss: 0.0000
[epoch  5][iter  380] loss: 112830.9453 RMSElog: 8.6221 grad_loss: 11274.4727 normal_loss: 0.0000
[epoch  5][iter  390] loss: 130719.2656 RMSElog: 8.7760 grad_loss: 13063.1504 normal_loss: 0.0000
[epoch  5][iter  400] loss: 100639.8516 RMSElog: 8.7642 grad_loss: 10055.2207 normal_loss: 0.0000
[epoch  5][iter  410] loss: 208794.8125 RMSElog: 8.8197 grad_loss: 20870.6602 normal_loss: 0.0000
[epoch  5][iter  420] loss: 136846.3125 RMSElog: 8.4504 grad_loss: 13676.1816 normal_loss: 0.0000
[epoch  5][iter  430] loss: 144632.8438 RMSElog: 8.7406 grad_loss: 14454.5449 normal_loss: 0.0000
[epoch  5][iter  440] loss: 153697.7500 RMSElog: 8.7682 grad_loss: 15361.0059 normal_loss: 0.0000
[epoch  5][iter  450] loss: 127606.4922 RMSElog: 8.5262 grad_loss: 12752.1230 normal_loss: 0.0000
[epoch  5][iter  460] loss: 147347.6562 RMSElog: 8.5743 grad_loss: 14726.1914 normal_loss: 0.0000
[epoch  5][iter  470] loss: 188690.6250 RMSElog: 8.7363 grad_loss: 18860.3262 normal_loss: 0.0000
[epoch  5][iter  480] loss: 170667.7344 RMSElog: 8.9364 grad_loss: 17057.8379 normal_loss: 0.0000
[epoch  5][iter  490] loss: 179623.2656 RMSElog: 8.9259 grad_loss: 17953.4004 normal_loss: 0.0000
[epoch  5][iter  500] loss: 175333.7812 RMSElog: 8.9505 grad_loss: 17524.4277 normal_loss: 0.0000
[epoch  5][iter  510] loss: 127439.3828 RMSElog: 8.4231 grad_loss: 12735.5156 normal_loss: 0.0000
[epoch  5][iter  520] loss: 105799.4219 RMSElog: 8.4401 grad_loss: 10571.5020 normal_loss: 0.0000
[epoch  5][iter  530] loss: 218521.7812 RMSElog: 9.1784 grad_loss: 21843.0000 normal_loss: 0.0000
[epoch  5][iter  540] loss: 128535.6250 RMSElog: 8.6993 grad_loss: 12844.8633 normal_loss: 0.0000
[epoch  5][iter  550] loss: 122653.8281 RMSElog: 8.7516 grad_loss: 12256.6309 normal_loss: 0.0000
[epoch  5][iter  560] loss: 102204.4844 RMSElog: 8.7204 grad_loss: 10211.7275 normal_loss: 0.0000
[epoch  5][iter  570] loss: 135016.1094 RMSElog: 8.3757 grad_loss: 13493.2344 normal_loss: 0.0000
[epoch  5][iter  580] loss: 163751.8125 RMSElog: 8.6875 grad_loss: 16366.4941 normal_loss: 0.0000
[epoch  5][iter  590] loss: 163368.6094 RMSElog: 8.8512 grad_loss: 16328.0098 normal_loss: 0.0000
[epoch  6][iter    0] loss: 148156.5312 RMSElog: 8.6798 grad_loss: 14806.9727 normal_loss: 0.0000
[epoch  6][iter   10] loss: 150270.8594 RMSElog: 8.6406 grad_loss: 15018.4453 normal_loss: 0.0000
[epoch  6][iter   20] loss: 177909.7656 RMSElog: 8.7782 grad_loss: 17782.1992 normal_loss: 0.0000
[epoch  6][iter   30] loss: 185472.2656 RMSElog: 8.9050 grad_loss: 18538.3223 normal_loss: 0.0000
[epoch  6][iter   40] loss: 121842.2734 RMSElog: 8.5501 grad_loss: 12175.6777 normal_loss: 0.0000
[epoch  6][iter   50] loss: 142569.7656 RMSElog: 8.6077 grad_loss: 14248.3691 normal_loss: 0.0000
[epoch  6][iter   60] loss: 165626.4219 RMSElog: 8.7967 grad_loss: 16553.8457 normal_loss: 0.0000
[epoch  6][iter   70] loss: 218597.9531 RMSElog: 8.8743 grad_loss: 21850.9199 normal_loss: 0.0000
[epoch  6][iter   80] loss: 167392.2812 RMSElog: 8.8440 grad_loss: 16730.3848 normal_loss: 0.0000
[epoch  6][iter   90] loss: 125658.9219 RMSElog: 8.6487 grad_loss: 12557.2441 normal_loss: 0.0000
[epoch  6][iter  100] loss: 206498.9844 RMSElog: 8.8139 grad_loss: 20641.0840 normal_loss: 0.0000
[epoch  6][iter  110] loss: 180635.7969 RMSElog: 8.9021 grad_loss: 18054.6777 normal_loss: 0.0000
[epoch  6][iter  120] loss: 142234.7500 RMSElog: 8.4177 grad_loss: 14215.0566 normal_loss: 0.0000
[epoch  6][iter  130] loss: 195749.6094 RMSElog: 8.8270 grad_loss: 19566.1348 normal_loss: 0.0000
[epoch  6][iter  140] loss: 180724.1875 RMSElog: 8.8798 grad_loss: 18063.5391 normal_loss: 0.0000
[epoch  6][iter  150] loss: 167930.6562 RMSElog: 8.6403 grad_loss: 16784.4258 normal_loss: 0.0000
[epoch  6][iter  160] loss: 241946.0938 RMSElog: 9.0438 grad_loss: 24185.5664 normal_loss: 0.0000
[epoch  6][iter  170] loss: 185748.1250 RMSElog: 8.8967 grad_loss: 18565.9160 normal_loss: 0.0000
[epoch  6][iter  180] loss: 163228.1094 RMSElog: 8.7382 grad_loss: 16314.0732 normal_loss: 0.0000
[epoch  6][iter  190] loss: 100952.6719 RMSElog: 8.1623 grad_loss: 10087.1055 normal_loss: 0.0000
[epoch  6][iter  200] loss: 203065.4531 RMSElog: 8.7327 grad_loss: 20297.8125 normal_loss: 0.0000
[epoch  6][iter  210] loss: 185119.7500 RMSElog: 8.9003 grad_loss: 18503.0742 normal_loss: 0.0000
[epoch  6][iter  220] loss: 141419.8125 RMSElog: 8.7000 grad_loss: 14133.2812 normal_loss: 0.0000
[epoch  6][iter  230] loss: 153698.4062 RMSElog: 8.8049 grad_loss: 15361.0361 normal_loss: 0.0000
[epoch  6][iter  240] loss: 208158.6250 RMSElog: 9.1467 grad_loss: 20806.7168 normal_loss: 0.0000
[epoch  6][iter  250] loss: 129181.8359 RMSElog: 8.8183 grad_loss: 12909.3652 normal_loss: 0.0000
[epoch  6][iter  260] loss: 163244.3906 RMSElog: 8.8901 grad_loss: 16315.5488 normal_loss: 0.0000
[epoch  6][iter  270] loss: 103786.2109 RMSElog: 8.1827 grad_loss: 10370.4385 normal_loss: 0.0000
[epoch  6][iter  280] loss: 121161.0859 RMSElog: 8.7453 grad_loss: 12107.3633 normal_loss: 0.0000
[epoch  6][iter  290] loss: 146757.8438 RMSElog: 8.7873 grad_loss: 14666.9971 normal_loss: 0.0000
[epoch  6][iter  300] loss: 176200.8594 RMSElog: 8.9970 grad_loss: 17611.0898 normal_loss: 0.0000
[epoch  6][iter  310] loss: 191666.6406 RMSElog: 8.9258 grad_loss: 19157.7383 normal_loss: 0.0000
[epoch  6][iter  320] loss: 217146.6875 RMSElog: 8.8780 grad_loss: 21705.7891 normal_loss: 0.0000
[epoch  6][iter  330] loss: 122887.8125 RMSElog: 8.6602 grad_loss: 12280.1211 normal_loss: 0.0000
[epoch  6][iter  340] loss: 104104.2109 RMSElog: 8.5581 grad_loss: 10401.8633 normal_loss: 0.0000
[epoch  6][iter  350] loss: 167771.0312 RMSElog: 8.8553 grad_loss: 16768.2480 normal_loss: 0.0000
[epoch  6][iter  360] loss: 130403.7891 RMSElog: 8.5275 grad_loss: 13031.8516 normal_loss: 0.0000
[epoch  6][iter  370] loss: 195387.9531 RMSElog: 8.9664 grad_loss: 19529.8281 normal_loss: 0.0000
[epoch  6][iter  380] loss: 112686.9375 RMSElog: 8.1347 grad_loss: 11260.5586 normal_loss: 0.0000
[epoch  6][iter  390] loss: 167716.7500 RMSElog: 8.8744 grad_loss: 16762.8008 normal_loss: 0.0000
[epoch  6][iter  400] loss: 104413.7578 RMSElog: 8.2783 grad_loss: 10433.0977 normal_loss: 0.0000
[epoch  6][iter  410] loss: 195973.3438 RMSElog: 8.9761 grad_loss: 19588.3574 normal_loss: 0.0000
[epoch  6][iter  420] loss: 115507.4688 RMSElog: 8.3553 grad_loss: 11542.3916 normal_loss: 0.0000
[epoch  6][iter  430] loss: 164820.6406 RMSElog: 8.7267 grad_loss: 16473.3379 normal_loss: 0.0000
[epoch  6][iter  440] loss: 120262.9297 RMSElog: 8.2454 grad_loss: 12018.0479 normal_loss: 0.0000
[epoch  6][iter  450] loss: 162497.2344 RMSElog: 8.8936 grad_loss: 16240.8301 normal_loss: 0.0000
[epoch  6][iter  460] loss: 150296.3906 RMSElog: 8.7259 grad_loss: 15020.9141 normal_loss: 0.0000
[epoch  6][iter  470] loss: 105676.2969 RMSElog: 8.0708 grad_loss: 10559.5586 normal_loss: 0.0000
[epoch  6][iter  480] loss: 165283.5312 RMSElog: 8.9011 grad_loss: 16519.4531 normal_loss: 0.0000
[epoch  6][iter  490] loss: 160002.9531 RMSElog: 8.8201 grad_loss: 15991.4756 normal_loss: 0.0000
[epoch  6][iter  500] loss: 120420.1641 RMSElog: 8.3792 grad_loss: 12033.6377 normal_loss: 0.0000
[epoch  6][iter  510] loss: 178055.2344 RMSElog: 8.9186 grad_loss: 17796.6055 normal_loss: 0.0000
[epoch  6][iter  520] loss: 128951.9219 RMSElog: 8.3060 grad_loss: 12886.8867 normal_loss: 0.0000
[epoch  6][iter  530] loss: 146920.5625 RMSElog: 8.5360 grad_loss: 14683.5195 normal_loss: 0.0000
[epoch  6][iter  540] loss: 135187.1250 RMSElog: 8.5496 grad_loss: 13510.1621 normal_loss: 0.0000
[epoch  6][iter  550] loss: 160719.8281 RMSElog: 8.7716 grad_loss: 16063.2109 normal_loss: 0.0000
[epoch  6][iter  560] loss: 165550.9531 RMSElog: 8.7975 grad_loss: 16546.2988 normal_loss: 0.0000
[epoch  6][iter  570] loss: 112667.5625 RMSElog: 8.1606 grad_loss: 11258.5957 normal_loss: 0.0000
[epoch  6][iter  580] loss: 116045.1953 RMSElog: 8.2507 grad_loss: 11596.2686 normal_loss: 0.0000
[epoch  6][iter  590] loss: 103283.5000 RMSElog: 8.5973 grad_loss: 10319.7520 normal_loss: 0.0000
[epoch  7][iter    0] loss: 105847.2266 RMSElog: 8.0634 grad_loss: 10576.6592 normal_loss: 0.0000
[epoch  7][iter   10] loss: 186932.2500 RMSElog: 8.8775 grad_loss: 18684.3477 normal_loss: 0.0000
[epoch  7][iter   20] loss: 135181.0469 RMSElog: 8.4643 grad_loss: 13509.6406 normal_loss: 0.0000
[epoch  7][iter   30] loss: 175975.0156 RMSElog: 9.1976 grad_loss: 17588.3047 normal_loss: 0.0000
[epoch  7][iter   40] loss: 140438.9844 RMSElog: 8.7921 grad_loss: 14035.1064 normal_loss: 0.0000
[epoch  7][iter   50] loss: 160443.5781 RMSElog: 8.8751 grad_loss: 16035.4824 normal_loss: 0.0000
[epoch  7][iter   60] loss: 106922.2188 RMSElog: 8.3062 grad_loss: 10683.9150 normal_loss: 0.0000
[epoch  7][iter   70] loss: 142234.7031 RMSElog: 8.4129 grad_loss: 14215.0576 normal_loss: 0.0000
[epoch  7][iter   80] loss: 140254.5625 RMSElog: 8.5562 grad_loss: 14016.9004 normal_loss: 0.0000
[epoch  7][iter   90] loss: 204623.3750 RMSElog: 8.9473 grad_loss: 20453.3906 normal_loss: 0.0000
[epoch  7][iter  100] loss: 167392.1250 RMSElog: 8.8259 grad_loss: 16730.3867 normal_loss: 0.0000
[epoch  7][iter  110] loss: 232745.3438 RMSElog: 9.0304 grad_loss: 23265.5039 normal_loss: 0.0000
[epoch  7][iter  120] loss: 151379.7188 RMSElog: 8.5071 grad_loss: 15129.4648 normal_loss: 0.0000
[epoch  7][iter  130] loss: 194379.8594 RMSElog: 8.7982 grad_loss: 19429.1875 normal_loss: 0.0000
[epoch  7][iter  140] loss: 149345.3125 RMSElog: 8.7619 grad_loss: 14925.7695 normal_loss: 0.0000
[epoch  7][iter  150] loss: 174605.8438 RMSElog: 8.8040 grad_loss: 17451.7793 normal_loss: 0.0000
[epoch  7][iter  160] loss: 216204.0938 RMSElog: 8.8497 grad_loss: 21611.5605 normal_loss: 0.0000
[epoch  7][iter  170] loss: 103929.2812 RMSElog: 8.3468 grad_loss: 10384.5811 normal_loss: 0.0000
[epoch  7][iter  180] loss: 177265.7500 RMSElog: 8.8743 grad_loss: 17717.6992 normal_loss: 0.0000
[epoch  7][iter  190] loss: 137301.3594 RMSElog: 8.5210 grad_loss: 13721.6152 normal_loss: 0.0000
[epoch  7][iter  200] loss: 162348.8438 RMSElog: 8.8114 grad_loss: 16226.0732 normal_loss: 0.0000
[epoch  7][iter  210] loss: 136038.5781 RMSElog: 8.3079 grad_loss: 13595.5508 normal_loss: 0.0000
[epoch  7][iter  220] loss: 135087.9219 RMSElog: 8.6197 grad_loss: 13500.1719 normal_loss: 0.0000
[epoch  7][iter  230] loss: 163674.4688 RMSElog: 8.8113 grad_loss: 16358.6348 normal_loss: 0.0000
[epoch  7][iter  240] loss: 160761.2188 RMSElog: 8.8279 grad_loss: 16067.2939 normal_loss: 0.0000
[epoch  7][iter  250] loss: 106152.5312 RMSElog: 8.1217 grad_loss: 10607.1309 normal_loss: 0.0000
[epoch  7][iter  260] loss: 144633.4688 RMSElog: 8.7948 grad_loss: 14454.5518 normal_loss: 0.0000
[epoch  7][iter  270] loss: 119898.9688 RMSElog: 8.5135 grad_loss: 11981.3828 normal_loss: 0.0000
[epoch  7][iter  280] loss: 165302.8906 RMSElog: 8.8649 grad_loss: 16521.4238 normal_loss: 0.0000
[epoch  7][iter  290] loss: 180295.3281 RMSElog: 8.8723 grad_loss: 18020.6602 normal_loss: 0.0000
[epoch  7][iter  300] loss: 115634.0234 RMSElog: 8.5879 grad_loss: 11554.8145 normal_loss: 0.0000
[epoch  7][iter  310] loss: 155886.2656 RMSElog: 8.6481 grad_loss: 15579.9785 normal_loss: 0.0000
[epoch  7][iter  320] loss: 126847.8438 RMSElog: 8.4963 grad_loss: 12676.2881 normal_loss: 0.0000
[epoch  7][iter  330] loss: 117379.5781 RMSElog: 8.4074 grad_loss: 11729.5508 normal_loss: 0.0000
[epoch  7][iter  340] loss: 167707.9688 RMSElog: 8.9835 grad_loss: 16761.8125 normal_loss: 0.0000
[epoch  7][iter  350] loss: 151697.6406 RMSElog: 8.7429 grad_loss: 15161.0215 normal_loss: 0.0000
[epoch  7][iter  360] loss: 176774.2344 RMSElog: 8.7693 grad_loss: 17668.6543 normal_loss: 0.0000
[epoch  7][iter  370] loss: 185877.9688 RMSElog: 8.8260 grad_loss: 18578.9707 normal_loss: 0.0000
[epoch  7][iter  380] loss: 159774.8594 RMSElog: 8.6991 grad_loss: 15968.7871 normal_loss: 0.0000
[epoch  7][iter  390] loss: 122903.0859 RMSElog: 8.5754 grad_loss: 12281.7334 normal_loss: 0.0000
[epoch  7][iter  400] loss: 203947.2500 RMSElog: 9.1548 grad_loss: 20385.5703 normal_loss: 0.0000
[epoch  7][iter  410] loss: 120262.8438 RMSElog: 8.2430 grad_loss: 12018.0410 normal_loss: 0.0000
[epoch  7][iter  420] loss: 103506.1406 RMSElog: 8.2433 grad_loss: 10342.3711 normal_loss: 0.0000
[epoch  7][iter  430] loss: 181983.2656 RMSElog: 9.0777 grad_loss: 18189.2480 normal_loss: 0.0000
[epoch  7][iter  440] loss: 110572.4297 RMSElog: 8.1015 grad_loss: 11049.1416 normal_loss: 0.0000
[epoch  7][iter  450] loss: 110204.4531 RMSElog: 8.0530 grad_loss: 11012.3926 normal_loss: 0.0000
[epoch  7][iter  460] loss: 232955.0625 RMSElog: 8.9595 grad_loss: 23286.5469 normal_loss: 0.0000
[epoch  7][iter  470] loss: 191180.1406 RMSElog: 8.8343 grad_loss: 19109.1797 normal_loss: 0.0000
[epoch  7][iter  480] loss: 208795.1406 RMSElog: 8.8386 grad_loss: 20870.6758 normal_loss: 0.0000
[epoch  7][iter  490] loss: 231948.7812 RMSElog: 9.0600 grad_loss: 23185.8184 normal_loss: 0.0000
[epoch  7][iter  500] loss: 138993.8438 RMSElog: 8.6402 grad_loss: 13890.7441 normal_loss: 0.0000
[epoch  7][iter  510] loss: 143435.8594 RMSElog: 8.6401 grad_loss: 14334.9463 normal_loss: 0.0000
[epoch  7][iter  520] loss: 183066.5000 RMSElog: 8.5975 grad_loss: 18298.0527 normal_loss: 0.0000
[epoch  7][iter  530] loss: 152493.8281 RMSElog: 8.7277 grad_loss: 15240.6553 normal_loss: 0.0000
[epoch  7][iter  540] loss: 163243.3281 RMSElog: 8.8267 grad_loss: 16315.5059 normal_loss: 0.0000
[epoch  7][iter  550] loss: 208157.9062 RMSElog: 9.1016 grad_loss: 20806.6895 normal_loss: 0.0000
[epoch  7][iter  560] loss: 205334.5000 RMSElog: 8.7901 grad_loss: 20524.6582 normal_loss: 0.0000
[epoch  7][iter  570] loss: 100619.4844 RMSElog: 8.0502 grad_loss: 10053.8984 normal_loss: 0.0000
[epoch  7][iter  580] loss: 155738.6875 RMSElog: 8.7627 grad_loss: 15565.1055 normal_loss: 0.0000
[epoch  7][iter  590] loss: 160049.6875 RMSElog: 8.6418 grad_loss: 15996.3271 normal_loss: 0.0000
[epoch  8][iter    0] loss: 137237.4844 RMSElog: 8.9370 grad_loss: 13714.0439 normal_loss: 0.7664
[epoch  8][iter   10] loss: 141426.4219 RMSElog: 8.6854 grad_loss: 14133.2539 normal_loss: 0.7029
[epoch  8][iter   20] loss: 149525.2656 RMSElog: 8.8471 grad_loss: 14942.9746 normal_loss: 0.7048
[epoch  8][iter   30] loss: 124229.8281 RMSElog: 8.6393 grad_loss: 12413.6465 normal_loss: 0.6965
[epoch  8][iter   40] loss: 133268.3750 RMSElog: 8.6790 grad_loss: 13317.4883 normal_loss: 0.6714
[epoch  8][iter   50] loss: 179618.3594 RMSElog: 9.0432 grad_loss: 17952.0605 normal_loss: 0.7328
[epoch  8][iter   60] loss: 112517.9766 RMSElog: 8.4950 grad_loss: 11242.6230 normal_loss: 0.6798
[epoch  8][iter   70] loss: 102075.5547 RMSElog: 8.3620 grad_loss: 10198.5195 normal_loss: 0.6738
[epoch  8][iter   80] loss: 163758.6250 RMSElog: 8.6809 grad_loss: 16366.4590 normal_loss: 0.7223
[epoch  8][iter   90] loss: 106159.3672 RMSElog: 8.1296 grad_loss: 10607.1348 normal_loss: 0.6719
[epoch  8][iter  100] loss: 221193.6094 RMSElog: 8.9005 grad_loss: 22109.7871 normal_loss: 0.6748
[epoch  8][iter  110] loss: 233376.7812 RMSElog: 9.0101 grad_loss: 23327.9844 normal_loss: 0.6844
[epoch  8][iter  120] loss: 177916.4062 RMSElog: 8.7851 grad_loss: 17782.1758 normal_loss: 0.6800
[epoch  8][iter  130] loss: 210500.8125 RMSElog: 8.9575 grad_loss: 21040.4668 normal_loss: 0.6579
[epoch  8][iter  140] loss: 120571.0938 RMSElog: 8.6476 grad_loss: 12047.7656 normal_loss: 0.6960
[epoch  8][iter  150] loss: 152529.3281 RMSElog: 8.7436 grad_loss: 15243.5293 normal_loss: 0.6598
[epoch  8][iter  160] loss: 182171.3438 RMSElog: 8.9769 grad_loss: 18207.4336 normal_loss: 0.7239
[epoch  8][iter  170] loss: 200934.0938 RMSElog: 8.8719 grad_loss: 20083.8379 normal_loss: 0.7016
[epoch  8][iter  180] loss: 156980.8438 RMSElog: 9.2400 grad_loss: 15687.9678 normal_loss: 0.8760
[epoch  8][iter  190] loss: 183073.5312 RMSElog: 8.6090 grad_loss: 18298.0449 normal_loss: 0.6986
[epoch  8][iter  200] loss: 143594.9688 RMSElog: 8.5920 grad_loss: 14350.1963 normal_loss: 0.7078
[epoch  8][iter  210] loss: 104725.8281 RMSElog: 8.1230 grad_loss: 10463.7969 normal_loss: 0.6630
[epoch  8][iter  220] loss: 154839.8125 RMSElog: 8.8230 grad_loss: 15474.4453 normal_loss: 0.7131
[epoch  8][iter  230] loss: 111901.5938 RMSElog: 8.4750 grad_loss: 11180.9629 normal_loss: 0.7217
[epoch  8][iter  240] loss: 127483.3750 RMSElog: 8.3110 grad_loss: 12739.3311 normal_loss: 0.6958
[epoch  8][iter  250] loss: 151351.0469 RMSElog: 8.6390 grad_loss: 15125.7539 normal_loss: 0.7118
[epoch  8][iter  260] loss: 144569.5156 RMSElog: 8.1271 grad_loss: 14448.0869 normal_loss: 0.7372
[epoch  8][iter  270] loss: 134796.7656 RMSElog: 8.3729 grad_loss: 13470.6016 normal_loss: 0.7025
[epoch  8][iter  280] loss: 150278.0938 RMSElog: 8.6764 grad_loss: 15018.4668 normal_loss: 0.6652
[epoch  8][iter  290] loss: 111645.7500 RMSElog: 8.6222 grad_loss: 11155.2051 normal_loss: 0.7479
[epoch  8][iter  300] loss: 154687.6562 RMSElog: 8.8183 grad_loss: 15459.2627 normal_loss: 0.6841
[epoch  8][iter  310] loss: 150588.5625 RMSElog: 8.7873 grad_loss: 15049.3984 normal_loss: 0.6707
[epoch  8][iter  320] loss: 163575.2656 RMSElog: 8.7584 grad_loss: 16348.0938 normal_loss: 0.6739
[epoch  8][iter  330] loss: 113185.7109 RMSElog: 8.5776 grad_loss: 11309.2656 normal_loss: 0.7281
[epoch  8][iter  340] loss: 188998.6875 RMSElog: 8.9084 grad_loss: 18890.2500 normal_loss: 0.7113
[epoch  8][iter  350] loss: 166191.0625 RMSElog: 8.8434 grad_loss: 16609.5664 normal_loss: 0.6962
[epoch  8][iter  360] loss: 140261.3750 RMSElog: 8.5796 grad_loss: 14016.8945 normal_loss: 0.6619
[epoch  8][iter  370] loss: 217154.0625 RMSElog: 8.9308 grad_loss: 21705.7988 normal_loss: 0.6764
[epoch  8][iter  380] loss: 195394.5312 RMSElog: 8.9682 grad_loss: 19529.8105 normal_loss: 0.6733
[epoch  8][iter  390] loss: 160864.1875 RMSElog: 8.8916 grad_loss: 16076.8359 normal_loss: 0.6914
[epoch  8][iter  400] loss: 137244.5312 RMSElog: 8.9035 grad_loss: 13714.7812 normal_loss: 0.7687
[epoch  8][iter  410] loss: 135679.9688 RMSElog: 8.3786 grad_loss: 13558.9121 normal_loss: 0.7061
[epoch  8][iter  420] loss: 226930.6250 RMSElog: 9.0039 grad_loss: 22683.3867 normal_loss: 0.6726
[epoch  8][iter  430] loss: 175340.6406 RMSElog: 8.9638 grad_loss: 17524.3926 normal_loss: 0.7090
[epoch  8][iter  440] loss: 135797.3125 RMSElog: 8.4433 grad_loss: 13570.5947 normal_loss: 0.6933
[epoch  8][iter  450] loss: 184355.6250 RMSElog: 9.0550 grad_loss: 18425.8008 normal_loss: 0.7076
[epoch  8][iter  460] loss: 164163.5312 RMSElog: 8.8672 grad_loss: 16406.8027 normal_loss: 0.6845
[epoch  8][iter  470] loss: 163769.2812 RMSElog: 8.7598 grad_loss: 16367.4912 normal_loss: 0.6765
[epoch  8][iter  480] loss: 121894.7188 RMSElog: 8.3266 grad_loss: 12180.4678 normal_loss: 0.6777
[epoch  8][iter  490] loss: 207666.8594 RMSElog: 8.8902 grad_loss: 20757.0801 normal_loss: 0.7142
[epoch  8][iter  500] loss: 122894.6406 RMSElog: 8.6629 grad_loss: 12280.1113 normal_loss: 0.6892
[epoch  8][iter  510] loss: 142677.6562 RMSElog: 8.7019 grad_loss: 14258.3730 normal_loss: 0.6905
[epoch  8][iter  520] loss: 166475.6406 RMSElog: 8.8092 grad_loss: 16638.0547 normal_loss: 0.7011
[epoch  8][iter  530] loss: 167938.0312 RMSElog: 8.6540 grad_loss: 16784.4375 normal_loss: 0.7115
[epoch  8][iter  540] loss: 197695.0781 RMSElog: 9.1346 grad_loss: 19759.6953 normal_loss: 0.6774
[epoch  8][iter  550] loss: 154146.2812 RMSElog: 8.8741 grad_loss: 15405.0859 normal_loss: 0.6688
[epoch  8][iter  560] loss: 129689.3828 RMSElog: 8.4880 grad_loss: 12959.7666 normal_loss: 0.6839
[epoch  8][iter  570] loss: 165557.8281 RMSElog: 8.8043 grad_loss: 16546.2852 normal_loss: 0.6930
[epoch  8][iter  580] loss: 159269.0469 RMSElog: 8.6377 grad_loss: 15917.5342 normal_loss: 0.7331
[epoch  8][iter  590] loss: 175282.7969 RMSElog: 8.6264 grad_loss: 17518.9492 normal_loss: 0.7030
[epoch  9][iter    0] loss: 216489.6875 RMSElog: 8.9471 grad_loss: 21639.3555 normal_loss: 0.6656
[epoch  9][iter   10] loss: 142241.7500 RMSElog: 8.4143 grad_loss: 14215.0439 normal_loss: 0.7175
[epoch  9][iter   20] loss: 114364.0156 RMSElog: 8.6011 grad_loss: 11427.0596 normal_loss: 0.7398
[epoch  9][iter   30] loss: 158015.4375 RMSElog: 8.8053 grad_loss: 15792.0645 normal_loss: 0.6739
[epoch  9][iter   40] loss: 133026.9062 RMSElog: 8.6762 grad_loss: 13293.3506 normal_loss: 0.6641
[epoch  9][iter   50] loss: 179885.1719 RMSElog: 8.8213 grad_loss: 17978.9824 normal_loss: 0.7132
[epoch  9][iter   60] loss: 173703.3125 RMSElog: 8.9044 grad_loss: 17360.7188 normal_loss: 0.7081
[epoch  9][iter   70] loss: 121968.6875 RMSElog: 8.4452 grad_loss: 12187.6836 normal_loss: 0.7407
[epoch  9][iter   80] loss: 175340.5938 RMSElog: 8.9611 grad_loss: 17524.3887 normal_loss: 0.7080
[epoch  9][iter   90] loss: 151335.7969 RMSElog: 8.7306 grad_loss: 15124.1846 normal_loss: 0.6644
[epoch  9][iter  100] loss: 149114.2812 RMSElog: 8.7864 grad_loss: 14901.9668 normal_loss: 0.6746
[epoch  9][iter  110] loss: 130624.5469 RMSElog: 8.7171 grad_loss: 13053.0605 normal_loss: 0.6782
[epoch  9][iter  120] loss: 181990.7969 RMSElog: 9.0970 grad_loss: 18189.2480 normal_loss: 0.7343
[epoch  9][iter  130] loss: 185051.9375 RMSElog: 8.7503 grad_loss: 18495.7344 normal_loss: 0.7083
[epoch  9][iter  140] loss: 103290.2969 RMSElog: 8.6054 grad_loss: 10319.7354 normal_loss: 0.6888
[epoch  9][iter  150] loss: 104067.0000 RMSElog: 8.5041 grad_loss: 10397.5234 normal_loss: 0.6731
[epoch  9][iter  160] loss: 214470.7031 RMSElog: 8.8989 grad_loss: 21437.5098 normal_loss: 0.6626
[epoch  9][iter  170] loss: 183854.7031 RMSElog: 9.0401 grad_loss: 18375.7129 normal_loss: 0.7167
[epoch  9][iter  180] loss: 128541.8125 RMSElog: 8.6426 grad_loss: 12844.8291 normal_loss: 0.7103
[epoch  9][iter  190] loss: 165957.2812 RMSElog: 8.7612 grad_loss: 16586.3105 normal_loss: 0.6557
[epoch  9][iter  200] loss: 154538.5938 RMSElog: 8.8730 grad_loss: 15444.2949 normal_loss: 0.6912
[epoch  9][iter  210] loss: 147525.3281 RMSElog: 8.7259 grad_loss: 14743.1396 normal_loss: 0.6679
[epoch  9][iter  220] loss: 130726.2344 RMSElog: 8.7388 grad_loss: 13063.1436 normal_loss: 0.7406
[epoch  9][iter  230] loss: 100729.6953 RMSElog: 8.6486 grad_loss: 10063.6484 normal_loss: 0.6729
[epoch  9][iter  240] loss: 162356.6719 RMSElog: 8.8812 grad_loss: 16226.0781 normal_loss: 0.7083
[epoch  9][iter  250] loss: 167715.1562 RMSElog: 9.0175 grad_loss: 16761.8066 normal_loss: 0.6923
[epoch  9][iter  260] loss: 218748.3750 RMSElog: 9.4059 grad_loss: 21864.5332 normal_loss: 0.8988
[epoch  9][iter  270] loss: 156963.6719 RMSElog: 8.8679 grad_loss: 15686.8398 normal_loss: 0.6589
[epoch  9][iter  280] loss: 158908.9219 RMSElog: 8.8194 grad_loss: 15881.3652 normal_loss: 0.7080
[epoch  9][iter  290] loss: 137745.5625 RMSElog: 8.6289 grad_loss: 13765.2656 normal_loss: 0.6610
[epoch  9][iter  300] loss: 194556.0781 RMSElog: 8.7755 grad_loss: 19446.1758 normal_loss: 0.6571
[epoch  9][iter  310] loss: 179617.6562 RMSElog: 9.0079 grad_loss: 17952.0371 normal_loss: 0.7209
[epoch  9][iter  320] loss: 175277.9531 RMSElog: 8.8023 grad_loss: 17518.3242 normal_loss: 0.6688
[epoch  9][iter  330] loss: 202097.1094 RMSElog: 8.7504 grad_loss: 20200.3047 normal_loss: 0.6567
[epoch  9][iter  340] loss: 139078.9531 RMSElog: 8.6454 grad_loss: 13898.5752 normal_loss: 0.6749
[epoch  9][iter  350] loss: 163681.9062 RMSElog: 8.8044 grad_loss: 16358.6328 normal_loss: 0.7533
[epoch  9][iter  360] loss: 140918.7656 RMSElog: 8.4501 grad_loss: 14082.7500 normal_loss: 0.6762
[epoch  9][iter  370] loss: 142119.4375 RMSElog: 8.5252 grad_loss: 14202.7129 normal_loss: 0.7051
[epoch  9][iter  380] loss: 139415.9688 RMSElog: 8.6795 grad_loss: 13932.2451 normal_loss: 0.6726
[epoch  9][iter  390] loss: 161615.4844 RMSElog: 8.9556 grad_loss: 16151.9199 normal_loss: 0.6724
[epoch  9][iter  400] loss: 121943.0625 RMSElog: 8.7085 grad_loss: 12184.9229 normal_loss: 0.6758
[epoch  9][iter  410] loss: 164750.5625 RMSElog: 8.8329 grad_loss: 16465.5098 normal_loss: 0.7157
[epoch  9][iter  420] loss: 103791.9922 RMSElog: 8.1168 grad_loss: 10370.4189 normal_loss: 0.6629
[epoch  9][iter  430] loss: 149352.1406 RMSElog: 8.7327 grad_loss: 14925.7383 normal_loss: 0.7431
[epoch  9][iter  440] loss: 175528.8125 RMSElog: 8.8350 grad_loss: 17543.3555 normal_loss: 0.6888
[epoch  9][iter  450] loss: 105872.3906 RMSElog: 8.0585 grad_loss: 10578.5254 normal_loss: 0.6552
[epoch  9][iter  460] loss: 160449.4688 RMSElog: 8.8380 grad_loss: 16035.4248 normal_loss: 0.6844
[epoch  9][iter  470] loss: 135193.5625 RMSElog: 8.5662 grad_loss: 13510.1289 normal_loss: 0.6600
[epoch  9][iter  480] loss: 147463.7344 RMSElog: 8.7594 grad_loss: 14736.9082 normal_loss: 0.7050
[epoch  9][iter  490] loss: 167327.3594 RMSElog: 8.7853 grad_loss: 16723.2461 normal_loss: 0.7046
[epoch  9][iter  500] loss: 105770.7344 RMSElog: 8.0945 grad_loss: 10568.3311 normal_loss: 0.6478
[epoch  9][iter  510] loss: 101431.5234 RMSElog: 8.6345 grad_loss: 10133.8389 normal_loss: 0.6791
[epoch  9][iter  520] loss: 128258.1406 RMSElog: 8.5159 grad_loss: 12816.6406 normal_loss: 0.6586
[epoch  9][iter  530] loss: 160420.5156 RMSElog: 8.9233 grad_loss: 16032.4648 normal_loss: 0.6643
[epoch  9][iter  540] loss: 129148.9062 RMSElog: 8.8850 grad_loss: 12905.3135 normal_loss: 0.6921
[epoch  9][iter  550] loss: 139000.4219 RMSElog: 8.6358 grad_loss: 13890.7246 normal_loss: 0.6820
[epoch  9][iter  560] loss: 144174.3438 RMSElog: 8.5951 grad_loss: 14408.1592 normal_loss: 0.6799
[epoch  9][iter  570] loss: 167067.9531 RMSElog: 8.8487 grad_loss: 16697.2441 normal_loss: 0.7014
[epoch  9][iter  580] loss: 149111.3906 RMSElog: 8.6636 grad_loss: 14901.7578 normal_loss: 0.7175
[epoch  9][iter  590] loss: 134614.7500 RMSElog: 8.7139 grad_loss: 13452.0742 normal_loss: 0.6870
[epoch 10][iter    0] loss: 148118.5781 RMSElog: 8.7601 grad_loss: 14802.3965 normal_loss: 0.7016
[epoch 10][iter   10] loss: 153947.7656 RMSElog: 8.9784 grad_loss: 15385.0410 normal_loss: 0.7571
[epoch 10][iter   20] loss: 72273.5781 RMSElog: 8.4832 grad_loss: 7218.2002 normal_loss: 0.6741
[epoch 10][iter   30] loss: 178062.2812 RMSElog: 8.8992 grad_loss: 17796.5977 normal_loss: 0.7330
[epoch 10][iter   40] loss: 195756.7812 RMSElog: 8.8725 grad_loss: 19566.1289 normal_loss: 0.6759
[epoch 10][iter   50] loss: 151335.5938 RMSElog: 8.7241 grad_loss: 15124.1748 normal_loss: 0.6602
[epoch 10][iter   60] loss: 188623.1094 RMSElog: 8.7450 grad_loss: 18852.8789 normal_loss: 0.6866
[epoch 10][iter   70] loss: 128258.0469 RMSElog: 8.5073 grad_loss: 12816.6396 normal_loss: 0.6578
[epoch 10][iter   80] loss: 155745.6875 RMSElog: 8.7743 grad_loss: 15565.0879 normal_loss: 0.7070
[epoch 10][iter   90] loss: 150277.7969 RMSElog: 8.6835 grad_loss: 15018.4463 normal_loss: 0.6508
[epoch 10][iter  100] loss: 154271.0469 RMSElog: 8.6669 grad_loss: 15417.7793 normal_loss: 0.6587
[epoch 10][iter  110] loss: 109236.9375 RMSElog: 8.1015 grad_loss: 10914.9199 normal_loss: 0.6718
[epoch 10][iter  120] loss: 161068.5625 RMSElog: 8.6721 grad_loss: 16097.5156 normal_loss: 0.6678
[epoch 10][iter  130] loss: 141149.6875 RMSElog: 8.3799 grad_loss: 14105.8867 normal_loss: 0.7020
[epoch 10][iter  140] loss: 160918.3125 RMSElog: 8.8353 grad_loss: 16082.2842 normal_loss: 0.7125
[epoch 10][iter  150] loss: 182296.1875 RMSElog: 8.9466 grad_loss: 18220.0176 normal_loss: 0.6538
[epoch 10][iter  160] loss: 107924.1719 RMSElog: 8.0621 grad_loss: 10783.6895 normal_loss: 0.6651
[epoch 10][iter  170] loss: 195421.8594 RMSElog: 8.5884 grad_loss: 19532.9062 normal_loss: 0.6920
[epoch 10][iter  180] loss: 165992.3750 RMSElog: 8.8766 grad_loss: 16589.6992 normal_loss: 0.6612
[epoch 10][iter  190] loss: 171571.5312 RMSElog: 8.6884 grad_loss: 17147.7402 normal_loss: 0.7244
[epoch 10][iter  200] loss: 114296.7266 RMSElog: 8.4887 grad_loss: 11420.4600 normal_loss: 0.7245
[epoch 10][iter  210] loss: 175983.4062 RMSElog: 9.1607 grad_loss: 17588.3008 normal_loss: 0.8780
[epoch 10][iter  220] loss: 145598.1406 RMSElog: 8.8238 grad_loss: 14550.3174 normal_loss: 0.6731
[epoch 10][iter  230] loss: 177273.3750 RMSElog: 8.9045 grad_loss: 17717.7168 normal_loss: 0.7177
[epoch 10][iter  240] loss: 201354.6094 RMSElog: 9.1313 grad_loss: 20125.6426 normal_loss: 0.6884
[epoch 10][iter  250] loss: 118724.8125 RMSElog: 8.3076 grad_loss: 11863.5010 normal_loss: 0.6727
[epoch 10][iter  260] loss: 135680.3750 RMSElog: 8.4131 grad_loss: 13558.9268 normal_loss: 0.6971
[epoch 10][iter  270] loss: 107933.4531 RMSElog: 8.4108 grad_loss: 10784.2578 normal_loss: 0.6772
[epoch 10][iter  280] loss: 170340.7500 RMSElog: 8.9683 grad_loss: 17024.4219 normal_loss: 0.6830
[epoch 10][iter  290] loss: 158614.2500 RMSElog: 8.8240 grad_loss: 15851.9062 normal_loss: 0.6943
[epoch 10][iter  300] loss: 152529.1406 RMSElog: 8.7363 grad_loss: 15243.5234 normal_loss: 0.6542
[epoch 10][iter  310] loss: 186938.7656 RMSElog: 8.8827 grad_loss: 18684.3301 normal_loss: 0.6647
[epoch 10][iter  320] loss: 195979.5156 RMSElog: 8.9605 grad_loss: 19588.3242 normal_loss: 0.6663
[epoch 10][iter  330] loss: 155463.7656 RMSElog: 8.9342 grad_loss: 15536.7822 normal_loss: 0.6602
[epoch 10][iter  340] loss: 180188.1562 RMSElog: 8.7507 grad_loss: 18009.3809 normal_loss: 0.6857
[epoch 10][iter  350] loss: 101203.1094 RMSElog: 8.6335 grad_loss: 10110.9727 normal_loss: 0.7040
[epoch 10][iter  360] loss: 114363.9453 RMSElog: 8.6002 grad_loss: 11427.0596 normal_loss: 0.7342
[epoch 10][iter  370] loss: 119451.4375 RMSElog: 8.3723 grad_loss: 11936.0938 normal_loss: 0.6776
[epoch 10][iter  380] loss: 123515.3047 RMSElog: 8.7140 grad_loss: 12342.0488 normal_loss: 0.7673
[epoch 10][iter  390] loss: 117051.7109 RMSElog: 8.6487 grad_loss: 11695.7861 normal_loss: 0.7366
[epoch 10][iter  400] loss: 100958.8281 RMSElog: 8.1534 grad_loss: 10087.0771 normal_loss: 0.6523
[epoch 10][iter  410] loss: 196255.0312 RMSElog: 8.7665 grad_loss: 19616.0781 normal_loss: 0.6604
[epoch 10][iter  420] loss: 196900.1406 RMSElog: 8.9905 grad_loss: 19680.3555 normal_loss: 0.6676
[epoch 10][iter  430] loss: 176938.1406 RMSElog: 9.0282 grad_loss: 17684.0781 normal_loss: 0.7083
[epoch 10][iter  440] loss: 191952.5781 RMSElog: 8.8666 grad_loss: 19185.6895 normal_loss: 0.7007
[epoch 10][iter  450] loss: 179617.5938 RMSElog: 9.0094 grad_loss: 17952.0352 normal_loss: 0.7152
[epoch 10][iter  460] loss: 202609.6875 RMSElog: 8.7520 grad_loss: 20251.5469 normal_loss: 0.6704
[epoch 10][iter  470] loss: 134614.6562 RMSElog: 8.7165 grad_loss: 13452.0693 normal_loss: 0.6791
[epoch 10][iter  480] loss: 130507.5000 RMSElog: 8.6125 grad_loss: 13041.4795 normal_loss: 0.6587
[epoch 10][iter  490] loss: 175507.4062 RMSElog: 9.0946 grad_loss: 17540.9160 normal_loss: 0.7302
[epoch 10][iter  500] loss: 181990.7188 RMSElog: 9.0920 grad_loss: 18189.2480 normal_loss: 0.7334
[epoch 10][iter  510] loss: 156550.3750 RMSElog: 9.1585 grad_loss: 15645.0088 normal_loss: 0.8704
[epoch 10][iter  520] loss: 216489.5000 RMSElog: 8.9458 grad_loss: 21639.3457 normal_loss: 0.6592
[epoch 10][iter  530] loss: 177920.3125 RMSElog: 9.0191 grad_loss: 17782.2871 normal_loss: 0.7255
[epoch 10][iter  540] loss: 208164.5625 RMSElog: 9.1110 grad_loss: 20806.6641 normal_loss: 0.6819
[epoch 10][iter  550] loss: 107458.0938 RMSElog: 8.2226 grad_loss: 10736.9004 normal_loss: 0.6865
[epoch 10][iter  560] loss: 164827.3438 RMSElog: 8.7345 grad_loss: 16473.3086 normal_loss: 0.6906
[epoch 10][iter  570] loss: 149352.0938 RMSElog: 8.7265 grad_loss: 14925.7383 normal_loss: 0.7447
[epoch 10][iter  580] loss: 168528.5156 RMSElog: 8.6710 grad_loss: 16843.4941 normal_loss: 0.6863
[epoch 10][iter  590] loss: 127756.1875 RMSElog: 8.6521 grad_loss: 12766.2832 normal_loss: 0.6835
[epoch 11][iter    0] loss: 140447.1719 RMSElog: 8.7914 grad_loss: 14035.0957 normal_loss: 0.8313
[epoch 11][iter   10] loss: 214787.5156 RMSElog: 8.8500 grad_loss: 21469.2422 normal_loss: 0.6598
[epoch 11][iter   20] loss: 154761.1875 RMSElog: 8.8007 grad_loss: 15466.5801 normal_loss: 0.7375
[epoch 11][iter   30] loss: 188623.9375 RMSElog: 8.7905 grad_loss: 18852.9121 normal_loss: 0.6915
[epoch 11][iter   40] loss: 114297.0391 RMSElog: 8.5075 grad_loss: 11420.4736 normal_loss: 0.7230
[epoch 11][iter   50] loss: 177273.2500 RMSElog: 8.9028 grad_loss: 17717.7090 normal_loss: 0.7128
[epoch 11][iter   60] loss: 126854.2344 RMSElog: 8.5060 grad_loss: 12676.2646 normal_loss: 0.6535
[epoch 11][iter   70] loss: 129689.2109 RMSElog: 8.4865 grad_loss: 12959.7598 normal_loss: 0.6750
[epoch 11][iter   80] loss: 105877.4844 RMSElog: 8.4274 grad_loss: 10578.6387 normal_loss: 0.6813
[epoch 11][iter   90] loss: 194971.8750 RMSElog: 8.8553 grad_loss: 19487.6055 normal_loss: 0.7275
[epoch 11][iter  100] loss: 189521.9375 RMSElog: 8.7803 grad_loss: 18942.7402 normal_loss: 0.6711
[epoch 11][iter  110] loss: 147893.5625 RMSElog: 8.6234 grad_loss: 14780.0762 normal_loss: 0.6558
[epoch 11][iter  120] loss: 124615.2266 RMSElog: 8.5117 grad_loss: 12452.3340 normal_loss: 0.6764
[epoch 11][iter  130] loss: 100967.2344 RMSElog: 8.1292 grad_loss: 10087.9336 normal_loss: 0.6611
[epoch 11][iter  140] loss: 218748.2188 RMSElog: 9.3981 grad_loss: 21864.5293 normal_loss: 0.8937
[epoch 11][iter  150] loss: 175983.2812 RMSElog: 9.1527 grad_loss: 17588.2988 normal_loss: 0.8768
[epoch 11][iter  160] loss: 183994.9688 RMSElog: 8.5748 grad_loss: 18390.2266 normal_loss: 0.6949
[epoch 11][iter  170] loss: 141845.8750 RMSElog: 8.4132 grad_loss: 14175.4688 normal_loss: 0.7057
[epoch 11][iter  180] loss: 127482.7734 RMSElog: 8.2801 grad_loss: 12739.3076 normal_loss: 0.6897
[epoch 11][iter  190] loss: 121816.4062 RMSElog: 8.2788 grad_loss: 12172.6875 normal_loss: 0.6750
[epoch 11][iter  200] loss: 151703.8594 RMSElog: 8.7519 grad_loss: 15160.9863 normal_loss: 0.6472
[epoch 11][iter  210] loss: 176207.9062 RMSElog: 8.9985 grad_loss: 17611.0820 normal_loss: 0.7100
[epoch 11][iter  220] loss: 121968.9531 RMSElog: 8.4710 grad_loss: 12187.6904 normal_loss: 0.7339
[epoch 11][iter  230] loss: 147354.4844 RMSElog: 8.6036 grad_loss: 14726.1465 normal_loss: 0.6987
[epoch 11][iter  240] loss: 208165.2344 RMSElog: 9.1494 grad_loss: 20806.6895 normal_loss: 0.6856
[epoch 11][iter  250] loss: 203541.8438 RMSElog: 8.8069 grad_loss: 20344.7090 normal_loss: 0.6675
[epoch 11][iter  260] loss: 227304.1562 RMSElog: 9.1563 grad_loss: 22720.5977 normal_loss: 0.6622
[epoch 11][iter  270] loss: 107933.2031 RMSElog: 8.4019 grad_loss: 10784.2441 normal_loss: 0.6734
[epoch 11][iter  280] loss: 157793.2188 RMSElog: 8.7655 grad_loss: 15769.8428 normal_loss: 0.7130
[epoch 11][iter  290] loss: 107458.0156 RMSElog: 8.2176 grad_loss: 10736.8965 normal_loss: 0.6876
[epoch 11][iter  300] loss: 175282.5312 RMSElog: 8.6277 grad_loss: 17518.9375 normal_loss: 0.6889
[epoch 11][iter  310] loss: 167442.0625 RMSElog: 8.8054 grad_loss: 16734.6641 normal_loss: 0.7377
[epoch 11][iter  320] loss: 173145.9219 RMSElog: 8.6114 grad_loss: 17305.2793 normal_loss: 0.7003
[epoch 11][iter  330] loss: 121893.6562 RMSElog: 8.2697 grad_loss: 12180.4277 normal_loss: 0.6676
[epoch 11][iter  340] loss: 110566.8750 RMSElog: 8.6095 grad_loss: 11047.3691 normal_loss: 0.7093
[epoch 11][iter  350] loss: 195421.8125 RMSElog: 8.5835 grad_loss: 19532.9043 normal_loss: 0.6937
[epoch 11][iter  360] loss: 129143.3672 RMSElog: 8.6691 grad_loss: 12904.9707 normal_loss: 0.6970
[epoch 11][iter  370] loss: 147190.2188 RMSElog: 8.8451 grad_loss: 14709.5098 normal_loss: 0.6683
[epoch 11][iter  380] loss: 154839.4062 RMSElog: 8.8134 grad_loss: 15474.4297 normal_loss: 0.6981
[epoch 11][iter  390] loss: 135951.5469 RMSElog: 8.7270 grad_loss: 13585.7090 normal_loss: 0.7192
[epoch 11][iter  400] loss: 158861.3906 RMSElog: 8.6972 grad_loss: 15876.7266 normal_loss: 0.7159
[epoch 11][iter  410] loss: 138589.7031 RMSElog: 8.5791 grad_loss: 13849.6982 normal_loss: 0.6922
[epoch 11][iter  420] loss: 170674.5625 RMSElog: 8.9362 grad_loss: 17057.8145 normal_loss: 0.7069
[epoch 11][iter  430] loss: 148523.6875 RMSElog: 8.7982 grad_loss: 14842.8701 normal_loss: 0.7000
[epoch 11][iter  440] loss: 153480.9062 RMSElog: 8.8384 grad_loss: 15338.6045 normal_loss: 0.6463
[epoch 11][iter  450] loss: 129688.7891 RMSElog: 8.7209 grad_loss: 12959.4844 normal_loss: 0.6742
[epoch 11][iter  460] loss: 104455.9219 RMSElog: 8.0548 grad_loss: 10436.8936 normal_loss: 0.6435
[epoch 11][iter  470] loss: 133447.5781 RMSElog: 8.6112 grad_loss: 13335.4668 normal_loss: 0.6795
[epoch 11][iter  480] loss: 129724.6406 RMSElog: 8.7315 grad_loss: 12963.0625 normal_loss: 0.6699
[epoch 11][iter  490] loss: 188641.6562 RMSElog: 8.9863 grad_loss: 18854.5098 normal_loss: 0.6708
[epoch 11][iter  500] loss: 216210.5469 RMSElog: 8.8604 grad_loss: 21611.5312 normal_loss: 0.6626
[epoch 11][iter  510] loss: 134657.5938 RMSElog: 8.5776 grad_loss: 13456.5127 normal_loss: 0.6688
[epoch 11][iter  520] loss: 167067.7500 RMSElog: 8.8368 grad_loss: 16697.2402 normal_loss: 0.6995
[epoch 11][iter  530] loss: 148227.7188 RMSElog: 8.7543 grad_loss: 14813.3223 normal_loss: 0.6967
[epoch 11][iter  540] loss: 152500.3594 RMSElog: 8.7018 grad_loss: 15240.6299 normal_loss: 0.7045
[epoch 11][iter  550] loss: 130410.1406 RMSElog: 8.5363 grad_loss: 13031.8223 normal_loss: 0.6556
[epoch 11][iter  560] loss: 182296.1250 RMSElog: 8.9530 grad_loss: 18220.0137 normal_loss: 0.6459
[epoch 11][iter  570] loss: 126799.7812 RMSElog: 8.4662 grad_loss: 12670.7959 normal_loss: 0.7168
[epoch 11][iter  580] loss: 101431.2344 RMSElog: 8.6140 grad_loss: 10133.8291 normal_loss: 0.6802
[epoch 11][iter  590] loss: 188998.5312 RMSElog: 8.9084 grad_loss: 18890.2441 normal_loss: 0.7016
[epoch 12][iter    0] loss: 187216.1250 RMSElog: 8.8103 grad_loss: 18712.1367 normal_loss: 0.6666
[epoch 12][iter   10] loss: 172949.9844 RMSElog: 8.8979 grad_loss: 17285.4180 normal_loss: 0.6814
[epoch 12][iter   20] loss: 180188.0625 RMSElog: 8.7464 grad_loss: 18009.3750 normal_loss: 0.6856
[epoch 12][iter   30] loss: 122313.1406 RMSElog: 8.6250 grad_loss: 12222.0264 normal_loss: 0.6632
[epoch 12][iter   40] loss: 178453.3125 RMSElog: 8.9139 grad_loss: 17835.7051 normal_loss: 0.7128
[epoch 12][iter   50] loss: 177273.1094 RMSElog: 8.8978 grad_loss: 17717.7031 normal_loss: 0.7096
[epoch 12][iter   60] loss: 202097.0000 RMSElog: 8.7444 grad_loss: 20200.3008 normal_loss: 0.6549
[epoch 12][iter   70] loss: 140447.1250 RMSElog: 8.7906 grad_loss: 14035.0977 normal_loss: 0.8231
[epoch 12][iter   80] loss: 149352.1094 RMSElog: 8.7281 grad_loss: 14925.7402 normal_loss: 0.7425
[epoch 12][iter   90] loss: 129188.4297 RMSElog: 8.8110 grad_loss: 12909.3535 normal_loss: 0.6775
[epoch 12][iter  100] loss: 162503.6094 RMSElog: 8.9012 grad_loss: 16240.8018 normal_loss: 0.6581
[epoch 12][iter  110] loss: 106157.8594 RMSElog: 8.0448 grad_loss: 10607.0801 normal_loss: 0.6612
[epoch 12][iter  120] loss: 191187.4062 RMSElog: 8.8390 grad_loss: 19109.1758 normal_loss: 0.7247
[epoch 12][iter  130] loss: 172667.7188 RMSElog: 8.9277 grad_loss: 17257.1445 normal_loss: 0.6985
[epoch 12][iter  140] loss: 122367.6250 RMSElog: 8.5964 grad_loss: 12227.5195 normal_loss: 0.6468
[epoch 12][iter  150] loss: 116562.6953 RMSElog: 8.4575 grad_loss: 11647.1367 normal_loss: 0.6757
[epoch 12][iter  160] loss: 228318.5312 RMSElog: 9.0065 grad_loss: 22822.1836 normal_loss: 0.6636
[epoch 12][iter  170] loss: 146195.6562 RMSElog: 8.7254 grad_loss: 14610.1650 normal_loss: 0.6751
[epoch 12][iter  180] loss: 156154.2344 RMSElog: 8.8526 grad_loss: 15605.9277 normal_loss: 0.6433
[epoch 12][iter  190] loss: 128864.6172 RMSElog: 8.6691 grad_loss: 12877.1191 normal_loss: 0.6735
[epoch 12][iter  200] loss: 112692.9766 RMSElog: 8.1027 grad_loss: 11260.5195 normal_loss: 0.6753
[epoch 12][iter  210] loss: 211516.1250 RMSElog: 8.8029 grad_loss: 21142.1582 normal_loss: 0.6526
[epoch 12][iter  220] loss: 103500.6250 RMSElog: 8.5687 grad_loss: 10340.8164 normal_loss: 0.6774
[epoch 12][iter  230] loss: 188623.9219 RMSElog: 8.7945 grad_loss: 18852.9121 normal_loss: 0.6861
[epoch 12][iter  240] loss: 195422.5938 RMSElog: 8.6267 grad_loss: 19532.9336 normal_loss: 0.6994
[epoch 12][iter  250] loss: 177044.6562 RMSElog: 9.0152 grad_loss: 17694.7832 normal_loss: 0.6658
[epoch 12][iter  260] loss: 181926.4844 RMSElog: 8.9097 grad_loss: 18183.0039 normal_loss: 0.7348
[epoch 12][iter  270] loss: 239433.3750 RMSElog: 9.0796 grad_loss: 23933.5781 normal_loss: 0.6798
[epoch 12][iter  280] loss: 198015.2500 RMSElog: 8.8934 grad_loss: 19791.9648 normal_loss: 0.6681
[epoch 12][iter  290] loss: 180408.2500 RMSElog: 8.9460 grad_loss: 18031.1680 normal_loss: 0.7108
[epoch 12][iter  300] loss: 202609.5312 RMSElog: 8.7502 grad_loss: 20251.5391 normal_loss: 0.6643
[epoch 12][iter  310] loss: 170339.7812 RMSElog: 8.9142 grad_loss: 17024.3887 normal_loss: 0.6764
[epoch 12][iter  320] loss: 128302.1875 RMSElog: 8.6144 grad_loss: 12820.9336 normal_loss: 0.6710
[epoch 12][iter  330] loss: 167191.7500 RMSElog: 8.7906 grad_loss: 16709.7012 normal_loss: 0.6827
[epoch 12][iter  340] loss: 164397.3281 RMSElog: 8.9606 grad_loss: 16430.0820 normal_loss: 0.6902
[epoch 12][iter  350] loss: 135187.8594 RMSElog: 8.4655 grad_loss: 13509.6172 normal_loss: 0.7031
[epoch 12][iter  360] loss: 158211.1875 RMSElog: 8.7878 grad_loss: 15811.6709 normal_loss: 0.6596
[epoch 12][iter  370] loss: 166526.7188 RMSElog: 8.9654 grad_loss: 16643.0156 normal_loss: 0.6915
[epoch 12][iter  380] loss: 104457.4062 RMSElog: 8.1640 grad_loss: 10436.9297 normal_loss: 0.6469
[epoch 12][iter  390] loss: 160010.7500 RMSElog: 8.8461 grad_loss: 15991.4854 normal_loss: 0.7444
[epoch 12][iter  400] loss: 122909.7969 RMSElog: 8.5430 grad_loss: 12281.7139 normal_loss: 0.7225
[epoch 12][iter  410] loss: 102976.6562 RMSElog: 8.3307 grad_loss: 10288.6670 normal_loss: 0.6682
[epoch 12][iter  420] loss: 162875.6875 RMSElog: 8.8546 grad_loss: 16278.0410 normal_loss: 0.6742
[epoch 12][iter  430] loss: 140918.5625 RMSElog: 8.4413 grad_loss: 14082.7451 normal_loss: 0.6690
[epoch 12][iter  440] loss: 162522.0781 RMSElog: 8.7847 grad_loss: 16242.7617 normal_loss: 0.6612
[epoch 12][iter  450] loss: 162699.5625 RMSElog: 8.9206 grad_loss: 16260.3008 normal_loss: 0.7341
[epoch 12][iter  460] loss: 173063.2031 RMSElog: 8.9886 grad_loss: 17296.6816 normal_loss: 0.6513
[epoch 12][iter  470] loss: 180729.7188 RMSElog: 8.8359 grad_loss: 18063.4727 normal_loss: 0.6650
[epoch 12][iter  480] loss: 155464.5156 RMSElog: 8.9940 grad_loss: 15536.7959 normal_loss: 0.6619
[epoch 12][iter  490] loss: 135951.4062 RMSElog: 8.7123 grad_loss: 13585.7041 normal_loss: 0.7250
[epoch 12][iter  500] loss: 196416.1719 RMSElog: 8.8213 grad_loss: 19632.1328 normal_loss: 0.6628
[epoch 12][iter  510] loss: 144961.6719 RMSElog: 8.8872 grad_loss: 14486.6172 normal_loss: 0.6627
[epoch 12][iter  520] loss: 101202.9688 RMSElog: 8.6319 grad_loss: 10110.9668 normal_loss: 0.6986
[epoch 12][iter  530] loss: 169000.3438 RMSElog: 8.6925 grad_loss: 16890.5859 normal_loss: 0.7566
[epoch 12][iter  540] loss: 223855.5625 RMSElog: 9.1115 grad_loss: 22375.7637 normal_loss: 0.6808
[epoch 12][iter  550] loss: 196656.5312 RMSElog: 8.7326 grad_loss: 19656.2617 normal_loss: 0.6583
[epoch 12][iter  560] loss: 191673.2656 RMSElog: 8.9319 grad_loss: 19157.7031 normal_loss: 0.6911
[epoch 12][iter  570] loss: 183854.5312 RMSElog: 9.0360 grad_loss: 18375.7090 normal_loss: 0.7098
[epoch 12][iter  580] loss: 137237.0000 RMSElog: 8.9361 grad_loss: 13714.0332 normal_loss: 0.7292
[epoch 12][iter  590] loss: 103935.8125 RMSElog: 8.3448 grad_loss: 10384.5518 normal_loss: 0.6846
[epoch 13][iter    0] loss: 103500.0703 RMSElog: 8.5439 grad_loss: 10340.7949 normal_loss: 0.6684
[epoch 13][iter   10] loss: 181273.1094 RMSElog: 8.7460 grad_loss: 18117.8926 normal_loss: 0.6725
[epoch 13][iter   20] loss: 107882.7969 RMSElog: 8.0601 grad_loss: 10779.5625 normal_loss: 0.6558
[epoch 13][iter   30] loss: 165992.9375 RMSElog: 8.9054 grad_loss: 16589.7188 normal_loss: 0.6685
[epoch 13][iter   40] loss: 207897.7969 RMSElog: 8.8308 grad_loss: 20780.2949 normal_loss: 0.6552
[epoch 13][iter   50] loss: 152543.9062 RMSElog: 8.6826 grad_loss: 15245.0439 normal_loss: 0.6638
[epoch 13][iter   60] loss: 166989.2188 RMSElog: 8.8055 grad_loss: 16689.4316 normal_loss: 0.6848
[epoch 13][iter   70] loss: 146559.6562 RMSElog: 8.5718 grad_loss: 14646.6914 normal_loss: 0.7018
[epoch 13][iter   80] loss: 158861.2344 RMSElog: 8.6916 grad_loss: 15876.7207 normal_loss: 0.7105
[epoch 13][iter   90] loss: 194556.6406 RMSElog: 8.8043 grad_loss: 19446.1992 normal_loss: 0.6607
[epoch 13][iter  100] loss: 167664.1562 RMSElog: 8.9657 grad_loss: 16756.7090 normal_loss: 0.7430
[epoch 13][iter  110] loss: 195979.3125 RMSElog: 8.9595 grad_loss: 19588.3125 normal_loss: 0.6603
[epoch 13][iter  120] loss: 165956.9062 RMSElog: 8.7485 grad_loss: 16586.2930 normal_loss: 0.6497
[epoch 13][iter  130] loss: 180302.1250 RMSElog: 8.8814 grad_loss: 18020.6328 normal_loss: 0.6984
[epoch 13][iter  140] loss: 106747.5391 RMSElog: 8.3114 grad_loss: 10665.7656 normal_loss: 0.6770
[epoch 13][iter  150] loss: 133005.2656 RMSElog: 8.5117 grad_loss: 13291.3789 normal_loss: 0.6353
[epoch 13][iter  160] loss: 183462.2031 RMSElog: 8.8559 grad_loss: 18336.6875 normal_loss: 0.6785
[epoch 13][iter  170] loss: 195393.8281 RMSElog: 8.9391 grad_loss: 19529.7793 normal_loss: 0.6640
[epoch 13][iter  180] loss: 189721.9688 RMSElog: 8.8338 grad_loss: 18962.7051 normal_loss: 0.6590
[epoch 13][iter  190] loss: 105717.4609 RMSElog: 8.0004 grad_loss: 10563.0801 normal_loss: 0.6656
[epoch 13][iter  200] loss: 164750.2969 RMSElog: 8.8200 grad_loss: 16465.4980 normal_loss: 0.7114
[epoch 13][iter  210] loss: 117051.4766 RMSElog: 8.6430 grad_loss: 11695.7754 normal_loss: 0.7299
[epoch 13][iter  220] loss: 180188.7500 RMSElog: 8.7657 grad_loss: 18009.4121 normal_loss: 0.6965
[epoch 13][iter  230] loss: 241952.9844 RMSElog: 9.0427 grad_loss: 24185.5664 normal_loss: 0.6896
[epoch 13][iter  240] loss: 114364.2266 RMSElog: 8.6081 grad_loss: 11427.0781 normal_loss: 0.7366
[epoch 13][iter  250] loss: 195651.4062 RMSElog: 8.8086 grad_loss: 19555.6543 normal_loss: 0.6785
[epoch 13][iter  260] loss: 100646.7188 RMSElog: 8.7987 grad_loss: 10055.1914 normal_loss: 0.6819
[epoch 13][iter  270] loss: 153704.4062 RMSElog: 8.7929 grad_loss: 15360.9941 normal_loss: 0.6542
[epoch 13][iter  280] loss: 90467.7500 RMSElog: 8.3913 grad_loss: 9037.6963 normal_loss: 0.6871
[epoch 13][iter  290] loss: 147525.4062 RMSElog: 8.7388 grad_loss: 14743.1387 normal_loss: 0.6622
[epoch 13][iter  300] loss: 178766.4688 RMSElog: 8.8836 grad_loss: 17867.0625 normal_loss: 0.7008
[epoch 13][iter  310] loss: 150282.4062 RMSElog: 8.6627 grad_loss: 15018.9199 normal_loss: 0.6568
[epoch 13][iter  320] loss: 114487.0625 RMSElog: 8.2446 grad_loss: 11439.7871 normal_loss: 0.6752
[epoch 13][iter  330] loss: 149524.8750 RMSElog: 8.8392 grad_loss: 14942.9600 normal_loss: 0.6894
[epoch 13][iter  340] loss: 163234.3906 RMSElog: 8.7295 grad_loss: 16314.0420 normal_loss: 0.6684
[epoch 13][iter  350] loss: 159323.0625 RMSElog: 8.8861 grad_loss: 15922.7266 normal_loss: 0.6945
[epoch 13][iter  360] loss: 150681.2188 RMSElog: 8.7331 grad_loss: 15058.6934 normal_loss: 0.6953
[epoch 13][iter  370] loss: 149789.4375 RMSElog: 8.7784 grad_loss: 14969.4668 normal_loss: 0.6990
[epoch 13][iter  380] loss: 197500.2812 RMSElog: 8.9172 grad_loss: 19740.4473 normal_loss: 0.6613
[epoch 13][iter  390] loss: 203000.5938 RMSElog: 8.8337 grad_loss: 20290.5703 normal_loss: 0.6544
[epoch 13][iter  400] loss: 138694.2500 RMSElog: 8.5484 grad_loss: 13860.1426 normal_loss: 0.7341
[epoch 13][iter  410] loss: 160862.9844 RMSElog: 8.8426 grad_loss: 16076.7881 normal_loss: 0.6674
[epoch 13][iter  420] loss: 105771.0781 RMSElog: 8.1136 grad_loss: 10568.3418 normal_loss: 0.6521
[epoch 13][iter  430] loss: 200933.9219 RMSElog: 8.8634 grad_loss: 20083.8340 normal_loss: 0.6944
[epoch 13][iter  440] loss: 134614.5469 RMSElog: 8.7116 grad_loss: 13452.0625 normal_loss: 0.6802
[epoch 13][iter  450] loss: 201482.4219 RMSElog: 8.9603 grad_loss: 20138.6074 normal_loss: 0.6733
[epoch 13][iter  460] loss: 131922.2188 RMSElog: 8.5448 grad_loss: 13183.0234 normal_loss: 0.6536
[epoch 13][iter  470] loss: 110566.8359 RMSElog: 8.6109 grad_loss: 11047.3691 normal_loss: 0.7034
[epoch 13][iter  480] loss: 239748.2812 RMSElog: 8.9721 grad_loss: 23965.1836 normal_loss: 0.6713
[epoch 13][iter  490] loss: 116051.5547 RMSElog: 8.2503 grad_loss: 11596.2402 normal_loss: 0.6647
[epoch 13][iter  500] loss: 147893.4531 RMSElog: 8.6205 grad_loss: 14780.0703 normal_loss: 0.6546
[epoch 13][iter  510] loss: 130507.4219 RMSElog: 8.6106 grad_loss: 13041.4756 normal_loss: 0.6563
[epoch 13][iter  520] loss: 130725.8203 RMSElog: 8.7264 grad_loss: 13063.1211 normal_loss: 0.7343
[epoch 13][iter  530] loss: 188623.8750 RMSElog: 8.7906 grad_loss: 18852.9082 normal_loss: 0.6866
[epoch 13][iter  540] loss: 142526.8594 RMSElog: 9.0247 grad_loss: 14242.9824 normal_loss: 0.6794
[epoch 13][iter  550] loss: 179617.5156 RMSElog: 9.0090 grad_loss: 17952.0312 normal_loss: 0.7106
[epoch 13][iter  560] loss: 167714.5938 RMSElog: 8.9874 grad_loss: 16761.7891 normal_loss: 0.6807
[epoch 13][iter  570] loss: 104066.9688 RMSElog: 8.5009 grad_loss: 10397.5254 normal_loss: 0.6706
[epoch 13][iter  580] loss: 148118.4375 RMSElog: 8.7477 grad_loss: 14802.3926 normal_loss: 0.7031
[epoch 13][iter  590] loss: 210634.3594 RMSElog: 9.1506 grad_loss: 21053.5957 normal_loss: 0.6900
[epoch 14][iter    0] loss: 176244.6250 RMSElog: 9.0363 grad_loss: 17614.7285 normal_loss: 0.6975
[epoch 14][iter   10] loss: 112517.5000 RMSElog: 8.4821 grad_loss: 11242.6064 normal_loss: 0.6612
[epoch 14][iter   20] loss: 211905.6406 RMSElog: 8.8651 grad_loss: 21181.0371 normal_loss: 0.6629
[epoch 14][iter   30] loss: 208165.0312 RMSElog: 9.1418 grad_loss: 20806.6816 normal_loss: 0.6804
[epoch 14][iter   40] loss: 200934.3750 RMSElog: 8.9077 grad_loss: 20083.8418 normal_loss: 0.6884
[epoch 14][iter   50] loss: 211618.8125 RMSElog: 8.9323 grad_loss: 21152.2871 normal_loss: 0.6620
[epoch 14][iter   60] loss: 107882.7812 RMSElog: 8.0606 grad_loss: 10779.5625 normal_loss: 0.6549
[epoch 14][iter   70] loss: 174359.6250 RMSElog: 9.0466 grad_loss: 17426.2148 normal_loss: 0.7002
[epoch 14][iter   80] loss: 198272.0469 RMSElog: 8.8034 grad_loss: 19817.7422 normal_loss: 0.6593
[epoch 14][iter   90] loss: 179283.8281 RMSElog: 8.7672 grad_loss: 17918.8906 normal_loss: 0.7241
[epoch 14][iter  100] loss: 121168.4219 RMSElog: 8.7423 grad_loss: 12107.3535 normal_loss: 0.7461
[epoch 14][iter  110] loss: 162961.0312 RMSElog: 8.6544 grad_loss: 16286.7598 normal_loss: 0.6881
[epoch 14][iter  120] loss: 232751.7188 RMSElog: 9.0215 grad_loss: 23265.4785 normal_loss: 0.6716
[epoch 14][iter  130] loss: 185884.1875 RMSElog: 8.7816 grad_loss: 18578.9375 normal_loss: 0.6987
[epoch 14][iter  140] loss: 129143.3906 RMSElog: 8.6838 grad_loss: 12904.9668 normal_loss: 0.6886
[epoch 14][iter  150] loss: 179884.2188 RMSElog: 8.7763 grad_loss: 17978.9473 normal_loss: 0.6984
[epoch 14][iter  160] loss: 117385.7656 RMSElog: 8.3892 grad_loss: 11729.5273 normal_loss: 0.6596
[epoch 14][iter  170] loss: 133447.5000 RMSElog: 8.6087 grad_loss: 13335.4639 normal_loss: 0.6780
[epoch 14][iter  180] loss: 156980.5312 RMSElog: 9.2238 grad_loss: 15687.9600 normal_loss: 0.8689
[epoch 14][iter  190] loss: 122366.7969 RMSElog: 8.5435 grad_loss: 12227.4893 normal_loss: 0.6463
[epoch 14][iter  200] loss: 101431.1406 RMSElog: 8.6136 grad_loss: 10133.8252 normal_loss: 0.6755
[epoch 14][iter  210] loss: 207666.5469 RMSElog: 8.8834 grad_loss: 20757.0703 normal_loss: 0.7011
[epoch 14][iter  220] loss: 174613.2031 RMSElog: 8.8340 grad_loss: 17451.7754 normal_loss: 0.7113
[epoch 14][iter  230] loss: 126005.4844 RMSElog: 8.4699 grad_loss: 12591.3975 normal_loss: 0.6814
[epoch 14][iter  240] loss: 127043.1094 RMSElog: 8.4142 grad_loss: 12695.2041 normal_loss: 0.6928
[epoch 14][iter  250] loss: 115646.4766 RMSElog: 8.5353 grad_loss: 11555.3809 normal_loss: 0.7312
[epoch 14][iter  260] loss: 185755.5312 RMSElog: 8.9251 grad_loss: 18565.9141 normal_loss: 0.7127
[epoch 14][iter  270] loss: 211516.8438 RMSElog: 8.8383 grad_loss: 21142.1855 normal_loss: 0.6603
[epoch 14][iter  280] loss: 215933.7188 RMSElog: 8.9224 grad_loss: 21583.7988 normal_loss: 0.6496
[epoch 14][iter  290] loss: 178766.4688 RMSElog: 8.8823 grad_loss: 17867.0625 normal_loss: 0.7007
[epoch 14][iter  300] loss: 196656.5469 RMSElog: 8.7299 grad_loss: 19656.2637 normal_loss: 0.6596
[epoch 14][iter  310] loss: 165557.5312 RMSElog: 8.7978 grad_loss: 16546.2734 normal_loss: 0.6829
[epoch 14][iter  320] loss: 122894.3594 RMSElog: 8.6538 grad_loss: 12280.1016 normal_loss: 0.6802
[epoch 14][iter  330] loss: 133005.2812 RMSElog: 8.5115 grad_loss: 13291.3809 normal_loss: 0.6348
[epoch 14][iter  340] loss: 133025.9688 RMSElog: 8.6303 grad_loss: 13293.3125 normal_loss: 0.6551
[epoch 14][iter  350] loss: 123514.8828 RMSElog: 8.6978 grad_loss: 12342.0312 normal_loss: 0.7584
[epoch 14][iter  360] loss: 173947.8438 RMSElog: 8.7614 grad_loss: 17385.3262 normal_loss: 0.6979
[epoch 14][iter  370] loss: 172739.7656 RMSElog: 8.9972 grad_loss: 17264.2676 normal_loss: 0.7102
[epoch 14][iter  380] loss: 95986.4453 RMSElog: 8.3192 grad_loss: 9589.6846 normal_loss: 0.6405
[epoch 14][iter  390] loss: 160727.8438 RMSElog: 8.9889 grad_loss: 16063.0938 normal_loss: 0.7024
[epoch 14][iter  400] loss: 165632.8906 RMSElog: 8.7824 grad_loss: 16553.8047 normal_loss: 0.7013
[epoch 14][iter  410] loss: 114903.2031 RMSElog: 8.2717 grad_loss: 11481.3828 normal_loss: 0.6665
[epoch 14][iter  420] loss: 206505.7969 RMSElog: 8.8503 grad_loss: 20641.0762 normal_loss: 0.6545
[epoch 14][iter  430] loss: 185127.0469 RMSElog: 8.9029 grad_loss: 18503.0625 normal_loss: 0.7409
[epoch 14][iter  440] loss: 198014.2969 RMSElog: 8.8373 grad_loss: 19791.9258 normal_loss: 0.6654
[epoch 14][iter  450] loss: 103486.1562 RMSElog: 8.3666 grad_loss: 10339.5879 normal_loss: 0.6608
[epoch 14][iter  460] loss: 72273.2969 RMSElog: 8.4745 grad_loss: 7218.1924 normal_loss: 0.6627
[epoch 14][iter  470] loss: 177043.3594 RMSElog: 8.9488 grad_loss: 17694.7344 normal_loss: 0.6519
[epoch 14][iter  480] loss: 154539.2812 RMSElog: 8.9228 grad_loss: 15444.3154 normal_loss: 0.6890
[epoch 14][iter  490] loss: 114363.7500 RMSElog: 8.5957 grad_loss: 11427.0527 normal_loss: 0.7263
[epoch 14][iter  500] loss: 182845.3438 RMSElog: 8.9947 grad_loss: 18274.8086 normal_loss: 0.7317
[epoch 14][iter  510] loss: 157792.9844 RMSElog: 8.7550 grad_loss: 15769.8340 normal_loss: 0.7091
[epoch 14][iter  520] loss: 195393.8125 RMSElog: 8.9382 grad_loss: 19529.7793 normal_loss: 0.6639
[epoch 14][iter  530] loss: 164827.5156 RMSElog: 8.7576 grad_loss: 16473.3105 normal_loss: 0.6843
[epoch 14][iter  540] loss: 228318.4688 RMSElog: 9.0047 grad_loss: 22822.1797 normal_loss: 0.6632
[epoch 14][iter  550] loss: 151386.5781 RMSElog: 8.5011 grad_loss: 15129.4385 normal_loss: 0.7179
[epoch 14][iter  560] loss: 140260.8750 RMSElog: 8.5601 grad_loss: 14016.8750 normal_loss: 0.6528
[epoch 14][iter  570] loss: 200447.7969 RMSElog: 8.7678 grad_loss: 20035.3613 normal_loss: 0.6510
[epoch 14][iter  580] loss: 197694.9375 RMSElog: 9.1366 grad_loss: 19759.6895 normal_loss: 0.6678
[epoch 14][iter  590] loss: 181273.1094 RMSElog: 8.7464 grad_loss: 18117.8926 normal_loss: 0.6720
[epoch 15][iter    0] loss: 172667.6719 RMSElog: 8.9281 grad_loss: 17257.1445 normal_loss: 0.6962
[epoch 15][iter   10] loss: 166070.9219 RMSElog: 8.9520 grad_loss: 16597.4805 normal_loss: 0.6592
[epoch 15][iter   20] loss: 182296.0781 RMSElog: 8.9390 grad_loss: 18220.0156 normal_loss: 0.6521
[epoch 15][iter   30] loss: 194556.5938 RMSElog: 8.7998 grad_loss: 19446.1992 normal_loss: 0.6621
[epoch 15][iter   40] loss: 106493.6875 RMSElog: 8.0558 grad_loss: 10640.6465 normal_loss: 0.6670
[epoch 15][iter   50] loss: 140261.3594 RMSElog: 8.5881 grad_loss: 14016.8896 normal_loss: 0.6579
[epoch 15][iter   60] loss: 122661.7656 RMSElog: 8.7036 grad_loss: 12256.6172 normal_loss: 0.8554
[epoch 15][iter   70] loss: 161614.2500 RMSElog: 8.9002 grad_loss: 16151.8662 normal_loss: 0.6594
[epoch 15][iter   80] loss: 103486.1328 RMSElog: 8.3680 grad_loss: 10339.5859 normal_loss: 0.6594
[epoch 15][iter   90] loss: 121849.1406 RMSElog: 8.5327 grad_loss: 12175.6592 normal_loss: 0.7229
[epoch 15][iter  100] loss: 160767.9375 RMSElog: 8.8132 grad_loss: 16067.2607 normal_loss: 0.7195
[epoch 15][iter  110] loss: 119637.5078 RMSElog: 8.5607 grad_loss: 11954.5459 normal_loss: 0.6443
[epoch 15][iter  120] loss: 174331.3906 RMSElog: 8.7529 grad_loss: 17423.6973 normal_loss: 0.6885
[epoch 15][iter  130] loss: 155892.5469 RMSElog: 8.6509 grad_loss: 15579.9492 normal_loss: 0.6545
[epoch 15][iter  140] loss: 162875.6875 RMSElog: 8.8551 grad_loss: 16278.0391 normal_loss: 0.6744
[epoch 15][iter  150] loss: 211905.6406 RMSElog: 8.8657 grad_loss: 21181.0371 normal_loss: 0.6626
[epoch 15][iter  160] loss: 103935.7891 RMSElog: 8.3438 grad_loss: 10384.5518 normal_loss: 0.6836
[epoch 15][iter  170] loss: 160862.9531 RMSElog: 8.8422 grad_loss: 16076.7881 normal_loss: 0.6656
[epoch 15][iter  180] loss: 198424.0312 RMSElog: 8.9044 grad_loss: 19832.8262 normal_loss: 0.6717
[epoch 15][iter  190] loss: 175282.4219 RMSElog: 8.6144 grad_loss: 17518.9375 normal_loss: 0.6900
[epoch 15][iter  200] loss: 149352.0625 RMSElog: 8.7272 grad_loss: 14925.7373 normal_loss: 0.7416
[epoch 15][iter  210] loss: 158908.4219 RMSElog: 8.7991 grad_loss: 15881.3486 normal_loss: 0.6947
[epoch 15][iter  220] loss: 208165.0156 RMSElog: 9.1423 grad_loss: 20806.6797 normal_loss: 0.6805
[epoch 15][iter  230] loss: 217153.6094 RMSElog: 8.9233 grad_loss: 21705.7793 normal_loss: 0.6582
[epoch 15][iter  240] loss: 152500.6875 RMSElog: 8.7250 grad_loss: 15240.6426 normal_loss: 0.7014
[epoch 15][iter  250] loss: 160034.2656 RMSElog: 8.8068 grad_loss: 15993.9199 normal_loss: 0.6999
[epoch 15][iter  260] loss: 224057.1875 RMSElog: 9.0583 grad_loss: 22395.9746 normal_loss: 0.6848
[epoch 15][iter  270] loss: 146196.0000 RMSElog: 8.7449 grad_loss: 14610.1768 normal_loss: 0.6781
[epoch 15][iter  280] loss: 199053.1094 RMSElog: 8.9062 grad_loss: 19895.7012 normal_loss: 0.7022
[epoch 15][iter  290] loss: 160010.4062 RMSElog: 8.8236 grad_loss: 15991.4756 normal_loss: 0.7414
[epoch 15][iter  300] loss: 239747.5312 RMSElog: 8.9358 grad_loss: 23965.1543 normal_loss: 0.6635
[epoch 15][iter  310] loss: 181446.1562 RMSElog: 8.8763 grad_loss: 18135.0469 normal_loss: 0.6923
[epoch 15][iter  320] loss: 146764.4062 RMSElog: 8.7690 grad_loss: 14666.9785 normal_loss: 0.6937
[epoch 15][iter  330] loss: 116051.5312 RMSElog: 8.2492 grad_loss: 11596.2393 normal_loss: 0.6646
[epoch 15][iter  340] loss: 160649.1719 RMSElog: 8.8642 grad_loss: 16055.3896 normal_loss: 0.6627
[epoch 15][iter  350] loss: 203071.5781 RMSElog: 8.7381 grad_loss: 20297.7695 normal_loss: 0.6503
[epoch 15][iter  360] loss: 122909.7969 RMSElog: 8.5439 grad_loss: 12281.7139 normal_loss: 0.7213
[epoch 15][iter  370] loss: 161267.1406 RMSElog: 8.7761 grad_loss: 16117.2793 normal_loss: 0.6579
[epoch 15][iter  380] loss: 194387.0000 RMSElog: 8.8345 grad_loss: 19429.1953 normal_loss: 0.6699
[epoch 15][iter  390] loss: 127445.5859 RMSElog: 8.4088 grad_loss: 12735.4844 normal_loss: 0.6648
[epoch 15][iter  400] loss: 160727.2188 RMSElog: 8.9459 grad_loss: 16063.0752 normal_loss: 0.7015
[epoch 15][iter  410] loss: 173145.7812 RMSElog: 8.6024 grad_loss: 17305.2754 normal_loss: 0.7011
[epoch 15][iter  420] loss: 201483.0625 RMSElog: 9.0032 grad_loss: 20138.6289 normal_loss: 0.6741
[epoch 15][iter  430] loss: 148118.4375 RMSElog: 8.7487 grad_loss: 14802.3926 normal_loss: 0.7023
[epoch 15][iter  440] loss: 172739.2031 RMSElog: 8.9668 grad_loss: 17264.2500 normal_loss: 0.7029
[epoch 15][iter  450] loss: 203540.6562 RMSElog: 8.7428 grad_loss: 20344.6699 normal_loss: 0.6549
[epoch 15][iter  460] loss: 131415.9688 RMSElog: 8.4647 grad_loss: 13132.4561 normal_loss: 0.6771
[epoch 15][iter  470] loss: 164750.3281 RMSElog: 8.8245 grad_loss: 16465.4980 normal_loss: 0.7104
[epoch 15][iter  480] loss: 109082.9297 RMSElog: 8.5145 grad_loss: 10899.0967 normal_loss: 0.6812
[epoch 15][iter  490] loss: 172949.9688 RMSElog: 8.8987 grad_loss: 17285.4180 normal_loss: 0.6801
[epoch 15][iter  500] loss: 165937.0312 RMSElog: 8.8814 grad_loss: 16584.1152 normal_loss: 0.7066
[epoch 15][iter  510] loss: 176477.0000 RMSElog: 8.8034 grad_loss: 17638.2051 normal_loss: 0.6906
[epoch 15][iter  520] loss: 134656.9375 RMSElog: 8.5344 grad_loss: 13456.4941 normal_loss: 0.6664
[epoch 15][iter  530] loss: 197500.1719 RMSElog: 8.9175 grad_loss: 19740.4414 normal_loss: 0.6588
[epoch 15][iter  540] loss: 115325.8516 RMSElog: 8.2718 grad_loss: 11523.6484 normal_loss: 0.6651
[epoch 15][iter  550] loss: 167723.1875 RMSElog: 8.8413 grad_loss: 16762.7812 normal_loss: 0.6958
[epoch 15][iter  560] loss: 163758.2344 RMSElog: 8.6696 grad_loss: 16366.4443 normal_loss: 0.7089
[epoch 15][iter  570] loss: 121816.2891 RMSElog: 8.2718 grad_loss: 12172.6816 normal_loss: 0.6757
[epoch 15][iter  580] loss: 108399.2500 RMSElog: 8.5990 grad_loss: 10830.5977 normal_loss: 0.7284
[epoch 15][iter  590] loss: 206741.2812 RMSElog: 8.7868 grad_loss: 20664.6836 normal_loss: 0.6574
[epoch 16][iter    0] loss: 151349.9688 RMSElog: 8.5962 grad_loss: 15125.7090 normal_loss: 0.6915
[epoch 16][iter   10] loss: 124228.9766 RMSElog: 8.6086 grad_loss: 12413.6055 normal_loss: 0.6837
[epoch 16][iter   20] loss: 129689.1328 RMSElog: 8.4822 grad_loss: 12959.7559 normal_loss: 0.6749
[epoch 16][iter   30] loss: 102075.9688 RMSElog: 8.4047 grad_loss: 10198.5273 normal_loss: 0.6653
[epoch 16][iter   40] loss: 144174.5781 RMSElog: 8.6181 grad_loss: 14408.1641 normal_loss: 0.6759
[epoch 16][iter   50] loss: 135094.0156 RMSElog: 8.6152 grad_loss: 13500.1348 normal_loss: 0.6517
[epoch 16][iter   60] loss: 183994.7031 RMSElog: 8.5626 grad_loss: 18390.2168 normal_loss: 0.6922
[epoch 16][iter   70] loss: 148035.5312 RMSElog: 8.6579 grad_loss: 14794.2461 normal_loss: 0.6490
[epoch 16][iter   80] loss: 207666.5312 RMSElog: 8.8813 grad_loss: 20757.0703 normal_loss: 0.7015
[epoch 16][iter   90] loss: 220675.8594 RMSElog: 8.8768 grad_loss: 22058.0195 normal_loss: 0.6900
[epoch 16][iter  100] loss: 158908.4219 RMSElog: 8.7990 grad_loss: 15881.3486 normal_loss: 0.6951
[epoch 16][iter  110] loss: 135448.4688 RMSElog: 8.7803 grad_loss: 13535.3926 normal_loss: 0.6746
[epoch 16][iter  120] loss: 104316.5312 RMSElog: 8.0535 grad_loss: 10422.9385 normal_loss: 0.6614
[epoch 16][iter  130] loss: 108776.7188 RMSElog: 8.1169 grad_loss: 10868.8848 normal_loss: 0.6696
[epoch 16][iter  140] loss: 175528.9688 RMSElog: 8.8619 grad_loss: 17543.3555 normal_loss: 0.6789
[epoch 16][iter  150] loss: 229061.6250 RMSElog: 9.0453 grad_loss: 22896.4453 normal_loss: 0.6718
[epoch 16][iter  160] loss: 160419.6250 RMSElog: 8.8807 grad_loss: 16032.4277 normal_loss: 0.6538
[epoch 16][iter  170] loss: 191673.2188 RMSElog: 8.9318 grad_loss: 19157.6992 normal_loss: 0.6908
[epoch 16][iter  180] loss: 151703.7656 RMSElog: 8.7516 grad_loss: 15160.9805 normal_loss: 0.6439
[epoch 16][iter  190] loss: 143822.8906 RMSElog: 8.7403 grad_loss: 14372.8730 normal_loss: 0.6756
[epoch 16][iter  200] loss: 106157.8203 RMSElog: 8.0439 grad_loss: 10607.0791 normal_loss: 0.6594
[epoch 16][iter  210] loss: 138915.3281 RMSElog: 8.6295 grad_loss: 13882.2441 normal_loss: 0.6580
[epoch 16][iter  220] loss: 129143.3750 RMSElog: 8.6826 grad_loss: 12904.9668 normal_loss: 0.6888
[epoch 16][iter  230] loss: 120269.9297 RMSElog: 8.2877 grad_loss: 12018.0381 normal_loss: 0.6668
[epoch 16][iter  240] loss: 149789.4062 RMSElog: 8.7779 grad_loss: 14969.4648 normal_loss: 0.6978
[epoch 16][iter  250] loss: 112693.5156 RMSElog: 8.1237 grad_loss: 11260.5488 normal_loss: 0.6788
[epoch 16][iter  260] loss: 182845.8125 RMSElog: 9.0188 grad_loss: 18274.8281 normal_loss: 0.7339
[epoch 16][iter  270] loss: 130124.6875 RMSElog: 8.3252 grad_loss: 13003.4717 normal_loss: 0.6719
[epoch 16][iter  280] loss: 103332.5859 RMSElog: 8.1088 grad_loss: 10324.4961 normal_loss: 0.6544
[epoch 16][iter  290] loss: 167191.7031 RMSElog: 8.7906 grad_loss: 16709.6992 normal_loss: 0.6805
[epoch 16][iter  300] loss: 196899.1406 RMSElog: 8.9420 grad_loss: 19680.3164 normal_loss: 0.6569
[epoch 16][iter  310] loss: 175282.4219 RMSElog: 8.6134 grad_loss: 17518.9375 normal_loss: 0.6905
[epoch 16][iter  320] loss: 100625.4062 RMSElog: 8.0334 grad_loss: 10053.8691 normal_loss: 0.6390
[epoch 16][iter  330] loss: 163234.3438 RMSElog: 8.7252 grad_loss: 16314.0410 normal_loss: 0.6684
[epoch 16][iter  340] loss: 154289.1250 RMSElog: 8.6206 grad_loss: 15419.6436 normal_loss: 0.6498
[epoch 16][iter  350] loss: 122427.3516 RMSElog: 8.3552 grad_loss: 12233.6895 normal_loss: 0.6906
[epoch 16][iter  360] loss: 195650.6250 RMSElog: 8.7659 grad_loss: 19555.6211 normal_loss: 0.6754
[epoch 16][iter  370] loss: 161794.1250 RMSElog: 8.8732 grad_loss: 16169.8848 normal_loss: 0.6548
[epoch 16][iter  380] loss: 138041.4844 RMSElog: 8.5296 grad_loss: 13794.9404 normal_loss: 0.6782
[epoch 16][iter  390] loss: 166072.2812 RMSElog: 9.0294 grad_loss: 16597.5312 normal_loss: 0.6686
[epoch 16][iter  400] loss: 107882.7656 RMSElog: 8.0581 grad_loss: 10779.5625 normal_loss: 0.6556
[epoch 16][iter  410] loss: 100729.4688 RMSElog: 8.6357 grad_loss: 10063.6436 normal_loss: 0.6681
[epoch 16][iter  420] loss: 218930.3906 RMSElog: 9.1557 grad_loss: 21883.2051 normal_loss: 0.6775
[epoch 16][iter  430] loss: 183462.1875 RMSElog: 8.8545 grad_loss: 18336.6855 normal_loss: 0.6781
[epoch 16][iter  440] loss: 198424.0312 RMSElog: 8.9034 grad_loss: 19832.8262 normal_loss: 0.6724
[epoch 16][iter  450] loss: 172739.2344 RMSElog: 8.9666 grad_loss: 17264.2539 normal_loss: 0.7028
[epoch 16][iter  460] loss: 90467.2656 RMSElog: 8.3474 grad_loss: 9037.6865 normal_loss: 0.6924
[epoch 16][iter  470] loss: 104433.9219 RMSElog: 8.3996 grad_loss: 10434.3340 normal_loss: 0.6587
[epoch 16][iter  480] loss: 191802.7344 RMSElog: 8.8166 grad_loss: 19170.7988 normal_loss: 0.6579
[epoch 16][iter  490] loss: 112673.8594 RMSElog: 8.1476 grad_loss: 11258.5752 normal_loss: 0.6633
[epoch 16][iter  500] loss: 146141.9844 RMSElog: 8.5806 grad_loss: 14604.8838 normal_loss: 0.7334
[epoch 16][iter  510] loss: 199586.9062 RMSElog: 8.8736 grad_loss: 19949.1504 normal_loss: 0.6687
[epoch 16][iter  520] loss: 149114.0312 RMSElog: 8.7798 grad_loss: 14901.9551 normal_loss: 0.6679
[epoch 16][iter  530] loss: 115480.5156 RMSElog: 8.5633 grad_loss: 11538.8145 normal_loss: 0.6743
[epoch 16][iter  540] loss: 127755.9531 RMSElog: 8.6391 grad_loss: 12766.2754 normal_loss: 0.6815
[epoch 16][iter  550] loss: 154538.2188 RMSElog: 8.8579 grad_loss: 15444.2832 normal_loss: 0.6810
[epoch 16][iter  560] loss: 137237.0000 RMSElog: 8.9378 grad_loss: 13714.0332 normal_loss: 0.7281
[epoch 16][iter  570] loss: 189721.9688 RMSElog: 8.8330 grad_loss: 18962.7051 normal_loss: 0.6587
[epoch 16][iter  580] loss: 180642.4844 RMSElog: 8.9093 grad_loss: 18054.6523 normal_loss: 0.6856
[epoch 16][iter  590] loss: 161266.5312 RMSElog: 8.7469 grad_loss: 16117.2510 normal_loss: 0.6558
[epoch 17][iter    0] loss: 202609.5312 RMSElog: 8.7483 grad_loss: 20251.5391 normal_loss: 0.6655
[epoch 17][iter   10] loss: 105770.4688 RMSElog: 8.0808 grad_loss: 10568.3213 normal_loss: 0.6441
[epoch 17][iter   20] loss: 198424.0312 RMSElog: 8.9030 grad_loss: 19832.8262 normal_loss: 0.6732
[epoch 17][iter   30] loss: 163759.1719 RMSElog: 8.7171 grad_loss: 16366.4805 normal_loss: 0.7193
[epoch 17][iter   40] loss: 109237.2734 RMSElog: 8.1297 grad_loss: 10914.9277 normal_loss: 0.6699
[epoch 17][iter   50] loss: 129149.6094 RMSElog: 8.7458 grad_loss: 12905.5508 normal_loss: 0.6637
[epoch 17][iter   60] loss: 154289.1406 RMSElog: 8.6212 grad_loss: 15419.6436 normal_loss: 0.6497
[epoch 17][iter   70] loss: 199052.5625 RMSElog: 8.8702 grad_loss: 19895.6797 normal_loss: 0.7056
[epoch 17][iter   80] loss: 216489.4375 RMSElog: 8.9428 grad_loss: 21639.3438 normal_loss: 0.6567
[epoch 17][iter   90] loss: 138915.9531 RMSElog: 8.6717 grad_loss: 13882.2598 normal_loss: 0.6627
[epoch 17][iter  100] loss: 176780.8750 RMSElog: 8.7703 grad_loss: 17668.6289 normal_loss: 0.6885
[epoch 17][iter  110] loss: 104110.5469 RMSElog: 8.5507 grad_loss: 10401.8262 normal_loss: 0.6782
[epoch 17][iter  120] loss: 110210.7734 RMSElog: 8.0520 grad_loss: 11012.3691 normal_loss: 0.6567
[epoch 17][iter  130] loss: 174612.8438 RMSElog: 8.8098 grad_loss: 17451.7617 normal_loss: 0.7134
[epoch 17][iter  140] loss: 178453.2812 RMSElog: 8.9124 grad_loss: 17835.7051 normal_loss: 0.7108
[epoch 17][iter  150] loss: 198724.3438 RMSElog: 8.8911 grad_loss: 19862.8418 normal_loss: 0.7007
[epoch 17][iter  160] loss: 135093.4531 RMSElog: 8.5742 grad_loss: 13500.1182 normal_loss: 0.6538
[epoch 17][iter  170] loss: 153703.5312 RMSElog: 8.7528 grad_loss: 15360.9609 normal_loss: 0.6391
[epoch 17][iter  180] loss: 163234.3438 RMSElog: 8.7235 grad_loss: 16314.0410 normal_loss: 0.6688
[epoch 17][iter  190] loss: 170040.0156 RMSElog: 8.7741 grad_loss: 16994.5469 normal_loss: 0.6812
[epoch 17][iter  200] loss: 172667.6719 RMSElog: 8.9287 grad_loss: 17257.1445 normal_loss: 0.6950
[epoch 17][iter  210] loss: 142575.3125 RMSElog: 8.5657 grad_loss: 14248.3086 normal_loss: 0.6577
[epoch 17][iter  220] loss: 106747.5312 RMSElog: 8.3093 grad_loss: 10665.7656 normal_loss: 0.6778
[epoch 17][iter  230] loss: 176477.0000 RMSElog: 8.8020 grad_loss: 17638.2051 normal_loss: 0.6923
[epoch 17][iter  240] loss: 138589.8750 RMSElog: 8.5954 grad_loss: 13849.7012 normal_loss: 0.6915
[epoch 17][iter  250] loss: 109914.5625 RMSElog: 8.4563 grad_loss: 10982.3086 normal_loss: 0.6914
[epoch 17][iter  260] loss: 137308.1562 RMSElog: 8.5531 grad_loss: 13721.6094 normal_loss: 0.6544
[epoch 17][iter  270] loss: 228033.6719 RMSElog: 8.9463 grad_loss: 22793.7383 normal_loss: 0.6809
[epoch 17][iter  280] loss: 105872.9688 RMSElog: 8.0900 grad_loss: 10578.5488 normal_loss: 0.6583
[epoch 17][iter  290] loss: 100729.3672 RMSElog: 8.6332 grad_loss: 10063.6377 normal_loss: 0.6663
[epoch 17][iter  300] loss: 212217.5000 RMSElog: 8.9106 grad_loss: 21212.1914 normal_loss: 0.6476
[epoch 17][iter  310] loss: 220567.6562 RMSElog: 9.0005 grad_loss: 22047.0859 normal_loss: 0.6797
[epoch 17][iter  320] loss: 201482.3750 RMSElog: 8.9608 grad_loss: 20138.6055 normal_loss: 0.6714
[epoch 17][iter  330] loss: 118723.9922 RMSElog: 8.2591 grad_loss: 11863.4707 normal_loss: 0.6702
[epoch 17][iter  340] loss: 211618.0000 RMSElog: 8.8847 grad_loss: 21152.2656 normal_loss: 0.6509
[epoch 17][iter  350] loss: 109123.3125 RMSElog: 8.2898 grad_loss: 10903.3750 normal_loss: 0.6657
[epoch 17][iter  360] loss: 135021.7500 RMSElog: 8.3260 grad_loss: 13493.1689 normal_loss: 0.6800
[epoch 17][iter  370] loss: 139079.5312 RMSElog: 8.6950 grad_loss: 13898.5928 normal_loss: 0.6649
[epoch 17][iter  380] loss: 139415.8438 RMSElog: 8.6706 grad_loss: 13932.2422 normal_loss: 0.6718
[epoch 17][iter  390] loss: 114487.7812 RMSElog: 8.2863 grad_loss: 11439.8145 normal_loss: 0.6775
[epoch 17][iter  400] loss: 72273.2812 RMSElog: 8.4739 grad_loss: 7218.1924 normal_loss: 0.6618
[epoch 17][iter  410] loss: 116562.5312 RMSElog: 8.4485 grad_loss: 11647.1299 normal_loss: 0.6750
[epoch 17][iter  420] loss: 153431.5312 RMSElog: 8.8747 grad_loss: 15333.5820 normal_loss: 0.6958
[epoch 17][iter  430] loss: 123073.1953 RMSElog: 8.2869 grad_loss: 12298.3486 normal_loss: 0.6831
[epoch 17][iter  440] loss: 102075.1406 RMSElog: 8.3459 grad_loss: 10198.5049 normal_loss: 0.6634
[epoch 17][iter  450] loss: 210500.4531 RMSElog: 8.9483 grad_loss: 21040.4512 normal_loss: 0.6449
[epoch 17][iter  460] loss: 175983.2031 RMSElog: 9.1484 grad_loss: 17588.2969 normal_loss: 0.8746
[epoch 17][iter  470] loss: 241952.2656 RMSElog: 9.0177 grad_loss: 24185.5312 normal_loss: 0.6785
[epoch 17][iter  480] loss: 116052.2578 RMSElog: 8.2987 grad_loss: 11596.2676 normal_loss: 0.6588
[epoch 17][iter  490] loss: 119637.5000 RMSElog: 8.5619 grad_loss: 11954.5459 normal_loss: 0.6423
[epoch 17][iter  500] loss: 128302.1094 RMSElog: 8.6103 grad_loss: 12820.9326 normal_loss: 0.6684
[epoch 17][iter  510] loss: 183073.6250 RMSElog: 8.6287 grad_loss: 18298.0430 normal_loss: 0.6907
[epoch 17][iter  520] loss: 144568.9219 RMSElog: 8.0976 grad_loss: 14448.0664 normal_loss: 0.7290
[epoch 17][iter  530] loss: 104457.1406 RMSElog: 8.1460 grad_loss: 10436.9229 normal_loss: 0.6454
[epoch 17][iter  540] loss: 233376.3281 RMSElog: 9.0005 grad_loss: 23327.9648 normal_loss: 0.6686
[epoch 17][iter  550] loss: 167191.6875 RMSElog: 8.7865 grad_loss: 16709.6992 normal_loss: 0.6822
[epoch 17][iter  560] loss: 112835.5391 RMSElog: 8.4678 grad_loss: 11274.4043 normal_loss: 0.6813
[epoch 17][iter  570] loss: 157043.0156 RMSElog: 8.8617 grad_loss: 15694.7832 normal_loss: 0.6573
[epoch 17][iter  580] loss: 156154.1875 RMSElog: 8.8532 grad_loss: 15605.9229 normal_loss: 0.6417
[epoch 17][iter  590] loss: 176892.2031 RMSElog: 8.9621 grad_loss: 17679.6016 normal_loss: 0.6566
[epoch 18][iter    0] loss: 137244.2344 RMSElog: 8.8836 grad_loss: 13714.7793 normal_loss: 0.7594
[epoch 18][iter   10] loss: 156154.1875 RMSElog: 8.8531 grad_loss: 15605.9229 normal_loss: 0.6417
[epoch 18][iter   20] loss: 165732.1719 RMSElog: 8.8410 grad_loss: 16563.7129 normal_loss: 0.6615
[epoch 18][iter   30] loss: 218748.1562 RMSElog: 9.3937 grad_loss: 21864.5273 normal_loss: 0.8937
[epoch 18][iter   40] loss: 154289.4062 RMSElog: 8.6400 grad_loss: 15419.6543 normal_loss: 0.6475
[epoch 18][iter   50] loss: 138589.8438 RMSElog: 8.5968 grad_loss: 13849.6982 normal_loss: 0.6894
[epoch 18][iter   60] loss: 124615.0000 RMSElog: 8.5033 grad_loss: 12452.3281 normal_loss: 0.6689
[epoch 18][iter   70] loss: 141942.3281 RMSElog: 8.4999 grad_loss: 14185.0547 normal_loss: 0.6779
[epoch 18][iter   80] loss: 139415.3750 RMSElog: 8.6540 grad_loss: 13932.2285 normal_loss: 0.6543
[epoch 18][iter   90] loss: 106928.9219 RMSElog: 8.3336 grad_loss: 10683.8945 normal_loss: 0.6642
[epoch 18][iter  100] loss: 111645.6172 RMSElog: 8.6174 grad_loss: 11155.2041 normal_loss: 0.7406
[epoch 18][iter  110] loss: 142523.0156 RMSElog: 8.8151 grad_loss: 14242.8330 normal_loss: 0.6536
[epoch 18][iter  120] loss: 176892.1875 RMSElog: 8.9610 grad_loss: 17679.6016 normal_loss: 0.6566
[epoch 18][iter  130] loss: 146764.3906 RMSElog: 8.7671 grad_loss: 14666.9775 normal_loss: 0.6934
[epoch 18][iter  140] loss: 139858.0469 RMSElog: 8.2703 grad_loss: 13976.7832 normal_loss: 0.7512
[epoch 18][iter  150] loss: 180962.8750 RMSElog: 8.9812 grad_loss: 18086.6191 normal_loss: 0.6870
[epoch 18][iter  160] loss: 112368.8672 RMSElog: 8.1199 grad_loss: 11228.0977 normal_loss: 0.6692
[epoch 18][iter  170] loss: 111900.7656 RMSElog: 8.4361 grad_loss: 11180.9336 normal_loss: 0.7059
[epoch 18][iter  180] loss: 121168.3906 RMSElog: 8.7411 grad_loss: 12107.3525 normal_loss: 0.7452
[epoch 18][iter  190] loss: 100646.2891 RMSElog: 8.7663 grad_loss: 10055.1777 normal_loss: 0.6849
[epoch 18][iter  200] loss: 161068.3281 RMSElog: 8.6725 grad_loss: 16097.5000 normal_loss: 0.6600
[epoch 18][iter  210] loss: 104316.4922 RMSElog: 8.0507 grad_loss: 10422.9375 normal_loss: 0.6613
[epoch 18][iter  220] loss: 210634.8594 RMSElog: 9.1791 grad_loss: 21053.6172 normal_loss: 0.6901
[epoch 18][iter  230] loss: 137275.7188 RMSElog: 8.5628 grad_loss: 13718.3311 normal_loss: 0.6773
[epoch 18][iter  240] loss: 158614.6562 RMSElog: 8.8493 grad_loss: 15851.9229 normal_loss: 0.6932
[epoch 18][iter  250] loss: 181446.7344 RMSElog: 8.8961 grad_loss: 18135.0762 normal_loss: 0.7015
[epoch 18][iter  260] loss: 165633.4531 RMSElog: 8.8099 grad_loss: 16553.8262 normal_loss: 0.7097
[epoch 18][iter  270] loss: 167728.5000 RMSElog: 8.8758 grad_loss: 16763.2734 normal_loss: 0.7008
[epoch 18][iter  280] loss: 166054.3125 RMSElog: 8.7677 grad_loss: 16595.9648 normal_loss: 0.6991
[epoch 18][iter  290] loss: 150132.2031 RMSElog: 8.7519 grad_loss: 15003.8066 normal_loss: 0.6608
[epoch 18][iter  300] loss: 95206.3984 RMSElog: 8.3174 grad_loss: 9511.6543 normal_loss: 0.6678
[epoch 18][iter  310] loss: 128257.8750 RMSElog: 8.5072 grad_loss: 12816.6299 normal_loss: 0.6507
[epoch 18][iter  320] loss: 113898.4297 RMSElog: 8.0929 grad_loss: 11381.0732 normal_loss: 0.6770
[epoch 18][iter  330] loss: 147441.0312 RMSElog: 8.4963 grad_loss: 14734.9131 normal_loss: 0.6934
[epoch 18][iter  340] loss: 114903.1953 RMSElog: 8.2716 grad_loss: 11481.3818 normal_loss: 0.6658
[epoch 18][iter  350] loss: 214470.3906 RMSElog: 8.8807 grad_loss: 21437.5000 normal_loss: 0.6587
[epoch 18][iter  360] loss: 175102.8906 RMSElog: 8.7097 grad_loss: 17500.8945 normal_loss: 0.6849
[epoch 18][iter  370] loss: 142242.1562 RMSElog: 8.4496 grad_loss: 14215.0625 normal_loss: 0.7045
[epoch 18][iter  380] loss: 145597.8281 RMSElog: 8.8225 grad_loss: 14550.2988 normal_loss: 0.6613
[epoch 18][iter  390] loss: 133026.5312 RMSElog: 8.6615 grad_loss: 13293.3359 normal_loss: 0.6565
[epoch 18][iter  400] loss: 120269.0938 RMSElog: 8.2399 grad_loss: 12018.0039 normal_loss: 0.6649
[epoch 18][iter  410] loss: 103500.0312 RMSElog: 8.5433 grad_loss: 10340.7930 normal_loss: 0.6670
[epoch 18][iter  420] loss: 150441.7344 RMSElog: 8.7145 grad_loss: 15034.7461 normal_loss: 0.7121
[epoch 18][iter  430] loss: 136045.2188 RMSElog: 8.3149 grad_loss: 13595.5195 normal_loss: 0.6889
[epoch 18][iter  440] loss: 224056.5000 RMSElog: 9.0302 grad_loss: 22395.9473 normal_loss: 0.6732
[epoch 18][iter  450] loss: 164750.1875 RMSElog: 8.7531 grad_loss: 16465.5742 normal_loss: 0.6921
[epoch 18][iter  460] loss: 153480.7656 RMSElog: 8.8323 grad_loss: 15338.5986 normal_loss: 0.6457
[epoch 18][iter  470] loss: 213396.0938 RMSElog: 8.9660 grad_loss: 21329.9590 normal_loss: 0.6833
[epoch 18][iter  480] loss: 125490.4609 RMSElog: 8.8237 grad_loss: 12539.5156 normal_loss: 0.7056
[epoch 18][iter  490] loss: 211905.6406 RMSElog: 8.8647 grad_loss: 21181.0371 normal_loss: 0.6625
[epoch 18][iter  500] loss: 144568.9219 RMSElog: 8.0961 grad_loss: 14448.0664 normal_loss: 0.7292
[epoch 18][iter  510] loss: 140919.0156 RMSElog: 8.4683 grad_loss: 14082.7637 normal_loss: 0.6687
[epoch 18][iter  520] loss: 167723.1562 RMSElog: 8.8399 grad_loss: 16762.7812 normal_loss: 0.6955
[epoch 18][iter  530] loss: 180730.4844 RMSElog: 8.8802 grad_loss: 18063.4980 normal_loss: 0.6696
[epoch 18][iter  540] loss: 214753.3125 RMSElog: 8.8272 grad_loss: 21465.8496 normal_loss: 0.6540
[epoch 18][iter  550] loss: 191801.6406 RMSElog: 8.7741 grad_loss: 19170.7500 normal_loss: 0.6414
[epoch 18][iter  560] loss: 174612.8438 RMSElog: 8.8087 grad_loss: 17451.7617 normal_loss: 0.7139
[epoch 18][iter  570] loss: 200933.8750 RMSElog: 8.8635 grad_loss: 20083.8320 normal_loss: 0.6921
[epoch 18][iter  580] loss: 160806.4375 RMSElog: 8.8164 grad_loss: 16071.1807 normal_loss: 0.6461
[epoch 18][iter  590] loss: 161266.5312 RMSElog: 8.7470 grad_loss: 16117.2510 normal_loss: 0.6549
[epoch 19][iter    0] loss: 147525.3594 RMSElog: 8.7397 grad_loss: 14743.1357 normal_loss: 0.6616
[epoch 19][iter   10] loss: 183994.6719 RMSElog: 8.5617 grad_loss: 18390.2129 normal_loss: 0.6911
[epoch 19][iter   20] loss: 138041.0000 RMSElog: 8.4986 grad_loss: 13794.9229 normal_loss: 0.6790
[epoch 19][iter   30] loss: 200934.3750 RMSElog: 8.9082 grad_loss: 20083.8418 normal_loss: 0.6874
[epoch 19][iter   40] loss: 148773.9688 RMSElog: 8.8061 grad_loss: 14867.9062 normal_loss: 0.6845
[epoch 19][iter   50] loss: 165733.4375 RMSElog: 8.8889 grad_loss: 16563.7637 normal_loss: 0.6910
[epoch 19][iter   60] loss: 135093.4531 RMSElog: 8.5730 grad_loss: 13500.1172 normal_loss: 0.6543
[epoch 19][iter   70] loss: 105682.3906 RMSElog: 8.0518 grad_loss: 10559.5273 normal_loss: 0.6604
[epoch 19][iter   80] loss: 173145.7812 RMSElog: 8.6008 grad_loss: 17305.2754 normal_loss: 0.7006
[epoch 19][iter   90] loss: 125808.1641 RMSElog: 8.3699 grad_loss: 12571.7686 normal_loss: 0.6776
[epoch 19][iter  100] loss: 105075.4062 RMSElog: 8.1074 grad_loss: 10498.7646 normal_loss: 0.6688
[epoch 19][iter  110] loss: 217892.0156 RMSElog: 8.8634 grad_loss: 21779.6914 normal_loss: 0.6458
[epoch 19][iter  120] loss: 114487.0469 RMSElog: 8.2421 grad_loss: 11439.7871 normal_loss: 0.6756
[epoch 19][iter  130] loss: 115479.1484 RMSElog: 8.4971 grad_loss: 11538.7549 normal_loss: 0.6630
[epoch 19][iter  140] loss: 145303.0156 RMSElog: 8.6264 grad_loss: 14521.0205 normal_loss: 0.6553
[epoch 19][iter  150] loss: 198272.0156 RMSElog: 8.8019 grad_loss: 19817.7402 normal_loss: 0.6587
[epoch 19][iter  160] loss: 153978.8281 RMSElog: 8.6078 grad_loss: 15388.6279 normal_loss: 0.6479
[epoch 19][iter  170] loss: 204628.5781 RMSElog: 8.8828 grad_loss: 20453.3184 normal_loss: 0.6557
[epoch 19][iter  180] loss: 190917.3125 RMSElog: 9.0334 grad_loss: 19082.0293 normal_loss: 0.6689
[epoch 19][iter  190] loss: 129149.1875 RMSElog: 8.7090 grad_loss: 12905.5381 normal_loss: 0.6723
[epoch 19][iter  200] loss: 227059.7188 RMSElog: 9.0941 grad_loss: 22696.2207 normal_loss: 0.6575
[epoch 19][iter  210] loss: 158014.0938 RMSElog: 8.7556 grad_loss: 15792.0020 normal_loss: 0.6527
[epoch 19][iter  220] loss: 170340.5000 RMSElog: 8.9567 grad_loss: 17024.4121 normal_loss: 0.6808
[epoch 19][iter  230] loss: 179630.0938 RMSElog: 8.9633 grad_loss: 17953.3555 normal_loss: 0.6906
[epoch 19][iter  240] loss: 179884.8438 RMSElog: 8.8189 grad_loss: 17978.9629 normal_loss: 0.7034
[epoch 19][iter  250] loss: 178062.0156 RMSElog: 8.8829 grad_loss: 17796.5879 normal_loss: 0.7305
[epoch 19][iter  260] loss: 164750.6094 RMSElog: 8.7747 grad_loss: 16465.5879 normal_loss: 0.6965
[epoch 19][iter  270] loss: 142242.1719 RMSElog: 8.4493 grad_loss: 14215.0625 normal_loss: 0.7048
[epoch 19][iter  280] loss: 130725.8594 RMSElog: 8.7316 grad_loss: 13063.1260 normal_loss: 0.7282
[epoch 19][iter  290] loss: 112835.5156 RMSElog: 8.4669 grad_loss: 11274.4043 normal_loss: 0.6809
[epoch 19][iter  300] loss: 131196.7344 RMSElog: 8.5790 grad_loss: 13110.3828 normal_loss: 0.7111
[epoch 19][iter  310] loss: 107882.7422 RMSElog: 8.0593 grad_loss: 10779.5605 normal_loss: 0.6544
[epoch 19][iter  320] loss: 167398.9219 RMSElog: 8.8027 grad_loss: 16730.3594 normal_loss: 0.7314
[epoch 19][iter  330] loss: 163249.8750 RMSElog: 8.8312 grad_loss: 16315.4824 normal_loss: 0.6743
[epoch 19][iter  340] loss: 121848.8984 RMSElog: 8.5166 grad_loss: 12175.6533 normal_loss: 0.7193
[epoch 19][iter  350] loss: 163235.8281 RMSElog: 8.8212 grad_loss: 16314.0762 normal_loss: 0.6856
[epoch 19][iter  360] loss: 175282.4219 RMSElog: 8.6128 grad_loss: 17518.9375 normal_loss: 0.6906
[epoch 19][iter  370] loss: 123515.1484 RMSElog: 8.7098 grad_loss: 12342.0430 normal_loss: 0.7614
[epoch 19][iter  380] loss: 203000.4844 RMSElog: 8.8288 grad_loss: 20290.5684 normal_loss: 0.6527
[epoch 19][iter  390] loss: 161267.0938 RMSElog: 8.7757 grad_loss: 16117.2773 normal_loss: 0.6570
[epoch 19][iter  400] loss: 162560.7812 RMSElog: 8.9154 grad_loss: 16246.4639 normal_loss: 0.6991
[epoch 19][iter  410] loss: 130410.0391 RMSElog: 8.5277 grad_loss: 13031.8184 normal_loss: 0.6582
[epoch 19][iter  420] loss: 210500.9375 RMSElog: 8.9675 grad_loss: 21040.4766 normal_loss: 0.6502
[epoch 19][iter  430] loss: 163681.8125 RMSElog: 8.8063 grad_loss: 16358.6289 normal_loss: 0.7462
[epoch 19][iter  440] loss: 136045.2344 RMSElog: 8.3150 grad_loss: 13595.5195 normal_loss: 0.6887
[epoch 19][iter  450] loss: 107923.9922 RMSElog: 8.0551 grad_loss: 10783.6826 normal_loss: 0.6620
[epoch 19][iter  460] loss: 232751.6406 RMSElog: 9.0185 grad_loss: 23265.4766 normal_loss: 0.6701
[epoch 19][iter  470] loss: 166190.8438 RMSElog: 8.8382 grad_loss: 16609.5586 normal_loss: 0.6867
[epoch 19][iter  480] loss: 120426.6797 RMSElog: 8.3734 grad_loss: 12033.6084 normal_loss: 0.6870
[epoch 19][iter  490] loss: 167777.8750 RMSElog: 8.8634 grad_loss: 16768.2324 normal_loss: 0.6909
[epoch 19][iter  500] loss: 197694.9062 RMSElog: 9.1330 grad_loss: 19759.6895 normal_loss: 0.6681
[epoch 19][iter  510] loss: 153481.7656 RMSElog: 8.8760 grad_loss: 15338.6426 normal_loss: 0.6577
[epoch 19][iter  520] loss: 100093.3203 RMSElog: 8.2967 grad_loss: 10000.3711 normal_loss: 0.6636
[epoch 19][iter  530] loss: 188998.9219 RMSElog: 8.9417 grad_loss: 18890.2578 normal_loss: 0.6930
[epoch 19][iter  540] loss: 214787.4062 RMSElog: 8.8489 grad_loss: 21469.2344 normal_loss: 0.6555
[epoch 19][iter  550] loss: 112673.8281 RMSElog: 8.1485 grad_loss: 11258.5732 normal_loss: 0.6616
[epoch 19][iter  560] loss: 111900.7500 RMSElog: 8.4357 grad_loss: 11180.9336 normal_loss: 0.7058
[epoch 19][iter  570] loss: 124228.9141 RMSElog: 8.6064 grad_loss: 12413.6025 normal_loss: 0.6831
[epoch 19][iter  580] loss: 122661.7812 RMSElog: 8.7050 grad_loss: 12256.6172 normal_loss: 0.8551
[epoch 19][iter  590] loss: 133005.2344 RMSElog: 8.5094 grad_loss: 13291.3789 normal_loss: 0.6343
[epoch 20][iter    0] loss: 106157.8125 RMSElog: 8.0429 grad_loss: 10607.0791 normal_loss: 0.6589
[epoch 20][iter   10] loss: 173836.4688 RMSElog: 8.8775 grad_loss: 17374.0723 normal_loss: 0.6980
[epoch 20][iter   20] loss: 143594.6875 RMSElog: 8.5870 grad_loss: 14350.1875 normal_loss: 0.6942
[epoch 20][iter   30] loss: 110864.7656 RMSElog: 8.6914 grad_loss: 11077.0518 normal_loss: 0.7332
[epoch 20][iter   40] loss: 142575.9531 RMSElog: 8.6049 grad_loss: 14248.3340 normal_loss: 0.6573
[epoch 20][iter   50] loss: 112693.4688 RMSElog: 8.1198 grad_loss: 11260.5459 normal_loss: 0.6804
[epoch 20][iter   60] loss: 129143.2812 RMSElog: 8.6660 grad_loss: 12904.9668 normal_loss: 0.6951
[epoch 20][iter   70] loss: 163235.8281 RMSElog: 8.8211 grad_loss: 16314.0762 normal_loss: 0.6855
[epoch 20][iter   80] loss: 162415.7344 RMSElog: 8.8348 grad_loss: 16232.0723 normal_loss: 0.6661
[epoch 20][iter   90] loss: 161794.1250 RMSElog: 8.8742 grad_loss: 16169.8848 normal_loss: 0.6530
[epoch 20][iter  100] loss: 72273.2734 RMSElog: 8.4746 grad_loss: 7218.1924 normal_loss: 0.6604
[epoch 20][iter  110] loss: 100958.6641 RMSElog: 8.1503 grad_loss: 10087.0684 normal_loss: 0.6470
[epoch 20][iter  120] loss: 150132.2031 RMSElog: 8.7516 grad_loss: 15003.8066 normal_loss: 0.6609
[epoch 20][iter  130] loss: 142523.0000 RMSElog: 8.8147 grad_loss: 14242.8330 normal_loss: 0.6536
[epoch 20][iter  140] loss: 113404.2031 RMSElog: 8.1836 grad_loss: 11331.5605 normal_loss: 0.6756
[epoch 20][iter  150] loss: 176244.5625 RMSElog: 9.0361 grad_loss: 17614.7246 normal_loss: 0.6963
[epoch 20][iter  160] loss: 120426.5547 RMSElog: 8.3581 grad_loss: 12033.6055 normal_loss: 0.6916
[epoch 20][iter  170] loss: 159323.0312 RMSElog: 8.8833 grad_loss: 15922.7256 normal_loss: 0.6941
[epoch 20][iter  180] loss: 201353.6875 RMSElog: 9.0794 grad_loss: 20125.6133 normal_loss: 0.6749
[epoch 20][iter  190] loss: 105872.1094 RMSElog: 8.0501 grad_loss: 10578.5137 normal_loss: 0.6472
[epoch 20][iter  200] loss: 125265.6875 RMSElog: 8.4421 grad_loss: 12517.3936 normal_loss: 0.7322
[epoch 20][iter  210] loss: 162874.9062 RMSElog: 8.8290 grad_loss: 16278.0000 normal_loss: 0.6623
[epoch 20][iter  220] loss: 160449.9375 RMSElog: 8.8725 grad_loss: 16035.4414 normal_loss: 0.6793
[epoch 20][iter  230] loss: 112518.0391 RMSElog: 8.5107 grad_loss: 11242.6270 normal_loss: 0.6663
[epoch 20][iter  240] loss: 190952.1250 RMSElog: 8.8476 grad_loss: 19085.6953 normal_loss: 0.6692
[epoch 20][iter  250] loss: 163759.1562 RMSElog: 8.7172 grad_loss: 16366.4795 normal_loss: 0.7191
[epoch 20][iter  260] loss: 165290.2969 RMSElog: 8.8961 grad_loss: 16519.4277 normal_loss: 0.7056
[epoch 20][iter  270] loss: 193003.2031 RMSElog: 8.6328 grad_loss: 19290.9941 normal_loss: 0.6933
[epoch 20][iter  280] loss: 194386.9531 RMSElog: 8.8334 grad_loss: 19429.1914 normal_loss: 0.6696
[epoch 20][iter  290] loss: 107049.5938 RMSElog: 8.0694 grad_loss: 10696.2256 normal_loss: 0.6642
[epoch 20][iter  300] loss: 160806.4219 RMSElog: 8.8165 grad_loss: 16071.1807 normal_loss: 0.6460
[epoch 20][iter  310] loss: 115641.2578 RMSElog: 8.5842 grad_loss: 11554.8145 normal_loss: 0.7277
[epoch 20][iter  320] loss: 123073.1953 RMSElog: 8.2881 grad_loss: 12298.3486 normal_loss: 0.6828
[epoch 20][iter  330] loss: 104725.2266 RMSElog: 8.0866 grad_loss: 10463.7803 normal_loss: 0.6550
[epoch 20][iter  340] loss: 162355.8281 RMSElog: 8.8226 grad_loss: 16226.0605 normal_loss: 0.7002
[epoch 20][iter  350] loss: 128958.4688 RMSElog: 8.3037 grad_loss: 12886.8594 normal_loss: 0.6834
[epoch 20][iter  360] loss: 170339.7500 RMSElog: 8.9149 grad_loss: 17024.3867 normal_loss: 0.6747
[epoch 20][iter  370] loss: 174986.3281 RMSElog: 8.8613 grad_loss: 17489.0605 normal_loss: 0.7118
[epoch 20][iter  380] loss: 141846.2188 RMSElog: 8.4371 grad_loss: 14175.4834 normal_loss: 0.7000
[epoch 20][iter  390] loss: 130410.4219 RMSElog: 8.5526 grad_loss: 13031.8320 normal_loss: 0.6568
[epoch 20][iter  400] loss: 177688.0938 RMSElog: 8.8194 grad_loss: 17759.2461 normal_loss: 0.7423
[epoch 20][iter  410] loss: 170471.5312 RMSElog: 8.7744 grad_loss: 17037.7109 normal_loss: 0.6664
[epoch 20][iter  420] loss: 109417.8438 RMSElog: 8.0673 grad_loss: 10933.0430 normal_loss: 0.6740
[epoch 20][iter  430] loss: 100625.3828 RMSElog: 8.0308 grad_loss: 10053.8682 normal_loss: 0.6388
[epoch 20][iter  440] loss: 186032.5625 RMSElog: 8.8528 grad_loss: 18593.7266 normal_loss: 0.6755
[epoch 20][iter  450] loss: 226929.5469 RMSElog: 8.9595 grad_loss: 22683.3359 normal_loss: 0.6610
[epoch 20][iter  460] loss: 178452.7969 RMSElog: 8.8895 grad_loss: 17835.6836 normal_loss: 0.7068
[epoch 20][iter  470] loss: 212217.5156 RMSElog: 8.9113 grad_loss: 21212.1914 normal_loss: 0.6480
[epoch 20][iter  480] loss: 175103.5312 RMSElog: 8.7374 grad_loss: 17500.9219 normal_loss: 0.6940
[epoch 20][iter  490] loss: 157043.0156 RMSElog: 8.8614 grad_loss: 15694.7832 normal_loss: 0.6573
[epoch 20][iter  500] loss: 165632.8750 RMSElog: 8.7815 grad_loss: 16553.8047 normal_loss: 0.7009
[epoch 20][iter  510] loss: 174548.3750 RMSElog: 8.7424 grad_loss: 17445.4062 normal_loss: 0.6887
[epoch 20][iter  520] loss: 214753.3125 RMSElog: 8.8272 grad_loss: 21465.8496 normal_loss: 0.6538
[epoch 20][iter  530] loss: 175917.2500 RMSElog: 9.0507 grad_loss: 17581.9629 normal_loss: 0.7113
[epoch 20][iter  540] loss: 103289.8281 RMSElog: 8.5838 grad_loss: 10319.7227 normal_loss: 0.6755
[epoch 20][iter  550] loss: 196819.8125 RMSElog: 8.7076 grad_loss: 19672.6211 normal_loss: 0.6519
[epoch 20][iter  560] loss: 202096.9688 RMSElog: 8.7439 grad_loss: 20200.3008 normal_loss: 0.6530
[epoch 20][iter  570] loss: 154145.9219 RMSElog: 8.8672 grad_loss: 15405.0693 normal_loss: 0.6555
[epoch 20][iter  580] loss: 105075.4062 RMSElog: 8.1071 grad_loss: 10498.7646 normal_loss: 0.6689
[epoch 20][iter  590] loss: 150440.8125 RMSElog: 8.6557 grad_loss: 15034.7168 normal_loss: 0.7099
[epoch 21][iter    0] loss: 163375.4219 RMSElog: 8.8311 grad_loss: 16327.9795 normal_loss: 0.7312
[epoch 21][iter   10] loss: 147747.3125 RMSElog: 8.6398 grad_loss: 14765.3564 normal_loss: 0.7351
[epoch 21][iter   20] loss: 188696.6562 RMSElog: 8.7152 grad_loss: 18860.2715 normal_loss: 0.6793
[epoch 21][iter   30] loss: 196003.9688 RMSElog: 8.8262 grad_loss: 19590.9082 normal_loss: 0.6629
[epoch 21][iter   40] loss: 172739.7188 RMSElog: 8.9944 grad_loss: 17264.2676 normal_loss: 0.7100
[epoch 21][iter   50] loss: 167192.5156 RMSElog: 8.8179 grad_loss: 16709.7441 normal_loss: 0.6889
[epoch 21][iter   60] loss: 100729.4141 RMSElog: 8.6347 grad_loss: 10063.6396 normal_loss: 0.6668
[epoch 21][iter   70] loss: 199052.5312 RMSElog: 8.8682 grad_loss: 19895.6797 normal_loss: 0.7062
[epoch 21][iter   80] loss: 161266.5312 RMSElog: 8.7469 grad_loss: 16117.2510 normal_loss: 0.6549
[epoch 21][iter   90] loss: 140447.2188 RMSElog: 8.7898 grad_loss: 14035.1064 normal_loss: 0.8257
[epoch 21][iter  100] loss: 112297.4219 RMSElog: 8.2072 grad_loss: 11220.8594 normal_loss: 0.6755
[epoch 21][iter  110] loss: 130410.0391 RMSElog: 8.5275 grad_loss: 13031.8184 normal_loss: 0.6584
[epoch 21][iter  120] loss: 177919.3438 RMSElog: 8.9593 grad_loss: 17782.2617 normal_loss: 0.7139
[epoch 21][iter  130] loss: 117673.9922 RMSElog: 8.3549 grad_loss: 11758.3711 normal_loss: 0.6738
[epoch 21][iter  140] loss: 124542.1094 RMSElog: 8.5168 grad_loss: 12445.0342 normal_loss: 0.6598
[epoch 21][iter  150] loss: 211617.9844 RMSElog: 8.8839 grad_loss: 21152.2637 normal_loss: 0.6512
[epoch 21][iter  160] loss: 112517.4922 RMSElog: 8.4823 grad_loss: 11242.6064 normal_loss: 0.6601
[epoch 21][iter  170] loss: 215522.2188 RMSElog: 9.1081 grad_loss: 21542.4395 normal_loss: 0.6749
[epoch 21][iter  180] loss: 140035.8750 RMSElog: 8.6432 grad_loss: 13994.2881 normal_loss: 0.6550
[epoch 21][iter  190] loss: 178452.7969 RMSElog: 8.8894 grad_loss: 17835.6836 normal_loss: 0.7067
[epoch 21][iter  200] loss: 103935.7656 RMSElog: 8.3432 grad_loss: 10384.5508 normal_loss: 0.6828
[epoch 21][iter  210] loss: 125664.1719 RMSElog: 8.5535 grad_loss: 12557.1719 normal_loss: 0.6913
[epoch 21][iter  220] loss: 193959.2188 RMSElog: 9.0665 grad_loss: 19386.1738 normal_loss: 0.6823
[epoch 21][iter  230] loss: 170340.5000 RMSElog: 8.9568 grad_loss: 17024.4121 normal_loss: 0.6808
[epoch 21][iter  240] loss: 133267.8438 RMSElog: 8.6645 grad_loss: 13317.4697 normal_loss: 0.6514
[epoch 21][iter  250] loss: 172667.9062 RMSElog: 8.9451 grad_loss: 17257.1504 normal_loss: 0.6954
[epoch 21][iter  260] loss: 104111.6562 RMSElog: 8.6245 grad_loss: 10401.8545 normal_loss: 0.6865
[epoch 21][iter  270] loss: 176476.9531 RMSElog: 8.8026 grad_loss: 17638.2012 normal_loss: 0.6911
[epoch 21][iter  280] loss: 166526.6562 RMSElog: 8.9622 grad_loss: 16643.0117 normal_loss: 0.6905
[epoch 21][iter  290] loss: 127096.7031 RMSElog: 8.6422 grad_loss: 12700.2891 normal_loss: 0.7387
[epoch 21][iter  300] loss: 125489.9219 RMSElog: 8.7874 grad_loss: 12539.5010 normal_loss: 0.7045
[epoch 21][iter  310] loss: 105877.3906 RMSElog: 8.4264 grad_loss: 10578.6348 normal_loss: 0.6781
[epoch 21][iter  320] loss: 126949.7734 RMSElog: 8.4499 grad_loss: 12685.8584 normal_loss: 0.6690
[epoch 21][iter  330] loss: 228033.1406 RMSElog: 8.9206 grad_loss: 22793.7188 normal_loss: 0.6767
[epoch 21][iter  340] loss: 151334.6562 RMSElog: 8.6861 grad_loss: 15124.1318 normal_loss: 0.6469
[epoch 21][iter  350] loss: 144638.3438 RMSElog: 8.7167 grad_loss: 14454.4873 normal_loss: 0.6298
[epoch 21][iter  360] loss: 120722.7344 RMSElog: 8.2590 grad_loss: 12063.3574 normal_loss: 0.6572
[epoch 21][iter  370] loss: 138589.8438 RMSElog: 8.5968 grad_loss: 13849.6982 normal_loss: 0.6895
[epoch 21][iter  380] loss: 187216.7812 RMSElog: 8.8463 grad_loss: 18712.1602 normal_loss: 0.6724
[epoch 21][iter  390] loss: 155464.4375 RMSElog: 8.9915 grad_loss: 15536.7920 normal_loss: 0.6608
[epoch 21][iter  400] loss: 115342.8828 RMSElog: 8.5145 grad_loss: 11525.0703 normal_loss: 0.7027
[epoch 21][iter  410] loss: 175528.5938 RMSElog: 8.8321 grad_loss: 17543.3457 normal_loss: 0.6811
[epoch 21][iter  420] loss: 133448.0000 RMSElog: 8.6459 grad_loss: 13335.4834 normal_loss: 0.6716
[epoch 21][iter  430] loss: 150347.3125 RMSElog: 8.8547 grad_loss: 15025.1875 normal_loss: 0.6895
[epoch 21][iter  440] loss: 174331.3438 RMSElog: 8.7536 grad_loss: 17423.6934 normal_loss: 0.6872
[epoch 21][iter  450] loss: 100625.3828 RMSElog: 8.0311 grad_loss: 10053.8682 normal_loss: 0.6386
[epoch 21][iter  460] loss: 144568.9219 RMSElog: 8.0966 grad_loss: 14448.0664 normal_loss: 0.7288
[epoch 21][iter  470] loss: 196255.0156 RMSElog: 8.7686 grad_loss: 19616.0762 normal_loss: 0.6568
[epoch 21][iter  480] loss: 173703.9062 RMSElog: 8.9514 grad_loss: 17360.7305 normal_loss: 0.7092
[epoch 21][iter  490] loss: 120426.5547 RMSElog: 8.3583 grad_loss: 12033.6055 normal_loss: 0.6916
[epoch 21][iter  500] loss: 128471.1719 RMSElog: 8.6923 grad_loss: 12837.7480 normal_loss: 0.6770
[epoch 21][iter  510] loss: 126854.5234 RMSElog: 8.5381 grad_loss: 12676.2705 normal_loss: 0.6433
[epoch 21][iter  520] loss: 190951.1875 RMSElog: 8.7963 grad_loss: 19085.6562 normal_loss: 0.6657
[epoch 21][iter  530] loss: 167765.9219 RMSElog: 8.7633 grad_loss: 16767.1113 normal_loss: 0.7167
[epoch 21][iter  540] loss: 119904.8594 RMSElog: 8.4955 grad_loss: 11981.3525 normal_loss: 0.6383
[epoch 21][iter  550] loss: 156154.1719 RMSElog: 8.8530 grad_loss: 15605.9229 normal_loss: 0.6416
[epoch 21][iter  560] loss: 191187.3594 RMSElog: 8.8388 grad_loss: 19109.1758 normal_loss: 0.7231
[epoch 21][iter  570] loss: 145597.5156 RMSElog: 8.7971 grad_loss: 14550.2881 normal_loss: 0.6671
[epoch 21][iter  580] loss: 148118.4062 RMSElog: 8.7471 grad_loss: 14802.3916 normal_loss: 0.7019
[epoch 21][iter  590] loss: 127482.6406 RMSElog: 8.2794 grad_loss: 12739.3027 normal_loss: 0.6821
[epoch 22][iter    0] loss: 131922.2031 RMSElog: 8.5440 grad_loss: 13183.0234 normal_loss: 0.6533
[epoch 22][iter   10] loss: 112835.5156 RMSElog: 8.4668 grad_loss: 11274.4043 normal_loss: 0.6810
[epoch 22][iter   20] loss: 149524.8281 RMSElog: 8.8364 grad_loss: 14942.9590 normal_loss: 0.6888
[epoch 22][iter   30] loss: 133448.0000 RMSElog: 8.6455 grad_loss: 13335.4824 normal_loss: 0.6718
[epoch 22][iter   40] loss: 159807.5625 RMSElog: 8.8447 grad_loss: 15971.2256 normal_loss: 0.6859
[epoch 22][iter   50] loss: 171571.7188 RMSElog: 8.7124 grad_loss: 17147.7383 normal_loss: 0.7209
[epoch 22][iter   60] loss: 185051.6406 RMSElog: 8.7494 grad_loss: 18495.7168 normal_loss: 0.6965
[epoch 22][iter   70] loss: 198724.3438 RMSElog: 8.8911 grad_loss: 19862.8418 normal_loss: 0.7003
[epoch 22][iter   80] loss: 164163.1562 RMSElog: 8.8566 grad_loss: 16406.7852 normal_loss: 0.6747
[epoch 22][iter   90] loss: 139858.0312 RMSElog: 8.2698 grad_loss: 13976.7832 normal_loss: 0.7513
[epoch 22][iter  100] loss: 155463.4688 RMSElog: 8.9289 grad_loss: 15536.7656 normal_loss: 0.6530
[epoch 22][iter  110] loss: 146195.5312 RMSElog: 8.7193 grad_loss: 14610.1611 normal_loss: 0.6728
[epoch 22][iter  120] loss: 139000.0938 RMSElog: 8.6245 grad_loss: 13890.7109 normal_loss: 0.6742
[epoch 22][iter  130] loss: 177272.4688 RMSElog: 8.8745 grad_loss: 17717.6758 normal_loss: 0.6961
[epoch 22][iter  140] loss: 140447.2188 RMSElog: 8.7897 grad_loss: 14035.1064 normal_loss: 0.8255
[epoch 22][iter  150] loss: 224923.8906 RMSElog: 8.8167 grad_loss: 22482.8867 normal_loss: 0.6848
[epoch 22][iter  160] loss: 110566.8281 RMSElog: 8.6099 grad_loss: 11047.3691 normal_loss: 0.7029
[epoch 22][iter  170] loss: 154270.8438 RMSElog: 8.6615 grad_loss: 15417.7725 normal_loss: 0.6512
[epoch 22][iter  180] loss: 154687.2500 RMSElog: 8.8054 grad_loss: 15459.2461 normal_loss: 0.6743
[epoch 22][iter  190] loss: 126004.6562 RMSElog: 8.4298 grad_loss: 12591.3623 normal_loss: 0.6741
[epoch 22][iter  200] loss: 155892.4844 RMSElog: 8.6518 grad_loss: 15579.9434 normal_loss: 0.6528
[epoch 22][iter  210] loss: 120269.0781 RMSElog: 8.2398 grad_loss: 12018.0039 normal_loss: 0.6645
[epoch 22][iter  220] loss: 115641.6406 RMSElog: 8.5962 grad_loss: 11554.8330 normal_loss: 0.7340
[epoch 22][iter  230] loss: 129724.5781 RMSElog: 8.7336 grad_loss: 12963.0586 normal_loss: 0.6661
[epoch 22][iter  240] loss: 105877.9375 RMSElog: 8.4748 grad_loss: 10578.6416 normal_loss: 0.6773
[epoch 22][iter  250] loss: 191952.9844 RMSElog: 8.8938 grad_loss: 19185.7090 normal_loss: 0.6955
[epoch 22][iter  260] loss: 156963.2500 RMSElog: 8.8586 grad_loss: 15686.8184 normal_loss: 0.6488
[epoch 22][iter  270] loss: 176476.9531 RMSElog: 8.8025 grad_loss: 17638.2012 normal_loss: 0.6912
[epoch 22][iter  280] loss: 191539.4375 RMSElog: 8.6708 grad_loss: 19144.5820 normal_loss: 0.6909
[epoch 22][iter  290] loss: 158908.4062 RMSElog: 8.7985 grad_loss: 15881.3477 normal_loss: 0.6941
[epoch 22][iter  300] loss: 106157.8047 RMSElog: 8.0433 grad_loss: 10607.0791 normal_loss: 0.6585
[epoch 22][iter  310] loss: 112297.4219 RMSElog: 8.2075 grad_loss: 11220.8604 normal_loss: 0.6752
[epoch 22][iter  320] loss: 193958.5312 RMSElog: 9.0228 grad_loss: 19386.1504 normal_loss: 0.6805
[epoch 22][iter  330] loss: 146764.3750 RMSElog: 8.7666 grad_loss: 14666.9775 normal_loss: 0.6933
[epoch 22][iter  340] loss: 127445.1094 RMSElog: 8.3838 grad_loss: 12735.4648 normal_loss: 0.6625
[epoch 22][iter  350] loss: 208164.4531 RMSElog: 9.1080 grad_loss: 20806.6582 normal_loss: 0.6797
[epoch 22][iter  360] loss: 160649.1406 RMSElog: 8.8636 grad_loss: 16055.3887 normal_loss: 0.6623
[epoch 22][iter  370] loss: 116052.2578 RMSElog: 8.2968 grad_loss: 11596.2686 normal_loss: 0.6600
[epoch 22][iter  380] loss: 178577.7344 RMSElog: 8.9075 grad_loss: 17848.1367 normal_loss: 0.7276
[epoch 22][iter  390] loss: 121303.1875 RMSElog: 8.6188 grad_loss: 12120.9570 normal_loss: 0.7421
[epoch 22][iter  400] loss: 141426.1250 RMSElog: 8.6849 grad_loss: 14133.2441 normal_loss: 0.6834
[epoch 22][iter  410] loss: 165937.0156 RMSElog: 8.8791 grad_loss: 16584.1152 normal_loss: 0.7063
[epoch 22][iter  420] loss: 114808.5078 RMSElog: 8.4930 grad_loss: 11471.6719 normal_loss: 0.6858
[epoch 22][iter  430] loss: 175507.2031 RMSElog: 9.0954 grad_loss: 17540.9023 normal_loss: 0.7228
[epoch 22][iter  440] loss: 103935.7734 RMSElog: 8.3433 grad_loss: 10384.5508 normal_loss: 0.6827
[epoch 22][iter  450] loss: 163710.1875 RMSElog: 8.6189 grad_loss: 16361.7002 normal_loss: 0.7001
[epoch 22][iter  460] loss: 202609.5156 RMSElog: 8.7488 grad_loss: 20251.5391 normal_loss: 0.6645
[epoch 22][iter  470] loss: 163235.8281 RMSElog: 8.8212 grad_loss: 16314.0762 normal_loss: 0.6854
[epoch 22][iter  480] loss: 153948.1719 RMSElog: 9.0119 grad_loss: 15385.0498 normal_loss: 0.7556
[epoch 22][iter  490] loss: 194555.7656 RMSElog: 8.7647 grad_loss: 19446.1582 normal_loss: 0.6515
[epoch 22][iter  500] loss: 142676.5938 RMSElog: 8.6456 grad_loss: 14258.3359 normal_loss: 0.6780
[epoch 22][iter  510] loss: 153575.7969 RMSElog: 8.6901 grad_loss: 15348.2373 normal_loss: 0.6523
[epoch 22][iter  520] loss: 125265.6719 RMSElog: 8.4421 grad_loss: 12517.3926 normal_loss: 0.7321
[epoch 22][iter  530] loss: 145303.0312 RMSElog: 8.6270 grad_loss: 14521.0215 normal_loss: 0.6549
[epoch 22][iter  540] loss: 148004.8750 RMSElog: 8.7203 grad_loss: 14791.1074 normal_loss: 0.6605
[epoch 22][iter  550] loss: 152574.4688 RMSElog: 8.5667 grad_loss: 15248.1797 normal_loss: 0.7005
[epoch 22][iter  560] loss: 166989.1875 RMSElog: 8.8055 grad_loss: 16689.4297 normal_loss: 0.6838
[epoch 22][iter  570] loss: 114139.3828 RMSElog: 8.6647 grad_loss: 11404.5322 normal_loss: 0.7407
[epoch 22][iter  580] loss: 118724.0000 RMSElog: 8.2583 grad_loss: 11863.4717 normal_loss: 0.6707
[epoch 22][iter  590] loss: 206381.3438 RMSElog: 9.0255 grad_loss: 20628.4258 normal_loss: 0.6840
[epoch 23][iter    0] loss: 176937.9688 RMSElog: 9.0212 grad_loss: 17684.0703 normal_loss: 0.7055
[epoch 23][iter   10] loss: 158014.0938 RMSElog: 8.7556 grad_loss: 15792.0020 normal_loss: 0.6527
[epoch 23][iter   20] loss: 109236.6250 RMSElog: 8.0865 grad_loss: 10914.9082 normal_loss: 0.6667
[epoch 23][iter   30] loss: 104420.5391 RMSElog: 8.3000 grad_loss: 10433.0898 normal_loss: 0.6639
[epoch 23][iter   40] loss: 153081.2656 RMSElog: 8.6014 grad_loss: 15298.8145 normal_loss: 0.7099
[epoch 23][iter   50] loss: 100646.6719 RMSElog: 8.7976 grad_loss: 10055.1895 normal_loss: 0.6802
[epoch 23][iter   60] loss: 121968.6016 RMSElog: 8.4452 grad_loss: 12187.6816 normal_loss: 0.7335
[epoch 23][iter   70] loss: 129148.3203 RMSElog: 8.8699 grad_loss: 12905.2852 normal_loss: 0.6772
[epoch 23][iter   80] loss: 204628.5781 RMSElog: 8.8827 grad_loss: 20453.3184 normal_loss: 0.6556
[epoch 23][iter   90] loss: 180302.5312 RMSElog: 8.9001 grad_loss: 18020.6484 normal_loss: 0.7053
[epoch 23][iter  100] loss: 182845.3125 RMSElog: 8.9946 grad_loss: 18274.8066 normal_loss: 0.7309
[epoch 23][iter  110] loss: 105717.3906 RMSElog: 7.9983 grad_loss: 10563.0771 normal_loss: 0.6640
[epoch 23][iter  120] loss: 194386.1094 RMSElog: 8.7980 grad_loss: 19429.1543 normal_loss: 0.6582
[epoch 23][iter  130] loss: 200447.7500 RMSElog: 8.7662 grad_loss: 20035.3594 normal_loss: 0.6513
[epoch 23][iter  140] loss: 152521.7031 RMSElog: 8.7277 grad_loss: 15242.7617 normal_loss: 0.6816
[epoch 23][iter  150] loss: 148118.4219 RMSElog: 8.7472 grad_loss: 14802.3926 normal_loss: 0.7018
[epoch 23][iter  160] loss: 127613.0391 RMSElog: 8.4663 grad_loss: 12752.0918 normal_loss: 0.7457
[epoch 23][iter  170] loss: 241952.2500 RMSElog: 9.0134 grad_loss: 24185.5312 normal_loss: 0.6794
[epoch 23][iter  180] loss: 217892.0156 RMSElog: 8.8634 grad_loss: 21779.6914 normal_loss: 0.6456
[epoch 23][iter  190] loss: 196495.8125 RMSElog: 8.6938 grad_loss: 19640.1992 normal_loss: 0.6891
[epoch 23][iter  200] loss: 221068.8906 RMSElog: 9.0495 grad_loss: 22097.1836 normal_loss: 0.6561
[epoch 23][iter  210] loss: 147747.3125 RMSElog: 8.6400 grad_loss: 14765.3564 normal_loss: 0.7349
[epoch 23][iter  220] loss: 167765.9219 RMSElog: 8.7631 grad_loss: 16767.1113 normal_loss: 0.7169
[epoch 23][iter  230] loss: 173949.0469 RMSElog: 8.8486 grad_loss: 17385.3496 normal_loss: 0.7052
[epoch 23][iter  240] loss: 162356.2656 RMSElog: 8.8635 grad_loss: 16226.0645 normal_loss: 0.6982
[epoch 23][iter  250] loss: 211618.7656 RMSElog: 8.9303 grad_loss: 21152.2871 normal_loss: 0.6610
[epoch 23][iter  260] loss: 160449.9375 RMSElog: 8.8721 grad_loss: 16035.4414 normal_loss: 0.6796
[epoch 23][iter  270] loss: 141943.3125 RMSElog: 8.5607 grad_loss: 14185.0801 normal_loss: 0.6918
[epoch 23][iter  280] loss: 108399.6562 RMSElog: 8.6209 grad_loss: 10830.6143 normal_loss: 0.7305
[epoch 23][iter  290] loss: 165436.8594 RMSElog: 8.9550 grad_loss: 16534.0117 normal_loss: 0.7190
[epoch 23][iter  300] loss: 211905.6406 RMSElog: 8.8647 grad_loss: 21181.0371 normal_loss: 0.6625
[epoch 23][iter  310] loss: 134386.5625 RMSElog: 8.7381 grad_loss: 13429.2480 normal_loss: 0.6704
[epoch 23][iter  320] loss: 132686.7500 RMSElog: 8.4944 grad_loss: 13259.5225 normal_loss: 0.6590
[epoch 23][iter  330] loss: 104110.5547 RMSElog: 8.5506 grad_loss: 10401.8262 normal_loss: 0.6783
[epoch 23][iter  340] loss: 135797.0625 RMSElog: 8.4421 grad_loss: 13570.5811 normal_loss: 0.6830
[epoch 23][iter  350] loss: 163768.9531 RMSElog: 8.7484 grad_loss: 16367.4795 normal_loss: 0.6684
[epoch 23][iter  360] loss: 143822.8750 RMSElog: 8.7400 grad_loss: 14372.8730 normal_loss: 0.6750
[epoch 23][iter  370] loss: 149525.4062 RMSElog: 8.8721 grad_loss: 14942.9727 normal_loss: 0.6949
[epoch 23][iter  380] loss: 162561.6875 RMSElog: 8.9569 grad_loss: 16246.4980 normal_loss: 0.7132
[epoch 23][iter  390] loss: 138694.4844 RMSElog: 8.5622 grad_loss: 13860.1514 normal_loss: 0.7346
[epoch 23][iter  400] loss: 72273.2734 RMSElog: 8.4746 grad_loss: 7218.1924 normal_loss: 0.6603
[epoch 23][iter  410] loss: 127445.1094 RMSElog: 8.3838 grad_loss: 12735.4648 normal_loss: 0.6624
[epoch 23][iter  420] loss: 180188.6875 RMSElog: 8.7637 grad_loss: 18009.4102 normal_loss: 0.6952
[epoch 23][iter  430] loss: 106181.8281 RMSElog: 8.4118 grad_loss: 10609.0713 normal_loss: 0.6993
[epoch 23][iter  440] loss: 105603.9844 RMSElog: 8.3317 grad_loss: 10551.3975 normal_loss: 0.6690
[epoch 23][iter  450] loss: 151386.5469 RMSElog: 8.5004 grad_loss: 15129.4385 normal_loss: 0.7169
[epoch 23][iter  460] loss: 150440.8125 RMSElog: 8.6563 grad_loss: 15034.7168 normal_loss: 0.7094
[epoch 23][iter  470] loss: 138589.6094 RMSElog: 8.5774 grad_loss: 13849.6934 normal_loss: 0.6904
[epoch 23][iter  480] loss: 138041.4688 RMSElog: 8.5314 grad_loss: 13794.9395 normal_loss: 0.6764
[epoch 23][iter  490] loss: 196002.9062 RMSElog: 8.7741 grad_loss: 19590.8633 normal_loss: 0.6537
[epoch 23][iter  500] loss: 198724.3125 RMSElog: 8.8912 grad_loss: 19862.8418 normal_loss: 0.7000
[epoch 23][iter  510] loss: 228033.6562 RMSElog: 8.9476 grad_loss: 22793.7383 normal_loss: 0.6792
[epoch 23][iter  520] loss: 167664.1406 RMSElog: 8.9648 grad_loss: 16756.7070 normal_loss: 0.7423
[epoch 23][iter  530] loss: 121303.1875 RMSElog: 8.6190 grad_loss: 12120.9570 normal_loss: 0.7419
[epoch 23][iter  540] loss: 101431.1250 RMSElog: 8.6134 grad_loss: 10133.8242 normal_loss: 0.6750
[epoch 23][iter  550] loss: 193958.5312 RMSElog: 9.0230 grad_loss: 19386.1504 normal_loss: 0.6804
[epoch 23][iter  560] loss: 159806.8906 RMSElog: 8.8102 grad_loss: 15971.1992 normal_loss: 0.6789
[epoch 23][iter  570] loss: 143514.0938 RMSElog: 8.4539 grad_loss: 14342.2578 normal_loss: 0.6972
[epoch 23][iter  580] loss: 143884.8438 RMSElog: 8.7168 grad_loss: 14379.0879 normal_loss: 0.6799
[epoch 23][iter  590] loss: 147525.3594 RMSElog: 8.7394 grad_loss: 14743.1357 normal_loss: 0.6612
[epoch 24][iter    0] loss: 142575.2969 RMSElog: 8.5646 grad_loss: 14248.3066 normal_loss: 0.6581
[epoch 24][iter   10] loss: 177915.9062 RMSElog: 8.7646 grad_loss: 17782.1543 normal_loss: 0.6712
[epoch 24][iter   20] loss: 122366.7656 RMSElog: 8.5424 grad_loss: 12227.4893 normal_loss: 0.6458
[epoch 24][iter   30] loss: 175917.2500 RMSElog: 9.0509 grad_loss: 17581.9629 normal_loss: 0.7112
[epoch 24][iter   40] loss: 144423.4062 RMSElog: 8.8214 grad_loss: 14432.8564 normal_loss: 0.6620
[epoch 24][iter   50] loss: 115646.4141 RMSElog: 8.5316 grad_loss: 11555.3799 normal_loss: 0.7305
[epoch 24][iter   60] loss: 231955.0312 RMSElog: 9.0412 grad_loss: 23185.7812 normal_loss: 0.6823
[epoch 24][iter   70] loss: 133005.2031 RMSElog: 8.5097 grad_loss: 13291.3770 normal_loss: 0.6339
[epoch 24][iter   80] loss: 141845.6719 RMSElog: 8.4052 grad_loss: 14175.4629 normal_loss: 0.6989
[epoch 24][iter   90] loss: 195651.3438 RMSElog: 8.8066 grad_loss: 19555.6504 normal_loss: 0.6769
[epoch 24][iter  100] loss: 124541.4141 RMSElog: 8.4793 grad_loss: 12445.0098 normal_loss: 0.6523
[epoch 24][iter  110] loss: 129149.1875 RMSElog: 8.7091 grad_loss: 12905.5381 normal_loss: 0.6719
[epoch 24][iter  120] loss: 131922.2031 RMSElog: 8.5440 grad_loss: 13183.0234 normal_loss: 0.6532
[epoch 24][iter  130] loss: 141426.1250 RMSElog: 8.6849 grad_loss: 14133.2441 normal_loss: 0.6833
[epoch 24][iter  140] loss: 110579.6172 RMSElog: 8.1406 grad_loss: 11049.1465 normal_loss: 0.6747
[epoch 24][iter  150] loss: 105877.3828 RMSElog: 8.4259 grad_loss: 10578.6348 normal_loss: 0.6782
[epoch 24][iter  160] loss: 131415.9531 RMSElog: 8.4629 grad_loss: 13132.4561 normal_loss: 0.6767
[epoch 24][iter  170] loss: 172667.6719 RMSElog: 8.9278 grad_loss: 17257.1445 normal_loss: 0.6954
[epoch 24][iter  180] loss: 131196.7188 RMSElog: 8.5779 grad_loss: 13110.3828 normal_loss: 0.7111
[epoch 24][iter  190] loss: 224056.5000 RMSElog: 9.0296 grad_loss: 22395.9473 normal_loss: 0.6729
[epoch 24][iter  200] loss: 114139.3750 RMSElog: 8.6648 grad_loss: 11404.5322 normal_loss: 0.7407
[epoch 24][iter  210] loss: 204628.5781 RMSElog: 8.8829 grad_loss: 20453.3184 normal_loss: 0.6555
[epoch 24][iter  220] loss: 103778.7891 RMSElog: 8.3415 grad_loss: 10368.8887 normal_loss: 0.6484
[epoch 24][iter  230] loss: 125297.1016 RMSElog: 8.6319 grad_loss: 12520.4395 normal_loss: 0.6386
[epoch 24][iter  240] loss: 198015.1719 RMSElog: 8.8927 grad_loss: 19791.9590 normal_loss: 0.6663
[epoch 24][iter  250] loss: 217892.9062 RMSElog: 8.9099 grad_loss: 21779.7227 normal_loss: 0.6575
[epoch 24][iter  260] loss: 137238.1562 RMSElog: 9.0285 grad_loss: 13714.0615 normal_loss: 0.7255
[epoch 24][iter  270] loss: 163710.5781 RMSElog: 8.6419 grad_loss: 16361.7100 normal_loss: 0.7057
[epoch 24][iter  280] loss: 158520.1250 RMSElog: 8.7412 grad_loss: 15842.5625 normal_loss: 0.7092
[epoch 24][iter  290] loss: 127096.7031 RMSElog: 8.6422 grad_loss: 12700.2891 normal_loss: 0.7387
[epoch 24][iter  300] loss: 121942.8125 RMSElog: 8.7030 grad_loss: 12184.9111 normal_loss: 0.6674
[epoch 24][iter  310] loss: 134795.4844 RMSElog: 8.3212 grad_loss: 13470.5449 normal_loss: 0.6816
[epoch 24][iter  320] loss: 155463.4688 RMSElog: 8.9288 grad_loss: 15536.7656 normal_loss: 0.6529
[epoch 24][iter  330] loss: 175277.6406 RMSElog: 8.7897 grad_loss: 17518.3125 normal_loss: 0.6628
[epoch 24][iter  340] loss: 213785.9219 RMSElog: 8.8518 grad_loss: 21369.0840 normal_loss: 0.6562
[epoch 24][iter  350] loss: 206381.3438 RMSElog: 9.0256 grad_loss: 20628.4258 normal_loss: 0.6839
[epoch 24][iter  360] loss: 142526.8125 RMSElog: 9.0244 grad_loss: 14242.9805 normal_loss: 0.6770
[epoch 24][iter  370] loss: 137275.7188 RMSElog: 8.5621 grad_loss: 13718.3311 normal_loss: 0.6778
[epoch 24][iter  380] loss: 228319.1562 RMSElog: 9.0375 grad_loss: 22822.2070 normal_loss: 0.6720
[epoch 24][iter  390] loss: 142242.1719 RMSElog: 8.4489 grad_loss: 14215.0635 normal_loss: 0.7049
[epoch 24][iter  400] loss: 181990.6094 RMSElog: 9.0908 grad_loss: 18189.2422 normal_loss: 0.7290
[epoch 24][iter  410] loss: 150132.1875 RMSElog: 8.7514 grad_loss: 15003.8066 normal_loss: 0.6609
[epoch 24][iter  420] loss: 179884.8438 RMSElog: 8.8188 grad_loss: 17978.9629 normal_loss: 0.7034
[epoch 24][iter  430] loss: 135448.4062 RMSElog: 8.7755 grad_loss: 13535.3906 normal_loss: 0.6746
[epoch 24][iter  440] loss: 147353.8438 RMSElog: 8.5689 grad_loss: 14726.1250 normal_loss: 0.6903
[epoch 24][iter  450] loss: 210634.3594 RMSElog: 9.1498 grad_loss: 21053.5957 normal_loss: 0.6894
[epoch 24][iter  460] loss: 178061.5938 RMSElog: 8.8634 grad_loss: 17796.5684 normal_loss: 0.7277
[epoch 24][iter  470] loss: 149524.8281 RMSElog: 8.8363 grad_loss: 14942.9590 normal_loss: 0.6886
[epoch 24][iter  480] loss: 191187.8438 RMSElog: 8.8699 grad_loss: 19109.1973 normal_loss: 0.7195
[epoch 24][iter  490] loss: 148004.8750 RMSElog: 8.7201 grad_loss: 14791.1074 normal_loss: 0.6606
[epoch 24][iter  500] loss: 112835.5156 RMSElog: 8.4665 grad_loss: 11274.4043 normal_loss: 0.6809
[epoch 24][iter  510] loss: 101078.7812 RMSElog: 8.3859 grad_loss: 10098.8496 normal_loss: 0.6423
[epoch 24][iter  520] loss: 196899.1406 RMSElog: 8.9397 grad_loss: 19680.3164 normal_loss: 0.6575
[epoch 24][iter  530] loss: 129724.5781 RMSElog: 8.7332 grad_loss: 12963.0586 normal_loss: 0.6663
[epoch 24][iter  540] loss: 218437.8750 RMSElog: 8.8759 grad_loss: 21834.2578 normal_loss: 0.6536
[epoch 24][iter  550] loss: 161793.4688 RMSElog: 8.8440 grad_loss: 16169.8525 normal_loss: 0.6506
[epoch 24][iter  560] loss: 153978.8125 RMSElog: 8.6079 grad_loss: 15388.6270 normal_loss: 0.6475
[epoch 24][iter  570] loss: 189563.4062 RMSElog: 8.7904 grad_loss: 18946.8594 normal_loss: 0.6894
[epoch 24][iter  580] loss: 124301.6797 RMSElog: 8.2778 grad_loss: 12421.2158 normal_loss: 0.6736
[epoch 24][iter  590] loss: 145302.6094 RMSElog: 8.5927 grad_loss: 14521.0078 normal_loss: 0.6605
[epoch 25][iter    0] loss: 135951.2969 RMSElog: 8.7094 grad_loss: 13585.6992 normal_loss: 0.7213
[epoch 25][iter   10] loss: 117385.6641 RMSElog: 8.3820 grad_loss: 11729.5254 normal_loss: 0.6594
[epoch 25][iter   20] loss: 137236.9688 RMSElog: 8.9351 grad_loss: 13714.0332 normal_loss: 0.7276
[epoch 25][iter   30] loss: 112518.0391 RMSElog: 8.5104 grad_loss: 11242.6270 normal_loss: 0.6664
[epoch 25][iter   40] loss: 233377.1875 RMSElog: 9.0379 grad_loss: 23328.0039 normal_loss: 0.6774
[epoch 25][iter   50] loss: 241952.8906 RMSElog: 9.0355 grad_loss: 24185.5645 normal_loss: 0.6886
[epoch 25][iter   60] loss: 182548.6562 RMSElog: 8.9015 grad_loss: 18245.2715 normal_loss: 0.6922
[epoch 25][iter   70] loss: 174331.3438 RMSElog: 8.7535 grad_loss: 17423.6934 normal_loss: 0.6873
[epoch 25][iter   80] loss: 176937.9688 RMSElog: 9.0213 grad_loss: 17684.0703 normal_loss: 0.7054
[epoch 25][iter   90] loss: 164750.6094 RMSElog: 8.7746 grad_loss: 16465.5879 normal_loss: 0.6965
[epoch 25][iter  100] loss: 175507.2031 RMSElog: 9.0954 grad_loss: 17540.9023 normal_loss: 0.7227
[epoch 25][iter  110] loss: 164579.5469 RMSElog: 8.8200 grad_loss: 16448.4590 normal_loss: 0.6766
[epoch 25][iter  120] loss: 162503.5625 RMSElog: 8.9003 grad_loss: 16240.7998 normal_loss: 0.6559
[epoch 25][iter  130] loss: 138915.2969 RMSElog: 8.6291 grad_loss: 13882.2422 normal_loss: 0.6579
[epoch 25][iter  140] loss: 107883.7188 RMSElog: 8.0994 grad_loss: 10779.6104 normal_loss: 0.6625
[epoch 25][iter  150] loss: 110210.7812 RMSElog: 8.0511 grad_loss: 11012.3711 normal_loss: 0.6567
[epoch 25][iter  160] loss: 213785.9219 RMSElog: 8.8518 grad_loss: 21369.0840 normal_loss: 0.6562
[epoch 25][iter  170] loss: 103500.0312 RMSElog: 8.5433 grad_loss: 10340.7930 normal_loss: 0.6669
[epoch 25][iter  180] loss: 141149.5000 RMSElog: 8.3741 grad_loss: 14105.8799 normal_loss: 0.6951
[epoch 25][iter  190] loss: 129148.3203 RMSElog: 8.8700 grad_loss: 12905.2852 normal_loss: 0.6770
[epoch 25][iter  200] loss: 145391.8906 RMSElog: 8.7154 grad_loss: 14529.7861 normal_loss: 0.6876
[epoch 25][iter  210] loss: 215522.2031 RMSElog: 9.1083 grad_loss: 21542.4395 normal_loss: 0.6745
[epoch 25][iter  220] loss: 216490.1250 RMSElog: 8.9768 grad_loss: 21639.3730 normal_loss: 0.6615
[epoch 25][iter  230] loss: 208164.9844 RMSElog: 9.1407 grad_loss: 20806.6797 normal_loss: 0.6787
[epoch 25][iter  240] loss: 160772.1719 RMSElog: 8.7993 grad_loss: 16067.7324 normal_loss: 0.6866
[epoch 25][iter  250] loss: 178767.1562 RMSElog: 8.9310 grad_loss: 17867.0801 normal_loss: 0.7023
[epoch 25][iter  260] loss: 115715.6953 RMSElog: 8.1545 grad_loss: 11562.7383 normal_loss: 0.6770
[epoch 25][iter  270] loss: 151350.6719 RMSElog: 8.6274 grad_loss: 15125.7402 normal_loss: 0.6997
[epoch 25][iter  280] loss: 145303.0312 RMSElog: 8.6267 grad_loss: 14521.0215 normal_loss: 0.6553
[epoch 25][iter  290] loss: 196899.1406 RMSElog: 8.9397 grad_loss: 19680.3164 normal_loss: 0.6575
[epoch 25][iter  300] loss: 172098.5000 RMSElog: 8.7346 grad_loss: 17200.4180 normal_loss: 0.6982
[epoch 25][iter  310] loss: 122312.4844 RMSElog: 8.5845 grad_loss: 12222.0029 normal_loss: 0.6599
[epoch 25][iter  320] loss: 202096.9375 RMSElog: 8.7442 grad_loss: 20200.2969 normal_loss: 0.6525
[epoch 25][iter  330] loss: 148227.0625 RMSElog: 8.7199 grad_loss: 14813.2959 normal_loss: 0.6902
[epoch 25][iter  340] loss: 150302.5938 RMSElog: 8.7215 grad_loss: 15020.8740 normal_loss: 0.6630
[epoch 25][iter  350] loss: 206505.1875 RMSElog: 8.8231 grad_loss: 20641.0469 normal_loss: 0.6503
[epoch 25][iter  360] loss: 145312.2031 RMSElog: 8.7563 grad_loss: 14521.8047 normal_loss: 0.6594
[epoch 25][iter  370] loss: 163769.6094 RMSElog: 8.7820 grad_loss: 16367.5010 normal_loss: 0.6775
[epoch 25][iter  380] loss: 144569.1875 RMSElog: 8.1139 grad_loss: 14448.0742 normal_loss: 0.7300
[epoch 25][iter  390] loss: 198424.2969 RMSElog: 8.9208 grad_loss: 19832.8379 normal_loss: 0.6712
[epoch 25][iter  400] loss: 176780.8594 RMSElog: 8.7698 grad_loss: 17668.6289 normal_loss: 0.6883
[epoch 25][iter  410] loss: 234926.2656 RMSElog: 8.7518 grad_loss: 23483.1641 normal_loss: 0.7100
[epoch 25][iter  420] loss: 139045.9219 RMSElog: 8.6423 grad_loss: 13895.2441 normal_loss: 0.7051
[epoch 25][iter  430] loss: 175282.4219 RMSElog: 8.6126 grad_loss: 17518.9375 normal_loss: 0.6904
[epoch 25][iter  440] loss: 165937.0156 RMSElog: 8.8791 grad_loss: 16584.1152 normal_loss: 0.7062
[epoch 25][iter  450] loss: 212217.5156 RMSElog: 8.9112 grad_loss: 21212.1914 normal_loss: 0.6478
[epoch 25][iter  460] loss: 162560.7812 RMSElog: 8.9152 grad_loss: 16246.4639 normal_loss: 0.6989
[epoch 25][iter  470] loss: 149352.0469 RMSElog: 8.7252 grad_loss: 14925.7373 normal_loss: 0.7420
[epoch 25][iter  480] loss: 154271.7031 RMSElog: 8.6992 grad_loss: 15417.8115 normal_loss: 0.6597
[epoch 25][iter  490] loss: 136780.4219 RMSElog: 8.6606 grad_loss: 13668.6816 normal_loss: 0.7005
[epoch 25][iter  500] loss: 119904.8594 RMSElog: 8.4954 grad_loss: 11981.3525 normal_loss: 0.6382
[epoch 25][iter  510] loss: 100959.4531 RMSElog: 8.1927 grad_loss: 10087.0986 normal_loss: 0.6546
[epoch 25][iter  520] loss: 155463.4688 RMSElog: 8.9288 grad_loss: 15536.7656 normal_loss: 0.6529
[epoch 25][iter  530] loss: 176245.0938 RMSElog: 9.0638 grad_loss: 17614.7422 normal_loss: 0.7039
[epoch 25][iter  540] loss: 167067.6562 RMSElog: 8.8305 grad_loss: 16697.2363 normal_loss: 0.7000
[epoch 25][iter  550] loss: 239432.7344 RMSElog: 9.0520 grad_loss: 23933.5430 normal_loss: 0.6769
[epoch 25][iter  560] loss: 149524.8281 RMSElog: 8.8363 grad_loss: 14942.9590 normal_loss: 0.6887
[epoch 25][iter  570] loss: 185127.0469 RMSElog: 8.9024 grad_loss: 18503.0625 normal_loss: 0.7405
[epoch 25][iter  580] loss: 131991.5156 RMSElog: 8.2599 grad_loss: 13190.2041 normal_loss: 0.6873
[epoch 25][iter  590] loss: 105603.9766 RMSElog: 8.3314 grad_loss: 10551.3975 normal_loss: 0.6691
[epoch 26][iter    0] loss: 215933.5312 RMSElog: 8.9020 grad_loss: 21583.8008 normal_loss: 0.6498
[epoch 26][iter   10] loss: 135873.4688 RMSElog: 8.6277 grad_loss: 13578.0713 normal_loss: 0.6474
[epoch 26][iter   20] loss: 167777.8750 RMSElog: 8.8635 grad_loss: 16768.2324 normal_loss: 0.6908
[epoch 26][iter   30] loss: 156853.5000 RMSElog: 8.8068 grad_loss: 15675.8613 normal_loss: 0.6817
[epoch 26][iter   40] loss: 178062.0156 RMSElog: 8.8827 grad_loss: 17796.5879 normal_loss: 0.7304
[epoch 26][iter   50] loss: 170525.0312 RMSElog: 8.6811 grad_loss: 17043.1016 normal_loss: 0.7211
[epoch 26][iter   60] loss: 217152.7812 RMSElog: 8.8756 grad_loss: 21705.7500 normal_loss: 0.6529
[epoch 26][iter   70] loss: 139415.3594 RMSElog: 8.6529 grad_loss: 13932.2285 normal_loss: 0.6541
[epoch 26][iter   80] loss: 105717.3906 RMSElog: 7.9984 grad_loss: 10563.0771 normal_loss: 0.6636
[epoch 26][iter   90] loss: 129724.5781 RMSElog: 8.7332 grad_loss: 12963.0586 normal_loss: 0.6662
[epoch 26][iter  100] loss: 136486.5938 RMSElog: 8.6966 grad_loss: 13639.3066 normal_loss: 0.6565
[epoch 26][iter  110] loss: 181273.0625 RMSElog: 8.7435 grad_loss: 18117.8906 normal_loss: 0.6717
[epoch 26][iter  120] loss: 203071.5781 RMSElog: 8.7358 grad_loss: 20297.7715 normal_loss: 0.6497
[epoch 26][iter  130] loss: 177915.9062 RMSElog: 8.7646 grad_loss: 17782.1543 normal_loss: 0.6712
[epoch 26][iter  140] loss: 110579.6406 RMSElog: 8.1406 grad_loss: 11049.1484 normal_loss: 0.6746
[epoch 26][iter  150] loss: 138929.5312 RMSElog: 8.6596 grad_loss: 13883.6406 normal_loss: 0.6537
[epoch 26][iter  160] loss: 146195.5312 RMSElog: 8.7192 grad_loss: 14610.1611 normal_loss: 0.6727
[epoch 26][iter  170] loss: 155951.7969 RMSElog: 8.6891 grad_loss: 15585.7510 normal_loss: 0.7393
[epoch 26][iter  180] loss: 161068.3125 RMSElog: 8.6718 grad_loss: 16097.5000 normal_loss: 0.6598
[epoch 26][iter  190] loss: 172098.5000 RMSElog: 8.7346 grad_loss: 17200.4180 normal_loss: 0.6982
[epoch 26][iter  200] loss: 130725.7109 RMSElog: 8.7210 grad_loss: 13063.1191 normal_loss: 0.7313
[epoch 26][iter  210] loss: 121168.3906 RMSElog: 8.7411 grad_loss: 12107.3525 normal_loss: 0.7451
[epoch 26][iter  220] loss: 106493.6875 RMSElog: 8.0595 grad_loss: 10640.6445 normal_loss: 0.6646
[epoch 26][iter  230] loss: 157043.5781 RMSElog: 8.8856 grad_loss: 15694.8027 normal_loss: 0.6698
[epoch 26][iter  240] loss: 139858.0312 RMSElog: 8.2697 grad_loss: 13976.7832 normal_loss: 0.7512
[epoch 26][iter  250] loss: 182845.7812 RMSElog: 9.0167 grad_loss: 18274.8262 normal_loss: 0.7340
[epoch 26][iter  260] loss: 100729.3047 RMSElog: 8.6337 grad_loss: 10063.6318 normal_loss: 0.6646
[epoch 26][iter  270] loss: 141426.5000 RMSElog: 8.6983 grad_loss: 14133.2568 normal_loss: 0.6947
[epoch 26][iter  280] loss: 201145.2188 RMSElog: 8.8705 grad_loss: 20104.9492 normal_loss: 0.7008
[epoch 26][iter  290] loss: 190951.1875 RMSElog: 8.7964 grad_loss: 19085.6562 normal_loss: 0.6655
[epoch 26][iter  300] loss: 135951.3125 RMSElog: 8.7094 grad_loss: 13585.7012 normal_loss: 0.7213
[epoch 26][iter  310] loss: 164750.1875 RMSElog: 8.7531 grad_loss: 16465.5742 normal_loss: 0.6916
[epoch 26][iter  320] loss: 117385.6641 RMSElog: 8.3820 grad_loss: 11729.5254 normal_loss: 0.6594
[epoch 26][iter  330] loss: 128471.1719 RMSElog: 8.6920 grad_loss: 12837.7480 normal_loss: 0.6768
[epoch 26][iter  340] loss: 145302.5938 RMSElog: 8.5927 grad_loss: 14521.0068 normal_loss: 0.6605
[epoch 26][iter  350] loss: 151349.9688 RMSElog: 8.5966 grad_loss: 15125.7090 normal_loss: 0.6905
[epoch 26][iter  360] loss: 103790.7656 RMSElog: 8.0553 grad_loss: 10370.3721 normal_loss: 0.6484
[epoch 26][iter  370] loss: 148163.2188 RMSElog: 8.7170 grad_loss: 14806.9453 normal_loss: 0.6604
[epoch 26][iter  380] loss: 179884.8438 RMSElog: 8.8188 grad_loss: 17978.9629 normal_loss: 0.7034
[epoch 26][iter  390] loss: 131991.9844 RMSElog: 8.2839 grad_loss: 13190.2236 normal_loss: 0.6905
[epoch 26][iter  400] loss: 105682.3906 RMSElog: 8.0519 grad_loss: 10559.5273 normal_loss: 0.6600
[epoch 26][iter  410] loss: 218437.8750 RMSElog: 8.8759 grad_loss: 21834.2578 normal_loss: 0.6536
[epoch 26][iter  420] loss: 146764.7188 RMSElog: 8.7850 grad_loss: 14666.9883 normal_loss: 0.6985
[epoch 26][iter  430] loss: 115479.1484 RMSElog: 8.4971 grad_loss: 11538.7549 normal_loss: 0.6628
[epoch 26][iter  440] loss: 142526.8125 RMSElog: 9.0244 grad_loss: 14242.9795 normal_loss: 0.6770
[epoch 26][iter  450] loss: 208164.4531 RMSElog: 9.1080 grad_loss: 20806.6582 normal_loss: 0.6796
[epoch 26][iter  460] loss: 129143.2734 RMSElog: 8.6659 grad_loss: 12904.9658 normal_loss: 0.6950
[epoch 26][iter  470] loss: 109417.1562 RMSElog: 8.0383 grad_loss: 10933.0166 normal_loss: 0.6616
[epoch 26][iter  480] loss: 156154.8594 RMSElog: 8.8881 grad_loss: 15605.9492 normal_loss: 0.6486
[epoch 26][iter  490] loss: 116051.4766 RMSElog: 8.2478 grad_loss: 11596.2354 normal_loss: 0.6636
[epoch 26][iter  500] loss: 124301.6797 RMSElog: 8.2779 grad_loss: 12421.2158 normal_loss: 0.6736
[epoch 26][iter  510] loss: 162961.3750 RMSElog: 8.6660 grad_loss: 16286.7773 normal_loss: 0.6933
[epoch 26][iter  520] loss: 160449.1094 RMSElog: 8.8204 grad_loss: 16035.4121 normal_loss: 0.6785
[epoch 26][iter  530] loss: 134657.4219 RMSElog: 8.5731 grad_loss: 13456.5059 normal_loss: 0.6631
[epoch 26][iter  540] loss: 90467.2500 RMSElog: 8.3458 grad_loss: 9037.6865 normal_loss: 0.6923
[epoch 26][iter  550] loss: 142676.5938 RMSElog: 8.6454 grad_loss: 14258.3359 normal_loss: 0.6780
[epoch 26][iter  560] loss: 150302.5938 RMSElog: 8.7215 grad_loss: 15020.8740 normal_loss: 0.6630
[epoch 26][iter  570] loss: 182170.7812 RMSElog: 8.9656 grad_loss: 18207.4062 normal_loss: 0.7071
[epoch 26][iter  580] loss: 137236.9688 RMSElog: 8.9351 grad_loss: 13714.0332 normal_loss: 0.7276
[epoch 26][iter  590] loss: 176476.6250 RMSElog: 8.7873 grad_loss: 17638.1875 normal_loss: 0.6883
[epoch 27][iter    0] loss: 135193.3906 RMSElog: 8.5547 grad_loss: 13510.1260 normal_loss: 0.6583
[epoch 27][iter   10] loss: 158211.1406 RMSElog: 8.7875 grad_loss: 15811.6689 normal_loss: 0.6583
[epoch 27][iter   20] loss: 106181.8281 RMSElog: 8.4117 grad_loss: 10609.0713 normal_loss: 0.6993
[epoch 27][iter   30] loss: 106493.6875 RMSElog: 8.0595 grad_loss: 10640.6445 normal_loss: 0.6646
[epoch 27][iter   40] loss: 173064.2812 RMSElog: 9.0394 grad_loss: 17296.7324 normal_loss: 0.6554
[epoch 27][iter   50] loss: 231678.4375 RMSElog: 9.1656 grad_loss: 23157.9922 normal_loss: 0.6861
[epoch 27][iter   60] loss: 204628.5781 RMSElog: 8.8829 grad_loss: 20453.3184 normal_loss: 0.6555
[epoch 27][iter   70] loss: 119450.1953 RMSElog: 8.2963 grad_loss: 11936.0488 normal_loss: 0.6748
[epoch 27][iter   80] loss: 182170.7812 RMSElog: 8.9656 grad_loss: 18207.4062 normal_loss: 0.7071
[epoch 27][iter   90] loss: 104434.6094 RMSElog: 8.4308 grad_loss: 10434.3652 normal_loss: 0.6652
[epoch 27][iter  100] loss: 144568.9062 RMSElog: 8.0962 grad_loss: 14448.0664 normal_loss: 0.7288
[epoch 27][iter  110] loss: 217891.9688 RMSElog: 8.8632 grad_loss: 21779.6895 normal_loss: 0.6455
[epoch 27][iter  120] loss: 159806.8750 RMSElog: 8.8100 grad_loss: 15971.1992 normal_loss: 0.6790
[epoch 27][iter  130] loss: 139078.5625 RMSElog: 8.6300 grad_loss: 13898.5605 normal_loss: 0.6652
[epoch 27][iter  140] loss: 241952.8906 RMSElog: 9.0356 grad_loss: 24185.5645 normal_loss: 0.6886
[epoch 27][iter  150] loss: 127755.9297 RMSElog: 8.6384 grad_loss: 12766.2734 normal_loss: 0.6807
[epoch 27][iter  160] loss: 138589.6094 RMSElog: 8.5773 grad_loss: 13849.6934 normal_loss: 0.6904
[epoch 27][iter  170] loss: 132686.7500 RMSElog: 8.4945 grad_loss: 13259.5225 normal_loss: 0.6588
[epoch 27][iter  180] loss: 200933.8750 RMSElog: 8.8635 grad_loss: 20083.8320 normal_loss: 0.6916
[epoch 27][iter  190] loss: 135093.4531 RMSElog: 8.5732 grad_loss: 13500.1172 normal_loss: 0.6539
[epoch 27][iter  200] loss: 214470.3906 RMSElog: 8.8802 grad_loss: 21437.5000 normal_loss: 0.6586
[epoch 27][iter  210] loss: 133005.2031 RMSElog: 8.5097 grad_loss: 13291.3770 normal_loss: 0.6338
[epoch 27][iter  220] loss: 142677.3125 RMSElog: 8.6913 grad_loss: 14258.3604 normal_loss: 0.6791
[epoch 27][iter  230] loss: 133026.5312 RMSElog: 8.6611 grad_loss: 13293.3359 normal_loss: 0.6567
[epoch 27][iter  240] loss: 208164.9844 RMSElog: 9.1406 grad_loss: 20806.6797 normal_loss: 0.6787
[epoch 27][iter  250] loss: 188482.6406 RMSElog: 8.9565 grad_loss: 18838.6367 normal_loss: 0.6692
[epoch 27][iter  260] loss: 129149.9922 RMSElog: 8.9762 grad_loss: 12905.3281 normal_loss: 0.6944
[epoch 27][iter  270] loss: 188998.9219 RMSElog: 8.9417 grad_loss: 18890.2578 normal_loss: 0.6930
[epoch 27][iter  280] loss: 140919.0156 RMSElog: 8.4684 grad_loss: 14082.7637 normal_loss: 0.6689
[epoch 27][iter  290] loss: 156980.5000 RMSElog: 9.2217 grad_loss: 15687.9600 normal_loss: 0.8683
[epoch 27][iter  300] loss: 215933.5312 RMSElog: 8.9020 grad_loss: 21583.8008 normal_loss: 0.6498
[epoch 27][iter  310] loss: 101431.1250 RMSElog: 8.6133 grad_loss: 10133.8242 normal_loss: 0.6750
[epoch 27][iter  320] loss: 175983.2500 RMSElog: 9.1518 grad_loss: 17588.2969 normal_loss: 0.8753
[epoch 27][iter  330] loss: 121816.2812 RMSElog: 8.2713 grad_loss: 12172.6816 normal_loss: 0.6744
[epoch 27][iter  340] loss: 139000.0625 RMSElog: 8.6245 grad_loss: 13890.7090 normal_loss: 0.6741
[epoch 27][iter  350] loss: 167191.6875 RMSElog: 8.7871 grad_loss: 16709.6992 normal_loss: 0.6813
[epoch 27][iter  360] loss: 106928.3438 RMSElog: 8.3060 grad_loss: 10683.8730 normal_loss: 0.6550
[epoch 27][iter  370] loss: 191539.4375 RMSElog: 8.6705 grad_loss: 19144.5820 normal_loss: 0.6910
[epoch 27][iter  380] loss: 163236.4062 RMSElog: 8.8368 grad_loss: 16314.1035 normal_loss: 0.7006
[epoch 27][iter  390] loss: 114903.7969 RMSElog: 8.3035 grad_loss: 11481.4111 normal_loss: 0.6651
[epoch 27][iter  400] loss: 165956.8906 RMSElog: 8.7471 grad_loss: 16586.2930 normal_loss: 0.6481
[epoch 27][iter  410] loss: 173703.1094 RMSElog: 8.9012 grad_loss: 17360.7109 normal_loss: 0.6997
[epoch 27][iter  420] loss: 109237.2500 RMSElog: 8.1283 grad_loss: 10914.9268 normal_loss: 0.6700
[epoch 27][iter  430] loss: 102075.1406 RMSElog: 8.3446 grad_loss: 10198.5049 normal_loss: 0.6644
[epoch 27][iter  440] loss: 107923.9844 RMSElog: 8.0546 grad_loss: 10783.6816 normal_loss: 0.6619
[epoch 27][iter  450] loss: 100958.6328 RMSElog: 8.1501 grad_loss: 10087.0664 normal_loss: 0.6469
[epoch 27][iter  460] loss: 167714.5469 RMSElog: 8.9870 grad_loss: 16761.7891 normal_loss: 0.6795
[epoch 27][iter  470] loss: 181446.1250 RMSElog: 8.8744 grad_loss: 18135.0469 normal_loss: 0.6917
[epoch 27][iter  480] loss: 164580.2500 RMSElog: 8.8475 grad_loss: 16448.4883 normal_loss: 0.6903
[epoch 27][iter  490] loss: 114363.7188 RMSElog: 8.5953 grad_loss: 11427.0518 normal_loss: 0.7248
[epoch 27][iter  500] loss: 207666.5312 RMSElog: 8.8811 grad_loss: 20757.0703 normal_loss: 0.7011
[epoch 27][iter  510] loss: 165290.2812 RMSElog: 8.8957 grad_loss: 16519.4258 normal_loss: 0.7057
[epoch 27][iter  520] loss: 203540.5938 RMSElog: 8.7406 grad_loss: 20344.6660 normal_loss: 0.6531
[epoch 27][iter  530] loss: 167728.5000 RMSElog: 8.8754 grad_loss: 16763.2734 normal_loss: 0.7013
[epoch 27][iter  540] loss: 125489.9219 RMSElog: 8.7870 grad_loss: 12539.5010 normal_loss: 0.7045
[epoch 27][iter  550] loss: 166475.3438 RMSElog: 8.8007 grad_loss: 16638.0449 normal_loss: 0.6888
[epoch 27][iter  560] loss: 104455.6875 RMSElog: 8.0465 grad_loss: 10436.8838 normal_loss: 0.6373
[epoch 27][iter  570] loss: 190917.3125 RMSElog: 9.0337 grad_loss: 19082.0293 normal_loss: 0.6686
[epoch 27][iter  580] loss: 137745.4219 RMSElog: 8.6268 grad_loss: 13765.2588 normal_loss: 0.6563
[epoch 27][iter  590] loss: 137236.9688 RMSElog: 8.9351 grad_loss: 13714.0332 normal_loss: 0.7277
[epoch 28][iter    0] loss: 142118.3125 RMSElog: 8.4643 grad_loss: 14202.6797 normal_loss: 0.6879
[epoch 28][iter   10] loss: 160649.1406 RMSElog: 8.8637 grad_loss: 16055.3887 normal_loss: 0.6621
[epoch 28][iter   20] loss: 172236.6250 RMSElog: 8.8302 grad_loss: 17214.0859 normal_loss: 0.7463
[epoch 28][iter   30] loss: 175528.9375 RMSElog: 8.8611 grad_loss: 17543.3555 normal_loss: 0.6781
[epoch 28][iter   40] loss: 182845.7812 RMSElog: 9.0167 grad_loss: 18274.8262 normal_loss: 0.7340
[epoch 28][iter   50] loss: 165290.2812 RMSElog: 8.8957 grad_loss: 16519.4258 normal_loss: 0.7056
[epoch 28][iter   60] loss: 118723.9922 RMSElog: 8.2582 grad_loss: 11863.4707 normal_loss: 0.6706
[epoch 28][iter   70] loss: 139000.0625 RMSElog: 8.6245 grad_loss: 13890.7090 normal_loss: 0.6741
[epoch 28][iter   80] loss: 188481.8594 RMSElog: 8.9086 grad_loss: 18838.6172 normal_loss: 0.6598
[epoch 28][iter   90] loss: 148163.2188 RMSElog: 8.7169 grad_loss: 14806.9453 normal_loss: 0.6604
[epoch 28][iter  100] loss: 114883.1250 RMSElog: 8.6509 grad_loss: 11478.9268 normal_loss: 0.7339
[epoch 28][iter  110] loss: 112835.5156 RMSElog: 8.4665 grad_loss: 11274.4043 normal_loss: 0.6809
[epoch 28][iter  120] loss: 166475.3438 RMSElog: 8.8007 grad_loss: 16638.0449 normal_loss: 0.6888
[epoch 28][iter  130] loss: 167327.2188 RMSElog: 8.7888 grad_loss: 16723.2402 normal_loss: 0.6942
[epoch 28][iter  140] loss: 156154.8594 RMSElog: 8.8881 grad_loss: 15605.9492 normal_loss: 0.6486
[epoch 28][iter  150] loss: 163681.8125 RMSElog: 8.8063 grad_loss: 16358.6289 normal_loss: 0.7460
[epoch 28][iter  160] loss: 134614.5000 RMSElog: 8.7124 grad_loss: 13452.0605 normal_loss: 0.6774
[epoch 28][iter  170] loss: 167727.9531 RMSElog: 8.8459 grad_loss: 16763.2598 normal_loss: 0.6896
[epoch 28][iter  180] loss: 140035.8594 RMSElog: 8.6433 grad_loss: 13994.2881 normal_loss: 0.6547
[epoch 28][iter  190] loss: 193958.5312 RMSElog: 9.0228 grad_loss: 19386.1504 normal_loss: 0.6804
[epoch 28][iter  200] loss: 181273.0625 RMSElog: 8.7434 grad_loss: 18117.8906 normal_loss: 0.6717
[epoch 28][iter  210] loss: 176244.5469 RMSElog: 9.0361 grad_loss: 17614.7246 normal_loss: 0.6962
[epoch 28][iter  220] loss: 196899.7656 RMSElog: 8.9802 grad_loss: 19680.3359 normal_loss: 0.6595
[epoch 28][iter  230] loss: 180302.5312 RMSElog: 8.9003 grad_loss: 18020.6484 normal_loss: 0.7053
[epoch 28][iter  240] loss: 162504.4375 RMSElog: 8.9399 grad_loss: 16240.8340 normal_loss: 0.6701
[epoch 28][iter  250] loss: 167765.9219 RMSElog: 8.7633 grad_loss: 16767.1113 normal_loss: 0.7168
[epoch 28][iter  260] loss: 202609.9844 RMSElog: 8.7789 grad_loss: 20251.5527 normal_loss: 0.6655
[epoch 28][iter  270] loss: 105877.9375 RMSElog: 8.4742 grad_loss: 10578.6416 normal_loss: 0.6774
[epoch 28][iter  280] loss: 156963.2656 RMSElog: 8.8586 grad_loss: 15686.8203 normal_loss: 0.6489
[epoch 28][iter  290] loss: 105804.9609 RMSElog: 8.3606 grad_loss: 10571.4668 normal_loss: 0.6689
[epoch 28][iter  300] loss: 164750.1875 RMSElog: 8.7531 grad_loss: 16465.5742 normal_loss: 0.6916
[epoch 28][iter  310] loss: 145391.8906 RMSElog: 8.7154 grad_loss: 14529.7861 normal_loss: 0.6875
[epoch 28][iter  320] loss: 167723.1562 RMSElog: 8.8400 grad_loss: 16762.7812 normal_loss: 0.6950
[epoch 28][iter  330] loss: 162355.8438 RMSElog: 8.8228 grad_loss: 16226.0605 normal_loss: 0.7000
[epoch 28][iter  340] loss: 135448.4062 RMSElog: 8.7754 grad_loss: 13535.3906 normal_loss: 0.6746
[epoch 28][iter  350] loss: 150303.8906 RMSElog: 8.8670 grad_loss: 15020.8398 normal_loss: 0.6828
[epoch 28][iter  360] loss: 177648.7188 RMSElog: 8.7827 grad_loss: 17755.3828 normal_loss: 0.7044
[epoch 28][iter  370] loss: 166054.3125 RMSElog: 8.7676 grad_loss: 16595.9648 normal_loss: 0.6993
[epoch 28][iter  380] loss: 122427.9609 RMSElog: 8.3860 grad_loss: 12233.7168 normal_loss: 0.6929
[epoch 28][iter  390] loss: 140623.7344 RMSElog: 8.6447 grad_loss: 14053.0684 normal_loss: 0.6608
[epoch 28][iter  400] loss: 170040.0156 RMSElog: 8.7737 grad_loss: 16994.5469 normal_loss: 0.6816
[epoch 28][iter  410] loss: 189563.4062 RMSElog: 8.7904 grad_loss: 18946.8594 normal_loss: 0.6894
[epoch 28][iter  420] loss: 186028.9688 RMSElog: 8.7860 grad_loss: 18593.4336 normal_loss: 0.6772
[epoch 28][iter  430] loss: 122894.3438 RMSElog: 8.6533 grad_loss: 12280.1016 normal_loss: 0.6801
[epoch 28][iter  440] loss: 150282.3125 RMSElog: 8.6605 grad_loss: 15018.9150 normal_loss: 0.6554
[epoch 28][iter  450] loss: 141210.3594 RMSElog: 8.7946 grad_loss: 14111.5762 normal_loss: 0.6652
[epoch 28][iter  460] loss: 104957.1172 RMSElog: 8.5603 grad_loss: 10486.4941 normal_loss: 0.6569
[epoch 28][iter  470] loss: 191187.3750 RMSElog: 8.8389 grad_loss: 19109.1758 normal_loss: 0.7229
[epoch 28][iter  480] loss: 131198.2500 RMSElog: 8.6984 grad_loss: 13110.4150 normal_loss: 0.7119
[epoch 28][iter  490] loss: 148222.1562 RMSElog: 8.7880 grad_loss: 14812.7676 normal_loss: 0.6593
[epoch 28][iter  500] loss: 162415.7344 RMSElog: 8.8347 grad_loss: 16232.0723 normal_loss: 0.6660
[epoch 28][iter  510] loss: 136046.1250 RMSElog: 8.3679 grad_loss: 13595.5449 normal_loss: 0.6991
[epoch 28][iter  520] loss: 200933.8750 RMSElog: 8.8635 grad_loss: 20083.8320 normal_loss: 0.6916
[epoch 28][iter  530] loss: 191802.6875 RMSElog: 8.8167 grad_loss: 19170.7969 normal_loss: 0.6562
[epoch 28][iter  540] loss: 227059.6875 RMSElog: 9.0943 grad_loss: 22696.2188 normal_loss: 0.6571
[epoch 28][iter  550] loss: 138041.0000 RMSElog: 8.4987 grad_loss: 13794.9229 normal_loss: 0.6785
[epoch 28][iter  560] loss: 127096.7031 RMSElog: 8.6423 grad_loss: 12700.2891 normal_loss: 0.7386
[epoch 28][iter  570] loss: 154687.2500 RMSElog: 8.8054 grad_loss: 15459.2461 normal_loss: 0.6742
[epoch 28][iter  580] loss: 202096.9375 RMSElog: 8.7443 grad_loss: 20200.2969 normal_loss: 0.6525
[epoch 28][iter  590] loss: 161614.2344 RMSElog: 8.8986 grad_loss: 16151.8662 normal_loss: 0.6590
[epoch 29][iter    0] loss: 195979.2500 RMSElog: 8.9566 grad_loss: 19588.3105 normal_loss: 0.6586
[epoch 29][iter   10] loss: 103500.0312 RMSElog: 8.5433 grad_loss: 10340.7930 normal_loss: 0.6669
[epoch 29][iter   20] loss: 143822.8750 RMSElog: 8.7399 grad_loss: 14372.8730 normal_loss: 0.6749
[epoch 29][iter   30] loss: 104456.9688 RMSElog: 8.1339 grad_loss: 10436.9189 normal_loss: 0.6441
[epoch 29][iter   40] loss: 227303.7812 RMSElog: 9.1408 grad_loss: 22720.5781 normal_loss: 0.6594
[epoch 29][iter   50] loss: 132023.8125 RMSElog: 8.8458 grad_loss: 13192.8525 normal_loss: 0.6835
[epoch 29][iter   60] loss: 112835.5156 RMSElog: 8.4664 grad_loss: 11274.4043 normal_loss: 0.6809
[epoch 29][iter   70] loss: 165557.5000 RMSElog: 8.7976 grad_loss: 16546.2715 normal_loss: 0.6824
[epoch 29][iter   80] loss: 140918.5000 RMSElog: 8.4399 grad_loss: 14082.7441 normal_loss: 0.6668
[epoch 29][iter   90] loss: 196004.0312 RMSElog: 8.8267 grad_loss: 19590.9141 normal_loss: 0.6630
[epoch 29][iter  100] loss: 175282.4062 RMSElog: 8.6126 grad_loss: 17518.9375 normal_loss: 0.6904
[epoch 29][iter  110] loss: 138041.0000 RMSElog: 8.4987 grad_loss: 13794.9229 normal_loss: 0.6785
[epoch 29][iter  120] loss: 205340.7656 RMSElog: 8.8006 grad_loss: 20524.6289 normal_loss: 0.6465
[epoch 29][iter  130] loss: 135448.4062 RMSElog: 8.7754 grad_loss: 13535.3906 normal_loss: 0.6746
[epoch 29][iter  140] loss: 174746.0000 RMSElog: 8.7799 grad_loss: 17465.1094 normal_loss: 0.7117
[epoch 29][iter  150] loss: 199052.5312 RMSElog: 8.8685 grad_loss: 19895.6797 normal_loss: 0.7057
[epoch 29][iter  160] loss: 147525.3594 RMSElog: 8.7392 grad_loss: 14743.1357 normal_loss: 0.6611
[epoch 29][iter  170] loss: 148227.0625 RMSElog: 8.7199 grad_loss: 14813.2959 normal_loss: 0.6902
[epoch 29][iter  180] loss: 183073.2031 RMSElog: 8.5994 grad_loss: 18298.0312 normal_loss: 0.6896
[epoch 29][iter  190] loss: 127482.6406 RMSElog: 8.2795 grad_loss: 12739.3027 normal_loss: 0.6818
[epoch 29][iter  200] loss: 182460.7500 RMSElog: 8.9574 grad_loss: 18236.4102 normal_loss: 0.7078
[epoch 29][iter  210] loss: 148118.4062 RMSElog: 8.7473 grad_loss: 14802.3926 normal_loss: 0.7016
[epoch 29][iter  220] loss: 133026.5312 RMSElog: 8.6611 grad_loss: 13293.3359 normal_loss: 0.6567
[epoch 29][iter  230] loss: 239748.2031 RMSElog: 8.9687 grad_loss: 23965.1797 normal_loss: 0.6712
[epoch 29][iter  240] loss: 109914.5469 RMSElog: 8.4566 grad_loss: 10982.3086 normal_loss: 0.6897
[epoch 29][iter  250] loss: 104434.6094 RMSElog: 8.4308 grad_loss: 10434.3652 normal_loss: 0.6651
[epoch 29][iter  260] loss: 218527.2031 RMSElog: 9.1198 grad_loss: 21842.9141 normal_loss: 0.6881
[epoch 29][iter  270] loss: 165290.2812 RMSElog: 8.8957 grad_loss: 16519.4258 normal_loss: 0.7057
[epoch 29][iter  280] loss: 117090.5391 RMSElog: 8.2817 grad_loss: 11700.1055 normal_loss: 0.6671
[epoch 29][iter  290] loss: 154538.2031 RMSElog: 8.8582 grad_loss: 15444.2822 normal_loss: 0.6795
[epoch 29][iter  300] loss: 135951.2969 RMSElog: 8.7095 grad_loss: 13585.6992 normal_loss: 0.7213
[epoch 29][iter  310] loss: 115325.8125 RMSElog: 8.2683 grad_loss: 11523.6465 normal_loss: 0.6656
[epoch 29][iter  320] loss: 124614.9922 RMSElog: 8.5019 grad_loss: 12452.3281 normal_loss: 0.6689
[epoch 29][iter  330] loss: 129149.1875 RMSElog: 8.7091 grad_loss: 12905.5381 normal_loss: 0.6718
[epoch 29][iter  340] loss: 232751.5938 RMSElog: 9.0182 grad_loss: 23265.4727 normal_loss: 0.6699
[epoch 29][iter  350] loss: 208800.7188 RMSElog: 8.8111 grad_loss: 20870.6094 normal_loss: 0.6522
[epoch 29][iter  360] loss: 164750.1875 RMSElog: 8.7531 grad_loss: 16465.5742 normal_loss: 0.6916
[epoch 29][iter  370] loss: 105805.7969 RMSElog: 8.4183 grad_loss: 10571.4883 normal_loss: 0.6734
[epoch 29][iter  380] loss: 126005.3281 RMSElog: 8.4625 grad_loss: 12591.3906 normal_loss: 0.6795
[epoch 29][iter  390] loss: 213396.6406 RMSElog: 9.0003 grad_loss: 21329.9805 normal_loss: 0.6841
[epoch 29][iter  400] loss: 183854.5000 RMSElog: 9.0350 grad_loss: 18375.7051 normal_loss: 0.7092
[epoch 29][iter  410] loss: 195755.9062 RMSElog: 8.8273 grad_loss: 19566.0938 normal_loss: 0.6677
[epoch 29][iter  420] loss: 153979.6094 RMSElog: 8.6388 grad_loss: 15388.6641 normal_loss: 0.6585
[epoch 29][iter  430] loss: 122312.4844 RMSElog: 8.5845 grad_loss: 12222.0029 normal_loss: 0.6599
[epoch 29][iter  440] loss: 103331.7656 RMSElog: 8.0592 grad_loss: 10324.4668 normal_loss: 0.6503
[epoch 29][iter  450] loss: 196899.1406 RMSElog: 8.9398 grad_loss: 19680.3164 normal_loss: 0.6575
[epoch 29][iter  460] loss: 130507.1016 RMSElog: 8.5754 grad_loss: 13041.4688 normal_loss: 0.6662
[epoch 29][iter  470] loss: 218604.3594 RMSElog: 8.8691 grad_loss: 21850.8965 normal_loss: 0.6700
[epoch 29][iter  480] loss: 162416.4375 RMSElog: 8.8700 grad_loss: 16232.0947 normal_loss: 0.6794
[epoch 29][iter  490] loss: 163574.9688 RMSElog: 8.7480 grad_loss: 16348.0811 normal_loss: 0.6672
[epoch 29][iter  500] loss: 170524.2188 RMSElog: 8.6305 grad_loss: 17043.0781 normal_loss: 0.7125
[epoch 29][iter  510] loss: 163759.1719 RMSElog: 8.7170 grad_loss: 16366.4805 normal_loss: 0.7192
[epoch 29][iter  520] loss: 122366.7656 RMSElog: 8.5424 grad_loss: 12227.4893 normal_loss: 0.6458
[epoch 29][iter  530] loss: 203000.4844 RMSElog: 8.8285 grad_loss: 20290.5684 normal_loss: 0.6530
[epoch 29][iter  540] loss: 210500.4375 RMSElog: 8.9463 grad_loss: 21040.4512 normal_loss: 0.6458
[epoch 29][iter  550] loss: 106157.8047 RMSElog: 8.0432 grad_loss: 10607.0791 normal_loss: 0.6584
[epoch 29][iter  560] loss: 100625.3750 RMSElog: 8.0304 grad_loss: 10053.8682 normal_loss: 0.6387
[epoch 29][iter  570] loss: 170471.5312 RMSElog: 8.7745 grad_loss: 17037.7109 normal_loss: 0.6661
[epoch 29][iter  580] loss: 107049.5938 RMSElog: 8.0690 grad_loss: 10696.2256 normal_loss: 0.6642
[epoch 29][iter  590] loss: 135679.7031 RMSElog: 8.3691 grad_loss: 13558.9014 normal_loss: 0.6988
###############
#epochs=80
###############
[epoch  0][iter    0] loss: 96.6826 RMSElog: 9.6683 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter   10] loss: 101.8512 RMSElog: 10.1851 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter   20] loss: 102.7256 RMSElog: 10.2726 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter   30] loss: 102.5333 RMSElog: 10.2533 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter   40] loss: 103.4060 RMSElog: 10.3406 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter   50] loss: 101.7941 RMSElog: 10.1794 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter   60] loss: 99.4623 RMSElog: 9.9462 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter   70] loss: 105.6677 RMSElog: 10.5668 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter   80] loss: 104.6128 RMSElog: 10.4613 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter   90] loss: 102.0681 RMSElog: 10.2068 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  100] loss: 103.5810 RMSElog: 10.3581 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  110] loss: 99.6793 RMSElog: 9.9679 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  120] loss: 106.1550 RMSElog: 10.6155 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  130] loss: 98.8509 RMSElog: 9.8851 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  140] loss: 102.4039 RMSElog: 10.2404 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  150] loss: 102.1226 RMSElog: 10.2123 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  160] loss: 99.4914 RMSElog: 9.9491 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  170] loss: 101.0637 RMSElog: 10.1064 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  180] loss: 102.0280 RMSElog: 10.2028 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  190] loss: 99.5242 RMSElog: 9.9524 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  200] loss: 96.0814 RMSElog: 9.6081 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  210] loss: 95.7316 RMSElog: 9.5732 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  220] loss: 101.7287 RMSElog: 10.1729 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  230] loss: 104.1326 RMSElog: 10.4133 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  240] loss: 103.4952 RMSElog: 10.3495 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  250] loss: 99.1753 RMSElog: 9.9175 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  260] loss: 99.6551 RMSElog: 9.9655 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  270] loss: 97.4319 RMSElog: 9.7432 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  280] loss: 99.7375 RMSElog: 9.9738 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  290] loss: 97.1322 RMSElog: 9.7132 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  300] loss: 102.8102 RMSElog: 10.2810 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  310] loss: 100.0774 RMSElog: 10.0077 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  320] loss: 96.2130 RMSElog: 9.6213 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  330] loss: 92.7374 RMSElog: 9.2737 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  340] loss: 97.4844 RMSElog: 9.7484 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  350] loss: 97.9433 RMSElog: 9.7943 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  360] loss: 96.9446 RMSElog: 9.6945 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  370] loss: 103.4173 RMSElog: 10.3417 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  380] loss: 99.2293 RMSElog: 9.9229 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  390] loss: 98.8696 RMSElog: 9.8870 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  400] loss: 100.6011 RMSElog: 10.0601 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  410] loss: 97.2120 RMSElog: 9.7212 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  420] loss: 102.2770 RMSElog: 10.2277 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  430] loss: 100.2353 RMSElog: 10.0235 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  440] loss: 101.7422 RMSElog: 10.1742 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  450] loss: 101.1643 RMSElog: 10.1164 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  460] loss: 98.5979 RMSElog: 9.8598 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  470] loss: 97.4061 RMSElog: 9.7406 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  480] loss: 100.1776 RMSElog: 10.0178 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  490] loss: 102.7446 RMSElog: 10.2745 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  500] loss: 97.5966 RMSElog: 9.7597 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  510] loss: 97.5013 RMSElog: 9.7501 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  520] loss: 98.3133 RMSElog: 9.8313 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  530] loss: 97.4836 RMSElog: 9.7484 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  540] loss: 97.4750 RMSElog: 9.7475 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  550] loss: 98.9282 RMSElog: 9.8928 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  560] loss: 97.4285 RMSElog: 9.7429 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  570] loss: 97.7570 RMSElog: 9.7757 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  580] loss: 101.8018 RMSElog: 10.1802 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  0][iter  590] loss: 103.8059 RMSElog: 10.3806 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter    0] loss: 99.1613 RMSElog: 9.9161 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter   10] loss: 97.7528 RMSElog: 9.7753 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter   20] loss: 99.0620 RMSElog: 9.9062 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter   30] loss: 97.7919 RMSElog: 9.7792 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter   40] loss: 97.9029 RMSElog: 9.7903 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter   50] loss: 99.7147 RMSElog: 9.9715 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter   60] loss: 99.1545 RMSElog: 9.9155 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter   70] loss: 97.6675 RMSElog: 9.7667 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter   80] loss: 99.8294 RMSElog: 9.9829 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter   90] loss: 96.8697 RMSElog: 9.6870 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  100] loss: 98.5712 RMSElog: 9.8571 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  110] loss: 96.6516 RMSElog: 9.6652 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  120] loss: 100.8189 RMSElog: 10.0819 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  130] loss: 91.8050 RMSElog: 9.1805 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  140] loss: 102.7508 RMSElog: 10.2751 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  150] loss: 97.4671 RMSElog: 9.7467 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  160] loss: 99.8099 RMSElog: 9.9810 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  170] loss: 94.3540 RMSElog: 9.4354 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  180] loss: 100.2277 RMSElog: 10.0228 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  190] loss: 101.0172 RMSElog: 10.1017 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  200] loss: 101.6594 RMSElog: 10.1659 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  210] loss: 99.7231 RMSElog: 9.9723 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  220] loss: 98.6099 RMSElog: 9.8610 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  230] loss: 100.0386 RMSElog: 10.0039 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  240] loss: 101.7815 RMSElog: 10.1781 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  250] loss: 94.2204 RMSElog: 9.4220 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  260] loss: 97.8211 RMSElog: 9.7821 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  270] loss: 93.2694 RMSElog: 9.3269 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  280] loss: 95.1448 RMSElog: 9.5145 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  290] loss: 99.9740 RMSElog: 9.9974 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  300] loss: 100.9375 RMSElog: 10.0938 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  310] loss: 99.4847 RMSElog: 9.9485 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  320] loss: 99.4786 RMSElog: 9.9479 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  330] loss: 96.8769 RMSElog: 9.6877 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  340] loss: 93.9718 RMSElog: 9.3972 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  350] loss: 99.2918 RMSElog: 9.9292 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  360] loss: 98.8900 RMSElog: 9.8890 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  370] loss: 100.2438 RMSElog: 10.0244 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  380] loss: 101.0081 RMSElog: 10.1008 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  390] loss: 102.4852 RMSElog: 10.2485 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  400] loss: 98.8006 RMSElog: 9.8801 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  410] loss: 98.2220 RMSElog: 9.8222 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  420] loss: 98.1105 RMSElog: 9.8111 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  430] loss: 100.3227 RMSElog: 10.0323 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  440] loss: 101.5933 RMSElog: 10.1593 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  450] loss: 97.3025 RMSElog: 9.7302 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  460] loss: 99.4989 RMSElog: 9.9499 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  470] loss: 99.4001 RMSElog: 9.9400 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  480] loss: 101.3661 RMSElog: 10.1366 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  490] loss: 99.0564 RMSElog: 9.9056 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  500] loss: 97.9668 RMSElog: 9.7967 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  510] loss: 96.6271 RMSElog: 9.6627 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  520] loss: 103.0374 RMSElog: 10.3037 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  530] loss: 96.9751 RMSElog: 9.6975 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  540] loss: 99.1122 RMSElog: 9.9112 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  550] loss: 97.2972 RMSElog: 9.7297 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  560] loss: 102.4609 RMSElog: 10.2461 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  570] loss: 96.9348 RMSElog: 9.6935 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  580] loss: 99.2774 RMSElog: 9.9277 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  1][iter  590] loss: 99.0421 RMSElog: 9.9042 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter    0] loss: 99.1679 RMSElog: 9.9168 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter   10] loss: 103.8317 RMSElog: 10.3832 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter   20] loss: 96.1357 RMSElog: 9.6136 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter   30] loss: 92.8975 RMSElog: 9.2898 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter   40] loss: 94.7136 RMSElog: 9.4714 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter   50] loss: 92.2853 RMSElog: 9.2285 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter   60] loss: 91.5769 RMSElog: 9.1577 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter   70] loss: 102.2206 RMSElog: 10.2221 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter   80] loss: 99.3237 RMSElog: 9.9324 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter   90] loss: 101.1983 RMSElog: 10.1198 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  100] loss: 97.0622 RMSElog: 9.7062 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  110] loss: 99.7166 RMSElog: 9.9717 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  120] loss: 98.0991 RMSElog: 9.8099 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  130] loss: 97.1192 RMSElog: 9.7119 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  140] loss: 98.4732 RMSElog: 9.8473 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  150] loss: 100.2014 RMSElog: 10.0201 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  160] loss: 97.6003 RMSElog: 9.7600 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  170] loss: 96.7008 RMSElog: 9.6701 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  180] loss: 95.8956 RMSElog: 9.5896 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  190] loss: 96.8870 RMSElog: 9.6887 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  200] loss: 104.0727 RMSElog: 10.4073 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  210] loss: 99.9804 RMSElog: 9.9980 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  220] loss: 96.5481 RMSElog: 9.6548 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  230] loss: 97.1177 RMSElog: 9.7118 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  240] loss: 100.6772 RMSElog: 10.0677 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  250] loss: 98.2893 RMSElog: 9.8289 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  260] loss: 91.1288 RMSElog: 9.1129 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  270] loss: 96.8654 RMSElog: 9.6865 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  280] loss: 98.2969 RMSElog: 9.8297 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  290] loss: 98.4339 RMSElog: 9.8434 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  300] loss: 93.9825 RMSElog: 9.3983 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  310] loss: 100.3815 RMSElog: 10.0381 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  320] loss: 95.5435 RMSElog: 9.5543 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  330] loss: 95.8800 RMSElog: 9.5880 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  340] loss: 102.2537 RMSElog: 10.2254 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  350] loss: 96.9657 RMSElog: 9.6966 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  360] loss: 100.5099 RMSElog: 10.0510 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  370] loss: 97.3420 RMSElog: 9.7342 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  380] loss: 99.3301 RMSElog: 9.9330 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  390] loss: 102.7829 RMSElog: 10.2783 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  400] loss: 102.2882 RMSElog: 10.2288 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  410] loss: 97.4234 RMSElog: 9.7423 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  420] loss: 97.6621 RMSElog: 9.7662 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  430] loss: 97.0313 RMSElog: 9.7031 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  440] loss: 99.1419 RMSElog: 9.9142 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  450] loss: 97.3766 RMSElog: 9.7377 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  460] loss: 96.2759 RMSElog: 9.6276 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  470] loss: 97.5261 RMSElog: 9.7526 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  480] loss: 99.6722 RMSElog: 9.9672 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  490] loss: 100.4557 RMSElog: 10.0456 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  500] loss: 104.1274 RMSElog: 10.4127 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  510] loss: 99.4066 RMSElog: 9.9407 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  520] loss: 97.1062 RMSElog: 9.7106 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  530] loss: 102.9953 RMSElog: 10.2995 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  540] loss: 99.6630 RMSElog: 9.9663 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  550] loss: 92.5311 RMSElog: 9.2531 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  560] loss: 101.2549 RMSElog: 10.1255 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  570] loss: 96.7327 RMSElog: 9.6733 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  580] loss: 98.0348 RMSElog: 9.8035 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  2][iter  590] loss: 96.9767 RMSElog: 9.6977 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter    0] loss: 99.3848 RMSElog: 9.9385 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter   10] loss: 98.8429 RMSElog: 9.8843 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter   20] loss: 97.5521 RMSElog: 9.7552 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter   30] loss: 96.2185 RMSElog: 9.6219 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter   40] loss: 99.1477 RMSElog: 9.9148 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter   50] loss: 96.5044 RMSElog: 9.6504 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter   60] loss: 99.2346 RMSElog: 9.9235 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter   70] loss: 98.0536 RMSElog: 9.8054 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter   80] loss: 100.2627 RMSElog: 10.0263 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter   90] loss: 98.5399 RMSElog: 9.8540 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  100] loss: 99.7144 RMSElog: 9.9714 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  110] loss: 98.1829 RMSElog: 9.8183 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  120] loss: 97.4644 RMSElog: 9.7464 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  130] loss: 101.4699 RMSElog: 10.1470 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  140] loss: 99.9806 RMSElog: 9.9981 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  150] loss: 102.4138 RMSElog: 10.2414 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  160] loss: 99.4019 RMSElog: 9.9402 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  170] loss: 101.2381 RMSElog: 10.1238 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  180] loss: 102.2882 RMSElog: 10.2288 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  190] loss: 101.9158 RMSElog: 10.1916 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  200] loss: 97.6821 RMSElog: 9.7682 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  210] loss: 100.2848 RMSElog: 10.0285 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  220] loss: 101.4907 RMSElog: 10.1491 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  230] loss: 97.4408 RMSElog: 9.7441 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  240] loss: 104.4796 RMSElog: 10.4480 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  250] loss: 98.2523 RMSElog: 9.8252 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  260] loss: 98.3247 RMSElog: 9.8325 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  270] loss: 97.8632 RMSElog: 9.7863 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  280] loss: 101.5924 RMSElog: 10.1592 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  290] loss: 100.9302 RMSElog: 10.0930 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  300] loss: 95.7654 RMSElog: 9.5765 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  310] loss: 101.2056 RMSElog: 10.1206 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  320] loss: 95.5603 RMSElog: 9.5560 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  330] loss: 101.7185 RMSElog: 10.1719 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  340] loss: 102.3188 RMSElog: 10.2319 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  350] loss: 97.8259 RMSElog: 9.7826 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  360] loss: 100.5099 RMSElog: 10.0510 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  370] loss: 98.3555 RMSElog: 9.8355 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  380] loss: 101.5307 RMSElog: 10.1531 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  390] loss: 98.0169 RMSElog: 9.8017 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  400] loss: 100.1311 RMSElog: 10.0131 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  410] loss: 96.0156 RMSElog: 9.6016 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  420] loss: 99.9277 RMSElog: 9.9928 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  430] loss: 96.7556 RMSElog: 9.6756 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  440] loss: 101.7802 RMSElog: 10.1780 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  450] loss: 94.2154 RMSElog: 9.4215 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  460] loss: 92.6056 RMSElog: 9.2606 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  470] loss: 97.7581 RMSElog: 9.7758 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  480] loss: 97.6624 RMSElog: 9.7662 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  490] loss: 94.4136 RMSElog: 9.4414 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  500] loss: 91.7611 RMSElog: 9.1761 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  510] loss: 99.3281 RMSElog: 9.9328 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  520] loss: 98.6529 RMSElog: 9.8653 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  530] loss: 92.7689 RMSElog: 9.2769 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  540] loss: 91.8944 RMSElog: 9.1894 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  550] loss: 99.5568 RMSElog: 9.9557 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  560] loss: 94.2778 RMSElog: 9.4278 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  570] loss: 101.8982 RMSElog: 10.1898 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  580] loss: 99.4045 RMSElog: 9.9405 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  3][iter  590] loss: 98.8814 RMSElog: 9.8881 grad_loss: 0.0000 normal_loss: 0.0000
[epoch  4][iter    0] loss: 161275.7500 RMSElog: 9.8339 grad_loss: 16117.7402 normal_loss: 0.0000
[epoch  4][iter   10] loss: 95212.1016 RMSElog: 9.1894 grad_loss: 9512.0205 normal_loss: 0.0000
[epoch  4][iter   20] loss: 150144.0156 RMSElog: 10.0554 grad_loss: 15004.3457 normal_loss: 0.0000
[epoch  4][iter   30] loss: 137287.1562 RMSElog: 9.9750 grad_loss: 13718.7412 normal_loss: 0.0000
[epoch  4][iter   40] loss: 188652.9531 RMSElog: 10.2592 grad_loss: 18855.0352 normal_loss: 0.0000
[epoch  4][iter   50] loss: 114817.7734 RMSElog: 9.7767 grad_loss: 11472.0010 normal_loss: 0.0000
[epoch  4][iter   60] loss: 108400.6172 RMSElog: 9.2105 grad_loss: 10830.8506 normal_loss: 0.0000
[epoch  4][iter   70] loss: 172674.3750 RMSElog: 9.9061 grad_loss: 17257.5312 normal_loss: 0.0000
[epoch  4][iter   80] loss: 162423.6719 RMSElog: 9.8183 grad_loss: 16232.5488 normal_loss: 0.0000
[epoch  4][iter   90] loss: 184006.8438 RMSElog: 9.9468 grad_loss: 18390.7363 normal_loss: 0.0000
[epoch  4][iter  100] loss: 163776.9219 RMSElog: 9.7419 grad_loss: 16367.9502 normal_loss: 0.0000
[epoch  4][iter  110] loss: 122376.6562 RMSElog: 9.6688 grad_loss: 12227.9971 normal_loss: 0.0000
[epoch  4][iter  120] loss: 135690.0938 RMSElog: 9.6754 grad_loss: 13559.3340 normal_loss: 0.0000
[epoch  4][iter  130] loss: 161079.5156 RMSElog: 9.9258 grad_loss: 16098.0254 normal_loss: 0.0000
[epoch  4][iter  140] loss: 105887.2734 RMSElog: 9.6983 grad_loss: 10579.0293 normal_loss: 0.0000
[epoch  4][iter  150] loss: 135194.2344 RMSElog: 9.4545 grad_loss: 13509.9697 normal_loss: 0.0000
[epoch  4][iter  160] loss: 172957.4375 RMSElog: 9.9430 grad_loss: 17285.8008 normal_loss: 0.0000
[epoch  4][iter  170] loss: 167075.2344 RMSElog: 9.8660 grad_loss: 16697.6582 normal_loss: 0.0000
[epoch  4][iter  180] loss: 103789.0000 RMSElog: 9.6193 grad_loss: 10369.2812 normal_loss: 0.0000
[epoch  4][iter  190] loss: 115757.1875 RMSElog: 9.7467 grad_loss: 11565.9717 normal_loss: 0.0000
[epoch  4][iter  200] loss: 179730.6562 RMSElog: 10.1206 grad_loss: 17962.9453 normal_loss: 0.0000
[epoch  4][iter  210] loss: 208812.3438 RMSElog: 10.1565 grad_loss: 20871.0781 normal_loss: 0.0000
[epoch  4][iter  220] loss: 104331.3984 RMSElog: 9.7692 grad_loss: 10423.3701 normal_loss: 0.0000
[epoch  4][iter  230] loss: 141856.0938 RMSElog: 9.7199 grad_loss: 14175.8896 normal_loss: 0.0000
[epoch  4][iter  240] loss: 175476.4844 RMSElog: 10.0393 grad_loss: 17537.6094 normal_loss: 0.0000
[epoch  4][iter  250] loss: 106943.6875 RMSElog: 10.0294 grad_loss: 10684.3398 normal_loss: 0.0000
[epoch  4][iter  260] loss: 149531.1562 RMSElog: 9.7408 grad_loss: 14943.3750 normal_loss: 0.0000
[epoch  4][iter  270] loss: 100654.6172 RMSElog: 9.9598 grad_loss: 10055.5020 normal_loss: 0.0000
[epoch  4][iter  280] loss: 113912.4375 RMSElog: 9.7251 grad_loss: 11381.5195 normal_loss: 0.0000
[epoch  4][iter  290] loss: 171578.8438 RMSElog: 9.8325 grad_loss: 17148.0527 normal_loss: 0.0000
[epoch  4][iter  300] loss: 142586.7188 RMSElog: 9.8922 grad_loss: 14248.7793 normal_loss: 0.0000
[epoch  4][iter  310] loss: 114500.0391 RMSElog: 9.7409 grad_loss: 11440.2627 normal_loss: 0.0000
[epoch  4][iter  320] loss: 160816.1094 RMSElog: 9.9290 grad_loss: 16071.6816 normal_loss: 0.0000
[epoch  4][iter  330] loss: 226855.7188 RMSElog: 10.2887 grad_loss: 22675.2832 normal_loss: 0.0000
[epoch  4][iter  340] loss: 139051.9688 RMSElog: 9.6201 grad_loss: 13895.5771 normal_loss: 0.0000
[epoch  4][iter  350] loss: 174751.8438 RMSElog: 9.7333 grad_loss: 17465.4512 normal_loss: 0.0000
[epoch  4][iter  360] loss: 215533.2500 RMSElog: 10.4358 grad_loss: 21542.8887 normal_loss: 0.0000
[epoch  4][iter  370] loss: 210645.0156 RMSElog: 10.4982 grad_loss: 21054.0039 normal_loss: 0.0000
[epoch  4][iter  380] loss: 218616.2812 RMSElog: 10.2839 grad_loss: 21851.3457 normal_loss: 0.0000
[epoch  4][iter  390] loss: 114164.8359 RMSElog: 9.3600 grad_loss: 11407.1230 normal_loss: 0.0000
[epoch  4][iter  400] loss: 139089.1250 RMSElog: 9.8619 grad_loss: 13899.0508 normal_loss: 0.0000
[epoch  4][iter  410] loss: 100971.7031 RMSElog: 9.6528 grad_loss: 10087.5176 normal_loss: 0.0000
[epoch  4][iter  420] loss: 153988.3281 RMSElog: 9.7620 grad_loss: 15389.0713 normal_loss: 0.0000
[epoch  4][iter  430] loss: 218747.5000 RMSElog: 10.1228 grad_loss: 21864.6270 normal_loss: 0.0000
[epoch  4][iter  440] loss: 103938.9062 RMSElog: 9.7608 grad_loss: 10384.1299 normal_loss: 0.0000
[epoch  4][iter  450] loss: 162547.0312 RMSElog: 9.9091 grad_loss: 16244.7939 normal_loss: 0.0000
[epoch  4][iter  460] loss: 106760.6094 RMSElog: 9.8688 grad_loss: 10666.1914 normal_loss: 0.0000
[epoch  4][iter  470] loss: 196016.1562 RMSElog: 10.2650 grad_loss: 19591.3496 normal_loss: 0.0000
[epoch  4][iter  480] loss: 207875.7500 RMSElog: 10.4422 grad_loss: 20777.1328 normal_loss: 0.0000
[epoch  4][iter  490] loss: 90473.3125 RMSElog: 9.3127 grad_loss: 9038.0186 normal_loss: 0.0000
[epoch  4][iter  500] loss: 154155.7969 RMSElog: 10.0187 grad_loss: 15405.5605 normal_loss: 0.0000
[epoch  4][iter  510] loss: 194399.2500 RMSElog: 10.2924 grad_loss: 19429.6328 normal_loss: 0.0000
[epoch  4][iter  520] loss: 141219.4531 RMSElog: 9.9926 grad_loss: 14111.9531 normal_loss: 0.0000
[epoch  4][iter  530] loss: 161803.7344 RMSElog: 9.9875 grad_loss: 16170.3857 normal_loss: 0.0000
[epoch  4][iter  540] loss: 221204.0000 RMSElog: 10.1740 grad_loss: 22110.2266 normal_loss: 0.0000
[epoch  4][iter  550] loss: 148784.5000 RMSElog: 10.1047 grad_loss: 14868.3447 normal_loss: 0.0000
[epoch  4][iter  560] loss: 159814.7812 RMSElog: 9.8973 grad_loss: 15971.5811 normal_loss: 0.0000
[epoch  4][iter  570] loss: 174366.1875 RMSElog: 10.0512 grad_loss: 17426.5684 normal_loss: 0.0000
[epoch  4][iter  580] loss: 128970.3984 RMSElog: 9.7049 grad_loss: 12887.3350 normal_loss: 0.0000
[epoch  4][iter  590] loss: 149026.6094 RMSElog: 9.7728 grad_loss: 14892.8887 normal_loss: 0.0000
[epoch  5][iter    0] loss: 213392.1562 RMSElog: 10.0568 grad_loss: 21329.1582 normal_loss: 0.0000
[epoch  5][iter   10] loss: 126864.5312 RMSElog: 9.7526 grad_loss: 12676.7002 normal_loss: 0.0000
[epoch  5][iter   20] loss: 146932.7656 RMSElog: 9.4227 grad_loss: 14683.8535 normal_loss: 0.0000
[epoch  5][iter   30] loss: 175349.6094 RMSElog: 10.1476 grad_loss: 17524.8125 normal_loss: 0.0000
[epoch  5][iter   40] loss: 214799.3125 RMSElog: 10.1872 grad_loss: 21469.7441 normal_loss: 0.0000
[epoch  5][iter   50] loss: 146567.5625 RMSElog: 9.6520 grad_loss: 14647.1045 normal_loss: 0.0000
[epoch  5][iter   60] loss: 162702.5469 RMSElog: 9.6160 grad_loss: 16260.6387 normal_loss: 0.0000
[epoch  5][iter   70] loss: 203083.8281 RMSElog: 10.1238 grad_loss: 20298.2598 normal_loss: 0.0000
[epoch  5][iter   80] loss: 106186.2422 RMSElog: 9.2531 grad_loss: 10609.3711 normal_loss: 0.0000
[epoch  5][iter   90] loss: 180415.6875 RMSElog: 10.0354 grad_loss: 18031.5332 normal_loss: 0.0000
[epoch  5][iter  100] loss: 197511.0781 RMSElog: 10.1663 grad_loss: 19740.9414 normal_loss: 0.0000
[epoch  5][iter  110] loss: 186039.4375 RMSElog: 10.0165 grad_loss: 18593.9277 normal_loss: 0.0000
[epoch  5][iter  120] loss: 214057.7969 RMSElog: 10.0580 grad_loss: 21395.7207 normal_loss: 0.0000
[epoch  5][iter  130] loss: 132925.8125 RMSElog: 9.9640 grad_loss: 13282.6172 normal_loss: 0.0000
[epoch  5][iter  140] loss: 221204.0000 RMSElog: 10.1740 grad_loss: 22110.2266 normal_loss: 0.0000
[epoch  5][iter  150] loss: 133750.3750 RMSElog: 10.0653 grad_loss: 13364.9717 normal_loss: 0.0000
[epoch  5][iter  160] loss: 191310.4688 RMSElog: 10.0978 grad_loss: 19120.9492 normal_loss: 0.0000
[epoch  5][iter  170] loss: 165443.4219 RMSElog: 9.9262 grad_loss: 16534.4160 normal_loss: 0.0000
[epoch  5][iter  180] loss: 220686.3125 RMSElog: 10.2208 grad_loss: 22058.4102 normal_loss: 0.0000
[epoch  5][iter  190] loss: 106507.5156 RMSElog: 9.6966 grad_loss: 10641.0557 normal_loss: 0.0000
[epoch  5][iter  200] loss: 195662.2031 RMSElog: 10.1080 grad_loss: 19556.1133 normal_loss: 0.0000
[epoch  5][iter  210] loss: 182178.2969 RMSElog: 10.0835 grad_loss: 18207.7461 normal_loss: 0.0000
[epoch  5][iter  220] loss: 130729.9062 RMSElog: 9.5471 grad_loss: 13063.4434 normal_loss: 0.0000
[epoch  5][iter  230] loss: 141109.0625 RMSElog: 10.0520 grad_loss: 14100.8545 normal_loss: 0.0000
[epoch  5][iter  240] loss: 115525.0469 RMSElog: 9.7196 grad_loss: 11542.7852 normal_loss: 0.0000
[epoch  5][iter  250] loss: 136860.4375 RMSElog: 9.5771 grad_loss: 13676.4668 normal_loss: 0.0000
[epoch  5][iter  260] loss: 103945.7812 RMSElog: 9.6256 grad_loss: 10384.9521 normal_loss: 0.0000
[epoch  5][iter  270] loss: 144977.7188 RMSElog: 9.8653 grad_loss: 14487.9062 normal_loss: 0.0000
[epoch  5][iter  280] loss: 205353.2500 RMSElog: 10.1686 grad_loss: 20525.1562 normal_loss: 0.0000
[epoch  5][iter  290] loss: 152540.2188 RMSElog: 9.9928 grad_loss: 15244.0293 normal_loss: 0.0000
[epoch  5][iter  300] loss: 112526.2812 RMSElog: 9.5823 grad_loss: 11243.0459 normal_loss: 0.0000
[epoch  5][iter  310] loss: 200943.7656 RMSElog: 10.1163 grad_loss: 20084.2598 normal_loss: 0.0000
[epoch  5][iter  320] loss: 104740.0000 RMSElog: 9.7512 grad_loss: 10464.2490 normal_loss: 0.0000
[epoch  5][iter  330] loss: 133036.2656 RMSElog: 9.8765 grad_loss: 13293.7490 normal_loss: 0.0000
[epoch  5][iter  340] loss: 163263.3125 RMSElog: 10.0261 grad_loss: 16316.3047 normal_loss: 0.0000
[epoch  5][iter  350] loss: 122661.5703 RMSElog: 9.4365 grad_loss: 12256.7207 normal_loss: 0.0000
[epoch  5][iter  360] loss: 105819.5781 RMSElog: 10.1032 grad_loss: 10571.8545 normal_loss: 0.0000
[epoch  5][iter  370] loss: 167784.5469 RMSElog: 9.8335 grad_loss: 16768.6211 normal_loss: 0.0000
[epoch  5][iter  380] loss: 195403.4531 RMSElog: 10.0493 grad_loss: 19530.2969 normal_loss: 0.0000
[epoch  5][iter  390] loss: 181456.2500 RMSElog: 10.1136 grad_loss: 18135.5117 normal_loss: 0.0000
[epoch  5][iter  400] loss: 113886.5312 RMSElog: 9.2606 grad_loss: 11379.3926 normal_loss: 0.0000
[epoch  5][iter  410] loss: 196428.2188 RMSElog: 10.2319 grad_loss: 19632.5898 normal_loss: 0.0000
[epoch  5][iter  420] loss: 107064.2031 RMSElog: 9.7289 grad_loss: 10696.6914 normal_loss: 0.0000
[epoch  5][iter  430] loss: 120431.3594 RMSElog: 9.1890 grad_loss: 12033.9463 normal_loss: 0.0000
[epoch  5][iter  440] loss: 142535.2188 RMSElog: 10.2185 grad_loss: 14243.3027 normal_loss: 0.0000
[epoch  5][iter  450] loss: 191962.0469 RMSElog: 10.0873 grad_loss: 19186.1172 normal_loss: 0.0000
[epoch  5][iter  460] loss: 156976.2500 RMSElog: 9.5765 grad_loss: 15688.0488 normal_loss: 0.0000
[epoch  5][iter  470] loss: 174555.5938 RMSElog: 9.7759 grad_loss: 17445.7832 normal_loss: 0.0000
[epoch  5][iter  480] loss: 202620.4844 RMSElog: 10.0417 grad_loss: 20252.0078 normal_loss: 0.0000
[epoch  5][iter  490] loss: 127489.9531 RMSElog: 9.2769 grad_loss: 12739.7178 normal_loss: 0.0000
[epoch  5][iter  500] loss: 115728.8125 RMSElog: 9.6968 grad_loss: 11563.1836 normal_loss: 0.0000
[epoch  5][iter  510] loss: 224934.3438 RMSElog: 10.1206 grad_loss: 22483.3125 normal_loss: 0.0000
[epoch  5][iter  520] loss: 177053.9062 RMSElog: 10.1532 grad_loss: 17695.2383 normal_loss: 0.0000
[epoch  5][iter  530] loss: 128874.0469 RMSElog: 9.8798 grad_loss: 12877.5244 normal_loss: 0.0000
[epoch  5][iter  540] loss: 172238.9062 RMSElog: 9.5166 grad_loss: 17214.3730 normal_loss: 0.0000
[epoch  5][iter  550] loss: 154280.7969 RMSElog: 9.8471 grad_loss: 15418.2334 normal_loss: 0.0000
[epoch  5][iter  560] loss: 189734.0625 RMSElog: 10.1862 grad_loss: 18963.2207 normal_loss: 0.0000
[epoch  5][iter  570] loss: 143451.3750 RMSElog: 9.7520 grad_loss: 14335.3848 normal_loss: 0.0000
[epoch  5][iter  580] loss: 111646.4453 RMSElog: 9.2050 grad_loss: 11155.4395 normal_loss: 0.0000
[epoch  5][iter  590] loss: 226940.0781 RMSElog: 10.2029 grad_loss: 22683.8047 normal_loss: 0.0000
[epoch  6][iter    0] loss: 182306.3281 RMSElog: 10.1291 grad_loss: 18220.5039 normal_loss: 0.0000
[epoch  6][iter   10] loss: 197705.4688 RMSElog: 10.4355 grad_loss: 19760.1113 normal_loss: 0.0000
[epoch  6][iter   20] loss: 154020.3750 RMSElog: 9.8645 grad_loss: 15392.1729 normal_loss: 0.0000
[epoch  6][iter   30] loss: 112687.7422 RMSElog: 9.7295 grad_loss: 11259.0449 normal_loss: 0.0000
[epoch  6][iter   40] loss: 114817.7734 RMSElog: 9.7767 grad_loss: 11472.0010 normal_loss: 0.0000
[epoch  6][iter   50] loss: 181283.8125 RMSElog: 10.0009 grad_loss: 18118.3809 normal_loss: 0.0000
[epoch  6][iter   60] loss: 142006.8594 RMSElog: 9.9191 grad_loss: 14190.7676 normal_loss: 0.0000
[epoch  6][iter   70] loss: 165010.7031 RMSElog: 9.7791 grad_loss: 16491.2910 normal_loss: 0.0000
[epoch  6][iter   80] loss: 218616.2812 RMSElog: 10.2839 grad_loss: 21851.3457 normal_loss: 0.0000
[epoch  6][iter   90] loss: 160816.1094 RMSElog: 9.9290 grad_loss: 16071.6816 normal_loss: 0.0000
[epoch  6][iter  100] loss: 147198.5625 RMSElog: 9.9779 grad_loss: 14709.8789 normal_loss: 0.0000
[epoch  6][iter  110] loss: 102214.2891 RMSElog: 9.4278 grad_loss: 10212.0010 normal_loss: 0.0000
[epoch  6][iter  120] loss: 121857.3828 RMSElog: 9.8422 grad_loss: 12175.8965 normal_loss: 0.0000
[epoch  6][iter  130] loss: 107463.1250 RMSElog: 9.0654 grad_loss: 10737.2471 normal_loss: 0.0000
[epoch  6][iter  140] loss: 144977.7188 RMSElog: 9.8653 grad_loss: 14487.9062 normal_loss: 0.0000
[epoch  6][iter  150] loss: 228043.6094 RMSElog: 10.2288 grad_loss: 22794.1328 normal_loss: 0.0000
[epoch  6][iter  160] loss: 185062.1719 RMSElog: 10.0456 grad_loss: 18496.1719 normal_loss: 0.0000
[epoch  6][iter  170] loss: 140324.4688 RMSElog: 9.8261 grad_loss: 14022.6211 normal_loss: 0.0000
[epoch  6][iter  180] loss: 161275.7500 RMSElog: 9.8339 grad_loss: 16117.7402 normal_loss: 0.0000
[epoch  6][iter  190] loss: 156163.8750 RMSElog: 9.9681 grad_loss: 15606.4199 normal_loss: 0.0000
[epoch  6][iter  200] loss: 218940.3750 RMSElog: 10.4462 grad_loss: 21883.5918 normal_loss: 0.0000
[epoch  6][iter  210] loss: 119461.2188 RMSElog: 9.6196 grad_loss: 11936.5020 normal_loss: 0.0000
[epoch  6][iter  220] loss: 161623.7656 RMSElog: 10.0107 grad_loss: 16152.3652 normal_loss: 0.0000
[epoch  6][iter  230] loss: 174751.8438 RMSElog: 9.7333 grad_loss: 17465.4512 normal_loss: 0.0000
[epoch  6][iter  240] loss: 104118.5625 RMSElog: 9.6242 grad_loss: 10402.2324 normal_loss: 0.0000
[epoch  6][iter  250] loss: 177924.8281 RMSElog: 9.8386 grad_loss: 17782.6445 normal_loss: 0.0000
[epoch  6][iter  260] loss: 170533.2656 RMSElog: 9.7855 grad_loss: 17043.5410 normal_loss: 0.0000
[epoch  6][iter  270] loss: 162569.9688 RMSElog: 10.1223 grad_loss: 16246.8740 normal_loss: 0.0000
[epoch  6][iter  280] loss: 149118.7969 RMSElog: 9.7262 grad_loss: 14902.1533 normal_loss: 0.0000
[epoch  6][iter  290] loss: 177690.3281 RMSElog: 9.4851 grad_loss: 17759.5488 normal_loss: 0.0000
[epoch  6][iter  300] loss: 149360.6875 RMSElog: 10.0930 grad_loss: 14925.9756 normal_loss: 0.0000
[epoch  6][iter  310] loss: 228329.2812 RMSElog: 10.2936 grad_loss: 22822.6348 normal_loss: 0.0000
[epoch  6][iter  320] loss: 135105.3281 RMSElog: 9.9265 grad_loss: 13500.6055 normal_loss: 0.0000
[epoch  6][iter  330] loss: 109432.2266 RMSElog: 9.7466 grad_loss: 10933.4756 normal_loss: 0.0000
[epoch  6][iter  340] loss: 125686.2578 RMSElog: 9.9422 grad_loss: 12558.6836 normal_loss: 0.0000
[epoch  6][iter  350] loss: 136860.4375 RMSElog: 9.5771 grad_loss: 13676.4668 normal_loss: 0.0000
[epoch  6][iter  360] loss: 114915.6172 RMSElog: 9.6925 grad_loss: 11481.8691 normal_loss: 0.0000
[epoch  6][iter  370] loss: 234935.6875 RMSElog: 10.0015 grad_loss: 23483.5664 normal_loss: 0.0000
[epoch  6][iter  380] loss: 160870.2812 RMSElog: 9.7521 grad_loss: 16077.2754 normal_loss: 0.0000
[epoch  6][iter  390] loss: 197511.0781 RMSElog: 10.1663 grad_loss: 19740.9414 normal_loss: 0.0000
[epoch  6][iter  400] loss: 139051.9688 RMSElog: 9.6201 grad_loss: 13895.5771 normal_loss: 0.0000
[epoch  6][iter  410] loss: 172859.2031 RMSElog: 10.0539 grad_loss: 17275.8652 normal_loss: 0.0000
[epoch  6][iter  420] loss: 115491.1562 RMSElog: 9.9924 grad_loss: 11539.1230 normal_loss: 0.0000
[epoch  6][iter  430] loss: 202620.4844 RMSElog: 10.0417 grad_loss: 20252.0078 normal_loss: 0.0000
[epoch  6][iter  440] loss: 155901.8281 RMSElog: 9.7863 grad_loss: 15580.3965 normal_loss: 0.0000
[epoch  6][iter  450] loss: 106943.6875 RMSElog: 10.0294 grad_loss: 10684.3398 normal_loss: 0.0000
[epoch  6][iter  460] loss: 153988.3281 RMSElog: 9.7620 grad_loss: 15389.0713 normal_loss: 0.0000
[epoch  6][iter  470] loss: 186950.9219 RMSElog: 10.2468 grad_loss: 18684.8457 normal_loss: 0.0000
[epoch  6][iter  480] loss: 133015.5938 RMSElog: 9.6937 grad_loss: 13291.8662 normal_loss: 0.0000
[epoch  6][iter  490] loss: 180309.3125 RMSElog: 9.9530 grad_loss: 18020.9785 normal_loss: 0.0000
[epoch  6][iter  500] loss: 164756.8750 RMSElog: 9.6928 grad_loss: 16465.9941 normal_loss: 0.0000
[epoch  6][iter  510] loss: 140929.4375 RMSElog: 9.7483 grad_loss: 14083.1953 normal_loss: 0.0000
[epoch  6][iter  520] loss: 175292.1875 RMSElog: 9.8017 grad_loss: 17519.4180 normal_loss: 0.0000
[epoch  6][iter  530] loss: 176214.0000 RMSElog: 9.9872 grad_loss: 17611.4141 normal_loss: 0.0000
[epoch  6][iter  540] loss: 103505.5547 RMSElog: 9.3451 grad_loss: 10341.2109 normal_loss: 0.0000
[epoch  6][iter  550] loss: 214483.6094 RMSElog: 10.3525 grad_loss: 21438.0078 normal_loss: 0.0000
[epoch  6][iter  560] loss: 223865.8750 RMSElog: 10.3929 grad_loss: 22376.1953 normal_loss: 0.0000
[epoch  6][iter  570] loss: 150311.9062 RMSElog: 9.9248 grad_loss: 15021.2656 normal_loss: 0.0000
[epoch  6][iter  580] loss: 146567.5625 RMSElog: 9.6520 grad_loss: 14647.1045 normal_loss: 0.0000
[epoch  6][iter  590] loss: 90473.3125 RMSElog: 9.3127 grad_loss: 9038.0186 normal_loss: 0.0000
[epoch  7][iter    0] loss: 177804.7500 RMSElog: 10.0854 grad_loss: 17770.3887 normal_loss: 0.0000
[epoch  7][iter   10] loss: 150599.4375 RMSElog: 10.0680 grad_loss: 15049.8750 normal_loss: 0.0000
[epoch  7][iter   20] loss: 99098.6641 RMSElog: 9.5230 grad_loss: 9900.3428 normal_loss: 0.0000
[epoch  7][iter   30] loss: 165967.0938 RMSElog: 9.9385 grad_loss: 16586.7695 normal_loss: 0.0000
[epoch  7][iter   40] loss: 103505.5469 RMSElog: 9.3451 grad_loss: 10341.2100 normal_loss: 0.0000
[epoch  7][iter   50] loss: 176788.5000 RMSElog: 9.8176 grad_loss: 17669.0312 normal_loss: 0.0000
[epoch  7][iter   60] loss: 148229.7500 RMSElog: 9.8055 grad_loss: 14813.1689 normal_loss: 0.0000
[epoch  7][iter   70] loss: 182309.4062 RMSElog: 10.1008 grad_loss: 18220.8398 normal_loss: 0.0000
[epoch  7][iter   80] loss: 127101.3359 RMSElog: 9.5273 grad_loss: 12700.6064 normal_loss: 0.0000
[epoch  7][iter   90] loss: 122661.5703 RMSElog: 9.4365 grad_loss: 12256.7207 normal_loss: 0.0000
[epoch  7][iter  100] loss: 157265.0000 RMSElog: 9.9277 grad_loss: 15716.5723 normal_loss: 0.0000
[epoch  7][iter  110] loss: 195431.3281 RMSElog: 9.7403 grad_loss: 19533.3926 normal_loss: 0.0000
[epoch  7][iter  120] loss: 176251.3438 RMSElog: 10.0638 grad_loss: 17615.0703 normal_loss: 0.0000
[epoch  7][iter  130] loss: 170533.2656 RMSElog: 9.7855 grad_loss: 17043.5410 normal_loss: 0.0000
[epoch  7][iter  140] loss: 227314.3594 RMSElog: 10.4159 grad_loss: 22721.0195 normal_loss: 0.0000
[epoch  7][iter  150] loss: 120431.3594 RMSElog: 9.1890 grad_loss: 12033.9463 normal_loss: 0.0000
[epoch  7][iter  160] loss: 179730.6562 RMSElog: 10.1206 grad_loss: 17962.9453 normal_loss: 0.0000
[epoch  7][iter  170] loss: 152762.9844 RMSElog: 9.8249 grad_loss: 15266.4736 normal_loss: 0.0000
[epoch  7][iter  180] loss: 113912.4375 RMSElog: 9.7251 grad_loss: 11381.5195 normal_loss: 0.0000
[epoch  7][iter  190] loss: 100980.9375 RMSElog: 9.7350 grad_loss: 10088.3584 normal_loss: 0.0000
[epoch  7][iter  200] loss: 133015.5938 RMSElog: 9.6937 grad_loss: 13291.8662 normal_loss: 0.0000
[epoch  7][iter  210] loss: 111646.4453 RMSElog: 9.2050 grad_loss: 11155.4395 normal_loss: 0.0000
[epoch  7][iter  220] loss: 183085.4688 RMSElog: 9.9762 grad_loss: 18298.5703 normal_loss: 0.0000
[epoch  7][iter  230] loss: 118736.9062 RMSElog: 9.7278 grad_loss: 11863.9629 normal_loss: 0.0000
[epoch  7][iter  240] loss: 159789.3438 RMSElog: 9.8075 grad_loss: 15969.1260 normal_loss: 0.0000
[epoch  7][iter  250] loss: 157800.9062 RMSElog: 9.9254 grad_loss: 15770.1641 normal_loss: 0.0000
[epoch  7][iter  260] loss: 172105.9219 RMSElog: 9.7682 grad_loss: 17200.8242 normal_loss: 0.0000
[epoch  7][iter  270] loss: 177186.0312 RMSElog: 10.1516 grad_loss: 17708.4512 normal_loss: 0.0000
[epoch  7][iter  280] loss: 114140.2969 RMSElog: 9.2655 grad_loss: 11404.7637 normal_loss: 0.0000
[epoch  7][iter  290] loss: 100971.7031 RMSElog: 9.6528 grad_loss: 10087.5176 normal_loss: 0.0000
[epoch  7][iter  300] loss: 146950.9219 RMSElog: 10.0979 grad_loss: 14684.9941 normal_loss: 0.0000
[epoch  7][iter  310] loss: 182849.3594 RMSElog: 9.8110 grad_loss: 18275.1250 normal_loss: 0.0000
[epoch  7][iter  320] loss: 108790.9062 RMSElog: 9.7538 grad_loss: 10869.3369 normal_loss: 0.0000
[epoch  7][iter  330] loss: 103945.7812 RMSElog: 9.6256 grad_loss: 10384.9521 normal_loss: 0.0000
[epoch  7][iter  340] loss: 196428.2188 RMSElog: 10.2319 grad_loss: 19632.5898 normal_loss: 0.0000
[epoch  7][iter  350] loss: 100740.3125 RMSElog: 9.9914 grad_loss: 10064.0400 normal_loss: 0.0000
[epoch  7][iter  360] loss: 150689.8906 RMSElog: 9.8567 grad_loss: 15059.1328 normal_loss: 0.0000
[epoch  7][iter  370] loss: 167730.8750 RMSElog: 9.8540 grad_loss: 16763.2344 normal_loss: 0.0000
[epoch  7][iter  380] loss: 128268.4531 RMSElog: 9.7565 grad_loss: 12817.0889 normal_loss: 0.0000
[epoch  7][iter  390] loss: 133276.7656 RMSElog: 9.7900 grad_loss: 13317.8867 normal_loss: 0.0000
[epoch  7][iter  400] loss: 129157.1406 RMSElog: 9.7364 grad_loss: 12905.9775 normal_loss: 0.0000
[epoch  7][iter  410] loss: 122350.0859 RMSElog: 9.7683 grad_loss: 12225.2402 normal_loss: 0.0000
[epoch  7][iter  420] loss: 112706.4375 RMSElog: 9.6867 grad_loss: 11260.9570 normal_loss: 0.0000
[epoch  7][iter  430] loss: 156163.8750 RMSElog: 9.9681 grad_loss: 15606.4199 normal_loss: 0.0000
[epoch  7][iter  440] loss: 128970.3984 RMSElog: 9.7049 grad_loss: 12887.3350 normal_loss: 0.0000
[epoch  7][iter  450] loss: 151392.3438 RMSElog: 9.4414 grad_loss: 15129.7930 normal_loss: 0.0000
[epoch  7][iter  460] loss: 106186.2422 RMSElog: 9.2531 grad_loss: 10609.3711 normal_loss: 0.0000
[epoch  7][iter  470] loss: 100107.2344 RMSElog: 9.9235 grad_loss: 10000.7998 normal_loss: 0.0000
[epoch  7][iter  480] loss: 132003.3906 RMSElog: 9.6802 grad_loss: 13190.6582 normal_loss: 0.0000
[epoch  7][iter  490] loss: 164834.3438 RMSElog: 9.7972 grad_loss: 16473.6367 normal_loss: 0.0000
[epoch  7][iter  500] loss: 191361.9375 RMSElog: 10.1918 grad_loss: 19126.0020 normal_loss: 0.0000
[epoch  7][iter  510] loss: 129150.1562 RMSElog: 9.6889 grad_loss: 12905.3271 normal_loss: 0.0000
[epoch  7][iter  520] loss: 227071.1094 RMSElog: 10.4127 grad_loss: 22696.6992 normal_loss: 0.0000
[epoch  7][iter  530] loss: 142535.0938 RMSElog: 10.1645 grad_loss: 14243.3457 normal_loss: 0.0000
[epoch  7][iter  540] loss: 191310.4688 RMSElog: 10.0978 grad_loss: 19120.9492 normal_loss: 0.0000
[epoch  7][iter  550] loss: 198733.7656 RMSElog: 10.0842 grad_loss: 19863.2930 normal_loss: 0.0000
[epoch  7][iter  560] loss: 177690.3281 RMSElog: 9.4851 grad_loss: 17759.5488 normal_loss: 0.0000
[epoch  7][iter  570] loss: 148047.6875 RMSElog: 9.9895 grad_loss: 14794.7803 normal_loss: 0.0000
[epoch  7][iter  580] loss: 162532.5156 RMSElog: 9.9393 grad_loss: 16243.3115 normal_loss: 0.0000
[epoch  7][iter  590] loss: 134396.6562 RMSElog: 9.9402 grad_loss: 13429.7256 normal_loss: 0.0000
[epoch  8][iter    0] loss: 181465.8125 RMSElog: 10.1136 grad_loss: 18135.5117 normal_loss: 0.9572
[epoch  8][iter   10] loss: 152540.7500 RMSElog: 9.9175 grad_loss: 15243.2090 normal_loss: 0.9474
[epoch  8][iter   20] loss: 144987.0938 RMSElog: 9.8653 grad_loss: 14487.9062 normal_loss: 0.9378
[epoch  8][iter   30] loss: 135966.2969 RMSElog: 9.6504 grad_loss: 13586.0273 normal_loss: 0.9522
[epoch  8][iter   40] loss: 223018.5625 RMSElog: 10.3592 grad_loss: 22290.5508 normal_loss: 0.9462
[epoch  8][iter   50] loss: 114924.9844 RMSElog: 9.6925 grad_loss: 11481.8691 normal_loss: 0.9360
[epoch  8][iter   60] loss: 178076.4688 RMSElog: 9.8778 grad_loss: 17796.8457 normal_loss: 0.9233
[epoch  8][iter   70] loss: 159422.2500 RMSElog: 9.7010 grad_loss: 15931.5938 normal_loss: 0.9299
[epoch  8][iter   80] loss: 122912.0625 RMSElog: 9.7328 grad_loss: 12280.5225 normal_loss: 0.9509
[epoch  8][iter   90] loss: 106769.7500 RMSElog: 9.8688 grad_loss: 10666.1914 normal_loss: 0.9140
[epoch  8][iter  100] loss: 139830.9062 RMSElog: 9.7552 grad_loss: 13972.3770 normal_loss: 0.9585
[epoch  8][iter  110] loss: 185495.4062 RMSElog: 9.9310 grad_loss: 18538.6641 normal_loss: 0.9455
[epoch  8][iter  120] loss: 115738.1953 RMSElog: 9.6968 grad_loss: 11563.1836 normal_loss: 0.9382
[epoch  8][iter  130] loss: 157119.7500 RMSElog: 9.8959 grad_loss: 15701.1318 normal_loss: 0.9480
[epoch  8][iter  140] loss: 164767.1875 RMSElog: 9.8398 grad_loss: 16465.9102 normal_loss: 0.9691
[epoch  8][iter  150] loss: 103955.0000 RMSElog: 9.6256 grad_loss: 10384.9521 normal_loss: 0.9222
[epoch  8][iter  160] loss: 148239.1406 RMSElog: 9.8055 grad_loss: 14813.1689 normal_loss: 0.9395
[epoch  8][iter  170] loss: 124246.3594 RMSElog: 9.7165 grad_loss: 12413.9834 normal_loss: 0.9360
[epoch  8][iter  180] loss: 150459.7656 RMSElog: 9.9429 grad_loss: 15035.0986 normal_loss: 0.9347
[epoch  8][iter  190] loss: 229081.2812 RMSElog: 10.2778 grad_loss: 22896.9102 normal_loss: 0.9420
[epoch  8][iter  200] loss: 152549.8750 RMSElog: 9.9928 grad_loss: 15244.0293 normal_loss: 0.9654
[epoch  8][iter  210] loss: 203093.2969 RMSElog: 10.1238 grad_loss: 20298.2598 normal_loss: 0.9478
[epoch  8][iter  220] loss: 194987.6250 RMSElog: 9.8416 grad_loss: 19487.9453 normal_loss: 0.9748
[epoch  8][iter  230] loss: 115766.5312 RMSElog: 9.7467 grad_loss: 11565.9717 normal_loss: 0.9344
[epoch  8][iter  240] loss: 199607.1562 RMSElog: 10.1504 grad_loss: 19949.6133 normal_loss: 0.9517
[epoch  8][iter  250] loss: 144657.9844 RMSElog: 9.9197 grad_loss: 14454.9258 normal_loss: 0.9518
[epoch  8][iter  260] loss: 155762.3594 RMSElog: 9.8487 grad_loss: 15565.4414 normal_loss: 0.9448
[epoch  8][iter  270] loss: 177699.9688 RMSElog: 9.4851 grad_loss: 17759.5488 normal_loss: 0.9621
[epoch  8][iter  280] loss: 163389.5625 RMSElog: 9.7702 grad_loss: 16328.2656 normal_loss: 0.9208
[epoch  8][iter  290] loss: 145409.6719 RMSElog: 9.8330 grad_loss: 14530.1904 normal_loss: 0.9445
[epoch  8][iter  300] loss: 177814.3125 RMSElog: 10.0854 grad_loss: 17770.3887 normal_loss: 0.9571
[epoch  8][iter  310] loss: 103514.7812 RMSElog: 9.3451 grad_loss: 10341.2109 normal_loss: 0.9229
[epoch  8][iter  320] loss: 161813.2500 RMSElog: 9.9875 grad_loss: 16170.3857 normal_loss: 0.9508
[epoch  8][iter  330] loss: 166207.1094 RMSElog: 9.7947 grad_loss: 16609.9668 normal_loss: 0.9492
[epoch  8][iter  340] loss: 120765.1875 RMSElog: 9.5441 grad_loss: 12066.0400 normal_loss: 0.9348
[epoch  8][iter  350] loss: 189017.5781 RMSElog: 10.1390 grad_loss: 18890.6602 normal_loss: 0.9589
[epoch  8][iter  360] loss: 167345.2500 RMSElog: 9.8843 grad_loss: 16723.6777 normal_loss: 0.9620
[epoch  8][iter  370] loss: 174629.2812 RMSElog: 9.9405 grad_loss: 17452.0625 normal_loss: 0.9229
[epoch  8][iter  380] loss: 184374.2188 RMSElog: 10.2641 grad_loss: 18426.1992 normal_loss: 0.9596
[epoch  8][iter  390] loss: 179740.0625 RMSElog: 10.1206 grad_loss: 17962.9453 normal_loss: 0.9397
[epoch  8][iter  400] loss: 185772.2500 RMSElog: 9.9702 grad_loss: 18566.2969 normal_loss: 0.9567
[epoch  8][iter  410] loss: 142138.5156 RMSElog: 9.7967 grad_loss: 14203.1260 normal_loss: 0.9291
[epoch  8][iter  420] loss: 179649.7188 RMSElog: 10.2020 grad_loss: 17953.8125 normal_loss: 0.9593
[epoch  8][iter  430] loss: 150153.5000 RMSElog: 10.0554 grad_loss: 15004.3457 normal_loss: 0.9478
[epoch  8][iter  440] loss: 160595.2188 RMSElog: 9.9514 grad_loss: 16048.6191 normal_loss: 0.9518
[epoch  8][iter  450] loss: 154029.7812 RMSElog: 9.8645 grad_loss: 15392.1729 normal_loss: 0.9405
[epoch  8][iter  460] loss: 147544.5000 RMSElog: 9.8913 grad_loss: 14743.6045 normal_loss: 0.9534
[epoch  8][iter  470] loss: 162372.7500 RMSElog: 9.8226 grad_loss: 16226.4922 normal_loss: 0.9603
[epoch  8][iter  480] loss: 188645.4531 RMSElog: 10.1590 grad_loss: 18853.4082 normal_loss: 0.9794
[epoch  8][iter  490] loss: 160466.8750 RMSElog: 9.9086 grad_loss: 16035.8252 normal_loss: 0.9540
[epoch  8][iter  500] loss: 153997.7188 RMSElog: 9.7620 grad_loss: 15389.0713 normal_loss: 0.9383
[epoch  8][iter  510] loss: 176260.7500 RMSElog: 10.0638 grad_loss: 17615.0703 normal_loss: 0.9395
[epoch  8][iter  520] loss: 178784.1250 RMSElog: 9.9460 grad_loss: 17867.5078 normal_loss: 0.9592
[epoch  8][iter  530] loss: 218625.8750 RMSElog: 10.2839 grad_loss: 21851.3457 normal_loss: 0.9592
[epoch  8][iter  540] loss: 175486.0156 RMSElog: 10.0393 grad_loss: 17537.6055 normal_loss: 0.9563
[epoch  8][iter  550] loss: 160782.5156 RMSElog: 9.7710 grad_loss: 16067.5547 normal_loss: 0.9244
[epoch  8][iter  560] loss: 177934.3750 RMSElog: 9.8386 grad_loss: 17782.6445 normal_loss: 0.9542
[epoch  8][iter  570] loss: 235211.2344 RMSElog: 10.3371 grad_loss: 23509.8398 normal_loss: 0.9452
[epoch  8][iter  580] loss: 146942.0781 RMSElog: 9.4227 grad_loss: 14683.8535 normal_loss: 0.9314
[epoch  8][iter  590] loss: 120590.1719 RMSElog: 9.9054 grad_loss: 12048.1719 normal_loss: 0.9408
[epoch  9][iter    0] loss: 150321.2812 RMSElog: 9.9248 grad_loss: 15021.2656 normal_loss: 0.9385
[epoch  9][iter   10] loss: 105896.9375 RMSElog: 9.7662 grad_loss: 10578.9961 normal_loss: 0.9310
[epoch  9][iter   20] loss: 195413.0312 RMSElog: 10.0493 grad_loss: 19530.3008 normal_loss: 0.9539
[epoch  9][iter   30] loss: 173339.3594 RMSElog: 9.8230 grad_loss: 17323.1914 normal_loss: 0.9212
[epoch  9][iter   40] loss: 170488.4688 RMSElog: 9.7164 grad_loss: 17038.1816 normal_loss: 0.9494
[epoch  9][iter   50] loss: 132012.6719 RMSElog: 9.6802 grad_loss: 13190.6582 normal_loss: 0.9291
[epoch  9][iter   60] loss: 125829.2891 RMSElog: 9.7844 grad_loss: 12572.2148 normal_loss: 0.9297
[epoch  9][iter   70] loss: 150153.5000 RMSElog: 10.0554 grad_loss: 15004.3457 normal_loss: 0.9478
[epoch  9][iter   80] loss: 175004.6875 RMSElog: 10.1774 grad_loss: 17489.3340 normal_loss: 0.9561
[epoch  9][iter   90] loss: 96006.2578 RMSElog: 9.5560 grad_loss: 9590.1406 normal_loss: 0.9294
[epoch  9][iter  100] loss: 149540.3750 RMSElog: 9.7408 grad_loss: 14943.3740 normal_loss: 0.9229
[epoch  9][iter  110] loss: 183095.1562 RMSElog: 9.9762 grad_loss: 18298.5703 normal_loss: 0.9696
[epoch  9][iter  120] loss: 115651.6406 RMSElog: 9.1877 grad_loss: 11555.0664 normal_loss: 0.9106
[epoch  9][iter  130] loss: 158630.9688 RMSElog: 9.8321 grad_loss: 15852.3232 normal_loss: 0.9428
[epoch  9][iter  140] loss: 163389.5625 RMSElog: 9.7702 grad_loss: 16328.2656 normal_loss: 0.9208
[epoch  9][iter  150] loss: 105794.8828 RMSElog: 9.7676 grad_loss: 10568.7939 normal_loss: 0.9267
[epoch  9][iter  160] loss: 135114.8750 RMSElog: 9.9265 grad_loss: 13500.6055 normal_loss: 0.9548
[epoch  9][iter  170] loss: 100749.6875 RMSElog: 9.9914 grad_loss: 10064.0400 normal_loss: 0.9377
[epoch  9][iter  180] loss: 133013.4531 RMSElog: 9.5635 grad_loss: 13290.8457 normal_loss: 0.9356
[epoch  9][iter  190] loss: 151723.8750 RMSElog: 9.9680 grad_loss: 15161.4746 normal_loss: 0.9449
[epoch  9][iter  200] loss: 120440.5938 RMSElog: 9.1890 grad_loss: 12033.9463 normal_loss: 0.9240
[epoch  9][iter  210] loss: 196920.5938 RMSElog: 10.2474 grad_loss: 19680.8340 normal_loss: 0.9760
[epoch  9][iter  220] loss: 179298.9062 RMSElog: 9.7220 grad_loss: 17919.2227 normal_loss: 0.9453
[epoch  9][iter  230] loss: 136869.7500 RMSElog: 9.5771 grad_loss: 13676.4668 normal_loss: 0.9315
[epoch  9][iter  240] loss: 133464.0312 RMSElog: 9.6242 grad_loss: 13335.8584 normal_loss: 0.9201
[epoch  9][iter  250] loss: 154310.0938 RMSElog: 9.9148 grad_loss: 15420.1445 normal_loss: 0.9495
[epoch  9][iter  260] loss: 234945.1719 RMSElog: 10.0015 grad_loss: 23483.5664 normal_loss: 0.9500
[epoch  9][iter  270] loss: 128883.6328 RMSElog: 9.8798 grad_loss: 12877.5244 normal_loss: 0.9593
[epoch  9][iter  280] loss: 119470.5156 RMSElog: 9.6196 grad_loss: 11936.5020 normal_loss: 0.9297
[epoch  9][iter  290] loss: 214067.1875 RMSElog: 10.0580 grad_loss: 21395.7207 normal_loss: 0.9394
[epoch  9][iter  300] loss: 194408.7812 RMSElog: 10.2924 grad_loss: 19429.6328 normal_loss: 0.9537
[epoch  9][iter  310] loss: 143904.7812 RMSElog: 10.0263 grad_loss: 14379.5068 normal_loss: 0.9457
[epoch  9][iter  320] loss: 150323.7656 RMSElog: 10.0510 grad_loss: 15021.3848 normal_loss: 0.9410
[epoch  9][iter  330] loss: 175547.8906 RMSElog: 10.1409 grad_loss: 17543.7148 normal_loss: 0.9342
[epoch  9][iter  340] loss: 149387.5469 RMSElog: 9.9550 grad_loss: 14927.8516 normal_loss: 0.9483
[epoch  9][iter  350] loss: 111712.9219 RMSElog: 9.7826 grad_loss: 11160.5811 normal_loss: 0.9290
[epoch  9][iter  360] loss: 138934.7188 RMSElog: 9.8450 grad_loss: 13882.6904 normal_loss: 0.9370
[epoch  9][iter  370] loss: 214808.7344 RMSElog: 10.1872 grad_loss: 21469.7441 normal_loss: 0.9423
[epoch  9][iter  380] loss: 141442.1250 RMSElog: 9.6437 grad_loss: 14133.6191 normal_loss: 0.9495
[epoch  9][iter  390] loss: 152549.8750 RMSElog: 9.9928 grad_loss: 15244.0293 normal_loss: 0.9654
[epoch  9][iter  400] loss: 208184.3438 RMSElog: 10.4061 grad_loss: 20807.0801 normal_loss: 0.9472
[epoch  9][iter  410] loss: 198293.9219 RMSElog: 10.2275 grad_loss: 19818.2148 normal_loss: 0.9521
[epoch  9][iter  420] loss: 144191.5938 RMSElog: 9.6650 grad_loss: 14408.5654 normal_loss: 0.9284
[epoch  9][iter  430] loss: 121838.3672 RMSElog: 9.7453 grad_loss: 12173.1494 normal_loss: 0.9425
[epoch  9][iter  440] loss: 141865.3906 RMSElog: 9.7199 grad_loss: 14175.8896 normal_loss: 0.9298
[epoch  9][iter  450] loss: 136507.7344 RMSElog: 10.0223 grad_loss: 13639.7900 normal_loss: 0.9608
[epoch  9][iter  460] loss: 185071.7031 RMSElog: 10.0456 grad_loss: 18496.1699 normal_loss: 0.9557
[epoch  9][iter  470] loss: 169013.5625 RMSElog: 9.5880 grad_loss: 16890.8262 normal_loss: 0.9419
[epoch  9][iter  480] loss: 121959.7344 RMSElog: 9.7378 grad_loss: 12185.3047 normal_loss: 0.9314
[epoch  9][iter  490] loss: 103948.1719 RMSElog: 9.7608 grad_loss: 10384.1299 normal_loss: 0.9264
[epoch  9][iter  500] loss: 142544.7031 RMSElog: 10.2185 grad_loss: 14243.3027 normal_loss: 0.9486
[epoch  9][iter  510] loss: 142016.3906 RMSElog: 9.9191 grad_loss: 14190.7676 normal_loss: 0.9526
[epoch  9][iter  520] loss: 156173.3594 RMSElog: 9.9681 grad_loss: 15606.4199 normal_loss: 0.9482
[epoch  9][iter  530] loss: 130145.6328 RMSElog: 9.7206 grad_loss: 13003.9082 normal_loss: 0.9344
[epoch  9][iter  540] loss: 175359.1875 RMSElog: 10.1476 grad_loss: 17524.8125 normal_loss: 0.9571
[epoch  9][iter  550] loss: 129205.9375 RMSElog: 9.8851 grad_loss: 12909.7734 normal_loss: 0.9356
[epoch  9][iter  560] loss: 155482.2031 RMSElog: 10.0818 grad_loss: 15537.1826 normal_loss: 0.9552
[epoch  9][iter  570] loss: 186960.2969 RMSElog: 10.2468 grad_loss: 18684.8457 normal_loss: 0.9380
[epoch  9][iter  580] loss: 161813.2500 RMSElog: 9.9875 grad_loss: 16170.3857 normal_loss: 0.9508
[epoch  9][iter  590] loss: 113921.7969 RMSElog: 9.7251 grad_loss: 11381.5195 normal_loss: 0.9357
[epoch 10][iter    0] loss: 72289.2656 RMSElog: 9.4677 grad_loss: 7218.5376 normal_loss: 0.9211
[epoch 10][iter   10] loss: 154776.6094 RMSElog: 9.8981 grad_loss: 15466.8418 normal_loss: 0.9209
[epoch 10][iter   20] loss: 145389.9375 RMSElog: 9.8784 grad_loss: 14528.1660 normal_loss: 0.9489
[epoch 10][iter   30] loss: 102998.8984 RMSElog: 9.8140 grad_loss: 10289.1318 normal_loss: 0.9431
[epoch 10][iter   40] loss: 196437.8125 RMSElog: 10.2319 grad_loss: 19632.5898 normal_loss: 0.9584
[epoch 10][iter   50] loss: 119429.4609 RMSElog: 9.6388 grad_loss: 11932.3740 normal_loss: 0.9339
[epoch 10][iter   60] loss: 213417.0312 RMSElog: 10.3416 grad_loss: 21330.4082 normal_loss: 0.9536
[epoch 10][iter   70] loss: 157274.4688 RMSElog: 9.9277 grad_loss: 15716.5723 normal_loss: 0.9461
[epoch 10][iter   80] loss: 188116.2812 RMSElog: 10.0888 grad_loss: 18800.5703 normal_loss: 0.9711
[epoch 10][iter   90] loss: 120590.1719 RMSElog: 9.9054 grad_loss: 12048.1719 normal_loss: 0.9408
[epoch 10][iter  100] loss: 206764.1875 RMSElog: 10.2849 grad_loss: 20665.1777 normal_loss: 0.9549
[epoch 10][iter  110] loss: 111914.6250 RMSElog: 9.3004 grad_loss: 11181.2402 normal_loss: 0.9218
[epoch 10][iter  120] loss: 134471.5938 RMSElog: 9.9200 grad_loss: 13436.3281 normal_loss: 0.9111
[epoch 10][iter  130] loss: 172755.6875 RMSElog: 10.0251 grad_loss: 17264.6016 normal_loss: 0.9419
[epoch 10][iter  140] loss: 176798.0312 RMSElog: 9.8176 grad_loss: 17669.0293 normal_loss: 0.9559
[epoch 10][iter  150] loss: 195440.9062 RMSElog: 9.7403 grad_loss: 19533.3926 normal_loss: 0.9572
[epoch 10][iter  160] loss: 144657.9844 RMSElog: 9.9197 grad_loss: 14454.9258 normal_loss: 0.9518
[epoch 10][iter  170] loss: 223018.5625 RMSElog: 10.3592 grad_loss: 22290.5508 normal_loss: 0.9462
[epoch 10][iter  180] loss: 191971.7188 RMSElog: 10.0873 grad_loss: 19186.1172 normal_loss: 0.9662
[epoch 10][iter  190] loss: 173574.0781 RMSElog: 10.1780 grad_loss: 17346.2734 normal_loss: 0.9575
[epoch 10][iter  200] loss: 121983.8203 RMSElog: 9.5225 grad_loss: 12187.9473 normal_loss: 0.9124
[epoch 10][iter  210] loss: 126874.0625 RMSElog: 9.7526 grad_loss: 12676.7002 normal_loss: 0.9530
[epoch 10][iter  220] loss: 122331.3906 RMSElog: 9.7180 grad_loss: 12222.4795 normal_loss: 0.9418
[epoch 10][iter  230] loss: 164767.1875 RMSElog: 9.8398 grad_loss: 16465.9102 normal_loss: 0.9691
[epoch 10][iter  240] loss: 112715.8125 RMSElog: 9.6867 grad_loss: 11260.9570 normal_loss: 0.9375
[epoch 10][iter  250] loss: 159340.4375 RMSElog: 9.9507 grad_loss: 15923.1426 normal_loss: 0.9500
[epoch 10][iter  260] loss: 163268.1250 RMSElog: 9.9218 grad_loss: 16315.9277 normal_loss: 0.9630
[epoch 10][iter  270] loss: 210654.5312 RMSElog: 10.4982 grad_loss: 21054.0039 normal_loss: 0.9517
[epoch 10][iter  280] loss: 105896.6094 RMSElog: 9.6983 grad_loss: 10579.0293 normal_loss: 0.9335
[epoch 10][iter  290] loss: 180982.8438 RMSElog: 10.2711 grad_loss: 18087.0605 normal_loss: 0.9528
[epoch 10][iter  300] loss: 132935.3906 RMSElog: 9.9640 grad_loss: 13282.6172 normal_loss: 0.9581
[epoch 10][iter  310] loss: 161084.5312 RMSElog: 9.9606 grad_loss: 16097.5410 normal_loss: 0.9512
[epoch 10][iter  320] loss: 104446.5625 RMSElog: 10.1932 grad_loss: 10433.5342 normal_loss: 0.9288
[epoch 10][iter  330] loss: 136796.3438 RMSElog: 9.6524 grad_loss: 13669.0273 normal_loss: 0.9549
[epoch 10][iter  340] loss: 152332.5469 RMSElog: 9.9813 grad_loss: 15222.3486 normal_loss: 0.9245
[epoch 10][iter  350] loss: 114893.3984 RMSElog: 9.2562 grad_loss: 11479.1777 normal_loss: 0.9067
[epoch 10][iter  360] loss: 154626.7656 RMSElog: 9.9098 grad_loss: 15451.8066 normal_loss: 0.9596
[epoch 10][iter  370] loss: 147371.3906 RMSElog: 9.6635 grad_loss: 14726.5488 normal_loss: 0.9280
[epoch 10][iter  380] loss: 179649.7188 RMSElog: 10.2020 grad_loss: 17953.8125 normal_loss: 0.9593
[epoch 10][iter  390] loss: 87697.6250 RMSElog: 9.9487 grad_loss: 8758.8750 normal_loss: 0.9395
[epoch 10][iter  400] loss: 175008.6562 RMSElog: 9.9496 grad_loss: 17489.9434 normal_loss: 0.9734
[epoch 10][iter  410] loss: 114509.4531 RMSElog: 9.7409 grad_loss: 11440.2627 normal_loss: 0.9413
[epoch 10][iter  420] loss: 161646.4688 RMSElog: 10.1105 grad_loss: 16153.5762 normal_loss: 0.9608
[epoch 10][iter  430] loss: 153447.6875 RMSElog: 9.9051 grad_loss: 15333.9209 normal_loss: 0.9425
[epoch 10][iter  440] loss: 105706.7031 RMSElog: 9.7501 grad_loss: 10559.9883 normal_loss: 0.9321
[epoch 10][iter  450] loss: 212239.7031 RMSElog: 10.2690 grad_loss: 21212.7383 normal_loss: 0.9638
[epoch 10][iter  460] loss: 124246.3594 RMSElog: 9.7165 grad_loss: 12413.9834 normal_loss: 0.9360
[epoch 10][iter  470] loss: 172966.9375 RMSElog: 9.9430 grad_loss: 17285.8008 normal_loss: 0.9485
[epoch 10][iter  480] loss: 130641.2188 RMSElog: 9.7184 grad_loss: 13053.4688 normal_loss: 0.9344
[epoch 10][iter  490] loss: 118746.3359 RMSElog: 9.7278 grad_loss: 11863.9629 normal_loss: 0.9433
[epoch 10][iter  500] loss: 223875.4062 RMSElog: 10.3929 grad_loss: 22376.1953 normal_loss: 0.9536
[epoch 10][iter  510] loss: 170542.9531 RMSElog: 9.7855 grad_loss: 17043.5410 normal_loss: 0.9688
[epoch 10][iter  520] loss: 176223.4531 RMSElog: 9.9872 grad_loss: 17611.4141 normal_loss: 0.9446
[epoch 10][iter  530] loss: 105742.0391 RMSElog: 9.7582 grad_loss: 10563.5234 normal_loss: 0.9230
[epoch 10][iter  540] loss: 167211.6250 RMSElog: 10.0285 grad_loss: 16710.1777 normal_loss: 0.9542
[epoch 10][iter  550] loss: 117061.6250 RMSElog: 9.2141 grad_loss: 11696.0439 normal_loss: 0.9046
[epoch 10][iter  560] loss: 143460.8594 RMSElog: 9.7520 grad_loss: 14335.3848 normal_loss: 0.9489
[epoch 10][iter  570] loss: 162372.7500 RMSElog: 9.8226 grad_loss: 16226.4922 normal_loss: 0.9603
[epoch 10][iter  580] loss: 182476.7344 RMSElog: 9.9672 grad_loss: 18236.7656 normal_loss: 0.9407
[epoch 10][iter  590] loss: 146960.1250 RMSElog: 10.0979 grad_loss: 14684.9922 normal_loss: 0.9220
[epoch 11][iter    0] loss: 122331.3906 RMSElog: 9.7180 grad_loss: 12222.4795 normal_loss: 0.9418
[epoch 11][iter   10] loss: 105742.0469 RMSElog: 9.7582 grad_loss: 10563.5244 normal_loss: 0.9230
[epoch 11][iter   20] loss: 105098.6875 RMSElog: 9.7426 grad_loss: 10499.2012 normal_loss: 0.9257
[epoch 11][iter   30] loss: 177934.3750 RMSElog: 9.8386 grad_loss: 17782.6445 normal_loss: 0.9542
[epoch 11][iter   40] loss: 168409.1562 RMSElog: 9.7031 grad_loss: 16830.2910 normal_loss: 0.9228
[epoch 11][iter   50] loss: 155482.2031 RMSElog: 10.0818 grad_loss: 15537.1826 normal_loss: 0.9552
[epoch 11][iter   60] loss: 189743.5625 RMSElog: 10.1862 grad_loss: 18963.2207 normal_loss: 0.9490
[epoch 11][iter   70] loss: 232982.0469 RMSElog: 10.2845 grad_loss: 23286.9707 normal_loss: 0.9500
[epoch 11][iter   80] loss: 152517.4844 RMSElog: 9.8233 grad_loss: 15241.0010 normal_loss: 0.9249
[epoch 11][iter   90] loss: 177292.4062 RMSElog: 10.1195 grad_loss: 17718.1602 normal_loss: 0.9615
[epoch 11][iter  100] loss: 139061.3750 RMSElog: 9.6201 grad_loss: 13895.5771 normal_loss: 0.9401
[epoch 11][iter  110] loss: 160024.9375 RMSElog: 9.8489 grad_loss: 15991.7236 normal_loss: 0.9215
[epoch 11][iter  120] loss: 196516.2188 RMSElog: 9.9615 grad_loss: 19640.7129 normal_loss: 0.9475
[epoch 11][iter  130] loss: 119923.2031 RMSElog: 9.5699 grad_loss: 11981.8154 normal_loss: 0.9342
[epoch 11][iter  140] loss: 182318.7812 RMSElog: 10.1008 grad_loss: 18220.8398 normal_loss: 0.9384
[epoch 11][iter  150] loss: 127463.1875 RMSElog: 9.4952 grad_loss: 12735.9082 normal_loss: 0.9153
[epoch 11][iter  160] loss: 170488.4688 RMSElog: 9.7164 grad_loss: 17038.1816 normal_loss: 0.9494
[epoch 11][iter  170] loss: 132425.4375 RMSElog: 10.0155 grad_loss: 13231.5801 normal_loss: 0.9468
[epoch 11][iter  180] loss: 115738.1953 RMSElog: 9.6968 grad_loss: 11563.1836 normal_loss: 0.9382
[epoch 11][iter  190] loss: 191825.0781 RMSElog: 10.3012 grad_loss: 19171.2500 normal_loss: 0.9570
[epoch 11][iter  200] loss: 115658.3125 RMSElog: 9.1856 grad_loss: 11555.7031 normal_loss: 0.9424
[epoch 11][iter  210] loss: 125486.8672 RMSElog: 9.7758 grad_loss: 12537.9834 normal_loss: 0.9281
[epoch 11][iter  220] loss: 177699.9688 RMSElog: 9.4851 grad_loss: 17759.5488 normal_loss: 0.9621
[epoch 11][iter  230] loss: 168545.0938 RMSElog: 9.6168 grad_loss: 16843.9434 normal_loss: 0.9495
[epoch 11][iter  240] loss: 156982.7188 RMSElog: 10.0093 grad_loss: 15687.3193 normal_loss: 0.9438
[epoch 11][iter  250] loss: 110603.0625 RMSElog: 9.8046 grad_loss: 11049.5654 normal_loss: 0.9364
[epoch 11][iter  260] loss: 154310.0938 RMSElog: 9.9148 grad_loss: 15420.1445 normal_loss: 0.9495
[epoch 11][iter  270] loss: 165750.3750 RMSElog: 9.8971 grad_loss: 16564.1895 normal_loss: 0.9518
[epoch 11][iter  280] loss: 177063.6250 RMSElog: 10.1532 grad_loss: 17695.2383 normal_loss: 0.9719
[epoch 11][iter  290] loss: 129708.8906 RMSElog: 10.0691 grad_loss: 12959.8711 normal_loss: 0.9481
[epoch 11][iter  300] loss: 152593.5000 RMSElog: 9.8773 grad_loss: 15248.5449 normal_loss: 0.9283
[epoch 11][iter  310] loss: 115534.3672 RMSElog: 9.7196 grad_loss: 11542.7842 normal_loss: 0.9325
[epoch 11][iter  320] loss: 159284.2188 RMSElog: 9.6028 grad_loss: 15917.8828 normal_loss: 0.9361
[epoch 11][iter  330] loss: 151723.8750 RMSElog: 9.9680 grad_loss: 15161.4746 normal_loss: 0.9449
[epoch 11][iter  340] loss: 116073.5938 RMSElog: 9.6844 grad_loss: 11596.7324 normal_loss: 0.9429
[epoch 11][iter  350] loss: 95221.2109 RMSElog: 9.1894 grad_loss: 9512.0205 normal_loss: 0.9115
[epoch 11][iter  360] loss: 104446.5625 RMSElog: 10.1932 grad_loss: 10433.5342 normal_loss: 0.9288
[epoch 11][iter  370] loss: 178593.9688 RMSElog: 10.0081 grad_loss: 17848.4395 normal_loss: 0.9490
[epoch 11][iter  380] loss: 147598.9844 RMSElog: 9.8981 grad_loss: 14749.0645 normal_loss: 0.9355
[epoch 11][iter  390] loss: 103815.4375 RMSElog: 9.7764 grad_loss: 10370.8340 normal_loss: 0.9338
[epoch 11][iter  400] loss: 148239.1406 RMSElog: 9.8055 grad_loss: 14813.1689 normal_loss: 0.9395
[epoch 11][iter  410] loss: 138934.7188 RMSElog: 9.8450 grad_loss: 13882.6904 normal_loss: 0.9370
[epoch 11][iter  420] loss: 167345.2500 RMSElog: 9.8843 grad_loss: 16723.6777 normal_loss: 0.9620
[epoch 11][iter  430] loss: 175122.9531 RMSElog: 9.9588 grad_loss: 17501.3789 normal_loss: 0.9573
[epoch 11][iter  440] loss: 149807.1562 RMSElog: 9.9060 grad_loss: 14969.8613 normal_loss: 0.9481
[epoch 11][iter  450] loss: 106952.9219 RMSElog: 10.0294 grad_loss: 10684.3398 normal_loss: 0.9231
[epoch 11][iter  460] loss: 150608.8906 RMSElog: 10.0680 grad_loss: 15049.8750 normal_loss: 0.9460
[epoch 11][iter  470] loss: 135114.8750 RMSElog: 9.9265 grad_loss: 13500.6055 normal_loss: 0.9548
[epoch 11][iter  480] loss: 135814.7969 RMSElog: 9.5386 grad_loss: 13571.0137 normal_loss: 0.9281
[epoch 11][iter  490] loss: 121983.8203 RMSElog: 9.5225 grad_loss: 12187.9473 normal_loss: 0.9124
[epoch 11][iter  500] loss: 125315.5547 RMSElog: 9.7124 grad_loss: 12520.9121 normal_loss: 0.9309
[epoch 11][iter  510] loss: 147765.9219 RMSElog: 10.0607 grad_loss: 14765.6104 normal_loss: 0.9219
[epoch 11][iter  520] loss: 174565.0781 RMSElog: 9.7759 grad_loss: 17445.7832 normal_loss: 0.9484
[epoch 11][iter  530] loss: 173855.4375 RMSElog: 10.0769 grad_loss: 17374.5098 normal_loss: 0.9566
[epoch 11][iter  540] loss: 173339.3594 RMSElog: 9.8230 grad_loss: 17323.1914 normal_loss: 0.9212
[epoch 11][iter  550] loss: 156709.9531 RMSElog: 9.8741 grad_loss: 15660.1758 normal_loss: 0.9453
[epoch 11][iter  560] loss: 174375.6875 RMSElog: 10.0512 grad_loss: 17426.5684 normal_loss: 0.9483
[epoch 11][iter  570] loss: 130739.4062 RMSElog: 9.5471 grad_loss: 13063.4434 normal_loss: 0.9498
[epoch 11][iter  580] loss: 121916.0000 RMSElog: 9.7464 grad_loss: 12180.9131 normal_loss: 0.9404
[epoch 11][iter  590] loss: 217845.4844 RMSElog: 10.3597 grad_loss: 21773.2129 normal_loss: 0.9774
[epoch 12][iter    0] loss: 159422.2656 RMSElog: 9.7010 grad_loss: 15931.5957 normal_loss: 0.9299
[epoch 12][iter   10] loss: 151723.8750 RMSElog: 9.9680 grad_loss: 15161.4746 normal_loss: 0.9449
[epoch 12][iter   20] loss: 150323.7656 RMSElog: 10.0510 grad_loss: 15021.3848 normal_loss: 0.9410
[epoch 12][iter   30] loss: 152562.3906 RMSElog: 9.8216 grad_loss: 15245.4785 normal_loss: 0.9391
[epoch 12][iter   40] loss: 142016.3906 RMSElog: 9.9191 grad_loss: 14190.7676 normal_loss: 0.9526
[epoch 12][iter   50] loss: 163171.1719 RMSElog: 9.7905 grad_loss: 16306.3613 normal_loss: 0.9652
[epoch 12][iter   60] loss: 167211.6250 RMSElog: 10.0285 grad_loss: 16710.1777 normal_loss: 0.9542
[epoch 12][iter   70] loss: 177934.3750 RMSElog: 9.8386 grad_loss: 17782.6445 normal_loss: 0.9542
[epoch 12][iter   80] loss: 208699.7031 RMSElog: 10.3419 grad_loss: 20858.6719 normal_loss: 0.9568
[epoch 12][iter   90] loss: 149135.0000 RMSElog: 10.0932 grad_loss: 14902.4570 normal_loss: 0.9500
[epoch 12][iter  100] loss: 162542.0469 RMSElog: 9.9393 grad_loss: 16243.3115 normal_loss: 0.9527
[epoch 12][iter  110] loss: 152149.7500 RMSElog: 9.9062 grad_loss: 15204.1221 normal_loss: 0.9462
[epoch 12][iter  120] loss: 159284.2188 RMSElog: 9.6028 grad_loss: 15917.8828 normal_loss: 0.9361
[epoch 12][iter  130] loss: 189507.2500 RMSElog: 10.2157 grad_loss: 18939.5586 normal_loss: 0.9510
[epoch 12][iter  140] loss: 95221.2109 RMSElog: 9.1894 grad_loss: 9512.0205 normal_loss: 0.9115
[epoch 12][iter  150] loss: 110579.5391 RMSElog: 9.3019 grad_loss: 11047.7109 normal_loss: 0.9413
[epoch 12][iter  160] loss: 117061.6250 RMSElog: 9.2141 grad_loss: 11696.0439 normal_loss: 0.9046
[epoch 12][iter  170] loss: 110603.0625 RMSElog: 9.8046 grad_loss: 11049.5654 normal_loss: 0.9364
[epoch 12][iter  180] loss: 206526.4062 RMSElog: 10.1531 grad_loss: 20641.5449 normal_loss: 0.9425
[epoch 12][iter  190] loss: 157706.0781 RMSElog: 9.9981 grad_loss: 15759.6602 normal_loss: 0.9499
[epoch 12][iter  200] loss: 170542.9531 RMSElog: 9.7855 grad_loss: 17043.5410 normal_loss: 0.9688
[epoch 12][iter  210] loss: 141442.1250 RMSElog: 9.6437 grad_loss: 14133.6191 normal_loss: 0.9495
[epoch 12][iter  220] loss: 206083.5312 RMSElog: 10.2501 grad_loss: 20597.1465 normal_loss: 0.9562
[epoch 12][iter  230] loss: 162579.5312 RMSElog: 10.1223 grad_loss: 16246.8740 normal_loss: 0.9574
[epoch 12][iter  240] loss: 130739.4062 RMSElog: 9.5471 grad_loss: 13063.4434 normal_loss: 0.9498
[epoch 12][iter  250] loss: 143613.7656 RMSElog: 9.8882 grad_loss: 14350.5566 normal_loss: 0.9315
[epoch 12][iter  260] loss: 164415.3438 RMSElog: 10.0829 grad_loss: 16430.4961 normal_loss: 0.9571
[epoch 12][iter  270] loss: 164767.1875 RMSElog: 9.8398 grad_loss: 16465.9102 normal_loss: 0.9691
[epoch 12][iter  280] loss: 112535.5625 RMSElog: 9.5823 grad_loss: 11243.0459 normal_loss: 0.9289
[epoch 12][iter  290] loss: 90482.6875 RMSElog: 9.3127 grad_loss: 9038.0186 normal_loss: 0.9378
[epoch 12][iter  300] loss: 135042.2031 RMSElog: 9.6763 grad_loss: 13493.6074 normal_loss: 0.9366
[epoch 12][iter  310] loss: 151368.4219 RMSElog: 9.7883 grad_loss: 15126.1289 normal_loss: 0.9251
[epoch 12][iter  320] loss: 150364.6250 RMSElog: 9.9123 grad_loss: 15025.6094 normal_loss: 0.9418
[epoch 12][iter  330] loss: 165750.3750 RMSElog: 9.8971 grad_loss: 16564.1895 normal_loss: 0.9518
[epoch 12][iter  340] loss: 119923.2031 RMSElog: 9.5699 grad_loss: 11981.8154 normal_loss: 0.9342
[epoch 12][iter  350] loss: 140282.6250 RMSElog: 9.9822 grad_loss: 14017.3145 normal_loss: 0.9650
[epoch 12][iter  360] loss: 119429.4609 RMSElog: 9.6388 grad_loss: 11932.3740 normal_loss: 0.9339
[epoch 12][iter  370] loss: 232771.8594 RMSElog: 10.2938 grad_loss: 23265.9453 normal_loss: 0.9464
[epoch 12][iter  380] loss: 196277.7188 RMSElog: 10.2453 grad_loss: 19616.5664 normal_loss: 0.9595
[epoch 12][iter  390] loss: 167784.2500 RMSElog: 9.9330 grad_loss: 16767.5391 normal_loss: 0.9539
[epoch 12][iter  400] loss: 160743.5156 RMSElog: 9.9654 grad_loss: 16063.4521 normal_loss: 0.9340
[epoch 12][iter  410] loss: 188645.4531 RMSElog: 10.1590 grad_loss: 18853.4082 normal_loss: 0.9794
[epoch 12][iter  420] loss: 208184.3125 RMSElog: 10.4061 grad_loss: 20807.0781 normal_loss: 0.9472
[epoch 12][iter  430] loss: 173339.3750 RMSElog: 9.8230 grad_loss: 17323.1934 normal_loss: 0.9212
[epoch 12][iter  440] loss: 144440.4531 RMSElog: 9.8843 grad_loss: 14433.2314 normal_loss: 0.9296
[epoch 12][iter  450] loss: 165648.1875 RMSElog: 9.7420 grad_loss: 16554.1289 normal_loss: 0.9479
[epoch 12][iter  460] loss: 231973.6719 RMSElog: 10.2099 grad_loss: 23186.2090 normal_loss: 0.9490
[epoch 12][iter  470] loss: 147765.9219 RMSElog: 10.0607 grad_loss: 14765.6104 normal_loss: 0.9219
[epoch 12][iter  480] loss: 108800.2422 RMSElog: 9.7538 grad_loss: 10869.3369 normal_loss: 0.9334
[epoch 12][iter  490] loss: 118495.6641 RMSElog: 9.2657 grad_loss: 11839.3906 normal_loss: 0.9103
[epoch 12][iter  500] loss: 115651.6406 RMSElog: 9.1877 grad_loss: 11555.0664 normal_loss: 0.9106
[epoch 12][iter  510] loss: 179649.7188 RMSElog: 10.2020 grad_loss: 17953.8125 normal_loss: 0.9593
[epoch 12][iter  520] loss: 138606.5781 RMSElog: 9.6389 grad_loss: 13850.0977 normal_loss: 0.9205
[epoch 12][iter  530] loss: 150296.1406 RMSElog: 9.8188 grad_loss: 15018.8555 normal_loss: 0.9407
[epoch 12][iter  540] loss: 109260.8984 RMSElog: 9.7819 grad_loss: 10915.3799 normal_loss: 0.9279
[epoch 12][iter  550] loss: 172755.6875 RMSElog: 10.0251 grad_loss: 17264.6016 normal_loss: 0.9419
[epoch 12][iter  560] loss: 166542.4219 RMSElog: 9.8913 grad_loss: 16643.4043 normal_loss: 0.9482
[epoch 12][iter  570] loss: 145321.5469 RMSElog: 9.7332 grad_loss: 14521.4590 normal_loss: 0.9632
[epoch 12][iter  580] loss: 182476.7344 RMSElog: 9.9672 grad_loss: 18236.7656 normal_loss: 0.9407
[epoch 12][iter  590] loss: 226949.5156 RMSElog: 10.2029 grad_loss: 22683.8047 normal_loss: 0.9430
[epoch 13][iter    0] loss: 106769.7500 RMSElog: 9.8688 grad_loss: 10666.1914 normal_loss: 0.9140
[epoch 13][iter   10] loss: 147765.9219 RMSElog: 10.0607 grad_loss: 14765.6104 normal_loss: 0.9219
[epoch 13][iter   20] loss: 105828.7812 RMSElog: 10.1032 grad_loss: 10571.8545 normal_loss: 0.9195
[epoch 13][iter   30] loss: 150608.8906 RMSElog: 10.0680 grad_loss: 15049.8750 normal_loss: 0.9460
[epoch 13][iter   40] loss: 175122.9531 RMSElog: 9.9588 grad_loss: 17501.3789 normal_loss: 0.9573
[epoch 13][iter   50] loss: 143904.7812 RMSElog: 10.0263 grad_loss: 14379.5068 normal_loss: 0.9457
[epoch 13][iter   60] loss: 115534.3750 RMSElog: 9.7196 grad_loss: 11542.7852 normal_loss: 0.9325
[epoch 13][iter   70] loss: 216232.0156 RMSElog: 10.2147 grad_loss: 21612.0215 normal_loss: 0.9640
[epoch 13][iter   80] loss: 148793.9688 RMSElog: 10.1047 grad_loss: 14868.3447 normal_loss: 0.9480
[epoch 13][iter   90] loss: 166492.4375 RMSElog: 9.8376 grad_loss: 16638.4512 normal_loss: 0.9558
[epoch 13][iter  100] loss: 218457.8281 RMSElog: 10.1255 grad_loss: 21834.7168 normal_loss: 0.9418
[epoch 13][iter  110] loss: 105628.3672 RMSElog: 10.0899 grad_loss: 10551.8262 normal_loss: 0.9213
[epoch 13][iter  120] loss: 147544.5000 RMSElog: 9.8913 grad_loss: 14743.6045 normal_loss: 0.9534
[epoch 13][iter  130] loss: 134677.6719 RMSElog: 9.8776 grad_loss: 13456.9443 normal_loss: 0.9441
[epoch 13][iter  140] loss: 198036.3750 RMSElog: 10.2619 grad_loss: 19792.4219 normal_loss: 0.9529
[epoch 13][iter  150] loss: 108800.2422 RMSElog: 9.7538 grad_loss: 10869.3369 normal_loss: 0.9334
[epoch 13][iter  160] loss: 141228.9219 RMSElog: 9.9926 grad_loss: 14111.9531 normal_loss: 0.9477
[epoch 13][iter  170] loss: 142138.5312 RMSElog: 9.7967 grad_loss: 14203.1270 normal_loss: 0.9291
[epoch 13][iter  180] loss: 134406.2031 RMSElog: 9.9402 grad_loss: 13429.7256 normal_loss: 0.9539
[epoch 13][iter  190] loss: 128277.9766 RMSElog: 9.7565 grad_loss: 12817.0889 normal_loss: 0.9518
[epoch 13][iter  200] loss: 224943.8906 RMSElog: 10.1206 grad_loss: 22483.3125 normal_loss: 0.9551
[epoch 13][iter  210] loss: 104086.5234 RMSElog: 9.7787 grad_loss: 10397.9453 normal_loss: 0.9289
[epoch 13][iter  220] loss: 135215.1250 RMSElog: 9.9407 grad_loss: 13510.6152 normal_loss: 0.9572
[epoch 13][iter  230] loss: 156709.9531 RMSElog: 9.8741 grad_loss: 15660.1758 normal_loss: 0.9453
[epoch 13][iter  240] loss: 162542.0469 RMSElog: 9.9393 grad_loss: 16243.3115 normal_loss: 0.9527
[epoch 13][iter  250] loss: 146576.9219 RMSElog: 9.6520 grad_loss: 14647.1045 normal_loss: 0.9356
[epoch 13][iter  260] loss: 99107.7500 RMSElog: 9.5230 grad_loss: 9900.3428 normal_loss: 0.9094
[epoch 13][iter  270] loss: 180929.5938 RMSElog: 10.0745 grad_loss: 18081.9297 normal_loss: 0.9546
[epoch 13][iter  280] loss: 137252.6406 RMSElog: 9.3538 grad_loss: 13714.9785 normal_loss: 0.9319
[epoch 13][iter  290] loss: 160782.5156 RMSElog: 9.7710 grad_loss: 16067.5547 normal_loss: 0.9244
[epoch 13][iter  300] loss: 218757.0312 RMSElog: 10.1228 grad_loss: 21864.6270 normal_loss: 0.9524
[epoch 13][iter  310] loss: 121959.7344 RMSElog: 9.7378 grad_loss: 12185.3047 normal_loss: 0.9314
[epoch 13][iter  320] loss: 110808.0469 RMSElog: 9.7171 grad_loss: 11070.1562 normal_loss: 0.9312
[epoch 13][iter  330] loss: 195777.0469 RMSElog: 10.1280 grad_loss: 19566.6035 normal_loss: 0.9726
[epoch 13][iter  340] loss: 159340.4219 RMSElog: 9.9507 grad_loss: 15923.1406 normal_loss: 0.9500
[epoch 13][iter  350] loss: 218949.8281 RMSElog: 10.4462 grad_loss: 21883.5918 normal_loss: 0.9447
[epoch 13][iter  360] loss: 163252.6875 RMSElog: 9.8236 grad_loss: 16314.5117 normal_loss: 0.9348
[epoch 13][iter  370] loss: 166207.1094 RMSElog: 9.7947 grad_loss: 16609.9668 normal_loss: 0.9492
[epoch 13][iter  380] loss: 107948.0312 RMSElog: 9.7377 grad_loss: 10784.1309 normal_loss: 0.9342
[epoch 13][iter  390] loss: 177936.2812 RMSElog: 10.0381 grad_loss: 17782.6348 normal_loss: 0.9545
[epoch 13][iter  400] loss: 181293.3438 RMSElog: 10.0009 grad_loss: 18118.3809 normal_loss: 0.9531
[epoch 13][iter  410] loss: 214808.7344 RMSElog: 10.1872 grad_loss: 21469.7441 normal_loss: 0.9423
[epoch 13][iter  420] loss: 111518.2734 RMSElog: 9.6257 grad_loss: 11141.2676 normal_loss: 0.9335
[epoch 13][iter  430] loss: 196516.2188 RMSElog: 9.9615 grad_loss: 19640.7129 normal_loss: 0.9475
[epoch 13][iter  440] loss: 149036.1562 RMSElog: 9.7728 grad_loss: 14892.8887 normal_loss: 0.9547
[epoch 13][iter  450] loss: 204650.3281 RMSElog: 10.1934 grad_loss: 20453.8633 normal_loss: 0.9763
[epoch 13][iter  460] loss: 145332.8438 RMSElog: 10.0385 grad_loss: 14522.2842 normal_loss: 0.9619
[epoch 13][iter  470] loss: 184016.5469 RMSElog: 9.9468 grad_loss: 18390.7363 normal_loss: 0.9712
[epoch 13][iter  480] loss: 195671.7031 RMSElog: 10.1080 grad_loss: 19556.1133 normal_loss: 0.9494
[epoch 13][iter  490] loss: 185495.4062 RMSElog: 9.9310 grad_loss: 18538.6641 normal_loss: 0.9455
[epoch 13][iter  500] loss: 185071.7031 RMSElog: 10.0456 grad_loss: 18496.1699 normal_loss: 0.9557
[epoch 13][iter  510] loss: 132623.8750 RMSElog: 9.3325 grad_loss: 13252.1152 normal_loss: 0.9391
[epoch 13][iter  520] loss: 154855.9062 RMSElog: 9.8054 grad_loss: 15474.8340 normal_loss: 0.9510
[epoch 13][iter  530] loss: 95221.2109 RMSElog: 9.1894 grad_loss: 9512.0205 normal_loss: 0.9115
[epoch 13][iter  540] loss: 147479.0312 RMSElog: 9.6781 grad_loss: 14737.2812 normal_loss: 0.9446
[epoch 13][iter  550] loss: 199070.3125 RMSElog: 9.9947 grad_loss: 19896.0605 normal_loss: 0.9757
[epoch 13][iter  560] loss: 126023.2344 RMSElog: 9.6219 grad_loss: 12591.7656 normal_loss: 0.9360
[epoch 13][iter  570] loss: 189017.5781 RMSElog: 10.1390 grad_loss: 18890.6602 normal_loss: 0.9589
[epoch 13][iter  580] loss: 102998.8984 RMSElog: 9.8140 grad_loss: 10289.1318 normal_loss: 0.9431
[epoch 13][iter  590] loss: 223875.4062 RMSElog: 10.3929 grad_loss: 22376.1953 normal_loss: 0.9536
[epoch 14][iter    0] loss: 224257.9688 RMSElog: 10.2842 grad_loss: 22414.5605 normal_loss: 0.9521
[epoch 14][iter   10] loss: 132707.3594 RMSElog: 9.7842 grad_loss: 13260.0049 normal_loss: 0.9472
[epoch 14][iter   20] loss: 149036.1562 RMSElog: 9.7728 grad_loss: 14892.8887 normal_loss: 0.9547
[epoch 14][iter   30] loss: 139154.8594 RMSElog: 9.5986 grad_loss: 13904.9707 normal_loss: 0.9164
[epoch 14][iter   40] loss: 123289.4609 RMSElog: 9.4106 grad_loss: 12318.5752 normal_loss: 0.9606
[epoch 14][iter   50] loss: 170357.3438 RMSElog: 9.9687 grad_loss: 17024.8008 normal_loss: 0.9644
[epoch 14][iter   60] loss: 147598.9844 RMSElog: 9.8981 grad_loss: 14749.0645 normal_loss: 0.9355
[epoch 14][iter   70] loss: 163171.1562 RMSElog: 9.7905 grad_loss: 16306.3594 normal_loss: 0.9652
[epoch 14][iter   80] loss: 151401.7031 RMSElog: 9.4414 grad_loss: 15129.7930 normal_loss: 0.9361
[epoch 14][iter   90] loss: 207885.1875 RMSElog: 10.4422 grad_loss: 20777.1328 normal_loss: 0.9460
[epoch 14][iter  100] loss: 153501.7031 RMSElog: 10.1127 grad_loss: 15339.1152 normal_loss: 0.9429
[epoch 14][iter  110] loss: 215955.5312 RMSElog: 10.2675 grad_loss: 21584.3203 normal_loss: 0.9654
[epoch 14][iter  120] loss: 189017.5781 RMSElog: 10.1390 grad_loss: 18890.6602 normal_loss: 0.9589
[epoch 14][iter  130] loss: 125695.5391 RMSElog: 9.9422 grad_loss: 12558.6836 normal_loss: 0.9278
[epoch 14][iter  140] loss: 142005.0938 RMSElog: 9.9168 grad_loss: 14189.6543 normal_loss: 0.9371
[epoch 14][iter  150] loss: 191371.4375 RMSElog: 10.1918 grad_loss: 19126.0020 normal_loss: 0.9504
[epoch 14][iter  160] loss: 178468.8906 RMSElog: 9.9039 grad_loss: 17836.0469 normal_loss: 0.9380
[epoch 14][iter  170] loss: 109101.1250 RMSElog: 9.7363 grad_loss: 10899.4590 normal_loss: 0.9167
[epoch 14][iter  180] loss: 159284.2188 RMSElog: 9.6028 grad_loss: 15917.8828 normal_loss: 0.9361
[epoch 14][iter  190] loss: 193978.6562 RMSElog: 10.3530 grad_loss: 19386.5605 normal_loss: 0.9518
[epoch 14][iter  200] loss: 175524.1250 RMSElog: 10.2056 grad_loss: 17541.2441 normal_loss: 0.9625
[epoch 14][iter  210] loss: 154310.0938 RMSElog: 9.9148 grad_loss: 15420.1445 normal_loss: 0.9495
[epoch 14][iter  220] loss: 176260.7500 RMSElog: 10.0638 grad_loss: 17615.0703 normal_loss: 0.9395
[epoch 14][iter  230] loss: 196677.7812 RMSElog: 10.0486 grad_loss: 19656.7715 normal_loss: 0.9571
[epoch 14][iter  240] loss: 105816.5000 RMSElog: 9.4266 grad_loss: 10571.3066 normal_loss: 0.9171
[epoch 14][iter  250] loss: 160051.0156 RMSElog: 9.8405 grad_loss: 15994.3105 normal_loss: 0.9506
[epoch 14][iter  260] loss: 113195.8281 RMSElog: 9.1761 grad_loss: 11309.5059 normal_loss: 0.9012
[epoch 14][iter  270] loss: 143534.0938 RMSElog: 9.7662 grad_loss: 14342.7061 normal_loss: 0.9374
[epoch 14][iter  280] loss: 100116.4922 RMSElog: 9.9235 grad_loss: 10000.7998 normal_loss: 0.9258
[epoch 14][iter  290] loss: 126821.0234 RMSElog: 10.1398 grad_loss: 12671.0410 normal_loss: 0.9215
[epoch 14][iter  300] loss: 178076.4688 RMSElog: 9.8778 grad_loss: 17796.8457 normal_loss: 0.9233
[epoch 14][iter  310] loss: 142544.5156 RMSElog: 10.1645 grad_loss: 14243.3457 normal_loss: 0.9425
[epoch 14][iter  320] loss: 177936.2812 RMSElog: 10.0381 grad_loss: 17782.6348 normal_loss: 0.9545
[epoch 14][iter  330] loss: 172966.9375 RMSElog: 9.9430 grad_loss: 17285.8008 normal_loss: 0.9485
[epoch 14][iter  340] loss: 193024.5000 RMSElog: 9.9639 grad_loss: 19291.5117 normal_loss: 0.9740
[epoch 14][iter  350] loss: 191825.0781 RMSElog: 10.3012 grad_loss: 19171.2500 normal_loss: 0.9570
[epoch 14][iter  360] loss: 127463.1875 RMSElog: 9.4952 grad_loss: 12735.9082 normal_loss: 0.9153
[epoch 14][iter  370] loss: 189743.5625 RMSElog: 10.1862 grad_loss: 18963.2207 normal_loss: 0.9490
[epoch 14][iter  380] loss: 132012.6719 RMSElog: 9.6802 grad_loss: 13190.6582 normal_loss: 0.9291
[epoch 14][iter  390] loss: 146576.9219 RMSElog: 9.6520 grad_loss: 14647.1045 normal_loss: 0.9356
[epoch 14][iter  400] loss: 115651.6406 RMSElog: 9.1877 grad_loss: 11555.0664 normal_loss: 0.9106
[epoch 14][iter  410] loss: 102223.4531 RMSElog: 9.4278 grad_loss: 10212.0010 normal_loss: 0.9169
[epoch 14][iter  420] loss: 158630.9688 RMSElog: 9.8321 grad_loss: 15852.3232 normal_loss: 0.9428
[epoch 14][iter  430] loss: 230519.3906 RMSElog: 10.3105 grad_loss: 23040.6836 normal_loss: 0.9459
[epoch 14][iter  440] loss: 143460.8594 RMSElog: 9.7520 grad_loss: 14335.3848 normal_loss: 0.9489
[epoch 14][iter  450] loss: 150608.8906 RMSElog: 10.0680 grad_loss: 15049.8750 normal_loss: 0.9460
[epoch 14][iter  460] loss: 195999.5625 RMSElog: 10.1749 grad_loss: 19588.8047 normal_loss: 0.9756
[epoch 14][iter  470] loss: 198743.4219 RMSElog: 10.0842 grad_loss: 19863.2930 normal_loss: 0.9643
[epoch 14][iter  480] loss: 203093.2969 RMSElog: 10.1238 grad_loss: 20298.2598 normal_loss: 0.9478
[epoch 14][iter  490] loss: 231973.6719 RMSElog: 10.2099 grad_loss: 23186.2090 normal_loss: 0.9490
[epoch 14][iter  500] loss: 121916.0000 RMSElog: 9.7464 grad_loss: 12180.9131 normal_loss: 0.9404
[epoch 14][iter  510] loss: 141228.9219 RMSElog: 9.9926 grad_loss: 14111.9531 normal_loss: 0.9477
[epoch 14][iter  520] loss: 136796.3438 RMSElog: 9.6524 grad_loss: 13669.0273 normal_loss: 0.9549
[epoch 14][iter  530] loss: 158534.6406 RMSElog: 9.6735 grad_loss: 15842.8672 normal_loss: 0.9232
[epoch 14][iter  540] loss: 106952.9219 RMSElog: 10.0294 grad_loss: 10684.3398 normal_loss: 0.9231
[epoch 14][iter  550] loss: 221213.4531 RMSElog: 10.1740 grad_loss: 22110.2305 normal_loss: 0.9415
[epoch 14][iter  560] loss: 205362.7344 RMSElog: 10.1686 grad_loss: 20525.1582 normal_loss: 0.9475
[epoch 14][iter  570] loss: 218547.0156 RMSElog: 10.4419 grad_loss: 21843.3086 normal_loss: 0.9515
[epoch 14][iter  580] loss: 160934.5156 RMSElog: 9.8035 grad_loss: 16082.6846 normal_loss: 0.9629
[epoch 14][iter  590] loss: 199607.0938 RMSElog: 10.0729 grad_loss: 19949.6758 normal_loss: 0.9604
[epoch 15][iter    0] loss: 114173.8984 RMSElog: 9.3600 grad_loss: 11407.1230 normal_loss: 0.9061
[epoch 15][iter   10] loss: 141962.4844 RMSElog: 9.7918 grad_loss: 14185.5273 normal_loss: 0.9289
[epoch 15][iter   20] loss: 136507.7344 RMSElog: 10.0223 grad_loss: 13639.7900 normal_loss: 0.9608
[epoch 15][iter   30] loss: 104749.2812 RMSElog: 9.7512 grad_loss: 10464.2490 normal_loss: 0.9281
[epoch 15][iter   40] loss: 150296.1562 RMSElog: 9.8188 grad_loss: 15018.8564 normal_loss: 0.9407
[epoch 15][iter   50] loss: 109260.8906 RMSElog: 9.7819 grad_loss: 10915.3789 normal_loss: 0.9279
[epoch 15][iter   60] loss: 158534.6406 RMSElog: 9.6735 grad_loss: 15842.8672 normal_loss: 0.9232
[epoch 15][iter   70] loss: 114149.3672 RMSElog: 9.2655 grad_loss: 11404.7637 normal_loss: 0.9073
[epoch 15][iter   80] loss: 214808.7344 RMSElog: 10.1872 grad_loss: 21469.7441 normal_loss: 0.9423
[epoch 15][iter   90] loss: 177063.6250 RMSElog: 10.1532 grad_loss: 17695.2383 normal_loss: 0.9719
[epoch 15][iter  100] loss: 116073.5938 RMSElog: 9.6844 grad_loss: 11596.7324 normal_loss: 0.9429
[epoch 15][iter  110] loss: 182005.7031 RMSElog: 10.0998 grad_loss: 18189.5195 normal_loss: 0.9507
[epoch 15][iter  120] loss: 125315.5547 RMSElog: 9.7124 grad_loss: 12520.9121 normal_loss: 0.9309
[epoch 15][iter  130] loss: 167740.4688 RMSElog: 9.8540 grad_loss: 16763.2344 normal_loss: 0.9591
[epoch 15][iter  140] loss: 150699.3750 RMSElog: 9.8567 grad_loss: 15059.1328 normal_loss: 0.9486
[epoch 15][iter  150] loss: 167412.9531 RMSElog: 9.7311 grad_loss: 16730.6387 normal_loss: 0.9260
[epoch 15][iter  160] loss: 114509.4531 RMSElog: 9.7409 grad_loss: 11440.2627 normal_loss: 0.9413
[epoch 15][iter  170] loss: 104456.8438 RMSElog: 10.0170 grad_loss: 10434.7461 normal_loss: 0.9214
[epoch 15][iter  180] loss: 175934.2344 RMSElog: 10.1481 grad_loss: 17582.3086 normal_loss: 0.9674
[epoch 15][iter  190] loss: 118495.6562 RMSElog: 9.2657 grad_loss: 11839.3896 normal_loss: 0.9103
[epoch 15][iter  200] loss: 189017.5781 RMSElog: 10.1390 grad_loss: 18890.6602 normal_loss: 0.9589
[epoch 15][iter  210] loss: 186053.2812 RMSElog: 10.1470 grad_loss: 18594.2305 normal_loss: 0.9504
[epoch 15][iter  220] loss: 136533.4688 RMSElog: 9.6756 grad_loss: 13642.7363 normal_loss: 0.9359
[epoch 15][iter  230] loss: 107956.6094 RMSElog: 10.0900 grad_loss: 10784.6504 normal_loss: 0.9209
[epoch 15][iter  240] loss: 149540.3750 RMSElog: 9.7408 grad_loss: 14943.3740 normal_loss: 0.9229
[epoch 15][iter  250] loss: 224257.9844 RMSElog: 10.2842 grad_loss: 22414.5625 normal_loss: 0.9521
[epoch 15][iter  260] loss: 221089.3594 RMSElog: 10.3457 grad_loss: 22097.6465 normal_loss: 0.9439
[epoch 15][iter  270] loss: 164415.3438 RMSElog: 10.0829 grad_loss: 16430.4961 normal_loss: 0.9571
[epoch 15][iter  280] loss: 131942.8750 RMSElog: 9.8414 grad_loss: 13183.5000 normal_loss: 0.9453
[epoch 15][iter  290] loss: 160595.2188 RMSElog: 9.9514 grad_loss: 16048.6191 normal_loss: 0.9518
[epoch 15][iter  300] loss: 180663.0469 RMSElog: 10.2319 grad_loss: 18055.1094 normal_loss: 0.9630
[epoch 15][iter  310] loss: 173339.3594 RMSElog: 9.8230 grad_loss: 17323.1914 normal_loss: 0.9212
[epoch 15][iter  320] loss: 128883.6328 RMSElog: 9.8798 grad_loss: 12877.5244 normal_loss: 0.9593
[epoch 15][iter  330] loss: 182315.9688 RMSElog: 10.1291 grad_loss: 18220.5039 normal_loss: 0.9652
[epoch 15][iter  340] loss: 112715.8125 RMSElog: 9.6867 grad_loss: 11260.9570 normal_loss: 0.9375
[epoch 15][iter  350] loss: 105828.7812 RMSElog: 10.1032 grad_loss: 10571.8545 normal_loss: 0.9195
[epoch 15][iter  360] loss: 133024.8594 RMSElog: 9.6937 grad_loss: 13291.8662 normal_loss: 0.9261
[epoch 15][iter  370] loss: 211640.2344 RMSElog: 10.2414 grad_loss: 21152.8145 normal_loss: 0.9675
[epoch 15][iter  380] loss: 185902.4219 RMSElog: 9.8998 grad_loss: 18579.3711 normal_loss: 0.9705
[epoch 15][iter  390] loss: 149128.2188 RMSElog: 9.7262 grad_loss: 14902.1533 normal_loss: 0.9419
[epoch 15][iter  400] loss: 189743.5625 RMSElog: 10.1862 grad_loss: 18963.2207 normal_loss: 0.9490
[epoch 15][iter  410] loss: 141118.5312 RMSElog: 10.0520 grad_loss: 14100.8545 normal_loss: 0.9477
[epoch 15][iter  420] loss: 175004.6719 RMSElog: 10.1774 grad_loss: 17489.3320 normal_loss: 0.9561
[epoch 15][iter  430] loss: 156173.3594 RMSElog: 9.9681 grad_loss: 15606.4199 normal_loss: 0.9482
[epoch 15][iter  440] loss: 160788.1406 RMSElog: 9.7539 grad_loss: 16068.1289 normal_loss: 0.9318
[epoch 15][iter  450] loss: 100980.7812 RMSElog: 9.6528 grad_loss: 10087.5176 normal_loss: 0.9085
[epoch 15][iter  460] loss: 189582.6719 RMSElog: 9.9918 grad_loss: 18947.3223 normal_loss: 0.9523
[epoch 15][iter  470] loss: 163389.5625 RMSElog: 9.7702 grad_loss: 16328.2656 normal_loss: 0.9208
[epoch 15][iter  480] loss: 128488.4375 RMSElog: 9.7402 grad_loss: 12838.1719 normal_loss: 0.9319
[epoch 15][iter  490] loss: 119655.7969 RMSElog: 9.6110 grad_loss: 11955.0312 normal_loss: 0.9378
[epoch 15][iter  500] loss: 230519.3906 RMSElog: 10.3105 grad_loss: 23040.6836 normal_loss: 0.9459
[epoch 15][iter  510] loss: 170488.4688 RMSElog: 9.7164 grad_loss: 17038.1816 normal_loss: 0.9494
[epoch 15][iter  520] loss: 176914.4531 RMSElog: 10.3802 grad_loss: 17680.1094 normal_loss: 0.9546
[epoch 15][iter  530] loss: 146161.1562 RMSElog: 10.0682 grad_loss: 14605.1289 normal_loss: 0.9190
[epoch 15][iter  540] loss: 125695.5391 RMSElog: 9.9422 grad_loss: 12558.6836 normal_loss: 0.9278
[epoch 15][iter  550] loss: 139830.9062 RMSElog: 9.7552 grad_loss: 13972.3770 normal_loss: 0.9585
[epoch 15][iter  560] loss: 229081.3125 RMSElog: 10.2778 grad_loss: 22896.9121 normal_loss: 0.9420
[epoch 15][iter  570] loss: 139433.9219 RMSElog: 9.8139 grad_loss: 13932.6191 normal_loss: 0.9591
[epoch 15][iter  580] loss: 170690.9219 RMSElog: 9.9328 grad_loss: 17058.2109 normal_loss: 0.9470
[epoch 15][iter  590] loss: 106952.9219 RMSElog: 10.0294 grad_loss: 10684.3398 normal_loss: 0.9231
[epoch 16][iter    0] loss: 150296.1562 RMSElog: 9.8188 grad_loss: 15018.8564 normal_loss: 0.9407
[epoch 16][iter   10] loss: 197715.0000 RMSElog: 10.4355 grad_loss: 19760.1113 normal_loss: 0.9532
[epoch 16][iter   20] loss: 128883.6328 RMSElog: 9.8798 grad_loss: 12877.5244 normal_loss: 0.9593
[epoch 16][iter   30] loss: 113195.8281 RMSElog: 9.1761 grad_loss: 11309.5059 normal_loss: 0.9012
[epoch 16][iter   40] loss: 194578.5781 RMSElog: 10.2312 grad_loss: 19446.6680 normal_loss: 0.9596
[epoch 16][iter   50] loss: 120765.1875 RMSElog: 9.5441 grad_loss: 12066.0400 normal_loss: 0.9348
[epoch 16][iter   60] loss: 169013.5312 RMSElog: 9.5880 grad_loss: 16890.8242 normal_loss: 0.9419
[epoch 16][iter   70] loss: 110579.5391 RMSElog: 9.3019 grad_loss: 11047.7109 normal_loss: 0.9413
[epoch 16][iter   80] loss: 215542.7500 RMSElog: 10.4358 grad_loss: 21542.8887 normal_loss: 0.9514
[epoch 16][iter   90] loss: 136507.7344 RMSElog: 10.0223 grad_loss: 13639.7900 normal_loss: 0.9608
[epoch 16][iter  100] loss: 125695.5391 RMSElog: 9.9422 grad_loss: 12558.6836 normal_loss: 0.9278
[epoch 16][iter  110] loss: 156985.6406 RMSElog: 9.5765 grad_loss: 15688.0488 normal_loss: 0.9385
[epoch 16][iter  120] loss: 144440.4531 RMSElog: 9.8843 grad_loss: 14433.2314 normal_loss: 0.9296
[epoch 16][iter  130] loss: 143613.7656 RMSElog: 9.8882 grad_loss: 14350.5566 normal_loss: 0.9315
[epoch 16][iter  140] loss: 235211.2344 RMSElog: 10.3371 grad_loss: 23509.8398 normal_loss: 0.9452
[epoch 16][iter  150] loss: 231973.6719 RMSElog: 10.2099 grad_loss: 23186.2090 normal_loss: 0.9490
[epoch 16][iter  160] loss: 180663.0469 RMSElog: 10.2319 grad_loss: 18055.1094 normal_loss: 0.9630
[epoch 16][iter  170] loss: 153501.6875 RMSElog: 10.1127 grad_loss: 15339.1133 normal_loss: 0.9429
[epoch 16][iter  180] loss: 117061.6250 RMSElog: 9.2141 grad_loss: 11696.0439 normal_loss: 0.9046
[epoch 16][iter  190] loss: 175122.9531 RMSElog: 9.9588 grad_loss: 17501.3789 normal_loss: 0.9573
[epoch 16][iter  200] loss: 122386.0234 RMSElog: 9.6688 grad_loss: 12227.9971 normal_loss: 0.9366
[epoch 16][iter  210] loss: 162542.0469 RMSElog: 9.9393 grad_loss: 16243.3115 normal_loss: 0.9527
[epoch 16][iter  220] loss: 162556.3906 RMSElog: 9.9091 grad_loss: 16244.7939 normal_loss: 0.9368
[epoch 16][iter  230] loss: 153099.1094 RMSElog: 9.8355 grad_loss: 15299.1445 normal_loss: 0.9304
[epoch 16][iter  240] loss: 122446.5469 RMSElog: 9.6438 grad_loss: 12234.0742 normal_loss: 0.9366
[epoch 16][iter  250] loss: 133045.7969 RMSElog: 9.8765 grad_loss: 13293.7480 normal_loss: 0.9538
[epoch 16][iter  260] loss: 143440.0000 RMSElog: 9.6404 grad_loss: 14333.4336 normal_loss: 0.9261
[epoch 16][iter  270] loss: 104086.5234 RMSElog: 9.7787 grad_loss: 10397.9453 normal_loss: 0.9289
[epoch 16][iter  280] loss: 165976.7344 RMSElog: 9.9385 grad_loss: 16586.7695 normal_loss: 0.9645
[epoch 16][iter  290] loss: 106769.7500 RMSElog: 9.8688 grad_loss: 10666.1914 normal_loss: 0.9140
[epoch 16][iter  300] loss: 227080.5938 RMSElog: 10.4127 grad_loss: 22696.6992 normal_loss: 0.9482
[epoch 16][iter  310] loss: 150608.8906 RMSElog: 10.0680 grad_loss: 15049.8750 normal_loss: 0.9460
[epoch 16][iter  320] loss: 135042.2031 RMSElog: 9.6763 grad_loss: 13493.6074 normal_loss: 0.9366
[epoch 16][iter  330] loss: 217914.6094 RMSElog: 10.2786 grad_loss: 21780.2246 normal_loss: 0.9575
[epoch 16][iter  340] loss: 167211.6250 RMSElog: 10.0285 grad_loss: 16710.1777 normal_loss: 0.9542
[epoch 16][iter  350] loss: 213401.8906 RMSElog: 10.0568 grad_loss: 21329.1582 normal_loss: 0.9749
[epoch 16][iter  360] loss: 182315.9688 RMSElog: 10.1291 grad_loss: 18220.5039 normal_loss: 0.9652
[epoch 16][iter  370] loss: 162522.4531 RMSElog: 9.9860 grad_loss: 16241.3066 normal_loss: 0.9518
[epoch 16][iter  380] loss: 175299.8438 RMSElog: 10.1975 grad_loss: 17518.8301 normal_loss: 0.9576
[epoch 16][iter  390] loss: 122670.8984 RMSElog: 9.4365 grad_loss: 12256.7207 normal_loss: 0.9331
[epoch 16][iter  400] loss: 180982.8438 RMSElog: 10.2711 grad_loss: 18087.0605 normal_loss: 0.9528
[epoch 16][iter  410] loss: 146942.0781 RMSElog: 9.4227 grad_loss: 14683.8535 normal_loss: 0.9314
[epoch 16][iter  420] loss: 130641.2188 RMSElog: 9.7184 grad_loss: 13053.4688 normal_loss: 0.9344
[epoch 16][iter  430] loss: 164766.2812 RMSElog: 9.6928 grad_loss: 16465.9941 normal_loss: 0.9411
[epoch 16][iter  440] loss: 233395.7188 RMSElog: 10.2368 grad_loss: 23328.3906 normal_loss: 0.9452
[epoch 16][iter  450] loss: 206526.4062 RMSElog: 10.1531 grad_loss: 20641.5449 normal_loss: 0.9425
[epoch 16][iter  460] loss: 110603.0781 RMSElog: 9.8046 grad_loss: 11049.5664 normal_loss: 0.9364
[epoch 16][iter  470] loss: 152772.4219 RMSElog: 9.8249 grad_loss: 15266.4736 normal_loss: 0.9438
[epoch 16][iter  480] loss: 206400.7812 RMSElog: 10.3027 grad_loss: 20628.8281 normal_loss: 0.9467
[epoch 16][iter  490] loss: 165648.1875 RMSElog: 9.7420 grad_loss: 16554.1289 normal_loss: 0.9479
[epoch 16][iter  500] loss: 208699.7031 RMSElog: 10.3419 grad_loss: 20858.6719 normal_loss: 0.9568
[epoch 16][iter  510] loss: 105706.7109 RMSElog: 9.7501 grad_loss: 10559.9893 normal_loss: 0.9321
[epoch 16][iter  520] loss: 153997.7188 RMSElog: 9.7620 grad_loss: 15389.0713 normal_loss: 0.9383
[epoch 16][iter  530] loss: 143904.7812 RMSElog: 10.0263 grad_loss: 14379.5068 normal_loss: 0.9457
[epoch 16][iter  540] loss: 231696.5312 RMSElog: 10.3202 grad_loss: 23158.3867 normal_loss: 0.9457
[epoch 16][iter  550] loss: 171588.0938 RMSElog: 9.8325 grad_loss: 17148.0527 normal_loss: 0.9237
[epoch 16][iter  560] loss: 137249.3438 RMSElog: 9.7342 grad_loss: 13714.2764 normal_loss: 0.9239
[epoch 16][iter  570] loss: 139878.0781 RMSElog: 9.8515 grad_loss: 13977.0234 normal_loss: 0.9327
[epoch 16][iter  580] loss: 174565.0781 RMSElog: 9.7759 grad_loss: 17445.7832 normal_loss: 0.9484
[epoch 16][iter  590] loss: 220587.5781 RMSElog: 10.2995 grad_loss: 22047.5117 normal_loss: 0.9466
[epoch 17][iter    0] loss: 216509.5938 RMSElog: 10.1942 grad_loss: 21639.8242 normal_loss: 0.9420
[epoch 17][iter   10] loss: 146781.9531 RMSElog: 9.8688 grad_loss: 14667.3818 normal_loss: 0.9440
[epoch 17][iter   20] loss: 114509.4531 RMSElog: 9.7409 grad_loss: 11440.2627 normal_loss: 0.9413
[epoch 17][iter   30] loss: 106195.4219 RMSElog: 9.2531 grad_loss: 10609.3711 normal_loss: 0.9179
[epoch 17][iter   40] loss: 157274.4688 RMSElog: 9.9277 grad_loss: 15716.5723 normal_loss: 0.9461
[epoch 17][iter   50] loss: 147207.9844 RMSElog: 9.9779 grad_loss: 14709.8789 normal_loss: 0.9413
[epoch 17][iter   60] loss: 175301.7344 RMSElog: 9.8017 grad_loss: 17519.4180 normal_loss: 0.9555
[epoch 17][iter   70] loss: 139878.0625 RMSElog: 9.8515 grad_loss: 13977.0225 normal_loss: 0.9327
[epoch 17][iter   80] loss: 144440.4531 RMSElog: 9.8843 grad_loss: 14433.2314 normal_loss: 0.9296
[epoch 17][iter   90] loss: 159422.2500 RMSElog: 9.7010 grad_loss: 15931.5938 normal_loss: 0.9299
[epoch 17][iter  100] loss: 138934.7188 RMSElog: 9.8450 grad_loss: 13882.6904 normal_loss: 0.9370
[epoch 17][iter  110] loss: 133013.4531 RMSElog: 9.5635 grad_loss: 13290.8457 normal_loss: 0.9356
[epoch 17][iter  120] loss: 130641.2188 RMSElog: 9.7184 grad_loss: 13053.4688 normal_loss: 0.9344
[epoch 17][iter  130] loss: 140938.8594 RMSElog: 9.7483 grad_loss: 14083.1953 normal_loss: 0.9424
[epoch 17][iter  140] loss: 144657.9844 RMSElog: 9.9197 grad_loss: 14454.9258 normal_loss: 0.9518
[epoch 17][iter  150] loss: 186960.2969 RMSElog: 10.2468 grad_loss: 18684.8457 normal_loss: 0.9380
[epoch 17][iter  160] loss: 183480.3438 RMSElog: 10.0091 grad_loss: 18337.0801 normal_loss: 0.9458
[epoch 17][iter  170] loss: 127633.3516 RMSElog: 10.0540 grad_loss: 12752.3496 normal_loss: 0.9317
[epoch 17][iter  180] loss: 155482.1875 RMSElog: 10.0818 grad_loss: 15537.1816 normal_loss: 0.9552
[epoch 17][iter  190] loss: 154705.2188 RMSElog: 9.9142 grad_loss: 15459.6543 normal_loss: 0.9527
[epoch 17][iter  200] loss: 127061.7500 RMSElog: 9.6726 grad_loss: 12695.5654 normal_loss: 0.9369
[epoch 17][iter  210] loss: 108800.2422 RMSElog: 9.7538 grad_loss: 10869.3369 normal_loss: 0.9334
[epoch 17][iter  220] loss: 173339.3594 RMSElog: 9.8230 grad_loss: 17323.1914 normal_loss: 0.9212
[epoch 17][iter  230] loss: 105098.6875 RMSElog: 9.7426 grad_loss: 10499.2012 normal_loss: 0.9257
[epoch 17][iter  240] loss: 139098.6406 RMSElog: 9.8619 grad_loss: 13899.0508 normal_loss: 0.9510
[epoch 17][iter  250] loss: 207919.9844 RMSElog: 10.2783 grad_loss: 20780.7637 normal_loss: 0.9578
[epoch 17][iter  260] loss: 167211.6250 RMSElog: 10.0285 grad_loss: 16710.1777 normal_loss: 0.9542
[epoch 17][iter  270] loss: 146444.9062 RMSElog: 10.1139 grad_loss: 14633.4365 normal_loss: 0.9403
[epoch 17][iter  280] loss: 109101.1250 RMSElog: 9.7363 grad_loss: 10899.4590 normal_loss: 0.9167
[epoch 17][iter  290] loss: 163268.1250 RMSElog: 9.9218 grad_loss: 16315.9277 normal_loss: 0.9630
[epoch 17][iter  300] loss: 160466.8750 RMSElog: 9.9086 grad_loss: 16035.8252 normal_loss: 0.9540
[epoch 17][iter  310] loss: 175547.9062 RMSElog: 10.1409 grad_loss: 17543.7168 normal_loss: 0.9342
[epoch 17][iter  320] loss: 226949.5156 RMSElog: 10.2029 grad_loss: 22683.8047 normal_loss: 0.9430
[epoch 17][iter  330] loss: 174629.2812 RMSElog: 9.9405 grad_loss: 17452.0625 normal_loss: 0.9229
[epoch 17][iter  340] loss: 140057.2812 RMSElog: 9.9766 grad_loss: 13994.7988 normal_loss: 0.9526
[epoch 17][iter  350] loss: 141118.5312 RMSElog: 10.0520 grad_loss: 14100.8545 normal_loss: 0.9477
[epoch 17][iter  360] loss: 149369.9062 RMSElog: 10.0930 grad_loss: 14925.9756 normal_loss: 0.9225
[epoch 17][iter  370] loss: 100650.2734 RMSElog: 9.7441 grad_loss: 10054.3555 normal_loss: 0.9279
[epoch 17][iter  380] loss: 105794.8828 RMSElog: 9.7676 grad_loss: 10568.7939 normal_loss: 0.9267
[epoch 17][iter  390] loss: 144589.9531 RMSElog: 9.7438 grad_loss: 14448.3340 normal_loss: 0.9167
[epoch 17][iter  400] loss: 158875.5625 RMSElog: 9.5853 grad_loss: 15877.0449 normal_loss: 0.9260
[epoch 17][iter  410] loss: 141442.1250 RMSElog: 9.6437 grad_loss: 14133.6191 normal_loss: 0.9495
[epoch 17][iter  420] loss: 130145.6328 RMSElog: 9.7206 grad_loss: 13003.9082 normal_loss: 0.9344
[epoch 17][iter  430] loss: 138469.0469 RMSElog: 9.4787 grad_loss: 13836.4834 normal_loss: 0.9438
[epoch 17][iter  440] loss: 185902.4219 RMSElog: 9.8998 grad_loss: 18579.3711 normal_loss: 0.9705
[epoch 17][iter  450] loss: 157061.8906 RMSElog: 9.9892 grad_loss: 15695.2461 normal_loss: 0.9531
[epoch 17][iter  460] loss: 196842.3281 RMSElog: 10.1249 grad_loss: 19673.1543 normal_loss: 0.9534
[epoch 17][iter  470] loss: 165020.2344 RMSElog: 9.7791 grad_loss: 16491.2910 normal_loss: 0.9530
[epoch 17][iter  480] loss: 122386.0234 RMSElog: 9.6688 grad_loss: 12227.9971 normal_loss: 0.9366
[epoch 17][iter  490] loss: 139433.9219 RMSElog: 9.8139 grad_loss: 13932.6191 normal_loss: 0.9591
[epoch 17][iter  500] loss: 107956.6094 RMSElog: 10.0900 grad_loss: 10784.6504 normal_loss: 0.9209
[epoch 17][iter  510] loss: 114924.9844 RMSElog: 9.6925 grad_loss: 11481.8691 normal_loss: 0.9360
[epoch 17][iter  520] loss: 178468.8906 RMSElog: 9.9039 grad_loss: 17836.0469 normal_loss: 0.9380
[epoch 17][iter  530] loss: 163389.5625 RMSElog: 9.7702 grad_loss: 16328.2656 normal_loss: 0.9208
[epoch 17][iter  540] loss: 175004.6719 RMSElog: 10.1774 grad_loss: 17489.3320 normal_loss: 0.9561
[epoch 17][iter  550] loss: 166492.4688 RMSElog: 9.8376 grad_loss: 16638.4531 normal_loss: 0.9558
[epoch 17][iter  560] loss: 105896.6094 RMSElog: 9.6983 grad_loss: 10579.0293 normal_loss: 0.9335
[epoch 17][iter  570] loss: 135215.1250 RMSElog: 9.9407 grad_loss: 13510.6152 normal_loss: 0.9572
[epoch 17][iter  580] loss: 154029.7812 RMSElog: 9.8645 grad_loss: 15392.1729 normal_loss: 0.9405
[epoch 17][iter  590] loss: 165954.0938 RMSElog: 9.8997 grad_loss: 16584.5449 normal_loss: 0.9646
[epoch 18][iter    0] loss: 129163.5156 RMSElog: 9.7674 grad_loss: 12905.6660 normal_loss: 0.9175
[epoch 18][iter   10] loss: 135814.7969 RMSElog: 9.5386 grad_loss: 13571.0137 normal_loss: 0.9281
[epoch 18][iter   20] loss: 120292.3359 RMSElog: 9.8042 grad_loss: 12018.4854 normal_loss: 0.9430
[epoch 18][iter   30] loss: 186053.2969 RMSElog: 10.1470 grad_loss: 18594.2324 normal_loss: 0.9504
[epoch 18][iter   40] loss: 153997.7188 RMSElog: 9.7620 grad_loss: 15389.0713 normal_loss: 0.9383
[epoch 18][iter   50] loss: 105896.6094 RMSElog: 9.6983 grad_loss: 10579.0293 normal_loss: 0.9335
[epoch 18][iter   60] loss: 166207.1094 RMSElog: 9.7947 grad_loss: 16609.9668 normal_loss: 0.9492
[epoch 18][iter   70] loss: 165976.7344 RMSElog: 9.9385 grad_loss: 16586.7695 normal_loss: 0.9645
[epoch 18][iter   80] loss: 122331.3906 RMSElog: 9.7180 grad_loss: 12222.4795 normal_loss: 0.9418
[epoch 18][iter   90] loss: 183095.1562 RMSElog: 9.9762 grad_loss: 18298.5703 normal_loss: 0.9696
[epoch 18][iter  100] loss: 157119.7500 RMSElog: 9.8959 grad_loss: 15701.1318 normal_loss: 0.9480
[epoch 18][iter  110] loss: 152549.8750 RMSElog: 9.9928 grad_loss: 15244.0293 normal_loss: 0.9654
[epoch 18][iter  120] loss: 152772.4062 RMSElog: 9.8249 grad_loss: 15266.4727 normal_loss: 0.9438
[epoch 18][iter  130] loss: 188116.2812 RMSElog: 10.0888 grad_loss: 18800.5703 normal_loss: 0.9711
[epoch 18][iter  140] loss: 228338.7500 RMSElog: 10.2936 grad_loss: 22822.6348 normal_loss: 0.9477
[epoch 18][iter  150] loss: 206526.4062 RMSElog: 10.1531 grad_loss: 20641.5449 normal_loss: 0.9425
[epoch 18][iter  160] loss: 163389.5625 RMSElog: 9.7702 grad_loss: 16328.2656 normal_loss: 0.9208
[epoch 18][iter  170] loss: 175004.6719 RMSElog: 10.1774 grad_loss: 17489.3320 normal_loss: 0.9561
[epoch 18][iter  180] loss: 180929.5938 RMSElog: 10.0745 grad_loss: 18081.9297 normal_loss: 0.9546
[epoch 18][iter  190] loss: 207919.9844 RMSElog: 10.2783 grad_loss: 20780.7637 normal_loss: 0.9578
[epoch 18][iter  200] loss: 142261.5781 RMSElog: 9.7627 grad_loss: 14215.4639 normal_loss: 0.9304
[epoch 18][iter  210] loss: 177063.6250 RMSElog: 10.1532 grad_loss: 17695.2383 normal_loss: 0.9719
[epoch 18][iter  220] loss: 190938.1094 RMSElog: 10.3028 grad_loss: 19082.5547 normal_loss: 0.9541
[epoch 18][iter  230] loss: 149128.2188 RMSElog: 9.7262 grad_loss: 14902.1533 normal_loss: 0.9419
[epoch 18][iter  240] loss: 157061.8906 RMSElog: 9.9892 grad_loss: 15695.2461 normal_loss: 0.9531
[epoch 18][iter  250] loss: 198743.4219 RMSElog: 10.0842 grad_loss: 19863.2930 normal_loss: 0.9643
[epoch 18][iter  260] loss: 133464.0312 RMSElog: 9.6242 grad_loss: 13335.8584 normal_loss: 0.9201
[epoch 18][iter  270] loss: 145321.5625 RMSElog: 9.7332 grad_loss: 14521.4600 normal_loss: 0.9632
[epoch 18][iter  280] loss: 96006.2578 RMSElog: 9.5560 grad_loss: 9590.1406 normal_loss: 0.9294
[epoch 18][iter  290] loss: 185772.2500 RMSElog: 9.9702 grad_loss: 18566.2969 normal_loss: 0.9567
[epoch 18][iter  300] loss: 139830.9062 RMSElog: 9.7552 grad_loss: 13972.3770 normal_loss: 0.9585
[epoch 18][iter  310] loss: 185071.7188 RMSElog: 10.0456 grad_loss: 18496.1719 normal_loss: 0.9557
[epoch 18][iter  320] loss: 191320.0938 RMSElog: 10.0978 grad_loss: 19120.9492 normal_loss: 0.9627
[epoch 18][iter  330] loss: 80097.4062 RMSElog: 9.2433 grad_loss: 7999.5806 normal_loss: 0.9166
[epoch 18][iter  340] loss: 181942.5625 RMSElog: 10.0291 grad_loss: 18183.2793 normal_loss: 0.9464
[epoch 18][iter  350] loss: 136869.7500 RMSElog: 9.5771 grad_loss: 13676.4668 normal_loss: 0.9315
[epoch 18][iter  360] loss: 110603.0781 RMSElog: 9.8046 grad_loss: 11049.5664 normal_loss: 0.9364
[epoch 18][iter  370] loss: 204037.7500 RMSElog: 10.1754 grad_loss: 20392.6582 normal_loss: 0.9420
[epoch 18][iter  380] loss: 177195.4844 RMSElog: 10.1516 grad_loss: 17708.4512 normal_loss: 0.9458
[epoch 18][iter  390] loss: 110579.5391 RMSElog: 9.3019 grad_loss: 11047.7109 normal_loss: 0.9413
[epoch 18][iter  400] loss: 130527.6172 RMSElog: 9.8914 grad_loss: 13041.9189 normal_loss: 0.9509
[epoch 18][iter  410] loss: 114149.3750 RMSElog: 9.2655 grad_loss: 11404.7646 normal_loss: 0.9073
[epoch 18][iter  420] loss: 206083.5312 RMSElog: 10.2501 grad_loss: 20597.1465 normal_loss: 0.9562
[epoch 18][iter  430] loss: 213401.8906 RMSElog: 10.0568 grad_loss: 21329.1582 normal_loss: 0.9749
[epoch 18][iter  440] loss: 113895.6328 RMSElog: 9.2606 grad_loss: 11379.3926 normal_loss: 0.9106
[epoch 18][iter  450] loss: 174761.2344 RMSElog: 9.7333 grad_loss: 17465.4512 normal_loss: 0.9393
[epoch 18][iter  460] loss: 185902.4219 RMSElog: 9.8998 grad_loss: 18579.3711 normal_loss: 0.9705
[epoch 18][iter  470] loss: 147479.0312 RMSElog: 9.6781 grad_loss: 14737.2812 normal_loss: 0.9446
[epoch 18][iter  480] loss: 157425.7188 RMSElog: 9.9235 grad_loss: 15731.7012 normal_loss: 0.9477
[epoch 18][iter  490] loss: 142138.5156 RMSElog: 9.7967 grad_loss: 14203.1260 normal_loss: 0.9291
[epoch 18][iter  500] loss: 122912.0625 RMSElog: 9.7328 grad_loss: 12280.5225 normal_loss: 0.9509
[epoch 18][iter  510] loss: 160879.7969 RMSElog: 9.7521 grad_loss: 16077.2764 normal_loss: 0.9513
[epoch 18][iter  520] loss: 143440.0000 RMSElog: 9.6404 grad_loss: 14333.4336 normal_loss: 0.9261
[epoch 18][iter  530] loss: 193978.6562 RMSElog: 10.3530 grad_loss: 19386.5605 normal_loss: 0.9518
[epoch 18][iter  540] loss: 165750.3750 RMSElog: 9.8971 grad_loss: 16564.1895 normal_loss: 0.9518
[epoch 18][iter  550] loss: 140938.8594 RMSElog: 9.7483 grad_loss: 14083.1953 normal_loss: 0.9424
[epoch 18][iter  560] loss: 141442.1250 RMSElog: 9.6437 grad_loss: 14133.6191 normal_loss: 0.9495
[epoch 18][iter  570] loss: 121866.5156 RMSElog: 9.8422 grad_loss: 12175.8965 normal_loss: 0.9131
[epoch 18][iter  580] loss: 178784.1250 RMSElog: 9.9460 grad_loss: 17867.5078 normal_loss: 0.9592
[epoch 18][iter  590] loss: 217845.4844 RMSElog: 10.3597 grad_loss: 21773.2129 normal_loss: 0.9774
[epoch 19][iter    0] loss: 193024.5000 RMSElog: 9.9639 grad_loss: 19291.5117 normal_loss: 0.9740
[epoch 19][iter   10] loss: 179649.7188 RMSElog: 10.2020 grad_loss: 17953.8125 normal_loss: 0.9593
[epoch 19][iter   20] loss: 156985.6250 RMSElog: 9.5765 grad_loss: 15688.0479 normal_loss: 0.9385
[epoch 19][iter   30] loss: 165954.0938 RMSElog: 9.8997 grad_loss: 16584.5449 normal_loss: 0.9646
[epoch 19][iter   40] loss: 125829.2891 RMSElog: 9.7844 grad_loss: 12572.2148 normal_loss: 0.9297
[epoch 19][iter   50] loss: 189743.5625 RMSElog: 10.1862 grad_loss: 18963.2207 normal_loss: 0.9490
[epoch 19][iter   60] loss: 218547.0156 RMSElog: 10.4419 grad_loss: 21843.3086 normal_loss: 0.9515
[epoch 19][iter   70] loss: 152549.8750 RMSElog: 9.9928 grad_loss: 15244.0293 normal_loss: 0.9654
[epoch 19][iter   80] loss: 103509.2188 RMSElog: 9.9787 grad_loss: 10340.0186 normal_loss: 0.9251
[epoch 19][iter   90] loss: 188116.2812 RMSElog: 10.0888 grad_loss: 18800.5703 normal_loss: 0.9711
[epoch 19][iter  100] loss: 122670.8984 RMSElog: 9.4365 grad_loss: 12256.7207 normal_loss: 0.9331
[epoch 19][iter  110] loss: 144657.9844 RMSElog: 9.9197 grad_loss: 14454.9258 normal_loss: 0.9518
[epoch 19][iter  120] loss: 206764.1875 RMSElog: 10.2849 grad_loss: 20665.1777 normal_loss: 0.9549
[epoch 19][iter  130] loss: 195440.9062 RMSElog: 9.7403 grad_loss: 19533.3926 normal_loss: 0.9572
[epoch 19][iter  140] loss: 213417.0312 RMSElog: 10.3416 grad_loss: 21330.4082 normal_loss: 0.9536
[epoch 19][iter  150] loss: 139154.8594 RMSElog: 9.5986 grad_loss: 13904.9707 normal_loss: 0.9164
[epoch 19][iter  160] loss: 174375.7031 RMSElog: 10.0512 grad_loss: 17426.5703 normal_loss: 0.9483
[epoch 19][iter  170] loss: 191559.9688 RMSElog: 9.9749 grad_loss: 19145.0742 normal_loss: 0.9477
[epoch 19][iter  180] loss: 196277.7188 RMSElog: 10.2453 grad_loss: 19616.5664 normal_loss: 0.9595
[epoch 19][iter  190] loss: 150699.3750 RMSElog: 9.8567 grad_loss: 15059.1328 normal_loss: 0.9486
[epoch 19][iter  200] loss: 180208.2188 RMSElog: 10.0490 grad_loss: 18009.8223 normal_loss: 0.9517
[epoch 19][iter  210] loss: 147371.3906 RMSElog: 9.6635 grad_loss: 14726.5488 normal_loss: 0.9280
[epoch 19][iter  220] loss: 164597.1094 RMSElog: 9.8631 grad_loss: 16448.8984 normal_loss: 0.9502
[epoch 19][iter  230] loss: 107073.5156 RMSElog: 9.7289 grad_loss: 10696.6914 normal_loss: 0.9313
[epoch 19][iter  240] loss: 210654.5312 RMSElog: 10.4982 grad_loss: 21054.0039 normal_loss: 0.9517
[epoch 19][iter  250] loss: 196516.1875 RMSElog: 9.9615 grad_loss: 19640.7109 normal_loss: 0.9475
[epoch 19][iter  260] loss: 115534.3672 RMSElog: 9.7196 grad_loss: 11542.7842 normal_loss: 0.9325
[epoch 19][iter  270] loss: 155482.1875 RMSElog: 10.0818 grad_loss: 15537.1816 normal_loss: 0.9552
[epoch 19][iter  280] loss: 144440.4531 RMSElog: 9.8843 grad_loss: 14433.2314 normal_loss: 0.9296
[epoch 19][iter  290] loss: 118495.6641 RMSElog: 9.2657 grad_loss: 11839.3906 normal_loss: 0.9103
[epoch 19][iter  300] loss: 120745.8203 RMSElog: 9.7706 grad_loss: 12063.8691 normal_loss: 0.9428
[epoch 19][iter  310] loss: 194987.6250 RMSElog: 9.8416 grad_loss: 19487.9453 normal_loss: 0.9748
[epoch 19][iter  320] loss: 125695.5391 RMSElog: 9.9422 grad_loss: 12558.6836 normal_loss: 0.9278
[epoch 19][iter  330] loss: 170542.9531 RMSElog: 9.7855 grad_loss: 17043.5410 normal_loss: 0.9688
[epoch 19][iter  340] loss: 217914.6094 RMSElog: 10.2786 grad_loss: 21780.2246 normal_loss: 0.9575
[epoch 19][iter  350] loss: 201503.0625 RMSElog: 10.3149 grad_loss: 20139.0410 normal_loss: 0.9514
[epoch 19][iter  360] loss: 223018.5625 RMSElog: 10.3592 grad_loss: 22290.5508 normal_loss: 0.9462
[epoch 19][iter  370] loss: 105828.7812 RMSElog: 10.1032 grad_loss: 10571.8545 normal_loss: 0.9195
[epoch 19][iter  380] loss: 148057.2500 RMSElog: 9.9895 grad_loss: 14794.7803 normal_loss: 0.9555
[epoch 19][iter  390] loss: 135114.8750 RMSElog: 9.9265 grad_loss: 13500.6055 normal_loss: 0.9548
[epoch 19][iter  400] loss: 109148.7344 RMSElog: 10.1172 grad_loss: 10903.8262 normal_loss: 0.9294
[epoch 19][iter  410] loss: 214808.7344 RMSElog: 10.1872 grad_loss: 21469.7441 normal_loss: 0.9423
[epoch 19][iter  420] loss: 136533.4688 RMSElog: 9.6756 grad_loss: 13642.7363 normal_loss: 0.9359
[epoch 19][iter  430] loss: 175008.6562 RMSElog: 9.9496 grad_loss: 17489.9434 normal_loss: 0.9734
[epoch 19][iter  440] loss: 149540.3750 RMSElog: 9.7408 grad_loss: 14943.3740 normal_loss: 0.9229
[epoch 19][iter  450] loss: 166093.7344 RMSElog: 10.4480 grad_loss: 16597.9766 normal_loss: 0.9489
[epoch 19][iter  460] loss: 195777.0469 RMSElog: 10.1280 grad_loss: 19566.6035 normal_loss: 0.9726
[epoch 19][iter  470] loss: 198743.4219 RMSElog: 10.0842 grad_loss: 19863.2930 normal_loss: 0.9643
[epoch 19][iter  480] loss: 111518.2734 RMSElog: 9.6257 grad_loss: 11141.2676 normal_loss: 0.9335
[epoch 19][iter  490] loss: 141118.5312 RMSElog: 10.0520 grad_loss: 14100.8545 normal_loss: 0.9477
[epoch 19][iter  500] loss: 119655.7969 RMSElog: 9.6110 grad_loss: 11955.0312 normal_loss: 0.9378
[epoch 19][iter  510] loss: 108409.6484 RMSElog: 9.2105 grad_loss: 10830.8506 normal_loss: 0.9033
[epoch 19][iter  520] loss: 135814.7969 RMSElog: 9.5386 grad_loss: 13571.0137 normal_loss: 0.9281
[epoch 19][iter  530] loss: 181983.5312 RMSElog: 10.0741 grad_loss: 18187.3320 normal_loss: 0.9464
[epoch 19][iter  540] loss: 205235.7188 RMSElog: 10.2875 grad_loss: 20512.3281 normal_loss: 0.9564
[epoch 19][iter  550] loss: 115495.2188 RMSElog: 9.6084 grad_loss: 11538.9688 normal_loss: 0.9442
[epoch 19][iter  560] loss: 168688.8125 RMSElog: 9.8493 grad_loss: 16858.0781 normal_loss: 0.9524
[epoch 19][iter  570] loss: 128277.9766 RMSElog: 9.7565 grad_loss: 12817.0889 normal_loss: 0.9518
[epoch 19][iter  580] loss: 167412.9531 RMSElog: 9.7311 grad_loss: 16730.6387 normal_loss: 0.9260
[epoch 19][iter  590] loss: 178076.4688 RMSElog: 9.8778 grad_loss: 17796.8457 normal_loss: 0.9233
[epoch 20][iter    0] loss: 108800.2422 RMSElog: 9.7538 grad_loss: 10869.3369 normal_loss: 0.9334
[epoch 20][iter   10] loss: 195440.9062 RMSElog: 9.7403 grad_loss: 19533.3926 normal_loss: 0.9572
[epoch 20][iter   20] loss: 110579.5391 RMSElog: 9.3019 grad_loss: 11047.7109 normal_loss: 0.9413
[epoch 20][iter   30] loss: 160743.5156 RMSElog: 9.9654 grad_loss: 16063.4521 normal_loss: 0.9340
[epoch 20][iter   40] loss: 172115.2812 RMSElog: 9.7682 grad_loss: 17200.8242 normal_loss: 0.9358
[epoch 20][iter   50] loss: 124323.5234 RMSElog: 9.7543 grad_loss: 12421.6660 normal_loss: 0.9322
[epoch 20][iter   60] loss: 167730.9531 RMSElog: 9.9663 grad_loss: 16762.1797 normal_loss: 0.9501
[epoch 20][iter   70] loss: 213417.0312 RMSElog: 10.3416 grad_loss: 21330.4082 normal_loss: 0.9536
[epoch 20][iter   80] loss: 233395.7188 RMSElog: 10.2368 grad_loss: 23328.3906 normal_loss: 0.9452
[epoch 20][iter   90] loss: 87697.6250 RMSElog: 9.9487 grad_loss: 8758.8750 normal_loss: 0.9395
[epoch 20][iter  100] loss: 152332.5469 RMSElog: 9.9813 grad_loss: 15222.3486 normal_loss: 0.9245
[epoch 20][iter  110] loss: 164415.3438 RMSElog: 10.0829 grad_loss: 16430.4961 normal_loss: 0.9571
[epoch 20][iter  120] loss: 101101.5781 RMSElog: 9.9274 grad_loss: 10099.3008 normal_loss: 0.9297
[epoch 20][iter  130] loss: 175486.0625 RMSElog: 10.0393 grad_loss: 17537.6094 normal_loss: 0.9563
[epoch 20][iter  140] loss: 162542.0312 RMSElog: 9.9393 grad_loss: 16243.3105 normal_loss: 0.9527
[epoch 20][iter  150] loss: 143460.8594 RMSElog: 9.7520 grad_loss: 14335.3848 normal_loss: 0.9489
[epoch 20][iter  160] loss: 207919.9844 RMSElog: 10.2783 grad_loss: 20780.7637 normal_loss: 0.9578
[epoch 20][iter  170] loss: 170357.3281 RMSElog: 9.9687 grad_loss: 17024.7988 normal_loss: 0.9644
[epoch 20][iter  180] loss: 113895.6328 RMSElog: 9.2606 grad_loss: 11379.3926 normal_loss: 0.9106
[epoch 20][iter  190] loss: 119923.2031 RMSElog: 9.5699 grad_loss: 11981.8154 normal_loss: 0.9342
[epoch 20][iter  200] loss: 161813.2500 RMSElog: 9.9875 grad_loss: 16170.3857 normal_loss: 0.9508
[epoch 20][iter  210] loss: 100663.8984 RMSElog: 9.9598 grad_loss: 10055.5020 normal_loss: 0.9277
[epoch 20][iter  220] loss: 108409.6484 RMSElog: 9.2105 grad_loss: 10830.8506 normal_loss: 0.9033
[epoch 20][iter  230] loss: 218547.0156 RMSElog: 10.4419 grad_loss: 21843.3086 normal_loss: 0.9515
[epoch 20][iter  240] loss: 145332.8438 RMSElog: 10.0385 grad_loss: 14522.2842 normal_loss: 0.9619
[epoch 20][iter  250] loss: 229081.2812 RMSElog: 10.2778 grad_loss: 22896.9102 normal_loss: 0.9420
[epoch 20][iter  260] loss: 175299.8438 RMSElog: 10.1975 grad_loss: 17518.8301 normal_loss: 0.9576
[epoch 20][iter  270] loss: 211640.2344 RMSElog: 10.2414 grad_loss: 21152.8145 normal_loss: 0.9675
[epoch 20][iter  280] loss: 126821.0234 RMSElog: 10.1398 grad_loss: 12671.0410 normal_loss: 0.9215
[epoch 20][iter  290] loss: 144440.4531 RMSElog: 9.8843 grad_loss: 14433.2314 normal_loss: 0.9296
[epoch 20][iter  300] loss: 109441.5234 RMSElog: 9.7466 grad_loss: 10933.4756 normal_loss: 0.9300
[epoch 20][iter  310] loss: 139061.3906 RMSElog: 9.6201 grad_loss: 13895.5781 normal_loss: 0.9401
[epoch 20][iter  320] loss: 195777.0156 RMSElog: 10.1280 grad_loss: 19566.5996 normal_loss: 0.9726
[epoch 20][iter  330] loss: 190034.7188 RMSElog: 9.9619 grad_loss: 18992.5625 normal_loss: 0.9485
[epoch 20][iter  340] loss: 170542.9531 RMSElog: 9.7855 grad_loss: 17043.5410 normal_loss: 0.9688
[epoch 20][iter  350] loss: 105828.7812 RMSElog: 10.1032 grad_loss: 10571.8545 normal_loss: 0.9195
[epoch 20][iter  360] loss: 196277.7188 RMSElog: 10.2453 grad_loss: 19616.5664 normal_loss: 0.9595
[epoch 20][iter  370] loss: 143440.0000 RMSElog: 9.6404 grad_loss: 14333.4336 normal_loss: 0.9261
[epoch 20][iter  380] loss: 124563.3906 RMSElog: 9.9514 grad_loss: 12445.4473 normal_loss: 0.9400
[epoch 20][iter  390] loss: 167008.4531 RMSElog: 10.0165 grad_loss: 16689.8750 normal_loss: 0.9557
[epoch 20][iter  400] loss: 156982.7188 RMSElog: 10.0093 grad_loss: 15687.3193 normal_loss: 0.9438
[epoch 20][iter  410] loss: 118746.3359 RMSElog: 9.7278 grad_loss: 11863.9629 normal_loss: 0.9433
[epoch 20][iter  420] loss: 140938.8594 RMSElog: 9.7483 grad_loss: 14083.1953 normal_loss: 0.9424
[epoch 20][iter  430] loss: 115500.4766 RMSElog: 9.9924 grad_loss: 11539.1230 normal_loss: 0.9328
[epoch 20][iter  440] loss: 189017.5781 RMSElog: 10.1390 grad_loss: 18890.6602 normal_loss: 0.9589
[epoch 20][iter  450] loss: 183095.1562 RMSElog: 9.9762 grad_loss: 18298.5703 normal_loss: 0.9696
[epoch 20][iter  460] loss: 134406.1875 RMSElog: 9.9402 grad_loss: 13429.7246 normal_loss: 0.9539
[epoch 20][iter  470] loss: 214808.7500 RMSElog: 10.1872 grad_loss: 21469.7461 normal_loss: 0.9423
[epoch 20][iter  480] loss: 162579.5312 RMSElog: 10.1223 grad_loss: 16246.8740 normal_loss: 0.9574
[epoch 20][iter  490] loss: 182476.7344 RMSElog: 9.9672 grad_loss: 18236.7656 normal_loss: 0.9407
[epoch 20][iter  500] loss: 160051.0156 RMSElog: 9.8405 grad_loss: 15994.3105 normal_loss: 0.9506
[epoch 20][iter  510] loss: 201373.6719 RMSElog: 10.3870 grad_loss: 20126.0312 normal_loss: 0.9491
[epoch 20][iter  520] loss: 135042.2031 RMSElog: 9.6763 grad_loss: 13493.6074 normal_loss: 0.9366
[epoch 20][iter  530] loss: 103955.0000 RMSElog: 9.6256 grad_loss: 10384.9521 normal_loss: 0.9222
[epoch 20][iter  540] loss: 217174.7812 RMSElog: 10.2254 grad_loss: 21706.2910 normal_loss: 0.9635
[epoch 20][iter  550] loss: 138934.7188 RMSElog: 9.8450 grad_loss: 13882.6904 normal_loss: 0.9370
[epoch 20][iter  560] loss: 103509.2188 RMSElog: 9.9787 grad_loss: 10340.0186 normal_loss: 0.9251
[epoch 20][iter  570] loss: 113195.8125 RMSElog: 9.1761 grad_loss: 11309.5039 normal_loss: 0.9012
[epoch 20][iter  580] loss: 140645.6875 RMSElog: 10.1118 grad_loss: 14053.4873 normal_loss: 0.9695
[epoch 20][iter  590] loss: 119429.4609 RMSElog: 9.6388 grad_loss: 11932.3740 normal_loss: 0.9339
[epoch 21][iter    0] loss: 176914.4531 RMSElog: 10.3802 grad_loss: 17680.1094 normal_loss: 0.9546
[epoch 21][iter   10] loss: 152517.4844 RMSElog: 9.8233 grad_loss: 15241.0010 normal_loss: 0.9249
[epoch 21][iter   20] loss: 178593.9688 RMSElog: 10.0081 grad_loss: 17848.4395 normal_loss: 0.9490
[epoch 21][iter   30] loss: 160642.9375 RMSElog: 9.9306 grad_loss: 16053.4121 normal_loss: 0.9504
[epoch 21][iter   40] loss: 190938.1250 RMSElog: 10.3028 grad_loss: 19082.5566 normal_loss: 0.9541
[epoch 21][iter   50] loss: 179901.4844 RMSElog: 9.8267 grad_loss: 17979.3633 normal_loss: 0.9589
[epoch 21][iter   60] loss: 153594.7031 RMSElog: 9.8426 grad_loss: 15348.6826 normal_loss: 0.9453
[epoch 21][iter   70] loss: 119429.4609 RMSElog: 9.6388 grad_loss: 11932.3740 normal_loss: 0.9339
[epoch 21][iter   80] loss: 206764.1875 RMSElog: 10.2849 grad_loss: 20665.1777 normal_loss: 0.9549
[epoch 21][iter   90] loss: 102998.8984 RMSElog: 9.8140 grad_loss: 10289.1318 normal_loss: 0.9431
[epoch 21][iter  100] loss: 189543.6719 RMSElog: 10.1592 grad_loss: 18943.2598 normal_loss: 0.9474
[epoch 21][iter  110] loss: 152149.7500 RMSElog: 9.9062 grad_loss: 15204.1221 normal_loss: 0.9462
[epoch 21][iter  120] loss: 115738.1953 RMSElog: 9.6968 grad_loss: 11563.1836 normal_loss: 0.9382
[epoch 21][iter  130] loss: 150153.5000 RMSElog: 10.0554 grad_loss: 15004.3457 normal_loss: 0.9478
[epoch 21][iter  140] loss: 122670.8984 RMSElog: 9.4365 grad_loss: 12256.7207 normal_loss: 0.9331
[epoch 21][iter  150] loss: 154310.0938 RMSElog: 9.9148 grad_loss: 15420.1445 normal_loss: 0.9495
[epoch 21][iter  160] loss: 134406.2031 RMSElog: 9.9402 grad_loss: 13429.7256 normal_loss: 0.9539
[epoch 21][iter  170] loss: 223018.5625 RMSElog: 10.3592 grad_loss: 22290.5508 normal_loss: 0.9462
[epoch 21][iter  180] loss: 115348.2734 RMSElog: 9.7235 grad_loss: 11524.1562 normal_loss: 0.9470
[epoch 21][iter  190] loss: 105816.5000 RMSElog: 9.4266 grad_loss: 10571.3066 normal_loss: 0.9171
[epoch 21][iter  200] loss: 178468.8906 RMSElog: 9.9039 grad_loss: 17836.0469 normal_loss: 0.9380
[epoch 21][iter  210] loss: 198293.9219 RMSElog: 10.2275 grad_loss: 19818.2148 normal_loss: 0.9521
[epoch 21][iter  220] loss: 163268.1250 RMSElog: 9.9218 grad_loss: 16315.9277 normal_loss: 0.9630
[epoch 21][iter  230] loss: 142596.2344 RMSElog: 9.8922 grad_loss: 14248.7793 normal_loss: 0.9508
[epoch 21][iter  240] loss: 115658.3125 RMSElog: 9.1856 grad_loss: 11555.7031 normal_loss: 0.9424
[epoch 21][iter  250] loss: 165648.1875 RMSElog: 9.7420 grad_loss: 16554.1289 normal_loss: 0.9479
[epoch 21][iter  260] loss: 158034.0469 RMSElog: 9.9527 grad_loss: 15792.5078 normal_loss: 0.9445
[epoch 21][iter  270] loss: 144191.5938 RMSElog: 9.6650 grad_loss: 14408.5654 normal_loss: 0.9284
[epoch 21][iter  280] loss: 103305.2500 RMSElog: 9.5543 grad_loss: 10320.0684 normal_loss: 0.9024
[epoch 21][iter  290] loss: 180208.2188 RMSElog: 10.0490 grad_loss: 18009.8223 normal_loss: 0.9517
[epoch 21][iter  300] loss: 186053.2812 RMSElog: 10.1470 grad_loss: 18594.2305 normal_loss: 0.9504
[epoch 21][iter  310] loss: 152549.8750 RMSElog: 9.9928 grad_loss: 15244.0293 normal_loss: 0.9654
[epoch 21][iter  320] loss: 149369.9219 RMSElog: 10.0930 grad_loss: 14925.9766 normal_loss: 0.9225
[epoch 21][iter  330] loss: 195777.0469 RMSElog: 10.1280 grad_loss: 19566.6035 normal_loss: 0.9726
[epoch 21][iter  340] loss: 80097.4062 RMSElog: 9.2433 grad_loss: 7999.5806 normal_loss: 0.9166
[epoch 21][iter  350] loss: 218949.8281 RMSElog: 10.4462 grad_loss: 21883.5918 normal_loss: 0.9447
[epoch 21][iter  360] loss: 121838.3672 RMSElog: 9.7453 grad_loss: 12173.1494 normal_loss: 0.9425
[epoch 21][iter  370] loss: 176223.4531 RMSElog: 9.9872 grad_loss: 17611.4141 normal_loss: 0.9446
[epoch 21][iter  380] loss: 188502.1875 RMSElog: 10.1669 grad_loss: 18839.0801 normal_loss: 0.9722
[epoch 21][iter  390] loss: 146781.9531 RMSElog: 9.8688 grad_loss: 14667.3818 normal_loss: 0.9440
[epoch 21][iter  400] loss: 120590.1719 RMSElog: 9.9054 grad_loss: 12048.1719 normal_loss: 0.9408
[epoch 21][iter  410] loss: 227080.5938 RMSElog: 10.4127 grad_loss: 22696.6992 normal_loss: 0.9482
[epoch 21][iter  420] loss: 164843.6719 RMSElog: 9.7972 grad_loss: 16473.6348 normal_loss: 0.9360
[epoch 21][iter  430] loss: 158875.5625 RMSElog: 9.5853 grad_loss: 15877.0449 normal_loss: 0.9260
[epoch 21][iter  440] loss: 162711.9688 RMSElog: 9.6160 grad_loss: 16260.6387 normal_loss: 0.9427
[epoch 21][iter  450] loss: 167740.4688 RMSElog: 9.8540 grad_loss: 16763.2344 normal_loss: 0.9591
[epoch 21][iter  460] loss: 184374.2188 RMSElog: 10.2641 grad_loss: 18426.1992 normal_loss: 0.9596
[epoch 21][iter  470] loss: 232982.0469 RMSElog: 10.2845 grad_loss: 23286.9707 normal_loss: 0.9500
[epoch 21][iter  480] loss: 157706.0781 RMSElog: 9.9981 grad_loss: 15759.6602 normal_loss: 0.9499
[epoch 21][iter  490] loss: 155762.3438 RMSElog: 9.8487 grad_loss: 15565.4404 normal_loss: 0.9448
[epoch 21][iter  500] loss: 178076.4688 RMSElog: 9.8778 grad_loss: 17796.8457 normal_loss: 0.9233
[epoch 21][iter  510] loss: 139098.6406 RMSElog: 9.8619 grad_loss: 13899.0508 normal_loss: 0.9510
[epoch 21][iter  520] loss: 140282.6250 RMSElog: 9.9822 grad_loss: 14017.3145 normal_loss: 0.9650
[epoch 21][iter  530] loss: 173855.4375 RMSElog: 10.0769 grad_loss: 17374.5098 normal_loss: 0.9566
[epoch 21][iter  540] loss: 167084.9219 RMSElog: 9.8660 grad_loss: 16697.6582 normal_loss: 0.9689
[epoch 21][iter  550] loss: 214067.1875 RMSElog: 10.0580 grad_loss: 21395.7207 normal_loss: 0.9394
[epoch 21][iter  560] loss: 121983.8203 RMSElog: 9.5225 grad_loss: 12187.9473 normal_loss: 0.9124
[epoch 21][iter  570] loss: 185902.4219 RMSElog: 9.8998 grad_loss: 18579.3711 normal_loss: 0.9705
[epoch 21][iter  580] loss: 211925.8438 RMSElog: 10.1863 grad_loss: 21181.4551 normal_loss: 0.9436
[epoch 21][iter  590] loss: 131434.0312 RMSElog: 9.5911 grad_loss: 13132.8770 normal_loss: 0.9360
[epoch 22][iter    0] loss: 154310.0938 RMSElog: 9.9148 grad_loss: 15420.1445 normal_loss: 0.9495
[epoch 22][iter   10] loss: 189582.6719 RMSElog: 9.9918 grad_loss: 18947.3223 normal_loss: 0.9523
[epoch 22][iter   20] loss: 181293.3438 RMSElog: 10.0009 grad_loss: 18118.3809 normal_loss: 0.9531
[epoch 22][iter   30] loss: 106182.2031 RMSElog: 9.7575 grad_loss: 10607.5332 normal_loss: 0.9294
[epoch 22][iter   40] loss: 241971.8750 RMSElog: 10.2390 grad_loss: 24186.0000 normal_loss: 0.9488
[epoch 22][iter   50] loss: 190034.7188 RMSElog: 9.9619 grad_loss: 18992.5625 normal_loss: 0.9485
[epoch 22][iter   60] loss: 162542.0469 RMSElog: 9.9393 grad_loss: 16243.3115 normal_loss: 0.9527
[epoch 22][iter   70] loss: 200470.3906 RMSElog: 10.2414 grad_loss: 20035.8379 normal_loss: 0.9585
[epoch 22][iter   80] loss: 136796.3438 RMSElog: 9.6524 grad_loss: 13669.0273 normal_loss: 0.9549
[epoch 22][iter   90] loss: 128979.7188 RMSElog: 9.7049 grad_loss: 12887.3340 normal_loss: 0.9327
[epoch 22][iter  100] loss: 157810.1406 RMSElog: 9.9254 grad_loss: 15770.1621 normal_loss: 0.9253
[epoch 22][iter  110] loss: 104127.8047 RMSElog: 9.6242 grad_loss: 10402.2324 normal_loss: 0.9237
[epoch 22][iter  120] loss: 160595.2188 RMSElog: 9.9514 grad_loss: 16048.6191 normal_loss: 0.9518
[epoch 22][iter  130] loss: 110874.4688 RMSElog: 9.2829 grad_loss: 11077.2607 normal_loss: 0.9034
[epoch 22][iter  140] loss: 119895.5078 RMSElog: 9.3187 grad_loss: 11979.3164 normal_loss: 0.9157
[epoch 22][iter  150] loss: 224077.6719 RMSElog: 10.3869 grad_loss: 22396.4219 normal_loss: 0.9584
[epoch 22][iter  160] loss: 167345.2500 RMSElog: 9.8843 grad_loss: 16723.6777 normal_loss: 0.9620
[epoch 22][iter  170] loss: 146465.6875 RMSElog: 10.2283 grad_loss: 14635.4062 normal_loss: 0.9337
[epoch 22][iter  180] loss: 190972.3281 RMSElog: 10.1152 grad_loss: 19086.1523 normal_loss: 0.9647
[epoch 22][iter  190] loss: 177814.3125 RMSElog: 10.0854 grad_loss: 17770.3887 normal_loss: 0.9571
[epoch 22][iter  200] loss: 125829.2891 RMSElog: 9.7844 grad_loss: 12572.2148 normal_loss: 0.9297
[epoch 22][iter  210] loss: 133024.8594 RMSElog: 9.6937 grad_loss: 13291.8662 normal_loss: 0.9261
[epoch 22][iter  220] loss: 205362.7188 RMSElog: 10.1686 grad_loss: 20525.1562 normal_loss: 0.9475
[epoch 22][iter  230] loss: 162835.5469 RMSElog: 9.9145 grad_loss: 16272.6914 normal_loss: 0.9492
[epoch 22][iter  240] loss: 142138.5156 RMSElog: 9.7967 grad_loss: 14203.1260 normal_loss: 0.9291
[epoch 22][iter  250] loss: 160743.5156 RMSElog: 9.9654 grad_loss: 16063.4521 normal_loss: 0.9340
[epoch 22][iter  260] loss: 162892.2969 RMSElog: 9.8252 grad_loss: 16278.4717 normal_loss: 0.9331
[epoch 22][iter  270] loss: 104086.5234 RMSElog: 9.7787 grad_loss: 10397.9453 normal_loss: 0.9289
[epoch 22][iter  280] loss: 185902.4219 RMSElog: 9.8998 grad_loss: 18579.3711 normal_loss: 0.9705
[epoch 22][iter  290] loss: 165648.1875 RMSElog: 9.7420 grad_loss: 16554.1289 normal_loss: 0.9479
[epoch 22][iter  300] loss: 96006.2500 RMSElog: 9.5560 grad_loss: 9590.1396 normal_loss: 0.9294
[epoch 22][iter  310] loss: 195440.9062 RMSElog: 9.7403 grad_loss: 19533.3926 normal_loss: 0.9572
[epoch 22][iter  320] loss: 105628.3594 RMSElog: 10.0899 grad_loss: 10551.8252 normal_loss: 0.9213
[epoch 22][iter  330] loss: 155968.4062 RMSElog: 9.9200 grad_loss: 15585.9980 normal_loss: 0.9232
[epoch 22][iter  340] loss: 204037.7500 RMSElog: 10.1754 grad_loss: 20392.6582 normal_loss: 0.9420
[epoch 22][iter  350] loss: 139878.0781 RMSElog: 9.8515 grad_loss: 13977.0234 normal_loss: 0.9327
[epoch 22][iter  360] loss: 211925.8438 RMSElog: 10.1863 grad_loss: 21181.4551 normal_loss: 0.9436
[epoch 22][iter  370] loss: 160879.7812 RMSElog: 9.7521 grad_loss: 16077.2754 normal_loss: 0.9513
[epoch 22][iter  380] loss: 196842.3281 RMSElog: 10.1249 grad_loss: 19673.1543 normal_loss: 0.9534
[epoch 22][iter  390] loss: 191559.9688 RMSElog: 9.9749 grad_loss: 19145.0742 normal_loss: 0.9477
[epoch 22][iter  400] loss: 166093.7344 RMSElog: 10.4480 grad_loss: 16597.9766 normal_loss: 0.9489
[epoch 22][iter  410] loss: 208699.7031 RMSElog: 10.3419 grad_loss: 20858.6719 normal_loss: 0.9568
[epoch 22][iter  420] loss: 126874.0625 RMSElog: 9.7526 grad_loss: 12676.7002 normal_loss: 0.9530
[epoch 22][iter  430] loss: 176217.8125 RMSElog: 10.1087 grad_loss: 17610.7246 normal_loss: 0.9466
[epoch 22][iter  440] loss: 152149.7500 RMSElog: 9.9062 grad_loss: 15204.1221 normal_loss: 0.9462
[epoch 22][iter  450] loss: 149540.3750 RMSElog: 9.7408 grad_loss: 14943.3740 normal_loss: 0.9229
[epoch 22][iter  460] loss: 233395.7188 RMSElog: 10.2368 grad_loss: 23328.3906 normal_loss: 0.9452
[epoch 22][iter  470] loss: 137249.3438 RMSElog: 9.7342 grad_loss: 13714.2764 normal_loss: 0.9239
[epoch 22][iter  480] loss: 163171.1719 RMSElog: 9.7905 grad_loss: 16306.3613 normal_loss: 0.9652
[epoch 22][iter  490] loss: 218949.8281 RMSElog: 10.4462 grad_loss: 21883.5918 normal_loss: 0.9447
[epoch 22][iter  500] loss: 142596.2344 RMSElog: 9.8922 grad_loss: 14248.7793 normal_loss: 0.9508
[epoch 22][iter  510] loss: 162522.4531 RMSElog: 9.9860 grad_loss: 16241.3066 normal_loss: 0.9518
[epoch 22][iter  520] loss: 122446.5469 RMSElog: 9.6438 grad_loss: 12234.0742 normal_loss: 0.9366
[epoch 22][iter  530] loss: 150153.5000 RMSElog: 10.0554 grad_loss: 15004.3457 normal_loss: 0.9478
[epoch 22][iter  540] loss: 111518.2734 RMSElog: 9.6257 grad_loss: 11141.2676 normal_loss: 0.9335
[epoch 22][iter  550] loss: 175122.9531 RMSElog: 9.9588 grad_loss: 17501.3789 normal_loss: 0.9573
[epoch 22][iter  560] loss: 102096.6484 RMSElog: 9.8437 grad_loss: 10198.8926 normal_loss: 0.9290
[epoch 22][iter  570] loss: 133760.0469 RMSElog: 10.0653 grad_loss: 13364.9717 normal_loss: 0.9680
[epoch 22][iter  580] loss: 103514.7734 RMSElog: 9.3451 grad_loss: 10341.2100 normal_loss: 0.9229
[epoch 22][iter  590] loss: 201162.3750 RMSElog: 9.9531 grad_loss: 20105.3145 normal_loss: 0.9706
[epoch 23][iter    0] loss: 176914.4531 RMSElog: 10.3802 grad_loss: 17680.1094 normal_loss: 0.9546
[epoch 23][iter   10] loss: 147765.9219 RMSElog: 10.0607 grad_loss: 14765.6104 normal_loss: 0.9219
[epoch 23][iter   20] loss: 154609.8281 RMSElog: 9.8745 grad_loss: 15450.1855 normal_loss: 0.9220
[epoch 23][iter   30] loss: 163252.6875 RMSElog: 9.8236 grad_loss: 16314.5117 normal_loss: 0.9348
[epoch 23][iter   40] loss: 157706.0781 RMSElog: 9.9981 grad_loss: 15759.6602 normal_loss: 0.9499
[epoch 23][iter   50] loss: 148057.2500 RMSElog: 9.9895 grad_loss: 14794.7803 normal_loss: 0.9555
[epoch 23][iter   60] loss: 145321.5469 RMSElog: 9.7332 grad_loss: 14521.4590 normal_loss: 0.9632
[epoch 23][iter   70] loss: 114310.3047 RMSElog: 9.4026 grad_loss: 11420.7217 normal_loss: 0.9063
[epoch 23][iter   80] loss: 160825.5938 RMSElog: 9.9290 grad_loss: 16071.6816 normal_loss: 0.9491
[epoch 23][iter   90] loss: 114893.3984 RMSElog: 9.2562 grad_loss: 11479.1777 normal_loss: 0.9067
[epoch 23][iter  100] loss: 172115.2812 RMSElog: 9.7682 grad_loss: 17200.8242 normal_loss: 0.9358
[epoch 23][iter  110] loss: 142544.7031 RMSElog: 10.2185 grad_loss: 14243.3027 normal_loss: 0.9486
[epoch 23][iter  120] loss: 112715.8125 RMSElog: 9.6867 grad_loss: 11260.9570 normal_loss: 0.9375
[epoch 23][iter  130] loss: 196920.5938 RMSElog: 10.2474 grad_loss: 19680.8340 normal_loss: 0.9760
[epoch 23][iter  140] loss: 155968.4062 RMSElog: 9.9200 grad_loss: 15585.9980 normal_loss: 0.9232
[epoch 23][iter  150] loss: 126821.0234 RMSElog: 10.1398 grad_loss: 12671.0410 normal_loss: 0.9215
[epoch 23][iter  160] loss: 160743.5156 RMSElog: 9.9654 grad_loss: 16063.4521 normal_loss: 0.9340
[epoch 23][iter  170] loss: 110808.0469 RMSElog: 9.7171 grad_loss: 11070.1562 normal_loss: 0.9312
[epoch 23][iter  180] loss: 145409.6719 RMSElog: 9.8330 grad_loss: 14530.1904 normal_loss: 0.9445
[epoch 23][iter  190] loss: 168688.7812 RMSElog: 9.8493 grad_loss: 16858.0762 normal_loss: 0.9524
[epoch 23][iter  200] loss: 204037.7500 RMSElog: 10.1754 grad_loss: 20392.6582 normal_loss: 0.9420
[epoch 23][iter  210] loss: 100116.4922 RMSElog: 9.9235 grad_loss: 10000.7998 normal_loss: 0.9258
[epoch 23][iter  220] loss: 194408.7812 RMSElog: 10.2924 grad_loss: 19429.6328 normal_loss: 0.9537
[epoch 23][iter  230] loss: 220695.7969 RMSElog: 10.2208 grad_loss: 22058.4082 normal_loss: 0.9520
[epoch 23][iter  240] loss: 221089.3594 RMSElog: 10.3457 grad_loss: 22097.6465 normal_loss: 0.9439
[epoch 23][iter  250] loss: 228053.0312 RMSElog: 10.2288 grad_loss: 22794.1328 normal_loss: 0.9422
[epoch 23][iter  260] loss: 154776.6250 RMSElog: 9.8981 grad_loss: 15466.8438 normal_loss: 0.9209
[epoch 23][iter  270] loss: 211640.2344 RMSElog: 10.2414 grad_loss: 21152.8145 normal_loss: 0.9675
[epoch 23][iter  280] loss: 180425.2344 RMSElog: 10.0354 grad_loss: 18031.5332 normal_loss: 0.9552
[epoch 23][iter  290] loss: 117693.8984 RMSElog: 9.6527 grad_loss: 11758.8105 normal_loss: 0.9267
[epoch 23][iter  300] loss: 133024.8594 RMSElog: 9.6937 grad_loss: 13291.8662 normal_loss: 0.9261
[epoch 23][iter  310] loss: 144440.4531 RMSElog: 9.8843 grad_loss: 14433.2314 normal_loss: 0.9296
[epoch 23][iter  320] loss: 174761.2500 RMSElog: 9.7333 grad_loss: 17465.4531 normal_loss: 0.9393
[epoch 23][iter  330] loss: 114924.9844 RMSElog: 9.6925 grad_loss: 11481.8691 normal_loss: 0.9360
[epoch 23][iter  340] loss: 104456.8438 RMSElog: 10.0170 grad_loss: 10434.7461 normal_loss: 0.9214
[epoch 23][iter  350] loss: 114149.3750 RMSElog: 9.2655 grad_loss: 11404.7646 normal_loss: 0.9073
[epoch 23][iter  360] loss: 147915.7812 RMSElog: 10.0131 grad_loss: 14780.6104 normal_loss: 0.9547
[epoch 23][iter  370] loss: 146576.9219 RMSElog: 9.6520 grad_loss: 14647.1045 normal_loss: 0.9356
[epoch 23][iter  380] loss: 206764.1875 RMSElog: 10.2849 grad_loss: 20665.1777 normal_loss: 0.9549
[epoch 23][iter  390] loss: 123289.4609 RMSElog: 9.4106 grad_loss: 12318.5752 normal_loss: 0.9606
[epoch 23][iter  400] loss: 149128.2188 RMSElog: 9.7262 grad_loss: 14902.1533 normal_loss: 0.9419
[epoch 23][iter  410] loss: 201503.0625 RMSElog: 10.3149 grad_loss: 20139.0410 normal_loss: 0.9514
[epoch 23][iter  420] loss: 175002.7344 RMSElog: 9.8652 grad_loss: 17489.4648 normal_loss: 0.9436
[epoch 23][iter  430] loss: 154855.9062 RMSElog: 9.8054 grad_loss: 15474.8340 normal_loss: 0.9510
[epoch 23][iter  440] loss: 147207.9844 RMSElog: 9.9779 grad_loss: 14709.8789 normal_loss: 0.9413
[epoch 23][iter  450] loss: 116582.6172 RMSElog: 9.7871 grad_loss: 11647.5430 normal_loss: 0.9314
[epoch 23][iter  460] loss: 175934.2344 RMSElog: 10.1481 grad_loss: 17582.3086 normal_loss: 0.9674
[epoch 23][iter  470] loss: 130527.6094 RMSElog: 9.8914 grad_loss: 13041.9180 normal_loss: 0.9509
[epoch 23][iter  480] loss: 160879.7812 RMSElog: 9.7521 grad_loss: 16077.2754 normal_loss: 0.9513
[epoch 23][iter  490] loss: 152540.7500 RMSElog: 9.9175 grad_loss: 15243.2090 normal_loss: 0.9474
[epoch 23][iter  500] loss: 149369.9062 RMSElog: 10.0930 grad_loss: 14925.9756 normal_loss: 0.9225
[epoch 23][iter  510] loss: 115500.4766 RMSElog: 9.9924 grad_loss: 11539.1230 normal_loss: 0.9328
[epoch 23][iter  520] loss: 120590.1719 RMSElog: 9.9054 grad_loss: 12048.1719 normal_loss: 0.9408
[epoch 23][iter  530] loss: 224943.8906 RMSElog: 10.1206 grad_loss: 22483.3125 normal_loss: 0.9551
[epoch 23][iter  540] loss: 100749.6875 RMSElog: 9.9914 grad_loss: 10064.0400 normal_loss: 0.9377
[epoch 23][iter  550] loss: 198036.3750 RMSElog: 10.2619 grad_loss: 19792.4219 normal_loss: 0.9529
[epoch 23][iter  560] loss: 173339.3750 RMSElog: 9.8230 grad_loss: 17323.1934 normal_loss: 0.9212
[epoch 23][iter  570] loss: 195671.7031 RMSElog: 10.1080 grad_loss: 19556.1133 normal_loss: 0.9494
[epoch 23][iter  580] loss: 196025.7031 RMSElog: 10.2650 grad_loss: 19591.3496 normal_loss: 0.9559
[epoch 23][iter  590] loss: 162976.7812 RMSElog: 9.6016 grad_loss: 16287.1572 normal_loss: 0.9201
[epoch 24][iter    0] loss: 100990.1562 RMSElog: 9.7350 grad_loss: 10088.3584 normal_loss: 0.9222
[epoch 24][iter   10] loss: 146444.9062 RMSElog: 10.1139 grad_loss: 14633.4365 normal_loss: 0.9403
[epoch 24][iter   20] loss: 138469.0469 RMSElog: 9.4787 grad_loss: 13836.4834 normal_loss: 0.9438
[epoch 24][iter   30] loss: 160788.1562 RMSElog: 9.7539 grad_loss: 16068.1299 normal_loss: 0.9318
[epoch 24][iter   40] loss: 131942.8750 RMSElog: 9.8414 grad_loss: 13183.5000 normal_loss: 0.9453
[epoch 24][iter   50] loss: 181942.5625 RMSElog: 10.0291 grad_loss: 18183.2793 normal_loss: 0.9464
[epoch 24][iter   60] loss: 165306.9531 RMSElog: 9.8881 grad_loss: 16519.8438 normal_loss: 0.9638
[epoch 24][iter   70] loss: 152549.8750 RMSElog: 9.9928 grad_loss: 15244.0293 normal_loss: 0.9654
[epoch 24][iter   80] loss: 155968.4062 RMSElog: 9.9200 grad_loss: 15585.9980 normal_loss: 0.9232
[epoch 24][iter   90] loss: 169013.5625 RMSElog: 9.5880 grad_loss: 16890.8262 normal_loss: 0.9419
[epoch 24][iter  100] loss: 166207.1094 RMSElog: 9.7947 grad_loss: 16609.9668 normal_loss: 0.9492
[epoch 24][iter  110] loss: 196842.3281 RMSElog: 10.1249 grad_loss: 19673.1543 normal_loss: 0.9534
[epoch 24][iter  120] loss: 165648.1875 RMSElog: 9.7420 grad_loss: 16554.1289 normal_loss: 0.9479
[epoch 24][iter  130] loss: 221089.3594 RMSElog: 10.3457 grad_loss: 22097.6465 normal_loss: 0.9439
[epoch 24][iter  140] loss: 198743.4219 RMSElog: 10.0842 grad_loss: 19863.2930 normal_loss: 0.9643
[epoch 24][iter  150] loss: 224943.8906 RMSElog: 10.1206 grad_loss: 22483.3125 normal_loss: 0.9551
[epoch 24][iter  160] loss: 173721.0312 RMSElog: 10.0355 grad_loss: 17361.1211 normal_loss: 0.9468
[epoch 24][iter  170] loss: 179649.7188 RMSElog: 10.2020 grad_loss: 17953.8125 normal_loss: 0.9593
[epoch 24][iter  180] loss: 153723.9844 RMSElog: 9.9609 grad_loss: 15361.4902 normal_loss: 0.9475
[epoch 24][iter  190] loss: 170542.9531 RMSElog: 9.7855 grad_loss: 17043.5410 normal_loss: 0.9688
[epoch 24][iter  200] loss: 211925.8438 RMSElog: 10.1863 grad_loss: 21181.4551 normal_loss: 0.9436
[epoch 24][iter  210] loss: 102096.6484 RMSElog: 9.8437 grad_loss: 10198.8926 normal_loss: 0.9290
[epoch 24][iter  220] loss: 176914.4531 RMSElog: 10.3802 grad_loss: 17680.1094 normal_loss: 0.9546
[epoch 24][iter  230] loss: 110579.5391 RMSElog: 9.3019 grad_loss: 11047.7109 normal_loss: 0.9413
[epoch 24][iter  240] loss: 167730.9531 RMSElog: 9.9663 grad_loss: 16762.1797 normal_loss: 0.9501
[epoch 24][iter  250] loss: 175359.1875 RMSElog: 10.1476 grad_loss: 17524.8125 normal_loss: 0.9571
[epoch 24][iter  260] loss: 218625.8750 RMSElog: 10.2839 grad_loss: 21851.3457 normal_loss: 0.9592
[epoch 24][iter  270] loss: 194408.7812 RMSElog: 10.2924 grad_loss: 19429.6328 normal_loss: 0.9537
[epoch 24][iter  280] loss: 203093.2969 RMSElog: 10.1238 grad_loss: 20298.2598 normal_loss: 0.9478
[epoch 24][iter  290] loss: 146465.6875 RMSElog: 10.2283 grad_loss: 14635.4062 normal_loss: 0.9337
[epoch 24][iter  300] loss: 104749.2812 RMSElog: 9.7512 grad_loss: 10464.2490 normal_loss: 0.9281
[epoch 24][iter  310] loss: 166010.9531 RMSElog: 9.9249 grad_loss: 16590.2227 normal_loss: 0.9481
[epoch 24][iter  320] loss: 161813.2500 RMSElog: 9.9875 grad_loss: 16170.3857 normal_loss: 0.9508
[epoch 24][iter  330] loss: 214493.2188 RMSElog: 10.3525 grad_loss: 21438.0078 normal_loss: 0.9611
[epoch 24][iter  340] loss: 182315.9688 RMSElog: 10.1291 grad_loss: 18220.5039 normal_loss: 0.9652
[epoch 24][iter  350] loss: 201373.6875 RMSElog: 10.3870 grad_loss: 20126.0332 normal_loss: 0.9491
[epoch 24][iter  360] loss: 127061.7500 RMSElog: 9.6726 grad_loss: 12695.5654 normal_loss: 0.9369
[epoch 24][iter  370] loss: 214067.1875 RMSElog: 10.0580 grad_loss: 21395.7207 normal_loss: 0.9394
[epoch 24][iter  380] loss: 160595.2188 RMSElog: 9.9514 grad_loss: 16048.6191 normal_loss: 0.9518
[epoch 24][iter  390] loss: 168545.0938 RMSElog: 9.6168 grad_loss: 16843.9434 normal_loss: 0.9495
[epoch 24][iter  400] loss: 163268.1250 RMSElog: 9.9218 grad_loss: 16315.9277 normal_loss: 0.9630
[epoch 24][iter  410] loss: 139830.9062 RMSElog: 9.7552 grad_loss: 13972.3770 normal_loss: 0.9585
[epoch 24][iter  420] loss: 106516.8047 RMSElog: 9.6966 grad_loss: 10641.0557 normal_loss: 0.9285
[epoch 24][iter  430] loss: 121983.8203 RMSElog: 9.5225 grad_loss: 12187.9473 normal_loss: 0.9124
[epoch 24][iter  440] loss: 137249.3438 RMSElog: 9.7342 grad_loss: 13714.2764 normal_loss: 0.9239
[epoch 24][iter  450] loss: 120745.8203 RMSElog: 9.7706 grad_loss: 12063.8691 normal_loss: 0.9428
[epoch 24][iter  460] loss: 154855.9062 RMSElog: 9.8054 grad_loss: 15474.8340 normal_loss: 0.9510
[epoch 24][iter  470] loss: 145389.9375 RMSElog: 9.8784 grad_loss: 14528.1660 normal_loss: 0.9489
[epoch 24][iter  480] loss: 141118.5312 RMSElog: 10.0520 grad_loss: 14100.8545 normal_loss: 0.9477
[epoch 24][iter  490] loss: 143440.0000 RMSElog: 9.6404 grad_loss: 14333.4336 normal_loss: 0.9261
[epoch 24][iter  500] loss: 199607.1562 RMSElog: 10.1504 grad_loss: 19949.6133 normal_loss: 0.9517
[epoch 24][iter  510] loss: 150323.7656 RMSElog: 10.0510 grad_loss: 15021.3848 normal_loss: 0.9410
[epoch 24][iter  520] loss: 128277.9766 RMSElog: 9.7565 grad_loss: 12817.0889 normal_loss: 0.9518
[epoch 24][iter  530] loss: 196677.7812 RMSElog: 10.0486 grad_loss: 19656.7715 normal_loss: 0.9571
[epoch 24][iter  540] loss: 162892.2969 RMSElog: 9.8252 grad_loss: 16278.4717 normal_loss: 0.9331
[epoch 24][iter  550] loss: 172248.5156 RMSElog: 9.5166 grad_loss: 17214.3730 normal_loss: 0.9619
[epoch 24][iter  560] loss: 231973.6719 RMSElog: 10.2099 grad_loss: 23186.2090 normal_loss: 0.9490
[epoch 24][iter  570] loss: 152517.4844 RMSElog: 9.8233 grad_loss: 15241.0010 normal_loss: 0.9249
[epoch 24][iter  580] loss: 106195.4219 RMSElog: 9.2531 grad_loss: 10609.3711 normal_loss: 0.9179
[epoch 24][iter  590] loss: 136507.7344 RMSElog: 10.0223 grad_loss: 13639.7900 normal_loss: 0.9608
[epoch 25][iter    0] loss: 126821.0234 RMSElog: 10.1398 grad_loss: 12671.0410 normal_loss: 0.9215
[epoch 25][iter   10] loss: 175359.1875 RMSElog: 10.1476 grad_loss: 17524.8125 normal_loss: 0.9571
[epoch 25][iter   20] loss: 105742.0391 RMSElog: 9.7582 grad_loss: 10563.5234 normal_loss: 0.9230
[epoch 25][iter   30] loss: 133464.0312 RMSElog: 9.6242 grad_loss: 13335.8584 normal_loss: 0.9201
[epoch 25][iter   40] loss: 172115.2812 RMSElog: 9.7682 grad_loss: 17200.8242 normal_loss: 0.9358
[epoch 25][iter   50] loss: 103955.0000 RMSElog: 9.6256 grad_loss: 10384.9521 normal_loss: 0.9222
[epoch 25][iter   60] loss: 180425.2344 RMSElog: 10.0354 grad_loss: 18031.5332 normal_loss: 0.9552
[epoch 25][iter   70] loss: 133760.0312 RMSElog: 10.0653 grad_loss: 13364.9707 normal_loss: 0.9680
[epoch 25][iter   80] loss: 140333.8125 RMSElog: 9.8261 grad_loss: 14022.6191 normal_loss: 0.9368
[epoch 25][iter   90] loss: 154776.6250 RMSElog: 9.8981 grad_loss: 15466.8438 normal_loss: 0.9209
[epoch 25][iter  100] loss: 157274.4688 RMSElog: 9.9277 grad_loss: 15716.5723 normal_loss: 0.9461
[epoch 25][iter  110] loss: 121838.3672 RMSElog: 9.7453 grad_loss: 12173.1494 normal_loss: 0.9425
[epoch 25][iter  120] loss: 169013.5312 RMSElog: 9.5880 grad_loss: 16890.8242 normal_loss: 0.9419
[epoch 25][iter  130] loss: 198036.3750 RMSElog: 10.2619 grad_loss: 19792.4219 normal_loss: 0.9529
[epoch 25][iter  140] loss: 167211.6250 RMSElog: 10.0285 grad_loss: 16710.1777 normal_loss: 0.9542
[epoch 25][iter  150] loss: 144191.5938 RMSElog: 9.6650 grad_loss: 14408.5654 normal_loss: 0.9284
[epoch 25][iter  160] loss: 164597.1094 RMSElog: 9.8631 grad_loss: 16448.8984 normal_loss: 0.9502
[epoch 25][iter  170] loss: 153997.7188 RMSElog: 9.7620 grad_loss: 15389.0713 normal_loss: 0.9383
[epoch 25][iter  180] loss: 155911.2344 RMSElog: 9.7863 grad_loss: 15580.3965 normal_loss: 0.9403
[epoch 25][iter  190] loss: 105816.5000 RMSElog: 9.4266 grad_loss: 10571.3066 normal_loss: 0.9171
[epoch 25][iter  200] loss: 191559.9688 RMSElog: 9.9749 grad_loss: 19145.0742 normal_loss: 0.9477
[epoch 25][iter  210] loss: 125281.0156 RMSElog: 9.5401 grad_loss: 12517.6484 normal_loss: 0.9128
[epoch 25][iter  220] loss: 214808.7500 RMSElog: 10.1872 grad_loss: 21469.7461 normal_loss: 0.9423
[epoch 25][iter  230] loss: 105828.7812 RMSElog: 10.1032 grad_loss: 10571.8545 normal_loss: 0.9195
[epoch 25][iter  240] loss: 115452.5781 RMSElog: 9.2301 grad_loss: 11535.1230 normal_loss: 0.9042
[epoch 25][iter  250] loss: 232158.2188 RMSElog: 10.1898 grad_loss: 23204.6875 normal_loss: 0.9455
[epoch 25][iter  260] loss: 228053.0312 RMSElog: 10.2288 grad_loss: 22794.1328 normal_loss: 0.9422
[epoch 25][iter  270] loss: 128319.4062 RMSElog: 9.6733 grad_loss: 12821.3389 normal_loss: 0.9287
[epoch 25][iter  280] loss: 164181.7344 RMSElog: 10.0104 grad_loss: 16407.1973 normal_loss: 0.9660
[epoch 25][iter  290] loss: 104086.5234 RMSElog: 9.7787 grad_loss: 10397.9453 normal_loss: 0.9289
[epoch 25][iter  300] loss: 150153.5000 RMSElog: 10.0554 grad_loss: 15004.3457 normal_loss: 0.9478
[epoch 25][iter  310] loss: 154855.9062 RMSElog: 9.8054 grad_loss: 15474.8340 normal_loss: 0.9510
[epoch 25][iter  320] loss: 152772.4219 RMSElog: 9.8249 grad_loss: 15266.4736 normal_loss: 0.9438
[epoch 25][iter  330] loss: 151723.8750 RMSElog: 9.9680 grad_loss: 15161.4746 normal_loss: 0.9449
[epoch 25][iter  340] loss: 122386.0234 RMSElog: 9.6688 grad_loss: 12227.9971 normal_loss: 0.9366
[epoch 25][iter  350] loss: 150323.7656 RMSElog: 10.0510 grad_loss: 15021.3848 normal_loss: 0.9410
[epoch 25][iter  360] loss: 104446.5625 RMSElog: 10.1932 grad_loss: 10433.5342 normal_loss: 0.9288
[epoch 25][iter  370] loss: 191825.0781 RMSElog: 10.3012 grad_loss: 19171.2500 normal_loss: 0.9570
[epoch 25][iter  380] loss: 103355.8125 RMSElog: 9.7106 grad_loss: 10324.9453 normal_loss: 0.9244
[epoch 25][iter  390] loss: 155762.3438 RMSElog: 9.8487 grad_loss: 15565.4404 normal_loss: 0.9448
[epoch 25][iter  400] loss: 121916.0000 RMSElog: 9.7464 grad_loss: 12180.9131 normal_loss: 0.9404
[epoch 25][iter  410] loss: 141442.1250 RMSElog: 9.6437 grad_loss: 14133.6191 normal_loss: 0.9495
[epoch 25][iter  420] loss: 174565.0781 RMSElog: 9.7759 grad_loss: 17445.7832 normal_loss: 0.9484
[epoch 25][iter  430] loss: 135042.2031 RMSElog: 9.6763 grad_loss: 13493.6074 normal_loss: 0.9366
[epoch 25][iter  440] loss: 156709.9531 RMSElog: 9.8741 grad_loss: 15660.1758 normal_loss: 0.9453
[epoch 25][iter  450] loss: 158630.9688 RMSElog: 9.8321 grad_loss: 15852.3232 normal_loss: 0.9428
[epoch 25][iter  460] loss: 134677.6719 RMSElog: 9.8776 grad_loss: 13456.9443 normal_loss: 0.9441
[epoch 25][iter  470] loss: 119655.7969 RMSElog: 9.6110 grad_loss: 11955.0312 normal_loss: 0.9378
[epoch 25][iter  480] loss: 162579.5312 RMSElog: 10.1223 grad_loss: 16246.8740 normal_loss: 0.9574
[epoch 25][iter  490] loss: 211640.2344 RMSElog: 10.2414 grad_loss: 21152.8145 normal_loss: 0.9675
[epoch 25][iter  500] loss: 154626.7656 RMSElog: 9.9098 grad_loss: 15451.8066 normal_loss: 0.9596
[epoch 25][iter  510] loss: 166207.1094 RMSElog: 9.7947 grad_loss: 16609.9668 normal_loss: 0.9492
[epoch 25][iter  520] loss: 176223.4531 RMSElog: 9.9872 grad_loss: 17611.4141 normal_loss: 0.9446
[epoch 25][iter  530] loss: 152149.7500 RMSElog: 9.9062 grad_loss: 15204.1230 normal_loss: 0.9462
[epoch 25][iter  540] loss: 226949.5156 RMSElog: 10.2029 grad_loss: 22683.8047 normal_loss: 0.9430
[epoch 25][iter  550] loss: 143460.8594 RMSElog: 9.7520 grad_loss: 14335.3848 normal_loss: 0.9489
[epoch 25][iter  560] loss: 160934.5156 RMSElog: 9.8035 grad_loss: 16082.6846 normal_loss: 0.9629
[epoch 25][iter  570] loss: 167008.4531 RMSElog: 10.0165 grad_loss: 16689.8750 normal_loss: 0.9557
[epoch 25][iter  580] loss: 103798.3281 RMSElog: 9.6193 grad_loss: 10369.2812 normal_loss: 0.9331
[epoch 25][iter  590] loss: 179298.9062 RMSElog: 9.7220 grad_loss: 17919.2227 normal_loss: 0.9453
[epoch 26][iter    0] loss: 227080.5938 RMSElog: 10.4127 grad_loss: 22696.6992 normal_loss: 0.9482
[epoch 26][iter   10] loss: 110874.4688 RMSElog: 9.2829 grad_loss: 11077.2607 normal_loss: 0.9034
[epoch 26][iter   20] loss: 158534.6406 RMSElog: 9.6735 grad_loss: 15842.8672 normal_loss: 0.9232
[epoch 26][iter   30] loss: 164415.3438 RMSElog: 10.0829 grad_loss: 16430.4961 normal_loss: 0.9571
[epoch 26][iter   40] loss: 108800.2422 RMSElog: 9.7538 grad_loss: 10869.3369 normal_loss: 0.9334
[epoch 26][iter   50] loss: 234945.1719 RMSElog: 10.0015 grad_loss: 23483.5664 normal_loss: 0.9500
[epoch 26][iter   60] loss: 188645.4531 RMSElog: 10.1590 grad_loss: 18853.4082 normal_loss: 0.9794
[epoch 26][iter   70] loss: 161813.2500 RMSElog: 9.9875 grad_loss: 16170.3857 normal_loss: 0.9508
[epoch 26][iter   80] loss: 167730.9531 RMSElog: 9.9663 grad_loss: 16762.1797 normal_loss: 0.9501
[epoch 26][iter   90] loss: 226949.5156 RMSElog: 10.2029 grad_loss: 22683.8047 normal_loss: 0.9430
[epoch 26][iter  100] loss: 211640.2344 RMSElog: 10.2414 grad_loss: 21152.8145 normal_loss: 0.9675
[epoch 26][iter  110] loss: 127075.1875 RMSElog: 10.1217 grad_loss: 12696.4443 normal_loss: 0.9524
[epoch 26][iter  120] loss: 109148.7344 RMSElog: 10.1172 grad_loss: 10903.8262 normal_loss: 0.9294
[epoch 26][iter  130] loss: 150321.2812 RMSElog: 9.9248 grad_loss: 15021.2656 normal_loss: 0.9385
[epoch 26][iter  140] loss: 180318.6719 RMSElog: 9.9530 grad_loss: 18020.9785 normal_loss: 0.9357
[epoch 26][iter  150] loss: 104749.2812 RMSElog: 9.7512 grad_loss: 10464.2490 normal_loss: 0.9281
[epoch 26][iter  160] loss: 105706.7109 RMSElog: 9.7501 grad_loss: 10559.9893 normal_loss: 0.9321
[epoch 26][iter  170] loss: 163171.1562 RMSElog: 9.7905 grad_loss: 16306.3594 normal_loss: 0.9652
[epoch 26][iter  180] loss: 165326.5625 RMSElog: 9.8832 grad_loss: 16521.8086 normal_loss: 0.9647
[epoch 26][iter  190] loss: 223018.5625 RMSElog: 10.3592 grad_loss: 22290.5508 normal_loss: 0.9462
[epoch 26][iter  200] loss: 233395.7188 RMSElog: 10.2368 grad_loss: 23328.3906 normal_loss: 0.9452
[epoch 26][iter  210] loss: 175301.7344 RMSElog: 9.8017 grad_loss: 17519.4180 normal_loss: 0.9555
[epoch 26][iter  220] loss: 232982.0469 RMSElog: 10.2845 grad_loss: 23286.9707 normal_loss: 0.9500
[epoch 26][iter  230] loss: 137252.6406 RMSElog: 9.3538 grad_loss: 13714.9795 normal_loss: 0.9319
[epoch 26][iter  240] loss: 143460.8594 RMSElog: 9.7520 grad_loss: 14335.3848 normal_loss: 0.9489
[epoch 26][iter  250] loss: 162711.9688 RMSElog: 9.6160 grad_loss: 16260.6387 normal_loss: 0.9427
[epoch 26][iter  260] loss: 167745.5625 RMSElog: 9.8817 grad_loss: 16763.7129 normal_loss: 0.9621
[epoch 26][iter  270] loss: 184374.1875 RMSElog: 10.2641 grad_loss: 18426.1953 normal_loss: 0.9596
[epoch 26][iter  280] loss: 129163.5156 RMSElog: 9.7674 grad_loss: 12905.6660 normal_loss: 0.9175
[epoch 26][iter  290] loss: 139154.8594 RMSElog: 9.5986 grad_loss: 13904.9707 normal_loss: 0.9164
[epoch 26][iter  300] loss: 109101.1250 RMSElog: 9.7363 grad_loss: 10899.4590 normal_loss: 0.9167
[epoch 26][iter  310] loss: 102998.8984 RMSElog: 9.8140 grad_loss: 10289.1318 normal_loss: 0.9431
[epoch 26][iter  320] loss: 103509.2188 RMSElog: 9.9787 grad_loss: 10340.0186 normal_loss: 0.9251
[epoch 26][iter  330] loss: 212429.2969 RMSElog: 10.2794 grad_loss: 21231.6719 normal_loss: 0.9784
[epoch 26][iter  340] loss: 191371.4375 RMSElog: 10.1918 grad_loss: 19126.0020 normal_loss: 0.9504
[epoch 26][iter  350] loss: 224077.6719 RMSElog: 10.3869 grad_loss: 22396.4219 normal_loss: 0.9584
[epoch 26][iter  360] loss: 156709.9531 RMSElog: 9.8741 grad_loss: 15660.1758 normal_loss: 0.9453
[epoch 26][iter  370] loss: 147544.5000 RMSElog: 9.8913 grad_loss: 14743.6045 normal_loss: 0.9534
[epoch 26][iter  380] loss: 214493.2188 RMSElog: 10.3525 grad_loss: 21438.0078 normal_loss: 0.9611
[epoch 26][iter  390] loss: 161285.2188 RMSElog: 9.8339 grad_loss: 16117.7402 normal_loss: 0.9476
[epoch 26][iter  400] loss: 201162.4062 RMSElog: 9.9531 grad_loss: 20105.3164 normal_loss: 0.9706
[epoch 26][iter  410] loss: 117693.8984 RMSElog: 9.6527 grad_loss: 11758.8105 normal_loss: 0.9267
[epoch 26][iter  420] loss: 188116.2812 RMSElog: 10.0888 grad_loss: 18800.5703 normal_loss: 0.9711
[epoch 26][iter  430] loss: 104086.5234 RMSElog: 9.7787 grad_loss: 10397.9453 normal_loss: 0.9289
[epoch 26][iter  440] loss: 191971.7188 RMSElog: 10.0873 grad_loss: 19186.1172 normal_loss: 0.9662
[epoch 26][iter  450] loss: 111712.9219 RMSElog: 9.7826 grad_loss: 11160.5811 normal_loss: 0.9290
[epoch 26][iter  460] loss: 173339.3750 RMSElog: 9.8230 grad_loss: 17323.1934 normal_loss: 0.9212
[epoch 26][iter  470] loss: 159798.6094 RMSElog: 9.8075 grad_loss: 15969.1260 normal_loss: 0.9263
[epoch 26][iter  480] loss: 121959.7344 RMSElog: 9.7378 grad_loss: 12185.3047 normal_loss: 0.9314
[epoch 26][iter  490] loss: 173086.7031 RMSElog: 10.4992 grad_loss: 17297.2227 normal_loss: 0.9480
[epoch 26][iter  500] loss: 108409.6484 RMSElog: 9.2105 grad_loss: 10830.8506 normal_loss: 0.9033
[epoch 26][iter  510] loss: 160438.2969 RMSElog: 9.9714 grad_loss: 16032.9092 normal_loss: 0.9487
[epoch 26][iter  520] loss: 110579.5391 RMSElog: 9.3019 grad_loss: 11047.7109 normal_loss: 0.9413
[epoch 26][iter  530] loss: 188502.1875 RMSElog: 10.1669 grad_loss: 18839.0801 normal_loss: 0.9722
[epoch 26][iter  540] loss: 203023.3438 RMSElog: 10.3036 grad_loss: 20291.0723 normal_loss: 0.9583
[epoch 26][iter  550] loss: 177936.2812 RMSElog: 10.0381 grad_loss: 17782.6348 normal_loss: 0.9545
[epoch 26][iter  560] loss: 112697.1016 RMSElog: 9.7295 grad_loss: 11259.0449 normal_loss: 0.9352
[epoch 26][iter  570] loss: 195412.9844 RMSElog: 10.0493 grad_loss: 19530.2969 normal_loss: 0.9539
[epoch 26][iter  580] loss: 142016.3906 RMSElog: 9.9191 grad_loss: 14190.7676 normal_loss: 0.9526
[epoch 26][iter  590] loss: 166093.7344 RMSElog: 10.4480 grad_loss: 16597.9766 normal_loss: 0.9489
[epoch 27][iter    0] loss: 151354.1562 RMSElog: 9.8837 grad_loss: 15124.5889 normal_loss: 0.9428
[epoch 27][iter   10] loss: 129708.8906 RMSElog: 10.0691 grad_loss: 12959.8711 normal_loss: 0.9481
[epoch 27][iter   20] loss: 146960.1250 RMSElog: 10.0979 grad_loss: 14684.9922 normal_loss: 0.9220
[epoch 27][iter   30] loss: 110579.5391 RMSElog: 9.3019 grad_loss: 11047.7109 normal_loss: 0.9413
[epoch 27][iter   40] loss: 110874.4688 RMSElog: 9.2829 grad_loss: 11077.2607 normal_loss: 0.9034
[epoch 27][iter   50] loss: 117061.6250 RMSElog: 9.2141 grad_loss: 11696.0439 normal_loss: 0.9046
[epoch 27][iter   60] loss: 143904.7812 RMSElog: 10.0263 grad_loss: 14379.5068 normal_loss: 0.9457
[epoch 27][iter   70] loss: 122670.8984 RMSElog: 9.4365 grad_loss: 12256.7207 normal_loss: 0.9331
[epoch 27][iter   80] loss: 129159.4375 RMSElog: 9.6889 grad_loss: 12905.3271 normal_loss: 0.9279
[epoch 27][iter   90] loss: 163268.1250 RMSElog: 9.9218 grad_loss: 16315.9277 normal_loss: 0.9630
[epoch 27][iter  100] loss: 160595.2188 RMSElog: 9.9514 grad_loss: 16048.6191 normal_loss: 0.9518
[epoch 27][iter  110] loss: 203973.6562 RMSElog: 10.4423 grad_loss: 20385.9727 normal_loss: 0.9505
[epoch 27][iter  120] loss: 172966.9375 RMSElog: 9.9430 grad_loss: 17285.8008 normal_loss: 0.9485
[epoch 27][iter  130] loss: 217174.7656 RMSElog: 10.2254 grad_loss: 21706.2891 normal_loss: 0.9635
[epoch 27][iter  140] loss: 87697.6250 RMSElog: 9.9487 grad_loss: 8758.8750 normal_loss: 0.9395
[epoch 27][iter  150] loss: 141118.5312 RMSElog: 10.0520 grad_loss: 14100.8545 normal_loss: 0.9477
[epoch 27][iter  160] loss: 104749.2812 RMSElog: 9.7512 grad_loss: 10464.2490 normal_loss: 0.9281
[epoch 27][iter  170] loss: 152149.7500 RMSElog: 9.9062 grad_loss: 15204.1230 normal_loss: 0.9462
[epoch 27][iter  180] loss: 135814.7656 RMSElog: 9.5386 grad_loss: 13571.0107 normal_loss: 0.9281
[epoch 27][iter  190] loss: 202630.0781 RMSElog: 10.0417 grad_loss: 20252.0078 normal_loss: 0.9589
[epoch 27][iter  200] loss: 162835.5469 RMSElog: 9.9145 grad_loss: 16272.6914 normal_loss: 0.9492
[epoch 27][iter  210] loss: 156557.3594 RMSElog: 9.6970 grad_loss: 15645.1025 normal_loss: 0.9363
[epoch 27][iter  220] loss: 125695.5391 RMSElog: 9.9422 grad_loss: 12558.6836 normal_loss: 0.9278
[epoch 27][iter  230] loss: 112715.8125 RMSElog: 9.6867 grad_loss: 11260.9570 normal_loss: 0.9375
[epoch 27][iter  240] loss: 175004.6875 RMSElog: 10.1774 grad_loss: 17489.3340 normal_loss: 0.9561
[epoch 27][iter  250] loss: 127633.3516 RMSElog: 10.0540 grad_loss: 12752.3496 normal_loss: 0.9317
[epoch 27][iter  260] loss: 128277.9766 RMSElog: 9.7565 grad_loss: 12817.0889 normal_loss: 0.9518
[epoch 27][iter  270] loss: 207919.9844 RMSElog: 10.2783 grad_loss: 20780.7637 normal_loss: 0.9578
[epoch 27][iter  280] loss: 175547.9062 RMSElog: 10.1409 grad_loss: 17543.7168 normal_loss: 0.9342
[epoch 27][iter  290] loss: 110808.0469 RMSElog: 9.7171 grad_loss: 11070.1562 normal_loss: 0.9312
[epoch 27][iter  300] loss: 165750.3750 RMSElog: 9.8971 grad_loss: 16564.1895 normal_loss: 0.9518
[epoch 27][iter  310] loss: 158534.6406 RMSElog: 9.6735 grad_loss: 15842.8672 normal_loss: 0.9232
[epoch 27][iter  320] loss: 188662.5156 RMSElog: 10.2592 grad_loss: 18855.0352 normal_loss: 0.9574
[epoch 27][iter  330] loss: 148793.9688 RMSElog: 10.1047 grad_loss: 14868.3447 normal_loss: 0.9480
[epoch 27][iter  340] loss: 130145.6250 RMSElog: 9.7206 grad_loss: 13003.9072 normal_loss: 0.9344
[epoch 27][iter  350] loss: 149369.9062 RMSElog: 10.0930 grad_loss: 14925.9756 normal_loss: 0.9225
[epoch 27][iter  360] loss: 122912.0625 RMSElog: 9.7328 grad_loss: 12280.5225 normal_loss: 0.9509
[epoch 27][iter  370] loss: 160024.9375 RMSElog: 9.8489 grad_loss: 15991.7236 normal_loss: 0.9215
[epoch 27][iter  380] loss: 221213.4531 RMSElog: 10.1740 grad_loss: 22110.2305 normal_loss: 0.9415
[epoch 27][iter  390] loss: 119429.4609 RMSElog: 9.6388 grad_loss: 11932.3740 normal_loss: 0.9339
[epoch 27][iter  400] loss: 178468.8906 RMSElog: 9.9039 grad_loss: 17836.0469 normal_loss: 0.9380
[epoch 27][iter  410] loss: 162433.1875 RMSElog: 9.8183 grad_loss: 16232.5488 normal_loss: 0.9510
[epoch 27][iter  420] loss: 156982.7188 RMSElog: 10.0093 grad_loss: 15687.3193 normal_loss: 0.9438
[epoch 27][iter  430] loss: 105628.3672 RMSElog: 10.0899 grad_loss: 10551.8262 normal_loss: 0.9213
[epoch 27][iter  440] loss: 124932.5703 RMSElog: 9.4215 grad_loss: 12482.8789 normal_loss: 0.9558
[epoch 27][iter  450] loss: 127776.8359 RMSElog: 10.0947 grad_loss: 12766.6396 normal_loss: 0.9490
[epoch 27][iter  460] loss: 185772.2500 RMSElog: 9.9702 grad_loss: 18566.2969 normal_loss: 0.9567
[epoch 27][iter  470] loss: 102096.6484 RMSElog: 9.8437 grad_loss: 10198.8926 normal_loss: 0.9290
[epoch 27][iter  480] loss: 181983.5312 RMSElog: 10.0741 grad_loss: 18187.3320 normal_loss: 0.9464
[epoch 27][iter  490] loss: 147371.3906 RMSElog: 9.6635 grad_loss: 14726.5488 normal_loss: 0.9280
[epoch 27][iter  500] loss: 214808.7344 RMSElog: 10.1872 grad_loss: 21469.7441 normal_loss: 0.9423
[epoch 27][iter  510] loss: 133045.7969 RMSElog: 9.8765 grad_loss: 13293.7480 normal_loss: 0.9538
[epoch 27][iter  520] loss: 150364.6250 RMSElog: 9.9123 grad_loss: 15025.6094 normal_loss: 0.9418
[epoch 27][iter  530] loss: 128979.7188 RMSElog: 9.7049 grad_loss: 12887.3340 normal_loss: 0.9327
[epoch 27][iter  540] loss: 177292.4062 RMSElog: 10.1195 grad_loss: 17718.1602 normal_loss: 0.9615
[epoch 27][iter  550] loss: 217845.5000 RMSElog: 10.3597 grad_loss: 21773.2148 normal_loss: 0.9774
[epoch 27][iter  560] loss: 180425.2344 RMSElog: 10.0354 grad_loss: 18031.5332 normal_loss: 0.9552
[epoch 27][iter  570] loss: 201503.0625 RMSElog: 10.3149 grad_loss: 20139.0410 normal_loss: 0.9514
[epoch 27][iter  580] loss: 141962.4844 RMSElog: 9.7918 grad_loss: 14185.5273 normal_loss: 0.9289
[epoch 27][iter  590] loss: 114310.3047 RMSElog: 9.4026 grad_loss: 11420.7217 normal_loss: 0.9063
[epoch 28][iter    0] loss: 101101.5781 RMSElog: 9.9274 grad_loss: 10099.3008 normal_loss: 0.9297
[epoch 28][iter   10] loss: 178593.9688 RMSElog: 10.0081 grad_loss: 17848.4395 normal_loss: 0.9490
[epoch 28][iter   20] loss: 180929.5938 RMSElog: 10.0745 grad_loss: 18081.9297 normal_loss: 0.9546
[epoch 28][iter   30] loss: 195440.9062 RMSElog: 9.7403 grad_loss: 19533.3926 normal_loss: 0.9572
[epoch 28][iter   40] loss: 139433.9219 RMSElog: 9.8139 grad_loss: 13932.6191 normal_loss: 0.9591
[epoch 28][iter   50] loss: 175008.6562 RMSElog: 9.9496 grad_loss: 17489.9434 normal_loss: 0.9734
[epoch 28][iter   60] loss: 138469.0469 RMSElog: 9.4787 grad_loss: 13836.4834 normal_loss: 0.9438
[epoch 28][iter   70] loss: 142016.3906 RMSElog: 9.9191 grad_loss: 14190.7676 normal_loss: 0.9526
[epoch 28][iter   80] loss: 158034.0625 RMSElog: 9.9527 grad_loss: 15792.5088 normal_loss: 0.9445
[epoch 28][iter   90] loss: 231696.5312 RMSElog: 10.3202 grad_loss: 23158.3867 normal_loss: 0.9457
[epoch 28][iter  100] loss: 163786.3906 RMSElog: 9.7419 grad_loss: 16367.9502 normal_loss: 0.9470
[epoch 28][iter  110] loss: 120440.5938 RMSElog: 9.1890 grad_loss: 12033.9463 normal_loss: 0.9240
[epoch 28][iter  120] loss: 139830.9062 RMSElog: 9.7552 grad_loss: 13972.3770 normal_loss: 0.9585
[epoch 28][iter  130] loss: 114509.4531 RMSElog: 9.7409 grad_loss: 11440.2627 normal_loss: 0.9413
[epoch 28][iter  140] loss: 156173.3594 RMSElog: 9.9681 grad_loss: 15606.4199 normal_loss: 0.9482
[epoch 28][iter  150] loss: 135042.2031 RMSElog: 9.6763 grad_loss: 13493.6074 normal_loss: 0.9366
[epoch 28][iter  160] loss: 161285.2188 RMSElog: 9.8339 grad_loss: 16117.7402 normal_loss: 0.9476
[epoch 28][iter  170] loss: 157119.7500 RMSElog: 9.8959 grad_loss: 15701.1318 normal_loss: 0.9480
[epoch 28][iter  180] loss: 170056.2812 RMSElog: 9.7423 grad_loss: 16994.9395 normal_loss: 0.9472
[epoch 28][iter  190] loss: 114149.3672 RMSElog: 9.2655 grad_loss: 11404.7637 normal_loss: 0.9073
[epoch 28][iter  200] loss: 128488.4375 RMSElog: 9.7402 grad_loss: 12838.1719 normal_loss: 0.9319
[epoch 28][iter  210] loss: 162363.8906 RMSElog: 9.7464 grad_loss: 16225.6777 normal_loss: 0.9661
[epoch 28][iter  220] loss: 124563.3672 RMSElog: 9.9514 grad_loss: 12445.4453 normal_loss: 0.9400
[epoch 28][iter  230] loss: 129743.7969 RMSElog: 9.9523 grad_loss: 12963.4785 normal_loss: 0.9492
[epoch 28][iter  240] loss: 148793.9688 RMSElog: 10.1047 grad_loss: 14868.3447 normal_loss: 0.9480
[epoch 28][iter  250] loss: 150459.7656 RMSElog: 9.9429 grad_loss: 15035.0986 normal_loss: 0.9347
[epoch 28][iter  260] loss: 105794.8906 RMSElog: 9.7676 grad_loss: 10568.7949 normal_loss: 0.9267
[epoch 28][iter  270] loss: 155762.3594 RMSElog: 9.8487 grad_loss: 15565.4414 normal_loss: 0.9448
[epoch 28][iter  280] loss: 116582.6250 RMSElog: 9.7871 grad_loss: 11647.5439 normal_loss: 0.9314
[epoch 28][iter  290] loss: 177814.3125 RMSElog: 10.0854 grad_loss: 17770.3887 normal_loss: 0.9571
[epoch 28][iter  300] loss: 133024.8594 RMSElog: 9.6937 grad_loss: 13291.8662 normal_loss: 0.9261
[epoch 28][iter  310] loss: 211925.8438 RMSElog: 10.1863 grad_loss: 21181.4551 normal_loss: 0.9436
[epoch 28][iter  320] loss: 178784.0938 RMSElog: 9.9460 grad_loss: 17867.5059 normal_loss: 0.9592
[epoch 28][iter  330] loss: 233395.7188 RMSElog: 10.2368 grad_loss: 23328.3906 normal_loss: 0.9452
[epoch 28][iter  340] loss: 174629.2812 RMSElog: 9.9405 grad_loss: 17452.0625 normal_loss: 0.9229
[epoch 28][iter  350] loss: 145409.6719 RMSElog: 9.8330 grad_loss: 14530.1904 normal_loss: 0.9445
[epoch 28][iter  360] loss: 153501.7031 RMSElog: 10.1127 grad_loss: 15339.1152 normal_loss: 0.9429
[epoch 28][iter  370] loss: 228053.0312 RMSElog: 10.2288 grad_loss: 22794.1328 normal_loss: 0.9422
[epoch 28][iter  380] loss: 195507.4062 RMSElog: 10.1854 grad_loss: 19539.6016 normal_loss: 0.9533
[epoch 28][iter  390] loss: 137764.4219 RMSElog: 9.8227 grad_loss: 13765.6758 normal_loss: 0.9442
[epoch 28][iter  400] loss: 156253.7656 RMSElog: 9.8231 grad_loss: 15614.6211 normal_loss: 0.9316
[epoch 28][iter  410] loss: 197520.7500 RMSElog: 10.1663 grad_loss: 19740.9414 normal_loss: 0.9662
[epoch 28][iter  420] loss: 104749.2812 RMSElog: 9.7512 grad_loss: 10464.2490 normal_loss: 0.9281
[epoch 28][iter  430] loss: 160051.0156 RMSElog: 9.8405 grad_loss: 15994.3105 normal_loss: 0.9506
[epoch 28][iter  440] loss: 131942.8750 RMSElog: 9.8414 grad_loss: 13183.5000 normal_loss: 0.9453
[epoch 28][iter  450] loss: 220695.7969 RMSElog: 10.2208 grad_loss: 22058.4082 normal_loss: 0.9520
[epoch 28][iter  460] loss: 132707.3594 RMSElog: 9.7842 grad_loss: 13260.0049 normal_loss: 0.9472
[epoch 28][iter  470] loss: 119429.4609 RMSElog: 9.6388 grad_loss: 11932.3740 normal_loss: 0.9339
[epoch 28][iter  480] loss: 118746.3359 RMSElog: 9.7278 grad_loss: 11863.9629 normal_loss: 0.9433
[epoch 28][iter  490] loss: 181465.8125 RMSElog: 10.1136 grad_loss: 18135.5117 normal_loss: 0.9572
[epoch 28][iter  500] loss: 207919.9844 RMSElog: 10.2783 grad_loss: 20780.7637 normal_loss: 0.9578
[epoch 28][iter  510] loss: 125683.7969 RMSElog: 9.8131 grad_loss: 12557.6123 normal_loss: 0.9537
[epoch 28][iter  520] loss: 212239.7031 RMSElog: 10.2690 grad_loss: 21212.7383 normal_loss: 0.9638
[epoch 28][iter  530] loss: 135814.7656 RMSElog: 9.5386 grad_loss: 13571.0107 normal_loss: 0.9281
[epoch 28][iter  540] loss: 196277.7188 RMSElog: 10.2453 grad_loss: 19616.5664 normal_loss: 0.9595
[epoch 28][iter  550] loss: 182476.7344 RMSElog: 9.9672 grad_loss: 18236.7656 normal_loss: 0.9407
[epoch 28][iter  560] loss: 177292.4062 RMSElog: 10.1195 grad_loss: 17718.1602 normal_loss: 0.9615
[epoch 28][iter  570] loss: 232982.0469 RMSElog: 10.2845 grad_loss: 23286.9707 normal_loss: 0.9500
[epoch 28][iter  580] loss: 132012.6719 RMSElog: 9.6802 grad_loss: 13190.6582 normal_loss: 0.9291
[epoch 28][iter  590] loss: 136869.7500 RMSElog: 9.5771 grad_loss: 13676.4668 normal_loss: 0.9315
[epoch 29][iter    0] loss: 165976.7500 RMSElog: 9.9385 grad_loss: 16586.7715 normal_loss: 0.9645
[epoch 29][iter   10] loss: 164766.2812 RMSElog: 9.6928 grad_loss: 16465.9941 normal_loss: 0.9411
[epoch 29][iter   20] loss: 100650.2734 RMSElog: 9.7441 grad_loss: 10054.3555 normal_loss: 0.9279
[epoch 29][iter   30] loss: 195777.0156 RMSElog: 10.1280 grad_loss: 19566.5996 normal_loss: 0.9726
[epoch 29][iter   40] loss: 100663.8984 RMSElog: 9.9598 grad_loss: 10055.5020 normal_loss: 0.9277
[epoch 29][iter   50] loss: 214067.1875 RMSElog: 10.0580 grad_loss: 21395.7207 normal_loss: 0.9394
[epoch 29][iter   60] loss: 116582.6172 RMSElog: 9.7871 grad_loss: 11647.5430 normal_loss: 0.9314
[epoch 29][iter   70] loss: 103514.7812 RMSElog: 9.3451 grad_loss: 10341.2109 normal_loss: 0.9229
[epoch 29][iter   80] loss: 198036.3750 RMSElog: 10.2619 grad_loss: 19792.4219 normal_loss: 0.9529
[epoch 29][iter   90] loss: 146576.9219 RMSElog: 9.6520 grad_loss: 14647.1045 normal_loss: 0.9356
[epoch 29][iter  100] loss: 167784.2500 RMSElog: 9.9330 grad_loss: 16767.5391 normal_loss: 0.9539
[epoch 29][iter  110] loss: 167745.5625 RMSElog: 9.8817 grad_loss: 16763.7129 normal_loss: 0.9621
[epoch 29][iter  120] loss: 217914.6094 RMSElog: 10.2786 grad_loss: 21780.2246 normal_loss: 0.9575
[epoch 29][iter  130] loss: 131942.8750 RMSElog: 9.8414 grad_loss: 13183.5000 normal_loss: 0.9453
[epoch 29][iter  140] loss: 125315.5625 RMSElog: 9.7124 grad_loss: 12520.9131 normal_loss: 0.9309
[epoch 29][iter  150] loss: 208821.7812 RMSElog: 10.1565 grad_loss: 20871.0781 normal_loss: 0.9431
[epoch 29][iter  160] loss: 150323.7656 RMSElog: 10.0510 grad_loss: 15021.3848 normal_loss: 0.9410
[epoch 29][iter  170] loss: 129163.5156 RMSElog: 9.7674 grad_loss: 12905.6660 normal_loss: 0.9175
[epoch 29][iter  180] loss: 202630.0781 RMSElog: 10.0417 grad_loss: 20252.0078 normal_loss: 0.9589
[epoch 29][iter  190] loss: 100116.4922 RMSElog: 9.9235 grad_loss: 10000.7998 normal_loss: 0.9258
[epoch 29][iter  200] loss: 108800.2422 RMSElog: 9.7538 grad_loss: 10869.3369 normal_loss: 0.9334
[epoch 29][iter  210] loss: 114827.1016 RMSElog: 9.7767 grad_loss: 11472.0010 normal_loss: 0.9329
[epoch 29][iter  220] loss: 194578.5781 RMSElog: 10.2312 grad_loss: 19446.6680 normal_loss: 0.9596
[epoch 29][iter  230] loss: 119923.2031 RMSElog: 9.5699 grad_loss: 11981.8154 normal_loss: 0.9342
[epoch 29][iter  240] loss: 147598.9844 RMSElog: 9.8981 grad_loss: 14749.0645 normal_loss: 0.9355
[epoch 29][iter  250] loss: 123525.9062 RMSElog: 9.3524 grad_loss: 12342.2930 normal_loss: 0.9450
[epoch 29][iter  260] loss: 141865.3906 RMSElog: 9.7199 grad_loss: 14175.8896 normal_loss: 0.9298
[epoch 29][iter  270] loss: 171588.0938 RMSElog: 9.8325 grad_loss: 17148.0527 normal_loss: 0.9237
[epoch 29][iter  280] loss: 141962.4844 RMSElog: 9.7918 grad_loss: 14185.5273 normal_loss: 0.9289
[epoch 29][iter  290] loss: 220695.7969 RMSElog: 10.2208 grad_loss: 22058.4082 normal_loss: 0.9520
[epoch 29][iter  300] loss: 196277.7188 RMSElog: 10.2453 grad_loss: 19616.5664 normal_loss: 0.9595
[epoch 29][iter  310] loss: 190972.3281 RMSElog: 10.1152 grad_loss: 19086.1523 normal_loss: 0.9647
[epoch 29][iter  320] loss: 127499.3281 RMSElog: 9.2769 grad_loss: 12739.7178 normal_loss: 0.9374
[epoch 29][iter  330] loss: 191559.9688 RMSElog: 9.9749 grad_loss: 19145.0742 normal_loss: 0.9477
[epoch 29][iter  340] loss: 106182.2188 RMSElog: 9.7575 grad_loss: 10607.5342 normal_loss: 0.9294
[epoch 29][iter  350] loss: 109441.5234 RMSElog: 9.7466 grad_loss: 10933.4756 normal_loss: 0.9300
[epoch 29][iter  360] loss: 159422.2656 RMSElog: 9.7010 grad_loss: 15931.5957 normal_loss: 0.9299
[epoch 29][iter  370] loss: 151354.1562 RMSElog: 9.8837 grad_loss: 15124.5889 normal_loss: 0.9428
[epoch 29][iter  380] loss: 177664.5312 RMSElog: 9.7808 grad_loss: 17755.7344 normal_loss: 0.9369
[epoch 29][iter  390] loss: 104340.6328 RMSElog: 9.7692 grad_loss: 10423.3701 normal_loss: 0.9234
[epoch 29][iter  400] loss: 213401.8906 RMSElog: 10.0568 grad_loss: 21329.1582 normal_loss: 0.9749
[epoch 29][iter  410] loss: 120745.8203 RMSElog: 9.7706 grad_loss: 12063.8691 normal_loss: 0.9428
[epoch 29][iter  420] loss: 134471.5781 RMSElog: 9.9200 grad_loss: 13436.3271 normal_loss: 0.9111
[epoch 29][iter  430] loss: 133024.8594 RMSElog: 9.6937 grad_loss: 13291.8662 normal_loss: 0.9261
[epoch 29][iter  440] loss: 109260.8906 RMSElog: 9.7819 grad_loss: 10915.3789 normal_loss: 0.9279
[epoch 29][iter  450] loss: 115766.5312 RMSElog: 9.7467 grad_loss: 11565.9717 normal_loss: 0.9344
[epoch 29][iter  460] loss: 114893.3984 RMSElog: 9.2562 grad_loss: 11479.1777 normal_loss: 0.9067
[epoch 29][iter  470] loss: 132623.8750 RMSElog: 9.3325 grad_loss: 13252.1152 normal_loss: 0.9391
[epoch 29][iter  480] loss: 160788.1406 RMSElog: 9.7539 grad_loss: 16068.1289 normal_loss: 0.9318
[epoch 29][iter  490] loss: 122386.0234 RMSElog: 9.6688 grad_loss: 12227.9971 normal_loss: 0.9366
[epoch 29][iter  500] loss: 169013.5312 RMSElog: 9.5880 grad_loss: 16890.8242 normal_loss: 0.9419
[epoch 29][iter  510] loss: 232771.8594 RMSElog: 10.2938 grad_loss: 23265.9453 normal_loss: 0.9464
[epoch 29][iter  520] loss: 151401.7031 RMSElog: 9.4414 grad_loss: 15129.7930 normal_loss: 0.9361
[epoch 29][iter  530] loss: 164415.3438 RMSElog: 10.0829 grad_loss: 16430.4961 normal_loss: 0.9571
[epoch 29][iter  540] loss: 194987.6250 RMSElog: 9.8416 grad_loss: 19487.9453 normal_loss: 0.9748
[epoch 29][iter  550] loss: 152562.3750 RMSElog: 9.8216 grad_loss: 15245.4775 normal_loss: 0.9391
[epoch 29][iter  560] loss: 175547.8906 RMSElog: 10.1409 grad_loss: 17543.7148 normal_loss: 0.9342
[epoch 29][iter  570] loss: 163253.2500 RMSElog: 9.8740 grad_loss: 16314.5010 normal_loss: 0.9499
[epoch 29][iter  580] loss: 109101.1250 RMSElog: 9.7363 grad_loss: 10899.4590 normal_loss: 0.9167
[epoch 29][iter  590] loss: 116073.6016 RMSElog: 9.6844 grad_loss: 11596.7334 normal_loss: 0.9429
[epoch 30][iter    0] loss: 141118.5312 RMSElog: 10.0520 grad_loss: 14100.8545 normal_loss: 0.9477
[epoch 30][iter   10] loss: 212429.2969 RMSElog: 10.2794 grad_loss: 21231.6719 normal_loss: 0.9784
[epoch 30][iter   20] loss: 196437.7969 RMSElog: 10.2319 grad_loss: 19632.5879 normal_loss: 0.9584
[epoch 30][iter   30] loss: 164597.1094 RMSElog: 9.8631 grad_loss: 16448.8984 normal_loss: 0.9502
[epoch 30][iter   40] loss: 165648.1875 RMSElog: 9.7420 grad_loss: 16554.1289 normal_loss: 0.9479
[epoch 30][iter   50] loss: 188502.1875 RMSElog: 10.1669 grad_loss: 18839.0801 normal_loss: 0.9722
[epoch 30][iter   60] loss: 213401.8906 RMSElog: 10.0568 grad_loss: 21329.1582 normal_loss: 0.9749
[epoch 30][iter   70] loss: 147765.9219 RMSElog: 10.0607 grad_loss: 14765.6104 normal_loss: 0.9219
[epoch 30][iter   80] loss: 159798.6094 RMSElog: 9.8075 grad_loss: 15969.1260 normal_loss: 0.9263
[epoch 30][iter   90] loss: 198293.9375 RMSElog: 10.2275 grad_loss: 19818.2168 normal_loss: 0.9521
[epoch 30][iter  100] loss: 109148.7344 RMSElog: 10.1172 grad_loss: 10903.8262 normal_loss: 0.9294
[epoch 30][iter  110] loss: 157061.8906 RMSElog: 9.9892 grad_loss: 15695.2461 normal_loss: 0.9531
[epoch 30][iter  120] loss: 160074.2031 RMSElog: 9.7506 grad_loss: 15996.7441 normal_loss: 0.9257
[epoch 30][iter  130] loss: 171588.0938 RMSElog: 9.8325 grad_loss: 17148.0527 normal_loss: 0.9237
[epoch 30][iter  140] loss: 133760.0312 RMSElog: 10.0653 grad_loss: 13364.9707 normal_loss: 0.9680
[epoch 30][iter  150] loss: 182005.7031 RMSElog: 10.0998 grad_loss: 18189.5195 normal_loss: 0.9507
[epoch 30][iter  160] loss: 204037.7500 RMSElog: 10.1754 grad_loss: 20392.6582 normal_loss: 0.9420
[epoch 30][iter  170] loss: 154029.7812 RMSElog: 9.8645 grad_loss: 15392.1729 normal_loss: 0.9405
[epoch 30][iter  180] loss: 188662.5156 RMSElog: 10.2592 grad_loss: 18855.0352 normal_loss: 0.9574
[epoch 30][iter  190] loss: 179634.8125 RMSElog: 10.0930 grad_loss: 17952.4258 normal_loss: 0.9607
[epoch 30][iter  200] loss: 103955.0000 RMSElog: 9.6256 grad_loss: 10384.9521 normal_loss: 0.9222
[epoch 30][iter  210] loss: 107956.6094 RMSElog: 10.0900 grad_loss: 10784.6504 normal_loss: 0.9209
[epoch 30][iter  220] loss: 115452.5781 RMSElog: 9.2301 grad_loss: 11535.1230 normal_loss: 0.9042
[epoch 30][iter  230] loss: 136869.7500 RMSElog: 9.5771 grad_loss: 13676.4668 normal_loss: 0.9315
[epoch 30][iter  240] loss: 135699.4688 RMSElog: 9.6754 grad_loss: 13559.3340 normal_loss: 0.9364
[epoch 30][iter  250] loss: 152562.3906 RMSElog: 9.8216 grad_loss: 15245.4785 normal_loss: 0.9391
[epoch 30][iter  260] loss: 198743.4219 RMSElog: 10.0842 grad_loss: 19863.2930 normal_loss: 0.9643
[epoch 30][iter  270] loss: 102096.6484 RMSElog: 9.8437 grad_loss: 10198.8926 normal_loss: 0.9290
[epoch 30][iter  280] loss: 164181.7344 RMSElog: 10.0104 grad_loss: 16407.1973 normal_loss: 0.9660
[epoch 30][iter  290] loss: 151401.7031 RMSElog: 9.4414 grad_loss: 15129.7930 normal_loss: 0.9361
[epoch 30][iter  300] loss: 175004.6875 RMSElog: 10.1774 grad_loss: 17489.3340 normal_loss: 0.9561
[epoch 30][iter  310] loss: 162835.5312 RMSElog: 9.9145 grad_loss: 16272.6904 normal_loss: 0.9492
[epoch 30][iter  320] loss: 218625.8750 RMSElog: 10.2839 grad_loss: 21851.3457 normal_loss: 0.9592
[epoch 30][iter  330] loss: 173721.0156 RMSElog: 10.0355 grad_loss: 17361.1191 normal_loss: 0.9468
[epoch 30][iter  340] loss: 161285.2188 RMSElog: 9.8339 grad_loss: 16117.7402 normal_loss: 0.9476
[epoch 30][iter  350] loss: 112715.8125 RMSElog: 9.6867 grad_loss: 11260.9570 normal_loss: 0.9375
[epoch 30][iter  360] loss: 142544.5156 RMSElog: 10.1645 grad_loss: 14243.3457 normal_loss: 0.9425
[epoch 30][iter  370] loss: 232982.0469 RMSElog: 10.2845 grad_loss: 23286.9707 normal_loss: 0.9500
[epoch 30][iter  380] loss: 184374.2188 RMSElog: 10.2641 grad_loss: 18426.1992 normal_loss: 0.9596
[epoch 30][iter  390] loss: 158534.6406 RMSElog: 9.6735 grad_loss: 15842.8672 normal_loss: 0.9232
[epoch 30][iter  400] loss: 195412.9844 RMSElog: 10.0493 grad_loss: 19530.2969 normal_loss: 0.9539
[epoch 30][iter  410] loss: 105098.6875 RMSElog: 9.7426 grad_loss: 10499.2012 normal_loss: 0.9257
[epoch 30][iter  420] loss: 173574.0781 RMSElog: 10.1780 grad_loss: 17346.2734 normal_loss: 0.9575
[epoch 30][iter  430] loss: 165326.5625 RMSElog: 9.8832 grad_loss: 16521.8086 normal_loss: 0.9647
[epoch 30][iter  440] loss: 100980.7812 RMSElog: 9.6528 grad_loss: 10087.5176 normal_loss: 0.9085
[epoch 30][iter  450] loss: 154609.8281 RMSElog: 9.8745 grad_loss: 15450.1855 normal_loss: 0.9220
[epoch 30][iter  460] loss: 175002.7344 RMSElog: 9.8652 grad_loss: 17489.4648 normal_loss: 0.9436
[epoch 30][iter  470] loss: 239768.0938 RMSElog: 10.2366 grad_loss: 23965.6250 normal_loss: 0.9466
[epoch 30][iter  480] loss: 128319.4141 RMSElog: 9.6733 grad_loss: 12821.3398 normal_loss: 0.9287
[epoch 30][iter  490] loss: 229081.3125 RMSElog: 10.2778 grad_loss: 22896.9121 normal_loss: 0.9420
[epoch 30][iter  500] loss: 160788.1406 RMSElog: 9.7539 grad_loss: 16068.1289 normal_loss: 0.9318
[epoch 30][iter  510] loss: 130641.2188 RMSElog: 9.7184 grad_loss: 13053.4688 normal_loss: 0.9344
[epoch 30][iter  520] loss: 143440.0000 RMSElog: 9.6404 grad_loss: 14333.4336 normal_loss: 0.9261
[epoch 30][iter  530] loss: 114509.4531 RMSElog: 9.7409 grad_loss: 11440.2627 normal_loss: 0.9413
[epoch 30][iter  540] loss: 108800.2422 RMSElog: 9.7538 grad_loss: 10869.3369 normal_loss: 0.9334
[epoch 30][iter  550] loss: 164415.3438 RMSElog: 10.0829 grad_loss: 16430.4961 normal_loss: 0.9571
[epoch 30][iter  560] loss: 206764.1875 RMSElog: 10.2849 grad_loss: 20665.1777 normal_loss: 0.9549
[epoch 30][iter  570] loss: 152549.8750 RMSElog: 9.9928 grad_loss: 15244.0293 normal_loss: 0.9654
[epoch 30][iter  580] loss: 149135.0000 RMSElog: 10.0932 grad_loss: 14902.4570 normal_loss: 0.9500
[epoch 30][iter  590] loss: 118746.3359 RMSElog: 9.7278 grad_loss: 11863.9629 normal_loss: 0.9433
[epoch 31][iter    0] loss: 127633.3516 RMSElog: 10.0540 grad_loss: 12752.3496 normal_loss: 0.9317
[epoch 31][iter   10] loss: 163272.7188 RMSElog: 10.0261 grad_loss: 16316.3047 normal_loss: 0.9400
[epoch 31][iter   20] loss: 139016.5781 RMSElog: 9.6276 grad_loss: 13891.1045 normal_loss: 0.9261
[epoch 31][iter   30] loss: 180663.0469 RMSElog: 10.2319 grad_loss: 18055.1094 normal_loss: 0.9630
[epoch 31][iter   40] loss: 152332.5469 RMSElog: 9.9813 grad_loss: 15222.3486 normal_loss: 0.9245
[epoch 31][iter   50] loss: 116582.6172 RMSElog: 9.7871 grad_loss: 11647.5430 normal_loss: 0.9314
[epoch 31][iter   60] loss: 114827.1016 RMSElog: 9.7767 grad_loss: 11472.0010 normal_loss: 0.9329
[epoch 31][iter   70] loss: 146465.6875 RMSElog: 10.2283 grad_loss: 14635.4062 normal_loss: 0.9337
[epoch 31][iter   80] loss: 177814.3125 RMSElog: 10.0854 grad_loss: 17770.3887 normal_loss: 0.9571
[epoch 31][iter   90] loss: 153723.9844 RMSElog: 9.9609 grad_loss: 15361.4902 normal_loss: 0.9475
[epoch 31][iter  100] loss: 160595.2188 RMSElog: 9.9514 grad_loss: 16048.6191 normal_loss: 0.9518
[epoch 31][iter  110] loss: 119923.2031 RMSElog: 9.5699 grad_loss: 11981.8154 normal_loss: 0.9342
[epoch 31][iter  120] loss: 146161.1562 RMSElog: 10.0682 grad_loss: 14605.1279 normal_loss: 0.9190
[epoch 31][iter  130] loss: 232982.0469 RMSElog: 10.2845 grad_loss: 23286.9707 normal_loss: 0.9500
[epoch 31][iter  140] loss: 221213.4219 RMSElog: 10.1740 grad_loss: 22110.2266 normal_loss: 0.9415
[epoch 31][iter  150] loss: 158534.6406 RMSElog: 9.6735 grad_loss: 15842.8672 normal_loss: 0.9232
[epoch 31][iter  160] loss: 177292.4062 RMSElog: 10.1195 grad_loss: 17718.1602 normal_loss: 0.9615
[epoch 31][iter  170] loss: 160743.5156 RMSElog: 9.9654 grad_loss: 16063.4521 normal_loss: 0.9340
[epoch 31][iter  180] loss: 148057.2500 RMSElog: 9.9895 grad_loss: 14794.7812 normal_loss: 0.9555
[epoch 31][iter  190] loss: 144440.4531 RMSElog: 9.8843 grad_loss: 14433.2314 normal_loss: 0.9296
[epoch 31][iter  200] loss: 115534.3750 RMSElog: 9.7196 grad_loss: 11542.7852 normal_loss: 0.9325
[epoch 31][iter  210] loss: 142261.5781 RMSElog: 9.7627 grad_loss: 14215.4639 normal_loss: 0.9304
[epoch 31][iter  220] loss: 178593.9688 RMSElog: 10.0081 grad_loss: 17848.4395 normal_loss: 0.9490
[epoch 31][iter  230] loss: 229081.3125 RMSElog: 10.2778 grad_loss: 22896.9121 normal_loss: 0.9420
[epoch 31][iter  240] loss: 207919.9844 RMSElog: 10.2783 grad_loss: 20780.7637 normal_loss: 0.9578
[epoch 31][iter  250] loss: 175486.0156 RMSElog: 10.0393 grad_loss: 17537.6055 normal_loss: 0.9563
[epoch 31][iter  260] loss: 172248.5156 RMSElog: 9.5166 grad_loss: 17214.3730 normal_loss: 0.9619
[epoch 31][iter  270] loss: 147544.5000 RMSElog: 9.8913 grad_loss: 14743.6045 normal_loss: 0.9534
[epoch 31][iter  280] loss: 170488.4688 RMSElog: 9.7164 grad_loss: 17038.1816 normal_loss: 0.9494
[epoch 31][iter  290] loss: 181293.3438 RMSElog: 10.0009 grad_loss: 18118.3809 normal_loss: 0.9531
[epoch 31][iter  300] loss: 103815.4375 RMSElog: 9.7764 grad_loss: 10370.8340 normal_loss: 0.9338
[epoch 31][iter  310] loss: 160788.1562 RMSElog: 9.7539 grad_loss: 16068.1299 normal_loss: 0.9318
[epoch 31][iter  320] loss: 176914.4531 RMSElog: 10.3802 grad_loss: 17680.1094 normal_loss: 0.9546
[epoch 31][iter  330] loss: 211925.8438 RMSElog: 10.1863 grad_loss: 21181.4551 normal_loss: 0.9436
[epoch 31][iter  340] loss: 232771.8594 RMSElog: 10.2938 grad_loss: 23265.9453 normal_loss: 0.9464
[epoch 31][iter  350] loss: 212239.7031 RMSElog: 10.2690 grad_loss: 21212.7383 normal_loss: 0.9638
[epoch 31][iter  360] loss: 135215.1250 RMSElog: 9.9407 grad_loss: 13510.6152 normal_loss: 0.9572
[epoch 31][iter  370] loss: 153501.7031 RMSElog: 10.1127 grad_loss: 15339.1152 normal_loss: 0.9429
[epoch 31][iter  380] loss: 129743.7969 RMSElog: 9.9523 grad_loss: 12963.4785 normal_loss: 0.9492
[epoch 31][iter  390] loss: 130641.2188 RMSElog: 9.7184 grad_loss: 13053.4688 normal_loss: 0.9344
[epoch 31][iter  400] loss: 153997.7188 RMSElog: 9.7620 grad_loss: 15389.0713 normal_loss: 0.9383
[epoch 31][iter  410] loss: 104340.6328 RMSElog: 9.7692 grad_loss: 10423.3701 normal_loss: 0.9234
[epoch 31][iter  420] loss: 215955.5625 RMSElog: 10.2675 grad_loss: 21584.3242 normal_loss: 0.9654
[epoch 31][iter  430] loss: 149540.3750 RMSElog: 9.7408 grad_loss: 14943.3740 normal_loss: 0.9229
[epoch 31][iter  440] loss: 114149.3750 RMSElog: 9.2655 grad_loss: 11404.7646 normal_loss: 0.9073
[epoch 31][iter  450] loss: 109148.7344 RMSElog: 10.1172 grad_loss: 10903.8262 normal_loss: 0.9294
[epoch 31][iter  460] loss: 160024.9375 RMSElog: 9.8489 grad_loss: 15991.7236 normal_loss: 0.9215
[epoch 31][iter  470] loss: 191371.4375 RMSElog: 10.1918 grad_loss: 19126.0020 normal_loss: 0.9504
[epoch 31][iter  480] loss: 149369.9062 RMSElog: 10.0930 grad_loss: 14925.9756 normal_loss: 0.9225
[epoch 31][iter  490] loss: 164766.2812 RMSElog: 9.6928 grad_loss: 16465.9941 normal_loss: 0.9411
[epoch 31][iter  500] loss: 96006.2500 RMSElog: 9.5560 grad_loss: 9590.1396 normal_loss: 0.9294
[epoch 31][iter  510] loss: 165306.9531 RMSElog: 9.8881 grad_loss: 16519.8438 normal_loss: 0.9638
[epoch 31][iter  520] loss: 108800.2422 RMSElog: 9.7538 grad_loss: 10869.3369 normal_loss: 0.9334
[epoch 31][iter  530] loss: 132425.4375 RMSElog: 10.0155 grad_loss: 13231.5801 normal_loss: 0.9468
[epoch 31][iter  540] loss: 175301.7344 RMSElog: 9.8017 grad_loss: 17519.4180 normal_loss: 0.9555
[epoch 31][iter  550] loss: 163389.5469 RMSElog: 9.7702 grad_loss: 16328.2637 normal_loss: 0.9208
[epoch 31][iter  560] loss: 131213.2969 RMSElog: 9.6860 grad_loss: 13110.7217 normal_loss: 0.9217
[epoch 31][iter  570] loss: 188645.4531 RMSElog: 10.1590 grad_loss: 18853.4082 normal_loss: 0.9794
[epoch 31][iter  580] loss: 189543.6719 RMSElog: 10.1592 grad_loss: 18943.2598 normal_loss: 0.9474
[epoch 31][iter  590] loss: 142544.6875 RMSElog: 10.2185 grad_loss: 14243.3018 normal_loss: 0.9486
[epoch 32][iter    0] loss: 131213.2969 RMSElog: 9.6860 grad_loss: 13110.7217 normal_loss: 0.9217
[epoch 32][iter   10] loss: 231973.6719 RMSElog: 10.2099 grad_loss: 23186.2090 normal_loss: 0.9490
[epoch 32][iter   20] loss: 143460.8594 RMSElog: 9.7520 grad_loss: 14335.3848 normal_loss: 0.9489
[epoch 32][iter   30] loss: 95221.2109 RMSElog: 9.1894 grad_loss: 9512.0205 normal_loss: 0.9115
[epoch 32][iter   40] loss: 180929.5938 RMSElog: 10.0745 grad_loss: 18081.9297 normal_loss: 0.9546
[epoch 32][iter   50] loss: 96006.2500 RMSElog: 9.5560 grad_loss: 9590.1396 normal_loss: 0.9294
[epoch 32][iter   60] loss: 105877.8750 RMSElog: 9.7638 grad_loss: 10577.0986 normal_loss: 0.9249
[epoch 32][iter   70] loss: 122446.5469 RMSElog: 9.6438 grad_loss: 12234.0742 normal_loss: 0.9366
[epoch 32][iter   80] loss: 162363.8906 RMSElog: 9.7464 grad_loss: 16225.6777 normal_loss: 0.9661
[epoch 32][iter   90] loss: 135966.2969 RMSElog: 9.6504 grad_loss: 13586.0273 normal_loss: 0.9522
[epoch 32][iter  100] loss: 104749.2812 RMSElog: 9.7512 grad_loss: 10464.2490 normal_loss: 0.9281
[epoch 32][iter  110] loss: 115534.3750 RMSElog: 9.7196 grad_loss: 11542.7852 normal_loss: 0.9325
[epoch 32][iter  120] loss: 103798.3281 RMSElog: 9.6193 grad_loss: 10369.2812 normal_loss: 0.9331
[epoch 32][iter  130] loss: 107073.5156 RMSElog: 9.7289 grad_loss: 10696.6914 normal_loss: 0.9313
[epoch 32][iter  140] loss: 154310.0938 RMSElog: 9.9148 grad_loss: 15420.1445 normal_loss: 0.9495
[epoch 32][iter  150] loss: 147371.3906 RMSElog: 9.6635 grad_loss: 14726.5488 normal_loss: 0.9280
[epoch 32][iter  160] loss: 109148.7344 RMSElog: 10.1172 grad_loss: 10903.8262 normal_loss: 0.9294
[epoch 32][iter  170] loss: 175002.7344 RMSElog: 9.8652 grad_loss: 17489.4648 normal_loss: 0.9436
[epoch 32][iter  180] loss: 115766.5312 RMSElog: 9.7467 grad_loss: 11565.9717 normal_loss: 0.9344
[epoch 32][iter  190] loss: 196842.3125 RMSElog: 10.1249 grad_loss: 19673.1523 normal_loss: 0.9534
[epoch 32][iter  200] loss: 120765.1875 RMSElog: 9.5441 grad_loss: 12066.0400 normal_loss: 0.9348
[epoch 32][iter  210] loss: 135814.7969 RMSElog: 9.5386 grad_loss: 13571.0137 normal_loss: 0.9281
[epoch 32][iter  220] loss: 196025.7031 RMSElog: 10.2650 grad_loss: 19591.3496 normal_loss: 0.9559
[epoch 32][iter  230] loss: 186053.2812 RMSElog: 10.1470 grad_loss: 18594.2305 normal_loss: 0.9504
[epoch 32][iter  240] loss: 162835.5469 RMSElog: 9.9145 grad_loss: 16272.6914 normal_loss: 0.9492
[epoch 32][iter  250] loss: 175122.9531 RMSElog: 9.9588 grad_loss: 17501.3789 normal_loss: 0.9573
[epoch 32][iter  260] loss: 161813.2500 RMSElog: 9.9875 grad_loss: 16170.3857 normal_loss: 0.9508
[epoch 32][iter  270] loss: 177195.4844 RMSElog: 10.1516 grad_loss: 17708.4512 normal_loss: 0.9458
[epoch 32][iter  280] loss: 177292.4062 RMSElog: 10.1195 grad_loss: 17718.1602 normal_loss: 0.9615
[epoch 32][iter  290] loss: 156253.7656 RMSElog: 9.8231 grad_loss: 15614.6211 normal_loss: 0.9316
[epoch 32][iter  300] loss: 162433.1875 RMSElog: 9.8183 grad_loss: 16232.5488 normal_loss: 0.9510
[epoch 32][iter  310] loss: 164597.1094 RMSElog: 9.8631 grad_loss: 16448.8984 normal_loss: 0.9502
[epoch 32][iter  320] loss: 185902.4219 RMSElog: 9.8998 grad_loss: 18579.3711 normal_loss: 0.9705
[epoch 32][iter  330] loss: 195440.9062 RMSElog: 9.7403 grad_loss: 19533.3926 normal_loss: 0.9572
[epoch 32][iter  340] loss: 174375.6875 RMSElog: 10.0512 grad_loss: 17426.5684 normal_loss: 0.9483
[epoch 32][iter  350] loss: 144589.9531 RMSElog: 9.7438 grad_loss: 14448.3340 normal_loss: 0.9167
[epoch 32][iter  360] loss: 119923.2031 RMSElog: 9.5699 grad_loss: 11981.8154 normal_loss: 0.9342
[epoch 32][iter  370] loss: 177699.9688 RMSElog: 9.4851 grad_loss: 17759.5488 normal_loss: 0.9621
[epoch 32][iter  380] loss: 111914.6250 RMSElog: 9.3004 grad_loss: 11181.2402 normal_loss: 0.9218
[epoch 32][iter  390] loss: 155762.3438 RMSElog: 9.8487 grad_loss: 15565.4404 normal_loss: 0.9448
[epoch 32][iter  400] loss: 119429.4609 RMSElog: 9.6388 grad_loss: 11932.3740 normal_loss: 0.9339
[epoch 32][iter  410] loss: 133013.4531 RMSElog: 9.5635 grad_loss: 13290.8457 normal_loss: 0.9356
[epoch 32][iter  420] loss: 159340.4375 RMSElog: 9.9507 grad_loss: 15923.1426 normal_loss: 0.9500
[epoch 32][iter  430] loss: 128979.7266 RMSElog: 9.7049 grad_loss: 12887.3350 normal_loss: 0.9327
[epoch 32][iter  440] loss: 182318.8125 RMSElog: 10.1008 grad_loss: 18220.8418 normal_loss: 0.9384
[epoch 32][iter  450] loss: 142696.6875 RMSElog: 9.9612 grad_loss: 14258.7588 normal_loss: 0.9493
[epoch 32][iter  460] loss: 218757.0312 RMSElog: 10.1228 grad_loss: 21864.6270 normal_loss: 0.9524
[epoch 32][iter  470] loss: 204650.3281 RMSElog: 10.1934 grad_loss: 20453.8633 normal_loss: 0.9762
[epoch 32][iter  480] loss: 139098.6406 RMSElog: 9.8619 grad_loss: 13899.0508 normal_loss: 0.9510
[epoch 32][iter  490] loss: 216232.0312 RMSElog: 10.2147 grad_loss: 21612.0234 normal_loss: 0.9640
[epoch 32][iter  500] loss: 115348.2734 RMSElog: 9.7235 grad_loss: 11524.1562 normal_loss: 0.9470
[epoch 32][iter  510] loss: 120292.3359 RMSElog: 9.8042 grad_loss: 12018.4854 normal_loss: 0.9430
[epoch 32][iter  520] loss: 131434.0312 RMSElog: 9.5911 grad_loss: 13132.8770 normal_loss: 0.9360
[epoch 32][iter  530] loss: 104340.6328 RMSElog: 9.7692 grad_loss: 10423.3701 normal_loss: 0.9234
[epoch 32][iter  540] loss: 161646.4688 RMSElog: 10.1105 grad_loss: 16153.5762 normal_loss: 0.9608
[epoch 32][iter  550] loss: 174761.2344 RMSElog: 9.7333 grad_loss: 17465.4512 normal_loss: 0.9393
[epoch 32][iter  560] loss: 138057.5156 RMSElog: 9.5188 grad_loss: 13795.3184 normal_loss: 0.9138
[epoch 32][iter  570] loss: 120440.5938 RMSElog: 9.1890 grad_loss: 12033.9463 normal_loss: 0.9240
[epoch 32][iter  580] loss: 128319.4141 RMSElog: 9.6733 grad_loss: 12821.3398 normal_loss: 0.9287
[epoch 32][iter  590] loss: 191825.0781 RMSElog: 10.3012 grad_loss: 19171.2500 normal_loss: 0.9570
[epoch 33][iter    0] loss: 189017.5781 RMSElog: 10.1390 grad_loss: 18890.6602 normal_loss: 0.9589
[epoch 33][iter   10] loss: 149128.2188 RMSElog: 9.7262 grad_loss: 14902.1533 normal_loss: 0.9419
[epoch 33][iter   20] loss: 113921.7969 RMSElog: 9.7251 grad_loss: 11381.5195 normal_loss: 0.9357
[epoch 33][iter   30] loss: 195440.9062 RMSElog: 9.7403 grad_loss: 19533.3926 normal_loss: 0.9572
[epoch 33][iter   40] loss: 103509.2188 RMSElog: 9.9787 grad_loss: 10340.0186 normal_loss: 0.9251
[epoch 33][iter   50] loss: 152549.8750 RMSElog: 9.9928 grad_loss: 15244.0293 normal_loss: 0.9654
[epoch 33][iter   60] loss: 177664.5312 RMSElog: 9.7808 grad_loss: 17755.7344 normal_loss: 0.9369
[epoch 33][iter   70] loss: 162372.7500 RMSElog: 9.8226 grad_loss: 16226.4922 normal_loss: 0.9603
[epoch 33][iter   80] loss: 194987.6250 RMSElog: 9.8416 grad_loss: 19487.9453 normal_loss: 0.9748
[epoch 33][iter   90] loss: 141442.1250 RMSElog: 9.6437 grad_loss: 14133.6191 normal_loss: 0.9495
[epoch 33][iter  100] loss: 214775.1719 RMSElog: 10.1719 grad_loss: 21466.3848 normal_loss: 0.9613
[epoch 33][iter  110] loss: 159798.6094 RMSElog: 9.8075 grad_loss: 15969.1260 normal_loss: 0.9263
[epoch 33][iter  120] loss: 178593.9688 RMSElog: 10.0081 grad_loss: 17848.4395 normal_loss: 0.9490
[epoch 33][iter  130] loss: 218457.8281 RMSElog: 10.1255 grad_loss: 21834.7168 normal_loss: 0.9418
[epoch 33][iter  140] loss: 139098.6406 RMSElog: 9.8619 grad_loss: 13899.0508 normal_loss: 0.9510
[epoch 33][iter  150] loss: 172868.6875 RMSElog: 10.0539 grad_loss: 17275.8652 normal_loss: 0.9500
[epoch 33][iter  160] loss: 177195.4844 RMSElog: 10.1516 grad_loss: 17708.4512 normal_loss: 0.9458
[epoch 33][iter  170] loss: 141118.5312 RMSElog: 10.0520 grad_loss: 14100.8545 normal_loss: 0.9477
[epoch 33][iter  180] loss: 106516.8047 RMSElog: 9.6966 grad_loss: 10641.0557 normal_loss: 0.9285
[epoch 33][iter  190] loss: 127499.3281 RMSElog: 9.2769 grad_loss: 12739.7178 normal_loss: 0.9374
[epoch 33][iter  200] loss: 103514.7734 RMSElog: 9.3451 grad_loss: 10341.2100 normal_loss: 0.9229
[epoch 33][iter  210] loss: 175301.7344 RMSElog: 9.8017 grad_loss: 17519.4180 normal_loss: 0.9555
[epoch 33][iter  220] loss: 143904.7812 RMSElog: 10.0263 grad_loss: 14379.5068 normal_loss: 0.9457
[epoch 33][iter  230] loss: 102223.4531 RMSElog: 9.4278 grad_loss: 10212.0010 normal_loss: 0.9169
[epoch 33][iter  240] loss: 163171.1719 RMSElog: 9.7905 grad_loss: 16306.3613 normal_loss: 0.9652
[epoch 33][iter  250] loss: 186053.2969 RMSElog: 10.1470 grad_loss: 18594.2324 normal_loss: 0.9504
[epoch 33][iter  260] loss: 144440.4531 RMSElog: 9.8843 grad_loss: 14433.2314 normal_loss: 0.9296
[epoch 33][iter  270] loss: 154705.2188 RMSElog: 9.9142 grad_loss: 15459.6543 normal_loss: 0.9527
[epoch 33][iter  280] loss: 196842.3125 RMSElog: 10.1249 grad_loss: 19673.1523 normal_loss: 0.9534
[epoch 33][iter  290] loss: 107073.5156 RMSElog: 9.7289 grad_loss: 10696.6914 normal_loss: 0.9313
[epoch 33][iter  300] loss: 132425.4375 RMSElog: 10.0155 grad_loss: 13231.5801 normal_loss: 0.9468
[epoch 33][iter  310] loss: 161633.2812 RMSElog: 10.0107 grad_loss: 16152.3652 normal_loss: 0.9518
[epoch 33][iter  320] loss: 104340.6328 RMSElog: 9.7692 grad_loss: 10423.3701 normal_loss: 0.9234
[epoch 33][iter  330] loss: 190972.3281 RMSElog: 10.1152 grad_loss: 19086.1523 normal_loss: 0.9647
[epoch 33][iter  340] loss: 178784.1250 RMSElog: 9.9460 grad_loss: 17867.5078 normal_loss: 0.9592
[epoch 33][iter  350] loss: 119923.2031 RMSElog: 9.5699 grad_loss: 11981.8154 normal_loss: 0.9342
[epoch 33][iter  360] loss: 175547.8906 RMSElog: 10.1409 grad_loss: 17543.7148 normal_loss: 0.9342
[epoch 33][iter  370] loss: 157425.7188 RMSElog: 9.9235 grad_loss: 15731.7012 normal_loss: 0.9477
[epoch 33][iter  380] loss: 140645.6875 RMSElog: 10.1118 grad_loss: 14053.4873 normal_loss: 0.9695
[epoch 33][iter  390] loss: 164843.6875 RMSElog: 9.7972 grad_loss: 16473.6367 normal_loss: 0.9360
[epoch 33][iter  400] loss: 179901.4844 RMSElog: 9.8267 grad_loss: 17979.3633 normal_loss: 0.9589
[epoch 33][iter  410] loss: 147371.3906 RMSElog: 9.6635 grad_loss: 14726.5488 normal_loss: 0.9280
[epoch 33][iter  420] loss: 103534.6562 RMSElog: 9.7670 grad_loss: 10342.7715 normal_loss: 0.9280
[epoch 33][iter  430] loss: 160642.9375 RMSElog: 9.9306 grad_loss: 16053.4121 normal_loss: 0.9504
[epoch 33][iter  440] loss: 158630.9688 RMSElog: 9.8321 grad_loss: 15852.3232 normal_loss: 0.9428
[epoch 33][iter  450] loss: 122912.0625 RMSElog: 9.7328 grad_loss: 12280.5225 normal_loss: 0.9509
[epoch 33][iter  460] loss: 111914.6250 RMSElog: 9.3004 grad_loss: 11181.2402 normal_loss: 0.9218
[epoch 33][iter  470] loss: 168409.1562 RMSElog: 9.7031 grad_loss: 16830.2910 normal_loss: 0.9228
[epoch 33][iter  480] loss: 235211.2344 RMSElog: 10.3371 grad_loss: 23509.8398 normal_loss: 0.9452
[epoch 33][iter  490] loss: 210654.5312 RMSElog: 10.4982 grad_loss: 21054.0039 normal_loss: 0.9517
[epoch 33][iter  500] loss: 180929.5938 RMSElog: 10.0745 grad_loss: 18081.9297 normal_loss: 0.9546
[epoch 33][iter  510] loss: 144589.9531 RMSElog: 9.7438 grad_loss: 14448.3340 normal_loss: 0.9167
[epoch 33][iter  520] loss: 108800.2422 RMSElog: 9.7538 grad_loss: 10869.3369 normal_loss: 0.9334
[epoch 33][iter  530] loss: 196920.5625 RMSElog: 10.2474 grad_loss: 19680.8320 normal_loss: 0.9760
[epoch 33][iter  540] loss: 133013.4531 RMSElog: 9.5635 grad_loss: 13290.8457 normal_loss: 0.9356
[epoch 33][iter  550] loss: 152149.7500 RMSElog: 9.9062 grad_loss: 15204.1221 normal_loss: 0.9462
[epoch 33][iter  560] loss: 108830.7344 RMSElog: 9.7359 grad_loss: 10872.4062 normal_loss: 0.9307
[epoch 33][iter  570] loss: 195507.4062 RMSElog: 10.1854 grad_loss: 19539.6016 normal_loss: 0.9533
[epoch 33][iter  580] loss: 162711.9688 RMSElog: 9.6160 grad_loss: 16260.6387 normal_loss: 0.9427
[epoch 33][iter  590] loss: 160024.9375 RMSElog: 9.8489 grad_loss: 15991.7236 normal_loss: 0.9215
[epoch 34][iter    0] loss: 224943.8906 RMSElog: 10.1206 grad_loss: 22483.3125 normal_loss: 0.9551
[epoch 34][iter   10] loss: 170690.9219 RMSElog: 9.9328 grad_loss: 17058.2109 normal_loss: 0.9470
[epoch 34][iter   20] loss: 183480.3438 RMSElog: 10.0091 grad_loss: 18337.0801 normal_loss: 0.9458
[epoch 34][iter   30] loss: 161088.9688 RMSElog: 9.9258 grad_loss: 16098.0254 normal_loss: 0.9455
[epoch 34][iter   40] loss: 118746.3359 RMSElog: 9.7278 grad_loss: 11863.9629 normal_loss: 0.9433
[epoch 34][iter   50] loss: 198036.3750 RMSElog: 10.2619 grad_loss: 19792.4219 normal_loss: 0.9529
[epoch 34][iter   60] loss: 180208.2188 RMSElog: 10.0490 grad_loss: 18009.8223 normal_loss: 0.9517
[epoch 34][iter   70] loss: 154626.7500 RMSElog: 9.9098 grad_loss: 15451.8057 normal_loss: 0.9596
[epoch 34][iter   80] loss: 202630.0781 RMSElog: 10.0417 grad_loss: 20252.0078 normal_loss: 0.9589
[epoch 34][iter   90] loss: 201503.0625 RMSElog: 10.3149 grad_loss: 20139.0410 normal_loss: 0.9514
[epoch 34][iter  100] loss: 239768.0938 RMSElog: 10.2366 grad_loss: 23965.6250 normal_loss: 0.9466
[epoch 34][iter  110] loss: 148136.9375 RMSElog: 9.9557 grad_loss: 14802.7871 normal_loss: 0.9498
[epoch 34][iter  120] loss: 136507.7344 RMSElog: 10.0223 grad_loss: 13639.7900 normal_loss: 0.9608
[epoch 34][iter  130] loss: 120440.5938 RMSElog: 9.1890 grad_loss: 12033.9463 normal_loss: 0.9240
[epoch 34][iter  140] loss: 123289.4609 RMSElog: 9.4106 grad_loss: 12318.5752 normal_loss: 0.9606
[epoch 34][iter  150] loss: 116073.5938 RMSElog: 9.6844 grad_loss: 11596.7324 normal_loss: 0.9429
[epoch 34][iter  160] loss: 160595.2188 RMSElog: 9.9514 grad_loss: 16048.6182 normal_loss: 0.9518
[epoch 34][iter  170] loss: 129163.5156 RMSElog: 9.7674 grad_loss: 12905.6660 normal_loss: 0.9175
[epoch 34][iter  180] loss: 122912.0625 RMSElog: 9.7328 grad_loss: 12280.5225 normal_loss: 0.9509
[epoch 34][iter  190] loss: 107472.2578 RMSElog: 9.0654 grad_loss: 10737.2480 normal_loss: 0.9124
[epoch 34][iter  200] loss: 176260.7500 RMSElog: 10.0638 grad_loss: 17615.0703 normal_loss: 0.9395
[epoch 34][iter  210] loss: 163268.1250 RMSElog: 9.9218 grad_loss: 16315.9277 normal_loss: 0.9630
[epoch 34][iter  220] loss: 149128.2188 RMSElog: 9.7262 grad_loss: 14902.1533 normal_loss: 0.9419
[epoch 34][iter  230] loss: 147479.0312 RMSElog: 9.6781 grad_loss: 14737.2812 normal_loss: 0.9446
[epoch 34][iter  240] loss: 142596.2344 RMSElog: 9.8922 grad_loss: 14248.7793 normal_loss: 0.9508
[epoch 34][iter  250] loss: 127499.3281 RMSElog: 9.2769 grad_loss: 12739.7178 normal_loss: 0.9374
[epoch 34][iter  260] loss: 191371.4375 RMSElog: 10.1918 grad_loss: 19126.0020 normal_loss: 0.9504
[epoch 34][iter  270] loss: 162835.5312 RMSElog: 9.9145 grad_loss: 16272.6904 normal_loss: 0.9492
[epoch 34][iter  280] loss: 114149.3672 RMSElog: 9.2655 grad_loss: 11404.7637 normal_loss: 0.9073
[epoch 34][iter  290] loss: 128277.9766 RMSElog: 9.7565 grad_loss: 12817.0889 normal_loss: 0.9518
[epoch 34][iter  300] loss: 120765.1875 RMSElog: 9.5441 grad_loss: 12066.0400 normal_loss: 0.9348
[epoch 34][iter  310] loss: 120292.3359 RMSElog: 9.8042 grad_loss: 12018.4854 normal_loss: 0.9430
[epoch 34][iter  320] loss: 146444.9062 RMSElog: 10.1139 grad_loss: 14633.4365 normal_loss: 0.9403
[epoch 34][iter  330] loss: 122386.0234 RMSElog: 9.6688 grad_loss: 12227.9971 normal_loss: 0.9366
[epoch 34][iter  340] loss: 151368.4219 RMSElog: 9.7883 grad_loss: 15126.1289 normal_loss: 0.9251
[epoch 34][iter  350] loss: 168545.0938 RMSElog: 9.6168 grad_loss: 16843.9434 normal_loss: 0.9495
[epoch 34][iter  360] loss: 156173.3594 RMSElog: 9.9681 grad_loss: 15606.4199 normal_loss: 0.9482
[epoch 34][iter  370] loss: 191559.9688 RMSElog: 9.9749 grad_loss: 19145.0742 normal_loss: 0.9477
[epoch 34][iter  380] loss: 172755.6875 RMSElog: 10.0251 grad_loss: 17264.6016 normal_loss: 0.9419
[epoch 34][iter  390] loss: 167784.2500 RMSElog: 9.9330 grad_loss: 16767.5391 normal_loss: 0.9539
[epoch 34][iter  400] loss: 180318.6719 RMSElog: 9.9530 grad_loss: 18020.9785 normal_loss: 0.9357
[epoch 34][iter  410] loss: 142544.5156 RMSElog: 10.1645 grad_loss: 14243.3457 normal_loss: 0.9425
[epoch 34][iter  420] loss: 111914.6406 RMSElog: 9.3004 grad_loss: 11181.2412 normal_loss: 0.9218
[epoch 34][iter  430] loss: 206083.5156 RMSElog: 10.2501 grad_loss: 20597.1445 normal_loss: 0.9562
[epoch 34][iter  440] loss: 142544.7031 RMSElog: 10.2185 grad_loss: 14243.3027 normal_loss: 0.9486
[epoch 34][iter  450] loss: 218547.0156 RMSElog: 10.4419 grad_loss: 21843.3086 normal_loss: 0.9515
[epoch 34][iter  460] loss: 131942.8750 RMSElog: 9.8414 grad_loss: 13183.5000 normal_loss: 0.9453
[epoch 34][iter  470] loss: 182476.7344 RMSElog: 9.9672 grad_loss: 18236.7656 normal_loss: 0.9407
[epoch 34][iter  480] loss: 174565.0781 RMSElog: 9.7759 grad_loss: 17445.7832 normal_loss: 0.9484
[epoch 34][iter  490] loss: 142005.0781 RMSElog: 9.9168 grad_loss: 14189.6533 normal_loss: 0.9371
[epoch 34][iter  500] loss: 127061.7500 RMSElog: 9.6726 grad_loss: 12695.5654 normal_loss: 0.9369
[epoch 34][iter  510] loss: 110579.5391 RMSElog: 9.3019 grad_loss: 11047.7109 normal_loss: 0.9413
[epoch 34][iter  520] loss: 226865.1406 RMSElog: 10.2887 grad_loss: 22675.2832 normal_loss: 0.9418
[epoch 34][iter  530] loss: 144987.0938 RMSElog: 9.8653 grad_loss: 14487.9062 normal_loss: 0.9378
[epoch 34][iter  540] loss: 115766.5312 RMSElog: 9.7467 grad_loss: 11565.9717 normal_loss: 0.9344
[epoch 34][iter  550] loss: 158034.0625 RMSElog: 9.9527 grad_loss: 15792.5088 normal_loss: 0.9445
[epoch 34][iter  560] loss: 103798.3281 RMSElog: 9.6193 grad_loss: 10369.2812 normal_loss: 0.9331
[epoch 34][iter  570] loss: 140645.6875 RMSElog: 10.1118 grad_loss: 14053.4873 normal_loss: 0.9695
[epoch 34][iter  580] loss: 173855.4375 RMSElog: 10.0769 grad_loss: 17374.5098 normal_loss: 0.9566
[epoch 34][iter  590] loss: 164766.2812 RMSElog: 9.6928 grad_loss: 16465.9941 normal_loss: 0.9411
[epoch 35][iter    0] loss: 233395.7188 RMSElog: 10.2368 grad_loss: 23328.3906 normal_loss: 0.9452
[epoch 35][iter   10] loss: 105628.3672 RMSElog: 10.0899 grad_loss: 10551.8262 normal_loss: 0.9213
[epoch 35][iter   20] loss: 139830.9062 RMSElog: 9.7552 grad_loss: 13972.3770 normal_loss: 0.9585
[epoch 35][iter   30] loss: 106769.7500 RMSElog: 9.8688 grad_loss: 10666.1914 normal_loss: 0.9140
[epoch 35][iter   40] loss: 111712.9219 RMSElog: 9.7826 grad_loss: 11160.5811 normal_loss: 0.9290
[epoch 35][iter   50] loss: 180318.6719 RMSElog: 9.9530 grad_loss: 18020.9785 normal_loss: 0.9357
[epoch 35][iter   60] loss: 178468.8906 RMSElog: 9.9039 grad_loss: 17836.0469 normal_loss: 0.9380
[epoch 35][iter   70] loss: 112697.1016 RMSElog: 9.7295 grad_loss: 11259.0449 normal_loss: 0.9352
[epoch 35][iter   80] loss: 198293.9219 RMSElog: 10.2275 grad_loss: 19818.2148 normal_loss: 0.9521
[epoch 35][iter   90] loss: 217845.4844 RMSElog: 10.3597 grad_loss: 21773.2129 normal_loss: 0.9774
[epoch 35][iter  100] loss: 115534.3750 RMSElog: 9.7196 grad_loss: 11542.7852 normal_loss: 0.9325
[epoch 35][iter  110] loss: 156253.7656 RMSElog: 9.8231 grad_loss: 15614.6211 normal_loss: 0.9316
[epoch 35][iter  120] loss: 188717.5625 RMSElog: 10.0336 grad_loss: 18860.7715 normal_loss: 0.9509
[epoch 35][iter  130] loss: 197520.7500 RMSElog: 10.1663 grad_loss: 19740.9414 normal_loss: 0.9662
[epoch 35][iter  140] loss: 143613.7656 RMSElog: 9.8882 grad_loss: 14350.5566 normal_loss: 0.9315
[epoch 35][iter  150] loss: 138469.0469 RMSElog: 9.4787 grad_loss: 13836.4834 normal_loss: 0.9438
[epoch 35][iter  160] loss: 163389.5625 RMSElog: 9.7702 grad_loss: 16328.2656 normal_loss: 0.9208
[epoch 35][iter  170] loss: 154626.7500 RMSElog: 9.9098 grad_loss: 15451.8057 normal_loss: 0.9596
[epoch 35][iter  180] loss: 201373.6719 RMSElog: 10.3870 grad_loss: 20126.0312 normal_loss: 0.9491
[epoch 35][iter  190] loss: 101101.5547 RMSElog: 9.9274 grad_loss: 10099.2979 normal_loss: 0.9297
[epoch 35][iter  200] loss: 234945.1719 RMSElog: 10.0015 grad_loss: 23483.5664 normal_loss: 0.9500
[epoch 35][iter  210] loss: 166010.9531 RMSElog: 9.9249 grad_loss: 16590.2227 normal_loss: 0.9481
[epoch 35][iter  220] loss: 139433.9219 RMSElog: 9.8139 grad_loss: 13932.6191 normal_loss: 0.9591
[epoch 35][iter  230] loss: 168688.8125 RMSElog: 9.8493 grad_loss: 16858.0781 normal_loss: 0.9524
[epoch 35][iter  240] loss: 133760.0469 RMSElog: 10.0653 grad_loss: 13364.9717 normal_loss: 0.9680
[epoch 35][iter  250] loss: 201503.0625 RMSElog: 10.3149 grad_loss: 20139.0410 normal_loss: 0.9514
[epoch 35][iter  260] loss: 141865.3906 RMSElog: 9.7199 grad_loss: 14175.8896 normal_loss: 0.9298
[epoch 35][iter  270] loss: 113426.9531 RMSElog: 9.7682 grad_loss: 11331.9902 normal_loss: 0.9362
[epoch 35][iter  280] loss: 106182.2031 RMSElog: 9.7575 grad_loss: 10607.5332 normal_loss: 0.9294
[epoch 35][iter  290] loss: 121916.0000 RMSElog: 9.7464 grad_loss: 12180.9131 normal_loss: 0.9404
[epoch 35][iter  300] loss: 183095.1562 RMSElog: 9.9762 grad_loss: 18298.5703 normal_loss: 0.9696
[epoch 35][iter  310] loss: 185071.7188 RMSElog: 10.0456 grad_loss: 18496.1719 normal_loss: 0.9557
[epoch 35][iter  320] loss: 194987.6250 RMSElog: 9.8416 grad_loss: 19487.9453 normal_loss: 0.9748
[epoch 35][iter  330] loss: 135966.2969 RMSElog: 9.6504 grad_loss: 13586.0273 normal_loss: 0.9522
[epoch 35][iter  340] loss: 170357.3438 RMSElog: 9.9687 grad_loss: 17024.8008 normal_loss: 0.9644
[epoch 35][iter  350] loss: 137252.6406 RMSElog: 9.3538 grad_loss: 13714.9795 normal_loss: 0.9319
[epoch 35][iter  360] loss: 130739.4062 RMSElog: 9.5471 grad_loss: 13063.4434 normal_loss: 0.9498
[epoch 35][iter  370] loss: 118746.3359 RMSElog: 9.7278 grad_loss: 11863.9629 normal_loss: 0.9433
[epoch 35][iter  380] loss: 156173.3594 RMSElog: 9.9681 grad_loss: 15606.4199 normal_loss: 0.9482
[epoch 35][iter  390] loss: 103355.8125 RMSElog: 9.7106 grad_loss: 10324.9453 normal_loss: 0.9244
[epoch 35][iter  400] loss: 137764.4219 RMSElog: 9.8227 grad_loss: 13765.6758 normal_loss: 0.9442
[epoch 35][iter  410] loss: 193978.6250 RMSElog: 10.3530 grad_loss: 19386.5586 normal_loss: 0.9518
[epoch 35][iter  420] loss: 134406.2031 RMSElog: 9.9402 grad_loss: 13429.7256 normal_loss: 0.9539
[epoch 35][iter  430] loss: 160782.5156 RMSElog: 9.7710 grad_loss: 16067.5547 normal_loss: 0.9244
[epoch 35][iter  440] loss: 99107.7500 RMSElog: 9.5230 grad_loss: 9900.3428 normal_loss: 0.9094
[epoch 35][iter  450] loss: 154029.7812 RMSElog: 9.8645 grad_loss: 15392.1729 normal_loss: 0.9405
[epoch 35][iter  460] loss: 138950.6406 RMSElog: 9.9777 grad_loss: 13884.1387 normal_loss: 0.9482
[epoch 35][iter  470] loss: 152593.5000 RMSElog: 9.8773 grad_loss: 15248.5439 normal_loss: 0.9283
[epoch 35][iter  480] loss: 184016.5469 RMSElog: 9.9468 grad_loss: 18390.7363 normal_loss: 0.9712
[epoch 35][iter  490] loss: 184374.2188 RMSElog: 10.2641 grad_loss: 18426.1992 normal_loss: 0.9596
[epoch 35][iter  500] loss: 154705.2344 RMSElog: 9.9142 grad_loss: 15459.6562 normal_loss: 0.9527
[epoch 35][iter  510] loss: 164415.3438 RMSElog: 10.0829 grad_loss: 16430.4961 normal_loss: 0.9571
[epoch 35][iter  520] loss: 128319.4141 RMSElog: 9.6733 grad_loss: 12821.3398 normal_loss: 0.9287
[epoch 35][iter  530] loss: 103948.1875 RMSElog: 9.7608 grad_loss: 10384.1309 normal_loss: 0.9264
[epoch 35][iter  540] loss: 113895.6328 RMSElog: 9.2606 grad_loss: 11379.3926 normal_loss: 0.9106
[epoch 35][iter  550] loss: 100663.8984 RMSElog: 9.9598 grad_loss: 10055.5020 normal_loss: 0.9277
[epoch 35][iter  560] loss: 159422.2656 RMSElog: 9.7010 grad_loss: 15931.5957 normal_loss: 0.9299
[epoch 35][iter  570] loss: 103305.2422 RMSElog: 9.5543 grad_loss: 10320.0674 normal_loss: 0.9024
[epoch 35][iter  580] loss: 208699.7031 RMSElog: 10.3419 grad_loss: 20858.6719 normal_loss: 0.9568
[epoch 35][iter  590] loss: 109441.5234 RMSElog: 9.7466 grad_loss: 10933.4756 normal_loss: 0.9300
[epoch 36][iter    0] loss: 146444.9062 RMSElog: 10.1139 grad_loss: 14633.4365 normal_loss: 0.9403
[epoch 36][iter   10] loss: 122446.5469 RMSElog: 9.6438 grad_loss: 12234.0742 normal_loss: 0.9366
[epoch 36][iter   20] loss: 149036.1562 RMSElog: 9.7728 grad_loss: 14892.8887 normal_loss: 0.9547
[epoch 36][iter   30] loss: 154290.2500 RMSElog: 9.8471 grad_loss: 15418.2334 normal_loss: 0.9458
[epoch 36][iter   40] loss: 149387.5469 RMSElog: 9.9550 grad_loss: 14927.8516 normal_loss: 0.9483
[epoch 36][iter   50] loss: 149540.3750 RMSElog: 9.7408 grad_loss: 14943.3740 normal_loss: 0.9229
[epoch 36][iter   60] loss: 160074.2031 RMSElog: 9.7506 grad_loss: 15996.7441 normal_loss: 0.9257
[epoch 36][iter   70] loss: 202630.0781 RMSElog: 10.0417 grad_loss: 20252.0078 normal_loss: 0.9589
[epoch 36][iter   80] loss: 195440.9062 RMSElog: 9.7403 grad_loss: 19533.3926 normal_loss: 0.9572
[epoch 36][iter   90] loss: 208770.3906 RMSElog: 10.1964 grad_loss: 20865.8984 normal_loss: 0.9442
[epoch 36][iter  100] loss: 208184.3438 RMSElog: 10.4061 grad_loss: 20807.0801 normal_loss: 0.9472
[epoch 36][iter  110] loss: 195412.9844 RMSElog: 10.0493 grad_loss: 19530.2969 normal_loss: 0.9539
[epoch 36][iter  120] loss: 216509.5938 RMSElog: 10.1942 grad_loss: 21639.8242 normal_loss: 0.9420
[epoch 36][iter  130] loss: 120440.5938 RMSElog: 9.1890 grad_loss: 12033.9463 normal_loss: 0.9240
[epoch 36][iter  140] loss: 143613.7656 RMSElog: 9.8882 grad_loss: 14350.5566 normal_loss: 0.9315
[epoch 36][iter  150] loss: 214775.1719 RMSElog: 10.1719 grad_loss: 21466.3848 normal_loss: 0.9613
[epoch 36][iter  160] loss: 147598.9844 RMSElog: 9.8981 grad_loss: 14749.0645 normal_loss: 0.9355
[epoch 36][iter  170] loss: 224943.8906 RMSElog: 10.1206 grad_loss: 22483.3125 normal_loss: 0.9551
[epoch 36][iter  180] loss: 166492.4375 RMSElog: 9.8376 grad_loss: 16638.4512 normal_loss: 0.9558
[epoch 36][iter  190] loss: 221213.4219 RMSElog: 10.1740 grad_loss: 22110.2266 normal_loss: 0.9415
[epoch 36][iter  200] loss: 147544.5000 RMSElog: 9.8913 grad_loss: 14743.6045 normal_loss: 0.9534
[epoch 36][iter  210] loss: 150321.2812 RMSElog: 9.9248 grad_loss: 15021.2656 normal_loss: 0.9385
[epoch 36][iter  220] loss: 218547.0156 RMSElog: 10.4419 grad_loss: 21843.3086 normal_loss: 0.9515
[epoch 36][iter  230] loss: 111655.4766 RMSElog: 9.2050 grad_loss: 11155.4395 normal_loss: 0.9029
[epoch 36][iter  240] loss: 100116.4922 RMSElog: 9.9235 grad_loss: 10000.7998 normal_loss: 0.9258
[epoch 36][iter  250] loss: 160743.5156 RMSElog: 9.9654 grad_loss: 16063.4521 normal_loss: 0.9340
[epoch 36][iter  260] loss: 153447.6875 RMSElog: 9.9051 grad_loss: 15333.9209 normal_loss: 0.9425
[epoch 36][iter  270] loss: 230519.3906 RMSElog: 10.3105 grad_loss: 23040.6836 normal_loss: 0.9459
[epoch 36][iter  280] loss: 133024.8594 RMSElog: 9.6937 grad_loss: 13291.8662 normal_loss: 0.9261
[epoch 36][iter  290] loss: 122331.3906 RMSElog: 9.7180 grad_loss: 12222.4795 normal_loss: 0.9418
[epoch 36][iter  300] loss: 115348.2734 RMSElog: 9.7235 grad_loss: 11524.1562 normal_loss: 0.9470
[epoch 36][iter  310] loss: 151354.1562 RMSElog: 9.8837 grad_loss: 15124.5889 normal_loss: 0.9428
[epoch 36][iter  320] loss: 121838.3672 RMSElog: 9.7453 grad_loss: 12173.1494 normal_loss: 0.9425
[epoch 36][iter  330] loss: 174375.6875 RMSElog: 10.0512 grad_loss: 17426.5684 normal_loss: 0.9483
[epoch 36][iter  340] loss: 162892.2969 RMSElog: 9.8252 grad_loss: 16278.4717 normal_loss: 0.9331
[epoch 36][iter  350] loss: 153724.0000 RMSElog: 9.9609 grad_loss: 15361.4922 normal_loss: 0.9475
[epoch 36][iter  360] loss: 217845.5000 RMSElog: 10.3597 grad_loss: 21773.2148 normal_loss: 0.9774
[epoch 36][iter  370] loss: 150699.3750 RMSElog: 9.8567 grad_loss: 15059.1328 normal_loss: 0.9486
[epoch 36][iter  380] loss: 158875.5625 RMSElog: 9.5853 grad_loss: 15877.0449 normal_loss: 0.9260
[epoch 36][iter  390] loss: 144987.0938 RMSElog: 9.8653 grad_loss: 14487.9062 normal_loss: 0.9378
[epoch 36][iter  400] loss: 115658.3125 RMSElog: 9.1856 grad_loss: 11555.7031 normal_loss: 0.9424
[epoch 36][iter  410] loss: 127075.1875 RMSElog: 10.1217 grad_loss: 12696.4443 normal_loss: 0.9524
[epoch 36][iter  420] loss: 127499.3281 RMSElog: 9.2769 grad_loss: 12739.7178 normal_loss: 0.9374
[epoch 36][iter  430] loss: 167412.9531 RMSElog: 9.7311 grad_loss: 16730.6387 normal_loss: 0.9260
[epoch 36][iter  440] loss: 137252.6406 RMSElog: 9.3538 grad_loss: 13714.9795 normal_loss: 0.9319
[epoch 36][iter  450] loss: 162542.0469 RMSElog: 9.9393 grad_loss: 16243.3115 normal_loss: 0.9527
[epoch 36][iter  460] loss: 196437.8125 RMSElog: 10.2319 grad_loss: 19632.5898 normal_loss: 0.9584
[epoch 36][iter  470] loss: 177195.4844 RMSElog: 10.1516 grad_loss: 17708.4512 normal_loss: 0.9458
[epoch 36][iter  480] loss: 226949.5156 RMSElog: 10.2029 grad_loss: 22683.8047 normal_loss: 0.9430
[epoch 36][iter  490] loss: 130145.6250 RMSElog: 9.7206 grad_loss: 13003.9072 normal_loss: 0.9344
[epoch 36][iter  500] loss: 151368.4219 RMSElog: 9.7883 grad_loss: 15126.1289 normal_loss: 0.9251
[epoch 36][iter  510] loss: 127061.7500 RMSElog: 9.6726 grad_loss: 12695.5654 normal_loss: 0.9369
[epoch 36][iter  520] loss: 165306.9531 RMSElog: 9.8881 grad_loss: 16519.8438 normal_loss: 0.9638
[epoch 36][iter  530] loss: 122670.8984 RMSElog: 9.4365 grad_loss: 12256.7207 normal_loss: 0.9331
[epoch 36][iter  540] loss: 140938.8594 RMSElog: 9.7483 grad_loss: 14083.1953 normal_loss: 0.9424
[epoch 36][iter  550] loss: 130739.4062 RMSElog: 9.5471 grad_loss: 13063.4434 normal_loss: 0.9498
[epoch 36][iter  560] loss: 211925.8438 RMSElog: 10.1863 grad_loss: 21181.4551 normal_loss: 0.9436
[epoch 36][iter  570] loss: 184374.2188 RMSElog: 10.2641 grad_loss: 18426.1992 normal_loss: 0.9596
[epoch 36][iter  580] loss: 165020.2344 RMSElog: 9.7791 grad_loss: 16491.2910 normal_loss: 0.9530
[epoch 36][iter  590] loss: 117693.8984 RMSElog: 9.6527 grad_loss: 11758.8105 normal_loss: 0.9267
[epoch 37][iter    0] loss: 114509.4531 RMSElog: 9.7409 grad_loss: 11440.2627 normal_loss: 0.9413
[epoch 37][iter   10] loss: 179740.0625 RMSElog: 10.1206 grad_loss: 17962.9453 normal_loss: 0.9397
[epoch 37][iter   20] loss: 150608.8906 RMSElog: 10.0680 grad_loss: 15049.8750 normal_loss: 0.9460
[epoch 37][iter   30] loss: 111518.2734 RMSElog: 9.6257 grad_loss: 11141.2676 normal_loss: 0.9335
[epoch 37][iter   40] loss: 114310.3047 RMSElog: 9.4026 grad_loss: 11420.7217 normal_loss: 0.9063
[epoch 37][iter   50] loss: 133760.0469 RMSElog: 10.0653 grad_loss: 13364.9717 normal_loss: 0.9680
[epoch 37][iter   60] loss: 182187.7188 RMSElog: 10.0835 grad_loss: 18207.7461 normal_loss: 0.9413
[epoch 37][iter   70] loss: 181983.5312 RMSElog: 10.0741 grad_loss: 18187.3320 normal_loss: 0.9464
[epoch 37][iter   80] loss: 185902.4219 RMSElog: 9.8998 grad_loss: 18579.3711 normal_loss: 0.9705
[epoch 37][iter   90] loss: 100749.6875 RMSElog: 9.9914 grad_loss: 10064.0400 normal_loss: 0.9377
[epoch 37][iter  100] loss: 232982.0469 RMSElog: 10.2845 grad_loss: 23286.9707 normal_loss: 0.9500
[epoch 37][iter  110] loss: 162976.7812 RMSElog: 9.6016 grad_loss: 16287.1572 normal_loss: 0.9201
[epoch 37][iter  120] loss: 228338.7500 RMSElog: 10.2936 grad_loss: 22822.6348 normal_loss: 0.9477
[epoch 37][iter  130] loss: 121983.8203 RMSElog: 9.5225 grad_loss: 12187.9473 normal_loss: 0.9124
[epoch 37][iter  140] loss: 150459.7656 RMSElog: 9.9429 grad_loss: 15035.0986 normal_loss: 0.9347
[epoch 37][iter  150] loss: 147544.5000 RMSElog: 9.8913 grad_loss: 14743.6045 normal_loss: 0.9534
[epoch 37][iter  160] loss: 180208.2188 RMSElog: 10.0490 grad_loss: 18009.8223 normal_loss: 0.9517
[epoch 37][iter  170] loss: 160466.8750 RMSElog: 9.9085 grad_loss: 16035.8252 normal_loss: 0.9540
[epoch 37][iter  180] loss: 115534.3672 RMSElog: 9.7196 grad_loss: 11542.7842 normal_loss: 0.9325
[epoch 37][iter  190] loss: 160438.2969 RMSElog: 9.9714 grad_loss: 16032.9092 normal_loss: 0.9487
[epoch 37][iter  200] loss: 202119.8125 RMSElog: 10.2544 grad_loss: 20200.7715 normal_loss: 0.9549
[epoch 37][iter  210] loss: 196677.7812 RMSElog: 10.0486 grad_loss: 19656.7715 normal_loss: 0.9571
[epoch 37][iter  220] loss: 155762.3594 RMSElog: 9.8487 grad_loss: 15565.4414 normal_loss: 0.9448
[epoch 37][iter  230] loss: 233395.7188 RMSElog: 10.2368 grad_loss: 23328.3906 normal_loss: 0.9452
[epoch 37][iter  240] loss: 128883.6406 RMSElog: 9.8798 grad_loss: 12877.5254 normal_loss: 0.9593
[epoch 37][iter  250] loss: 160788.1562 RMSElog: 9.7539 grad_loss: 16068.1299 normal_loss: 0.9318
[epoch 37][iter  260] loss: 158875.5312 RMSElog: 9.5853 grad_loss: 15877.0430 normal_loss: 0.9260
[epoch 37][iter  270] loss: 199607.0938 RMSElog: 10.0729 grad_loss: 19949.6758 normal_loss: 0.9604
[epoch 37][iter  280] loss: 151354.1562 RMSElog: 9.8837 grad_loss: 15124.5889 normal_loss: 0.9428
[epoch 37][iter  290] loss: 158034.0625 RMSElog: 9.9527 grad_loss: 15792.5088 normal_loss: 0.9445
[epoch 37][iter  300] loss: 118746.3359 RMSElog: 9.7278 grad_loss: 11863.9629 normal_loss: 0.9433
[epoch 37][iter  310] loss: 173855.4375 RMSElog: 10.0769 grad_loss: 17374.5098 normal_loss: 0.9566
[epoch 37][iter  320] loss: 162556.3906 RMSElog: 9.9091 grad_loss: 16244.7939 normal_loss: 0.9368
[epoch 37][iter  330] loss: 198036.3750 RMSElog: 10.2619 grad_loss: 19792.4219 normal_loss: 0.9529
[epoch 37][iter  340] loss: 106195.4219 RMSElog: 9.2531 grad_loss: 10609.3711 normal_loss: 0.9179
[epoch 37][iter  350] loss: 160051.0156 RMSElog: 9.8405 grad_loss: 15994.3105 normal_loss: 0.9506
[epoch 37][iter  360] loss: 229081.3125 RMSElog: 10.2778 grad_loss: 22896.9121 normal_loss: 0.9420
[epoch 37][iter  370] loss: 173086.7031 RMSElog: 10.4992 grad_loss: 17297.2227 normal_loss: 0.9480
[epoch 37][iter  380] loss: 154855.9062 RMSElog: 9.8054 grad_loss: 15474.8340 normal_loss: 0.9510
[epoch 37][iter  390] loss: 189017.5781 RMSElog: 10.1390 grad_loss: 18890.6602 normal_loss: 0.9589
[epoch 37][iter  400] loss: 186053.2812 RMSElog: 10.1470 grad_loss: 18594.2305 normal_loss: 0.9504
[epoch 37][iter  410] loss: 188502.1875 RMSElog: 10.1669 grad_loss: 18839.0801 normal_loss: 0.9722
[epoch 37][iter  420] loss: 217914.6094 RMSElog: 10.2786 grad_loss: 21780.2246 normal_loss: 0.9575
[epoch 37][iter  430] loss: 177292.4062 RMSElog: 10.1195 grad_loss: 17718.1602 normal_loss: 0.9615
[epoch 37][iter  440] loss: 160743.4062 RMSElog: 9.7965 grad_loss: 16063.6025 normal_loss: 0.9413
[epoch 37][iter  450] loss: 155911.2344 RMSElog: 9.7863 grad_loss: 15580.3965 normal_loss: 0.9403
[epoch 37][iter  460] loss: 130641.2188 RMSElog: 9.7184 grad_loss: 13053.4688 normal_loss: 0.9344
[epoch 37][iter  470] loss: 165648.1875 RMSElog: 9.7420 grad_loss: 16554.1289 normal_loss: 0.9479
[epoch 37][iter  480] loss: 122359.4531 RMSElog: 9.7683 grad_loss: 12225.2402 normal_loss: 0.9361
[epoch 37][iter  490] loss: 154609.8281 RMSElog: 9.8745 grad_loss: 15450.1855 normal_loss: 0.9220
[epoch 37][iter  500] loss: 168545.0938 RMSElog: 9.6168 grad_loss: 16843.9434 normal_loss: 0.9495
[epoch 37][iter  510] loss: 107948.0156 RMSElog: 9.7377 grad_loss: 10784.1299 normal_loss: 0.9342
[epoch 37][iter  520] loss: 218457.8281 RMSElog: 10.1255 grad_loss: 21834.7168 normal_loss: 0.9418
[epoch 37][iter  530] loss: 156557.3594 RMSElog: 9.6970 grad_loss: 15645.1025 normal_loss: 0.9363
[epoch 37][iter  540] loss: 163268.1250 RMSElog: 9.9218 grad_loss: 16315.9277 normal_loss: 0.9630
[epoch 37][iter  550] loss: 155968.4062 RMSElog: 9.9200 grad_loss: 15585.9980 normal_loss: 0.9232
[epoch 37][iter  560] loss: 108830.7344 RMSElog: 9.7359 grad_loss: 10872.4062 normal_loss: 0.9307
[epoch 37][iter  570] loss: 115658.3125 RMSElog: 9.1856 grad_loss: 11555.7031 normal_loss: 0.9424
[epoch 37][iter  580] loss: 111914.6250 RMSElog: 9.3004 grad_loss: 11181.2402 normal_loss: 0.9218
[epoch 37][iter  590] loss: 168409.1562 RMSElog: 9.7031 grad_loss: 16830.2910 normal_loss: 0.9228
[epoch 38][iter    0] loss: 80097.4062 RMSElog: 9.2433 grad_loss: 7999.5806 normal_loss: 0.9166
[epoch 38][iter   10] loss: 202630.0781 RMSElog: 10.0417 grad_loss: 20252.0078 normal_loss: 0.9589
[epoch 38][iter   20] loss: 201162.3750 RMSElog: 9.9531 grad_loss: 20105.3145 normal_loss: 0.9706
[epoch 38][iter   30] loss: 114149.3672 RMSElog: 9.2655 grad_loss: 11404.7637 normal_loss: 0.9073
[epoch 38][iter   40] loss: 145409.6719 RMSElog: 9.8330 grad_loss: 14530.1904 normal_loss: 0.9445
[epoch 38][iter   50] loss: 90482.6875 RMSElog: 9.3127 grad_loss: 9038.0186 normal_loss: 0.9378
[epoch 38][iter   60] loss: 217174.7656 RMSElog: 10.2254 grad_loss: 21706.2891 normal_loss: 0.9635
[epoch 38][iter   70] loss: 199607.1562 RMSElog: 10.1504 grad_loss: 19949.6133 normal_loss: 0.9517
[epoch 38][iter   80] loss: 146960.1406 RMSElog: 10.0979 grad_loss: 14684.9941 normal_loss: 0.9220
[epoch 38][iter   90] loss: 176260.7500 RMSElog: 10.0638 grad_loss: 17615.0703 normal_loss: 0.9395
[epoch 38][iter  100] loss: 234945.1719 RMSElog: 10.0015 grad_loss: 23483.5664 normal_loss: 0.9500
[epoch 38][iter  110] loss: 115534.3750 RMSElog: 9.7196 grad_loss: 11542.7852 normal_loss: 0.9325
[epoch 38][iter  120] loss: 189507.2500 RMSElog: 10.2157 grad_loss: 18939.5586 normal_loss: 0.9510
[epoch 38][iter  130] loss: 162372.7500 RMSElog: 9.8226 grad_loss: 16226.4922 normal_loss: 0.9603
[epoch 38][iter  140] loss: 120292.3359 RMSElog: 9.8042 grad_loss: 12018.4854 normal_loss: 0.9430
[epoch 38][iter  150] loss: 132707.3594 RMSElog: 9.7842 grad_loss: 13260.0049 normal_loss: 0.9472
[epoch 38][iter  160] loss: 179634.8125 RMSElog: 10.0930 grad_loss: 17952.4258 normal_loss: 0.9607
[epoch 38][iter  170] loss: 127463.1953 RMSElog: 9.4952 grad_loss: 12735.9092 normal_loss: 0.9153
[epoch 38][iter  180] loss: 160788.1562 RMSElog: 9.7539 grad_loss: 16068.1299 normal_loss: 0.9318
[epoch 38][iter  190] loss: 154776.6250 RMSElog: 9.8981 grad_loss: 15466.8438 normal_loss: 0.9209
[epoch 38][iter  200] loss: 132623.8750 RMSElog: 9.3325 grad_loss: 13252.1152 normal_loss: 0.9391
[epoch 38][iter  210] loss: 172966.9375 RMSElog: 9.9430 grad_loss: 17285.8008 normal_loss: 0.9485
[epoch 38][iter  220] loss: 194408.7812 RMSElog: 10.2924 grad_loss: 19429.6328 normal_loss: 0.9537
[epoch 38][iter  230] loss: 182859.0469 RMSElog: 9.8110 grad_loss: 18275.1250 normal_loss: 0.9678
[epoch 38][iter  240] loss: 147598.9844 RMSElog: 9.8981 grad_loss: 14749.0645 normal_loss: 0.9355
[epoch 38][iter  250] loss: 224077.7188 RMSElog: 10.3869 grad_loss: 22396.4258 normal_loss: 0.9584
[epoch 38][iter  260] loss: 103305.2500 RMSElog: 9.5543 grad_loss: 10320.0684 normal_loss: 0.9024
[epoch 38][iter  270] loss: 103514.7812 RMSElog: 9.3451 grad_loss: 10341.2109 normal_loss: 0.9229
[epoch 38][iter  280] loss: 160642.9375 RMSElog: 9.9306 grad_loss: 16053.4121 normal_loss: 0.9504
[epoch 38][iter  290] loss: 207885.1875 RMSElog: 10.4422 grad_loss: 20777.1328 normal_loss: 0.9460
[epoch 38][iter  300] loss: 148239.1406 RMSElog: 9.8055 grad_loss: 14813.1689 normal_loss: 0.9395
[epoch 38][iter  310] loss: 182318.7812 RMSElog: 10.1008 grad_loss: 18220.8398 normal_loss: 0.9384
[epoch 38][iter  320] loss: 223875.4062 RMSElog: 10.3929 grad_loss: 22376.1953 normal_loss: 0.9536
[epoch 38][iter  330] loss: 138934.7188 RMSElog: 9.8450 grad_loss: 13882.6904 normal_loss: 0.9370
[epoch 38][iter  340] loss: 126023.2344 RMSElog: 9.6219 grad_loss: 12591.7656 normal_loss: 0.9360
[epoch 38][iter  350] loss: 157274.4688 RMSElog: 9.9277 grad_loss: 15716.5723 normal_loss: 0.9461
[epoch 38][iter  360] loss: 160743.5156 RMSElog: 9.9654 grad_loss: 16063.4521 normal_loss: 0.9340
[epoch 38][iter  370] loss: 104749.2812 RMSElog: 9.7512 grad_loss: 10464.2490 normal_loss: 0.9281
[epoch 38][iter  380] loss: 214808.7344 RMSElog: 10.1872 grad_loss: 21469.7441 normal_loss: 0.9423
[epoch 38][iter  390] loss: 138057.5156 RMSElog: 9.5188 grad_loss: 13795.3184 normal_loss: 0.9138
[epoch 38][iter  400] loss: 163272.7188 RMSElog: 10.0261 grad_loss: 16316.3047 normal_loss: 0.9400
[epoch 38][iter  410] loss: 186960.2969 RMSElog: 10.2468 grad_loss: 18684.8457 normal_loss: 0.9380
[epoch 38][iter  420] loss: 158230.6094 RMSElog: 9.9234 grad_loss: 15812.1895 normal_loss: 0.9483
[epoch 38][iter  430] loss: 149387.5469 RMSElog: 9.9550 grad_loss: 14927.8516 normal_loss: 0.9483
[epoch 38][iter  440] loss: 195507.4062 RMSElog: 10.1854 grad_loss: 19539.6016 normal_loss: 0.9533
[epoch 38][iter  450] loss: 127776.8359 RMSElog: 10.0947 grad_loss: 12766.6396 normal_loss: 0.9490
[epoch 38][iter  460] loss: 156709.9531 RMSElog: 9.8741 grad_loss: 15660.1758 normal_loss: 0.9453
[epoch 38][iter  470] loss: 201503.0625 RMSElog: 10.3149 grad_loss: 20139.0410 normal_loss: 0.9514
[epoch 38][iter  480] loss: 127499.3281 RMSElog: 9.2769 grad_loss: 12739.7178 normal_loss: 0.9374
[epoch 38][iter  490] loss: 168409.1562 RMSElog: 9.7031 grad_loss: 16830.2910 normal_loss: 0.9228
[epoch 38][iter  500] loss: 212239.7031 RMSElog: 10.2690 grad_loss: 21212.7383 normal_loss: 0.9638
[epoch 38][iter  510] loss: 142596.2344 RMSElog: 9.8922 grad_loss: 14248.7803 normal_loss: 0.9508
[epoch 38][iter  520] loss: 105816.5000 RMSElog: 9.4266 grad_loss: 10571.3066 normal_loss: 0.9171
[epoch 38][iter  530] loss: 107073.5156 RMSElog: 9.7289 grad_loss: 10696.6914 normal_loss: 0.9313
[epoch 38][iter  540] loss: 235211.2344 RMSElog: 10.3371 grad_loss: 23509.8398 normal_loss: 0.9452
[epoch 38][iter  550] loss: 146576.9219 RMSElog: 9.6520 grad_loss: 14647.1045 normal_loss: 0.9356
[epoch 38][iter  560] loss: 163268.1250 RMSElog: 9.9218 grad_loss: 16315.9277 normal_loss: 0.9630
[epoch 38][iter  570] loss: 157706.0781 RMSElog: 9.9981 grad_loss: 15759.6602 normal_loss: 0.9499
[epoch 38][iter  580] loss: 159340.4219 RMSElog: 9.9507 grad_loss: 15923.1406 normal_loss: 0.9500
[epoch 38][iter  590] loss: 120440.5938 RMSElog: 9.1890 grad_loss: 12033.9463 normal_loss: 0.9240
[epoch 39][iter    0] loss: 160825.5938 RMSElog: 9.9290 grad_loss: 16071.6816 normal_loss: 0.9491
[epoch 39][iter   10] loss: 87697.6250 RMSElog: 9.9487 grad_loss: 8758.8750 normal_loss: 0.9395
[epoch 39][iter   20] loss: 203023.3438 RMSElog: 10.3036 grad_loss: 20291.0723 normal_loss: 0.9583
[epoch 39][iter   30] loss: 135966.2969 RMSElog: 9.6504 grad_loss: 13586.0273 normal_loss: 0.9522
[epoch 39][iter   40] loss: 168688.8125 RMSElog: 9.8493 grad_loss: 16858.0781 normal_loss: 0.9524
[epoch 39][iter   50] loss: 109441.5234 RMSElog: 9.7466 grad_loss: 10933.4756 normal_loss: 0.9300
[epoch 39][iter   60] loss: 154626.7656 RMSElog: 9.9098 grad_loss: 15451.8066 normal_loss: 0.9596
[epoch 39][iter   70] loss: 167084.9219 RMSElog: 9.8660 grad_loss: 16697.6582 normal_loss: 0.9689
[epoch 39][iter   80] loss: 213417.0312 RMSElog: 10.3416 grad_loss: 21330.4082 normal_loss: 0.9536
[epoch 39][iter   90] loss: 148136.9375 RMSElog: 9.9557 grad_loss: 14802.7871 normal_loss: 0.9498
[epoch 39][iter  100] loss: 160438.2812 RMSElog: 9.9714 grad_loss: 16032.9082 normal_loss: 0.9487
[epoch 39][iter  110] loss: 191559.9688 RMSElog: 9.9749 grad_loss: 19145.0742 normal_loss: 0.9477
[epoch 39][iter  120] loss: 103534.6562 RMSElog: 9.7670 grad_loss: 10342.7715 normal_loss: 0.9280
[epoch 39][iter  130] loss: 164597.1094 RMSElog: 9.8631 grad_loss: 16448.8984 normal_loss: 0.9502
[epoch 39][iter  140] loss: 140645.6875 RMSElog: 10.1118 grad_loss: 14053.4873 normal_loss: 0.9695
[epoch 39][iter  150] loss: 139433.9219 RMSElog: 9.8139 grad_loss: 13932.6191 normal_loss: 0.9591
[epoch 39][iter  160] loss: 102998.8984 RMSElog: 9.8140 grad_loss: 10289.1318 normal_loss: 0.9431
[epoch 39][iter  170] loss: 147598.9844 RMSElog: 9.8981 grad_loss: 14749.0645 normal_loss: 0.9355
[epoch 39][iter  180] loss: 172683.9219 RMSElog: 9.9061 grad_loss: 17257.5312 normal_loss: 0.9550
[epoch 39][iter  190] loss: 125486.8672 RMSElog: 9.7758 grad_loss: 12537.9834 normal_loss: 0.9281
[epoch 39][iter  200] loss: 133045.7969 RMSElog: 9.8765 grad_loss: 13293.7490 normal_loss: 0.9538
[epoch 39][iter  210] loss: 129708.8906 RMSElog: 10.0691 grad_loss: 12959.8711 normal_loss: 0.9481
[epoch 39][iter  220] loss: 176260.7500 RMSElog: 10.0638 grad_loss: 17615.0703 normal_loss: 0.9395
[epoch 39][iter  230] loss: 120765.1875 RMSElog: 9.5441 grad_loss: 12066.0400 normal_loss: 0.9348
[epoch 39][iter  240] loss: 110579.5391 RMSElog: 9.3019 grad_loss: 11047.7109 normal_loss: 0.9413
[epoch 39][iter  250] loss: 119923.2031 RMSElog: 9.5699 grad_loss: 11981.8154 normal_loss: 0.9342
[epoch 39][iter  260] loss: 194578.5781 RMSElog: 10.2312 grad_loss: 19446.6680 normal_loss: 0.9596
[epoch 39][iter  270] loss: 103355.8203 RMSElog: 9.7106 grad_loss: 10324.9463 normal_loss: 0.9244
[epoch 39][iter  280] loss: 196677.7812 RMSElog: 10.0486 grad_loss: 19656.7715 normal_loss: 0.9571
[epoch 39][iter  290] loss: 162372.7500 RMSElog: 9.8226 grad_loss: 16226.4922 normal_loss: 0.9603
[epoch 39][iter  300] loss: 118495.6641 RMSElog: 9.2657 grad_loss: 11839.3906 normal_loss: 0.9103
[epoch 39][iter  310] loss: 191371.4375 RMSElog: 10.1918 grad_loss: 19126.0020 normal_loss: 0.9504
[epoch 39][iter  320] loss: 156985.6250 RMSElog: 9.5765 grad_loss: 15688.0479 normal_loss: 0.9385
[epoch 39][iter  330] loss: 175301.7344 RMSElog: 9.8017 grad_loss: 17519.4180 normal_loss: 0.9555
[epoch 39][iter  340] loss: 206083.5156 RMSElog: 10.2501 grad_loss: 20597.1445 normal_loss: 0.9562
[epoch 39][iter  350] loss: 177814.3125 RMSElog: 10.0854 grad_loss: 17770.3887 normal_loss: 0.9571
[epoch 39][iter  360] loss: 195507.4062 RMSElog: 10.1854 grad_loss: 19539.6016 normal_loss: 0.9533
[epoch 39][iter  370] loss: 162433.1875 RMSElog: 9.8183 grad_loss: 16232.5488 normal_loss: 0.9510
[epoch 39][iter  380] loss: 151401.7031 RMSElog: 9.4414 grad_loss: 15129.7930 normal_loss: 0.9361
[epoch 39][iter  390] loss: 139016.5781 RMSElog: 9.6276 grad_loss: 13891.1045 normal_loss: 0.9261
[epoch 39][iter  400] loss: 198036.3750 RMSElog: 10.2619 grad_loss: 19792.4219 normal_loss: 0.9529
[epoch 39][iter  410] loss: 167008.4531 RMSElog: 10.0165 grad_loss: 16689.8750 normal_loss: 0.9557
[epoch 39][iter  420] loss: 129166.5312 RMSElog: 9.7364 grad_loss: 12905.9775 normal_loss: 0.9391
[epoch 39][iter  430] loss: 169013.5312 RMSElog: 9.5880 grad_loss: 16890.8242 normal_loss: 0.9419
[epoch 39][iter  440] loss: 140333.8125 RMSElog: 9.8261 grad_loss: 14022.6191 normal_loss: 0.9368
[epoch 39][iter  450] loss: 147371.3906 RMSElog: 9.6635 grad_loss: 14726.5488 normal_loss: 0.9280
[epoch 39][iter  460] loss: 224943.8906 RMSElog: 10.1206 grad_loss: 22483.3125 normal_loss: 0.9551
[epoch 39][iter  470] loss: 189743.5625 RMSElog: 10.1862 grad_loss: 18963.2207 normal_loss: 0.9490
[epoch 39][iter  480] loss: 105816.5000 RMSElog: 9.4266 grad_loss: 10571.3066 normal_loss: 0.9171
[epoch 39][iter  490] loss: 227323.8281 RMSElog: 10.4159 grad_loss: 22721.0195 normal_loss: 0.9477
[epoch 39][iter  500] loss: 162892.2969 RMSElog: 9.8252 grad_loss: 16278.4717 normal_loss: 0.9331
[epoch 39][iter  510] loss: 152540.7500 RMSElog: 9.9175 grad_loss: 15243.2090 normal_loss: 0.9474
[epoch 39][iter  520] loss: 206400.7812 RMSElog: 10.3027 grad_loss: 20628.8281 normal_loss: 0.9467
[epoch 39][iter  530] loss: 200953.4062 RMSElog: 10.1163 grad_loss: 20084.2598 normal_loss: 0.9623
[epoch 39][iter  540] loss: 229081.2812 RMSElog: 10.2778 grad_loss: 22896.9102 normal_loss: 0.9420
[epoch 39][iter  550] loss: 157274.4688 RMSElog: 9.9277 grad_loss: 15716.5723 normal_loss: 0.9461
[epoch 39][iter  560] loss: 160466.8750 RMSElog: 9.9086 grad_loss: 16035.8252 normal_loss: 0.9540
[epoch 39][iter  570] loss: 141442.1250 RMSElog: 9.6437 grad_loss: 14133.6191 normal_loss: 0.9495
[epoch 39][iter  580] loss: 175486.0625 RMSElog: 10.0393 grad_loss: 17537.6094 normal_loss: 0.9563
[epoch 39][iter  590] loss: 103509.2188 RMSElog: 9.9787 grad_loss: 10340.0186 normal_loss: 0.9251
[epoch 40][iter    0] loss: 150364.6250 RMSElog: 9.9123 grad_loss: 15025.6094 normal_loss: 0.9418
[epoch 40][iter   10] loss: 105628.3672 RMSElog: 10.0899 grad_loss: 10551.8262 normal_loss: 0.9213
[epoch 40][iter   20] loss: 153997.7188 RMSElog: 9.7620 grad_loss: 15389.0713 normal_loss: 0.9383
[epoch 40][iter   30] loss: 158630.9688 RMSElog: 9.8321 grad_loss: 15852.3232 normal_loss: 0.9428
[epoch 40][iter   40] loss: 106195.4219 RMSElog: 9.2531 grad_loss: 10609.3711 normal_loss: 0.9179
[epoch 40][iter   50] loss: 167730.9531 RMSElog: 9.9663 grad_loss: 16762.1797 normal_loss: 0.9501
[epoch 40][iter   60] loss: 162542.0469 RMSElog: 9.9393 grad_loss: 16243.3115 normal_loss: 0.9527
[epoch 40][iter   70] loss: 196842.3125 RMSElog: 10.1249 grad_loss: 19673.1523 normal_loss: 0.9534
[epoch 40][iter   80] loss: 183480.3438 RMSElog: 10.0091 grad_loss: 18337.0801 normal_loss: 0.9458
[epoch 40][iter   90] loss: 138934.7188 RMSElog: 9.8450 grad_loss: 13882.6904 normal_loss: 0.9370
[epoch 40][iter  100] loss: 195777.0469 RMSElog: 10.1280 grad_loss: 19566.6035 normal_loss: 0.9726
[epoch 40][iter  110] loss: 239768.0938 RMSElog: 10.2366 grad_loss: 23965.6250 normal_loss: 0.9466
[epoch 40][iter  120] loss: 170357.3438 RMSElog: 9.9687 grad_loss: 17024.8008 normal_loss: 0.9644
[epoch 40][iter  130] loss: 152772.4062 RMSElog: 9.8249 grad_loss: 15266.4727 normal_loss: 0.9438
[epoch 40][iter  140] loss: 214775.1719 RMSElog: 10.1719 grad_loss: 21466.3848 normal_loss: 0.9613
[epoch 40][iter  150] loss: 142138.5156 RMSElog: 9.7967 grad_loss: 14203.1260 normal_loss: 0.9291
[epoch 40][iter  160] loss: 198293.9375 RMSElog: 10.2275 grad_loss: 19818.2168 normal_loss: 0.9521
[epoch 40][iter  170] loss: 166207.1094 RMSElog: 9.7947 grad_loss: 16609.9668 normal_loss: 0.9492
[epoch 40][iter  180] loss: 114374.2344 RMSElog: 9.1913 grad_loss: 11427.3242 normal_loss: 0.9087
[epoch 40][iter  190] loss: 160825.5938 RMSElog: 9.9290 grad_loss: 16071.6816 normal_loss: 0.9491
[epoch 40][iter  200] loss: 151401.7031 RMSElog: 9.4414 grad_loss: 15129.7930 normal_loss: 0.9361
[epoch 40][iter  210] loss: 175547.8906 RMSElog: 10.1409 grad_loss: 17543.7148 normal_loss: 0.9342
[epoch 40][iter  220] loss: 177063.6250 RMSElog: 10.1532 grad_loss: 17695.2383 normal_loss: 0.9719
[epoch 40][iter  230] loss: 148136.9375 RMSElog: 9.9557 grad_loss: 14802.7871 normal_loss: 0.9498
[epoch 40][iter  240] loss: 195412.9844 RMSElog: 10.0493 grad_loss: 19530.2969 normal_loss: 0.9539
[epoch 40][iter  250] loss: 138469.0469 RMSElog: 9.4787 grad_loss: 13836.4834 normal_loss: 0.9438
[epoch 40][iter  260] loss: 175934.2344 RMSElog: 10.1481 grad_loss: 17582.3086 normal_loss: 0.9674
[epoch 40][iter  270] loss: 173721.0312 RMSElog: 10.0355 grad_loss: 17361.1211 normal_loss: 0.9468
[epoch 40][iter  280] loss: 212429.2969 RMSElog: 10.2794 grad_loss: 21231.6719 normal_loss: 0.9784
[epoch 40][iter  290] loss: 201503.0625 RMSElog: 10.3149 grad_loss: 20139.0410 normal_loss: 0.9514
[epoch 40][iter  300] loss: 104446.5625 RMSElog: 10.1932 grad_loss: 10433.5342 normal_loss: 0.9288
[epoch 40][iter  310] loss: 189582.6719 RMSElog: 9.9918 grad_loss: 18947.3223 normal_loss: 0.9523
[epoch 40][iter  320] loss: 134471.5781 RMSElog: 9.9200 grad_loss: 13436.3271 normal_loss: 0.9111
[epoch 40][iter  330] loss: 159824.3594 RMSElog: 9.8973 grad_loss: 15971.5811 normal_loss: 0.9569
[epoch 40][iter  340] loss: 150608.8906 RMSElog: 10.0680 grad_loss: 15049.8750 normal_loss: 0.9460
[epoch 40][iter  350] loss: 100980.7812 RMSElog: 9.6528 grad_loss: 10087.5176 normal_loss: 0.9085
[epoch 40][iter  360] loss: 111655.4766 RMSElog: 9.2050 grad_loss: 11155.4395 normal_loss: 0.9029
[epoch 40][iter  370] loss: 221213.4531 RMSElog: 10.1740 grad_loss: 22110.2305 normal_loss: 0.9415
[epoch 40][iter  380] loss: 199607.1562 RMSElog: 10.1504 grad_loss: 19949.6133 normal_loss: 0.9517
[epoch 40][iter  390] loss: 213417.0312 RMSElog: 10.3416 grad_loss: 21330.4082 normal_loss: 0.9536
[epoch 40][iter  400] loss: 211640.2344 RMSElog: 10.2414 grad_loss: 21152.8145 normal_loss: 0.9675
[epoch 40][iter  410] loss: 152149.7500 RMSElog: 9.9062 grad_loss: 15204.1221 normal_loss: 0.9462
[epoch 40][iter  420] loss: 179634.8125 RMSElog: 10.0930 grad_loss: 17952.4258 normal_loss: 0.9607
[epoch 40][iter  430] loss: 200470.3906 RMSElog: 10.2414 grad_loss: 20035.8379 normal_loss: 0.9585
[epoch 40][iter  440] loss: 133013.4531 RMSElog: 9.5635 grad_loss: 13290.8457 normal_loss: 0.9356
[epoch 40][iter  450] loss: 145321.5469 RMSElog: 9.7332 grad_loss: 14521.4590 normal_loss: 0.9632
[epoch 40][iter  460] loss: 224257.9688 RMSElog: 10.2842 grad_loss: 22414.5605 normal_loss: 0.9521
[epoch 40][iter  470] loss: 189543.6719 RMSElog: 10.1592 grad_loss: 18943.2598 normal_loss: 0.9474
[epoch 40][iter  480] loss: 175002.7344 RMSElog: 9.8652 grad_loss: 17489.4648 normal_loss: 0.9436
[epoch 40][iter  490] loss: 165020.2344 RMSElog: 9.7791 grad_loss: 16491.2910 normal_loss: 0.9530
[epoch 40][iter  500] loss: 195671.7031 RMSElog: 10.1080 grad_loss: 19556.1133 normal_loss: 0.9494
[epoch 40][iter  510] loss: 167455.8750 RMSElog: 9.7229 grad_loss: 16734.9395 normal_loss: 0.9256
[epoch 40][iter  520] loss: 163695.2656 RMSElog: 9.7037 grad_loss: 16358.8877 normal_loss: 0.9345
[epoch 40][iter  530] loss: 157810.1562 RMSElog: 9.9254 grad_loss: 15770.1641 normal_loss: 0.9253
[epoch 40][iter  540] loss: 144987.0938 RMSElog: 9.8653 grad_loss: 14487.9062 normal_loss: 0.9378
[epoch 40][iter  550] loss: 135042.2031 RMSElog: 9.6763 grad_loss: 13493.6074 normal_loss: 0.9366
[epoch 40][iter  560] loss: 165648.1875 RMSElog: 9.7420 grad_loss: 16554.1289 normal_loss: 0.9479
[epoch 40][iter  570] loss: 148239.1406 RMSElog: 9.8055 grad_loss: 14813.1689 normal_loss: 0.9395
[epoch 40][iter  580] loss: 107073.5156 RMSElog: 9.7289 grad_loss: 10696.6914 normal_loss: 0.9313
[epoch 40][iter  590] loss: 152540.7500 RMSElog: 9.9175 grad_loss: 15243.2090 normal_loss: 0.9474
[epoch 41][iter    0] loss: 127110.9375 RMSElog: 9.5273 grad_loss: 12700.6064 normal_loss: 0.9604
[epoch 41][iter   10] loss: 153997.7188 RMSElog: 9.7620 grad_loss: 15389.0713 normal_loss: 0.9383
[epoch 41][iter   20] loss: 185902.4219 RMSElog: 9.8998 grad_loss: 18579.3711 normal_loss: 0.9705
[epoch 41][iter   30] loss: 154776.6250 RMSElog: 9.8981 grad_loss: 15466.8438 normal_loss: 0.9209
[epoch 41][iter   40] loss: 103305.2422 RMSElog: 9.5543 grad_loss: 10320.0674 normal_loss: 0.9024
[epoch 41][iter   50] loss: 156253.7656 RMSElog: 9.8231 grad_loss: 15614.6211 normal_loss: 0.9316
[epoch 41][iter   60] loss: 160934.5156 RMSElog: 9.8035 grad_loss: 16082.6846 normal_loss: 0.9629
[epoch 41][iter   70] loss: 152149.7500 RMSElog: 9.9062 grad_loss: 15204.1230 normal_loss: 0.9462
[epoch 41][iter   80] loss: 142138.5156 RMSElog: 9.7967 grad_loss: 14203.1260 normal_loss: 0.9291
[epoch 41][iter   90] loss: 121983.8203 RMSElog: 9.5225 grad_loss: 12187.9473 normal_loss: 0.9124
[epoch 41][iter  100] loss: 190972.3281 RMSElog: 10.1152 grad_loss: 19086.1523 normal_loss: 0.9647
[epoch 41][iter  110] loss: 180982.8438 RMSElog: 10.2711 grad_loss: 18087.0605 normal_loss: 0.9528
[epoch 41][iter  120] loss: 111518.2734 RMSElog: 9.6257 grad_loss: 11141.2676 normal_loss: 0.9335
[epoch 41][iter  130] loss: 173855.4375 RMSElog: 10.0769 grad_loss: 17374.5098 normal_loss: 0.9566
[epoch 41][iter  140] loss: 157425.7500 RMSElog: 9.9235 grad_loss: 15731.7031 normal_loss: 0.9477
[epoch 41][iter  150] loss: 175524.1250 RMSElog: 10.2056 grad_loss: 17541.2441 normal_loss: 0.9625
[epoch 41][iter  160] loss: 220587.5781 RMSElog: 10.2995 grad_loss: 22047.5117 normal_loss: 0.9466
[epoch 41][iter  170] loss: 114173.9062 RMSElog: 9.3600 grad_loss: 11407.1240 normal_loss: 0.9061
[epoch 41][iter  180] loss: 232982.0469 RMSElog: 10.2845 grad_loss: 23286.9707 normal_loss: 0.9500
[epoch 41][iter  190] loss: 180663.0469 RMSElog: 10.2319 grad_loss: 18055.1094 normal_loss: 0.9630
[epoch 41][iter  200] loss: 120745.8203 RMSElog: 9.7706 grad_loss: 12063.8691 normal_loss: 0.9428
[epoch 41][iter  210] loss: 105628.3672 RMSElog: 10.0899 grad_loss: 10551.8262 normal_loss: 0.9213
[epoch 41][iter  220] loss: 110808.0469 RMSElog: 9.7171 grad_loss: 11070.1562 normal_loss: 0.9312
[epoch 41][iter  230] loss: 127463.1875 RMSElog: 9.4952 grad_loss: 12735.9082 normal_loss: 0.9153
[epoch 41][iter  240] loss: 114149.3750 RMSElog: 9.2655 grad_loss: 11404.7646 normal_loss: 0.9073
[epoch 41][iter  250] loss: 177195.4844 RMSElog: 10.1516 grad_loss: 17708.4512 normal_loss: 0.9458
[epoch 41][iter  260] loss: 162522.4531 RMSElog: 9.9860 grad_loss: 16241.3066 normal_loss: 0.9518
[epoch 41][iter  270] loss: 175008.6562 RMSElog: 9.9496 grad_loss: 17489.9434 normal_loss: 0.9734
[epoch 41][iter  280] loss: 211640.2344 RMSElog: 10.2414 grad_loss: 21152.8145 normal_loss: 0.9675
[epoch 41][iter  290] loss: 172966.9375 RMSElog: 9.9430 grad_loss: 17285.8008 normal_loss: 0.9485
[epoch 41][iter  300] loss: 149036.1562 RMSElog: 9.7728 grad_loss: 14892.8887 normal_loss: 0.9547
[epoch 41][iter  310] loss: 139878.0625 RMSElog: 9.8515 grad_loss: 13977.0225 normal_loss: 0.9327
[epoch 41][iter  320] loss: 136533.4688 RMSElog: 9.6756 grad_loss: 13642.7363 normal_loss: 0.9359
[epoch 41][iter  330] loss: 114827.1016 RMSElog: 9.7767 grad_loss: 11472.0010 normal_loss: 0.9329
[epoch 41][iter  340] loss: 147598.9844 RMSElog: 9.8981 grad_loss: 14749.0645 normal_loss: 0.9355
[epoch 41][iter  350] loss: 172115.2812 RMSElog: 9.7682 grad_loss: 17200.8242 normal_loss: 0.9358
[epoch 41][iter  360] loss: 148793.9688 RMSElog: 10.1047 grad_loss: 14868.3447 normal_loss: 0.9480
[epoch 41][iter  370] loss: 87697.6250 RMSElog: 9.9487 grad_loss: 8758.8750 normal_loss: 0.9395
[epoch 41][iter  380] loss: 110603.0781 RMSElog: 9.8046 grad_loss: 11049.5664 normal_loss: 0.9364
[epoch 41][iter  390] loss: 174375.6875 RMSElog: 10.0512 grad_loss: 17426.5684 normal_loss: 0.9483
[epoch 41][iter  400] loss: 119429.4609 RMSElog: 9.6388 grad_loss: 11932.3740 normal_loss: 0.9339
[epoch 41][iter  410] loss: 133045.7969 RMSElog: 9.8765 grad_loss: 13293.7490 normal_loss: 0.9538
[epoch 41][iter  420] loss: 184374.1875 RMSElog: 10.2641 grad_loss: 18426.1953 normal_loss: 0.9596
[epoch 41][iter  430] loss: 191559.9688 RMSElog: 9.9749 grad_loss: 19145.0742 normal_loss: 0.9477
[epoch 41][iter  440] loss: 105706.7031 RMSElog: 9.7501 grad_loss: 10559.9883 normal_loss: 0.9321
[epoch 41][iter  450] loss: 126023.2344 RMSElog: 9.6219 grad_loss: 12591.7656 normal_loss: 0.9360
[epoch 41][iter  460] loss: 172755.6875 RMSElog: 10.0251 grad_loss: 17264.6016 normal_loss: 0.9419
[epoch 41][iter  470] loss: 146942.0781 RMSElog: 9.4227 grad_loss: 14683.8535 normal_loss: 0.9314
[epoch 41][iter  480] loss: 129708.8906 RMSElog: 10.0691 grad_loss: 12959.8711 normal_loss: 0.9481
[epoch 41][iter  490] loss: 107472.2500 RMSElog: 9.0654 grad_loss: 10737.2471 normal_loss: 0.9124
[epoch 41][iter  500] loss: 127499.3281 RMSElog: 9.2769 grad_loss: 12739.7178 normal_loss: 0.9374
[epoch 41][iter  510] loss: 133013.4531 RMSElog: 9.5635 grad_loss: 13290.8457 normal_loss: 0.9356
[epoch 41][iter  520] loss: 177936.2812 RMSElog: 10.0381 grad_loss: 17782.6348 normal_loss: 0.9545
[epoch 41][iter  530] loss: 132935.3906 RMSElog: 9.9640 grad_loss: 13282.6172 normal_loss: 0.9581
[epoch 41][iter  540] loss: 158034.0469 RMSElog: 9.9527 grad_loss: 15792.5078 normal_loss: 0.9445
[epoch 41][iter  550] loss: 201373.6875 RMSElog: 10.3870 grad_loss: 20126.0332 normal_loss: 0.9491
[epoch 41][iter  560] loss: 134677.6719 RMSElog: 9.8776 grad_loss: 13456.9443 normal_loss: 0.9441
[epoch 41][iter  570] loss: 125683.7969 RMSElog: 9.8131 grad_loss: 12557.6123 normal_loss: 0.9537
[epoch 41][iter  580] loss: 166492.4688 RMSElog: 9.8376 grad_loss: 16638.4531 normal_loss: 0.9558
[epoch 41][iter  590] loss: 138950.6406 RMSElog: 9.9777 grad_loss: 13884.1387 normal_loss: 0.9482
[epoch 42][iter    0] loss: 180425.2344 RMSElog: 10.0354 grad_loss: 18031.5332 normal_loss: 0.9552
[epoch 42][iter   10] loss: 161633.2656 RMSElog: 10.0107 grad_loss: 16152.3643 normal_loss: 0.9518
[epoch 42][iter   20] loss: 167740.4688 RMSElog: 9.8540 grad_loss: 16763.2344 normal_loss: 0.9591
[epoch 42][iter   30] loss: 146942.0781 RMSElog: 9.4227 grad_loss: 14683.8535 normal_loss: 0.9314
[epoch 42][iter   40] loss: 110808.0469 RMSElog: 9.7171 grad_loss: 11070.1562 normal_loss: 0.9312
[epoch 42][iter   50] loss: 175002.7344 RMSElog: 9.8652 grad_loss: 17489.4648 normal_loss: 0.9436
[epoch 42][iter   60] loss: 156985.6406 RMSElog: 9.5765 grad_loss: 15688.0488 normal_loss: 0.9385
[epoch 42][iter   70] loss: 181293.3438 RMSElog: 10.0009 grad_loss: 18118.3809 normal_loss: 0.9531
[epoch 42][iter   80] loss: 205362.7344 RMSElog: 10.1686 grad_loss: 20525.1582 normal_loss: 0.9475
[epoch 42][iter   90] loss: 147915.7812 RMSElog: 10.0131 grad_loss: 14780.6104 normal_loss: 0.9547
[epoch 42][iter  100] loss: 132037.4375 RMSElog: 9.6100 grad_loss: 13193.2246 normal_loss: 0.9082
[epoch 42][iter  110] loss: 125281.0156 RMSElog: 9.5401 grad_loss: 12517.6484 normal_loss: 0.9128
[epoch 42][iter  120] loss: 221089.3594 RMSElog: 10.3457 grad_loss: 22097.6465 normal_loss: 0.9439
[epoch 42][iter  130] loss: 203093.2969 RMSElog: 10.1238 grad_loss: 20298.2598 normal_loss: 0.9478
[epoch 42][iter  140] loss: 178593.9688 RMSElog: 10.0081 grad_loss: 17848.4395 normal_loss: 0.9490
[epoch 42][iter  150] loss: 130145.6250 RMSElog: 9.7206 grad_loss: 13003.9072 normal_loss: 0.9344
[epoch 42][iter  160] loss: 113195.8281 RMSElog: 9.1761 grad_loss: 11309.5059 normal_loss: 0.9012
[epoch 42][iter  170] loss: 110235.3906 RMSElog: 9.7805 grad_loss: 11012.8271 normal_loss: 0.9312
[epoch 42][iter  180] loss: 186960.2969 RMSElog: 10.2468 grad_loss: 18684.8457 normal_loss: 0.9380
[epoch 42][iter  190] loss: 140938.8594 RMSElog: 9.7483 grad_loss: 14083.1953 normal_loss: 0.9424
[epoch 42][iter  200] loss: 230519.3906 RMSElog: 10.3105 grad_loss: 23040.6836 normal_loss: 0.9459
[epoch 42][iter  210] loss: 163389.5469 RMSElog: 9.7702 grad_loss: 16328.2637 normal_loss: 0.9208
[epoch 42][iter  220] loss: 226865.1406 RMSElog: 10.2887 grad_loss: 22675.2832 normal_loss: 0.9418
[epoch 42][iter  230] loss: 195671.7031 RMSElog: 10.1080 grad_loss: 19556.1133 normal_loss: 0.9494
[epoch 42][iter  240] loss: 143613.7656 RMSElog: 9.8882 grad_loss: 14350.5566 normal_loss: 0.9315
[epoch 42][iter  250] loss: 160642.9375 RMSElog: 9.9306 grad_loss: 16053.4121 normal_loss: 0.9504
[epoch 42][iter  260] loss: 167794.0469 RMSElog: 9.8335 grad_loss: 16768.6211 normal_loss: 0.9500
[epoch 42][iter  270] loss: 214808.7500 RMSElog: 10.1872 grad_loss: 21469.7461 normal_loss: 0.9423
[epoch 42][iter  280] loss: 145389.9375 RMSElog: 9.8784 grad_loss: 14528.1660 normal_loss: 0.9489
[epoch 42][iter  290] loss: 149036.1562 RMSElog: 9.7728 grad_loss: 14892.8887 normal_loss: 0.9547
[epoch 42][iter  300] loss: 160743.5156 RMSElog: 9.9654 grad_loss: 16063.4521 normal_loss: 0.9340
[epoch 42][iter  310] loss: 103798.3281 RMSElog: 9.6193 grad_loss: 10369.2812 normal_loss: 0.9331
[epoch 42][iter  320] loss: 132012.6719 RMSElog: 9.6802 grad_loss: 13190.6582 normal_loss: 0.9291
[epoch 42][iter  330] loss: 207919.9844 RMSElog: 10.2783 grad_loss: 20780.7637 normal_loss: 0.9578
[epoch 42][iter  340] loss: 99107.7500 RMSElog: 9.5230 grad_loss: 9900.3428 normal_loss: 0.9094
[epoch 42][iter  350] loss: 162892.2969 RMSElog: 9.8252 grad_loss: 16278.4717 normal_loss: 0.9331
[epoch 42][iter  360] loss: 161285.2188 RMSElog: 9.8339 grad_loss: 16117.7402 normal_loss: 0.9476
[epoch 42][iter  370] loss: 166207.1094 RMSElog: 9.7947 grad_loss: 16609.9668 normal_loss: 0.9492
[epoch 42][iter  380] loss: 179298.9062 RMSElog: 9.7220 grad_loss: 17919.2227 normal_loss: 0.9453
[epoch 42][iter  390] loss: 214067.1875 RMSElog: 10.0580 grad_loss: 21395.7207 normal_loss: 0.9394
[epoch 42][iter  400] loss: 142596.2344 RMSElog: 9.8922 grad_loss: 14248.7793 normal_loss: 0.9508
[epoch 42][iter  410] loss: 180208.2188 RMSElog: 10.0490 grad_loss: 18009.8223 normal_loss: 0.9517
[epoch 42][iter  420] loss: 186053.2969 RMSElog: 10.1470 grad_loss: 18594.2324 normal_loss: 0.9504
[epoch 42][iter  430] loss: 158034.0625 RMSElog: 9.9527 grad_loss: 15792.5088 normal_loss: 0.9445
[epoch 42][iter  440] loss: 190938.1250 RMSElog: 10.3028 grad_loss: 19082.5566 normal_loss: 0.9541
[epoch 42][iter  450] loss: 167455.8750 RMSElog: 9.7229 grad_loss: 16734.9395 normal_loss: 0.9256
[epoch 42][iter  460] loss: 213401.8906 RMSElog: 10.0568 grad_loss: 21329.1582 normal_loss: 0.9749
[epoch 42][iter  470] loss: 131213.2969 RMSElog: 9.6860 grad_loss: 13110.7217 normal_loss: 0.9217
[epoch 42][iter  480] loss: 124563.3906 RMSElog: 9.9514 grad_loss: 12445.4473 normal_loss: 0.9400
[epoch 42][iter  490] loss: 139433.9219 RMSElog: 9.8139 grad_loss: 13932.6191 normal_loss: 0.9591
[epoch 42][iter  500] loss: 114509.4531 RMSElog: 9.7409 grad_loss: 11440.2627 normal_loss: 0.9413
[epoch 42][iter  510] loss: 189743.5625 RMSElog: 10.1862 grad_loss: 18963.2207 normal_loss: 0.9490
[epoch 42][iter  520] loss: 179649.7188 RMSElog: 10.2020 grad_loss: 17953.8125 normal_loss: 0.9593
[epoch 42][iter  530] loss: 140057.2812 RMSElog: 9.9766 grad_loss: 13994.7988 normal_loss: 0.9526
[epoch 42][iter  540] loss: 100990.1562 RMSElog: 9.7350 grad_loss: 10088.3584 normal_loss: 0.9222
[epoch 42][iter  550] loss: 165326.5625 RMSElog: 9.8832 grad_loss: 16521.8086 normal_loss: 0.9647
[epoch 42][iter  560] loss: 154705.2344 RMSElog: 9.9142 grad_loss: 15459.6562 normal_loss: 0.9527
[epoch 42][iter  570] loss: 116582.6172 RMSElog: 9.7871 grad_loss: 11647.5430 normal_loss: 0.9314
[epoch 42][iter  580] loss: 198743.4219 RMSElog: 10.0842 grad_loss: 19863.2930 normal_loss: 0.9643
[epoch 42][iter  590] loss: 162711.9688 RMSElog: 9.6160 grad_loss: 16260.6387 normal_loss: 0.9427
[epoch 43][iter    0] loss: 174375.6875 RMSElog: 10.0512 grad_loss: 17426.5684 normal_loss: 0.9483
[epoch 43][iter   10] loss: 185902.4219 RMSElog: 9.8998 grad_loss: 18579.3711 normal_loss: 0.9705
[epoch 43][iter   20] loss: 102096.6484 RMSElog: 9.8437 grad_loss: 10198.8926 normal_loss: 0.9290
[epoch 43][iter   30] loss: 150699.3750 RMSElog: 9.8567 grad_loss: 15059.1328 normal_loss: 0.9486
[epoch 43][iter   40] loss: 152540.7500 RMSElog: 9.9175 grad_loss: 15243.2090 normal_loss: 0.9474
[epoch 43][iter   50] loss: 103305.2422 RMSElog: 9.5543 grad_loss: 10320.0674 normal_loss: 0.9024
[epoch 43][iter   60] loss: 189582.6719 RMSElog: 9.9918 grad_loss: 18947.3223 normal_loss: 0.9523
[epoch 43][iter   70] loss: 163171.1562 RMSElog: 9.7905 grad_loss: 16306.3594 normal_loss: 0.9652
[epoch 43][iter   80] loss: 114149.3750 RMSElog: 9.2655 grad_loss: 11404.7646 normal_loss: 0.9073
[epoch 43][iter   90] loss: 115534.3672 RMSElog: 9.7196 grad_loss: 11542.7842 normal_loss: 0.9325
[epoch 43][iter  100] loss: 114509.4531 RMSElog: 9.7409 grad_loss: 11440.2627 normal_loss: 0.9413
[epoch 43][iter  110] loss: 133464.0312 RMSElog: 9.6242 grad_loss: 13335.8584 normal_loss: 0.9201
[epoch 43][iter  120] loss: 155762.3438 RMSElog: 9.8487 grad_loss: 15565.4404 normal_loss: 0.9448
[epoch 43][iter  130] loss: 181983.5312 RMSElog: 10.0741 grad_loss: 18187.3320 normal_loss: 0.9464
[epoch 43][iter  140] loss: 195999.5625 RMSElog: 10.1749 grad_loss: 19588.8047 normal_loss: 0.9756
[epoch 43][iter  150] loss: 152562.3750 RMSElog: 9.8216 grad_loss: 15245.4775 normal_loss: 0.9391
[epoch 43][iter  160] loss: 146465.6875 RMSElog: 10.2283 grad_loss: 14635.4062 normal_loss: 0.9337
[epoch 43][iter  170] loss: 133024.8594 RMSElog: 9.6937 grad_loss: 13291.8662 normal_loss: 0.9261
[epoch 43][iter  180] loss: 124323.5156 RMSElog: 9.7543 grad_loss: 12421.6650 normal_loss: 0.9322
[epoch 43][iter  190] loss: 195440.9062 RMSElog: 9.7403 grad_loss: 19533.3926 normal_loss: 0.9572
[epoch 43][iter  200] loss: 199070.3125 RMSElog: 9.9947 grad_loss: 19896.0605 normal_loss: 0.9757
[epoch 43][iter  210] loss: 208821.7812 RMSElog: 10.1565 grad_loss: 20871.0781 normal_loss: 0.9431
[epoch 43][iter  220] loss: 144440.4531 RMSElog: 9.8843 grad_loss: 14433.2314 normal_loss: 0.9296
[epoch 43][iter  230] loss: 129743.7969 RMSElog: 9.9523 grad_loss: 12963.4785 normal_loss: 0.9492
[epoch 43][iter  240] loss: 154855.9062 RMSElog: 9.8054 grad_loss: 15474.8340 normal_loss: 0.9510
[epoch 43][iter  250] loss: 140938.8594 RMSElog: 9.7483 grad_loss: 14083.1953 normal_loss: 0.9424
[epoch 43][iter  260] loss: 156985.6250 RMSElog: 9.5765 grad_loss: 15688.0479 normal_loss: 0.9385
[epoch 43][iter  270] loss: 190034.7188 RMSElog: 9.9619 grad_loss: 18992.5625 normal_loss: 0.9485
[epoch 43][iter  280] loss: 185495.4062 RMSElog: 9.9310 grad_loss: 18538.6641 normal_loss: 0.9455
[epoch 43][iter  290] loss: 113921.7891 RMSElog: 9.7251 grad_loss: 11381.5186 normal_loss: 0.9357
[epoch 43][iter  300] loss: 152517.4844 RMSElog: 9.8233 grad_loss: 15241.0010 normal_loss: 0.9249
[epoch 43][iter  310] loss: 161088.9688 RMSElog: 9.9258 grad_loss: 16098.0254 normal_loss: 0.9455
[epoch 43][iter  320] loss: 160934.5156 RMSElog: 9.8035 grad_loss: 16082.6846 normal_loss: 0.9629
[epoch 43][iter  330] loss: 207919.9844 RMSElog: 10.2783 grad_loss: 20780.7637 normal_loss: 0.9578
[epoch 43][iter  340] loss: 175359.1875 RMSElog: 10.1476 grad_loss: 17524.8125 normal_loss: 0.9571
[epoch 43][iter  350] loss: 112535.5625 RMSElog: 9.5823 grad_loss: 11243.0459 normal_loss: 0.9289
[epoch 43][iter  360] loss: 200470.3906 RMSElog: 10.2414 grad_loss: 20035.8379 normal_loss: 0.9585
[epoch 43][iter  370] loss: 130527.6172 RMSElog: 9.8914 grad_loss: 13041.9189 normal_loss: 0.9509
[epoch 43][iter  380] loss: 215955.5312 RMSElog: 10.2675 grad_loss: 21584.3203 normal_loss: 0.9654
[epoch 43][iter  390] loss: 211925.8438 RMSElog: 10.1863 grad_loss: 21181.4551 normal_loss: 0.9436
[epoch 43][iter  400] loss: 185071.7031 RMSElog: 10.0456 grad_loss: 18496.1699 normal_loss: 0.9557
[epoch 43][iter  410] loss: 100980.7812 RMSElog: 9.6528 grad_loss: 10087.5176 normal_loss: 0.9085
[epoch 43][iter  420] loss: 163253.2500 RMSElog: 9.8740 grad_loss: 16314.5010 normal_loss: 0.9499
[epoch 43][iter  430] loss: 186049.0312 RMSElog: 10.0165 grad_loss: 18593.9277 normal_loss: 0.9585
[epoch 43][iter  440] loss: 123289.4609 RMSElog: 9.4106 grad_loss: 12318.5752 normal_loss: 0.9606
[epoch 43][iter  450] loss: 138469.0469 RMSElog: 9.4787 grad_loss: 13836.4834 normal_loss: 0.9438
[epoch 43][iter  460] loss: 180929.5938 RMSElog: 10.0745 grad_loss: 18081.9297 normal_loss: 0.9546
[epoch 43][iter  470] loss: 136533.4688 RMSElog: 9.6756 grad_loss: 13642.7363 normal_loss: 0.9359
[epoch 43][iter  480] loss: 105628.3672 RMSElog: 10.0899 grad_loss: 10551.8262 normal_loss: 0.9213
[epoch 43][iter  490] loss: 115348.2734 RMSElog: 9.7235 grad_loss: 11524.1562 normal_loss: 0.9470
[epoch 43][iter  500] loss: 142138.5156 RMSElog: 9.7967 grad_loss: 14203.1260 normal_loss: 0.9291
[epoch 43][iter  510] loss: 138714.8906 RMSElog: 10.1491 grad_loss: 13860.4043 normal_loss: 0.9360
[epoch 43][iter  520] loss: 115495.2188 RMSElog: 9.6084 grad_loss: 11538.9688 normal_loss: 0.9442
[epoch 43][iter  530] loss: 136796.3594 RMSElog: 9.6524 grad_loss: 13669.0283 normal_loss: 0.9549
[epoch 43][iter  540] loss: 162711.9688 RMSElog: 9.6160 grad_loss: 16260.6387 normal_loss: 0.9427
[epoch 43][iter  550] loss: 189743.5625 RMSElog: 10.1862 grad_loss: 18963.2207 normal_loss: 0.9490
[epoch 43][iter  560] loss: 103355.8125 RMSElog: 9.7106 grad_loss: 10324.9453 normal_loss: 0.9244
[epoch 43][iter  570] loss: 115738.1953 RMSElog: 9.6968 grad_loss: 11563.1836 normal_loss: 0.9382
[epoch 43][iter  580] loss: 125695.5391 RMSElog: 9.9422 grad_loss: 12558.6836 normal_loss: 0.9278
[epoch 43][iter  590] loss: 118746.3359 RMSElog: 9.7278 grad_loss: 11863.9629 normal_loss: 0.9433
[epoch 44][iter    0] loss: 162892.2969 RMSElog: 9.8252 grad_loss: 16278.4717 normal_loss: 0.9331
[epoch 44][iter   10] loss: 163272.7188 RMSElog: 10.0261 grad_loss: 16316.3047 normal_loss: 0.9400
[epoch 44][iter   20] loss: 215955.5625 RMSElog: 10.2675 grad_loss: 21584.3242 normal_loss: 0.9654
[epoch 44][iter   30] loss: 164415.3438 RMSElog: 10.0829 grad_loss: 16430.4961 normal_loss: 0.9571
[epoch 44][iter   40] loss: 196437.8125 RMSElog: 10.2319 grad_loss: 19632.5898 normal_loss: 0.9584
[epoch 44][iter   50] loss: 202119.8125 RMSElog: 10.2544 grad_loss: 20200.7715 normal_loss: 0.9549
[epoch 44][iter   60] loss: 120440.5938 RMSElog: 9.1890 grad_loss: 12033.9463 normal_loss: 0.9240
[epoch 44][iter   70] loss: 115452.5781 RMSElog: 9.2301 grad_loss: 11535.1230 normal_loss: 0.9042
[epoch 44][iter   80] loss: 129743.7969 RMSElog: 9.9523 grad_loss: 12963.4785 normal_loss: 0.9492
[epoch 44][iter   90] loss: 121838.3672 RMSElog: 9.7453 grad_loss: 12173.1494 normal_loss: 0.9425
[epoch 44][iter  100] loss: 125486.8672 RMSElog: 9.7758 grad_loss: 12537.9834 normal_loss: 0.9281
[epoch 44][iter  110] loss: 113895.6328 RMSElog: 9.2606 grad_loss: 11379.3926 normal_loss: 0.9106
[epoch 44][iter  120] loss: 124246.3594 RMSElog: 9.7165 grad_loss: 12413.9834 normal_loss: 0.9360
[epoch 44][iter  130] loss: 115495.2188 RMSElog: 9.6084 grad_loss: 11538.9688 normal_loss: 0.9442
[epoch 44][iter  140] loss: 177936.2812 RMSElog: 10.0381 grad_loss: 17782.6348 normal_loss: 0.9545
[epoch 44][iter  150] loss: 166492.4688 RMSElog: 9.8376 grad_loss: 16638.4531 normal_loss: 0.9558
[epoch 44][iter  160] loss: 194408.7812 RMSElog: 10.2924 grad_loss: 19429.6328 normal_loss: 0.9537
[epoch 44][iter  170] loss: 207919.9844 RMSElog: 10.2783 grad_loss: 20780.7637 normal_loss: 0.9578
[epoch 44][iter  180] loss: 153447.6875 RMSElog: 9.9051 grad_loss: 15333.9209 normal_loss: 0.9425
[epoch 44][iter  190] loss: 159340.4375 RMSElog: 9.9507 grad_loss: 15923.1426 normal_loss: 0.9500
[epoch 44][iter  200] loss: 101101.5781 RMSElog: 9.9274 grad_loss: 10099.3008 normal_loss: 0.9297
[epoch 44][iter  210] loss: 161646.4688 RMSElog: 10.1105 grad_loss: 16153.5762 normal_loss: 0.9608
[epoch 44][iter  220] loss: 110874.4688 RMSElog: 9.2829 grad_loss: 11077.2607 normal_loss: 0.9034
[epoch 44][iter  230] loss: 158630.9688 RMSElog: 9.8321 grad_loss: 15852.3232 normal_loss: 0.9428
[epoch 44][iter  240] loss: 105628.3672 RMSElog: 10.0899 grad_loss: 10551.8262 normal_loss: 0.9213
[epoch 44][iter  250] loss: 160642.9375 RMSElog: 9.9306 grad_loss: 16053.4121 normal_loss: 0.9504
[epoch 44][iter  260] loss: 162372.7500 RMSElog: 9.8226 grad_loss: 16226.4922 normal_loss: 0.9603
[epoch 44][iter  270] loss: 150364.6250 RMSElog: 9.9123 grad_loss: 15025.6094 normal_loss: 0.9418
[epoch 44][iter  280] loss: 106195.4219 RMSElog: 9.2531 grad_loss: 10609.3711 normal_loss: 0.9179
[epoch 44][iter  290] loss: 175122.9531 RMSElog: 9.9588 grad_loss: 17501.3789 normal_loss: 0.9573
[epoch 44][iter  300] loss: 136796.3438 RMSElog: 9.6524 grad_loss: 13669.0273 normal_loss: 0.9549
[epoch 44][iter  310] loss: 161633.2812 RMSElog: 10.0107 grad_loss: 16152.3652 normal_loss: 0.9518
[epoch 44][iter  320] loss: 231696.5312 RMSElog: 10.3202 grad_loss: 23158.3867 normal_loss: 0.9457
[epoch 44][iter  330] loss: 130145.6250 RMSElog: 9.7206 grad_loss: 13003.9072 normal_loss: 0.9344
[epoch 44][iter  340] loss: 189507.2500 RMSElog: 10.2157 grad_loss: 18939.5586 normal_loss: 0.9510
[epoch 44][iter  350] loss: 161084.5469 RMSElog: 9.9606 grad_loss: 16097.5430 normal_loss: 0.9512
[epoch 44][iter  360] loss: 126821.0234 RMSElog: 10.1398 grad_loss: 12671.0410 normal_loss: 0.9215
[epoch 44][iter  370] loss: 173574.0781 RMSElog: 10.1780 grad_loss: 17346.2734 normal_loss: 0.9575
[epoch 44][iter  380] loss: 114310.3047 RMSElog: 9.4026 grad_loss: 11420.7217 normal_loss: 0.9063
[epoch 44][iter  390] loss: 134471.5938 RMSElog: 9.9200 grad_loss: 13436.3281 normal_loss: 0.9111
[epoch 44][iter  400] loss: 170488.4688 RMSElog: 9.7164 grad_loss: 17038.1816 normal_loss: 0.9494
[epoch 44][iter  410] loss: 114827.1016 RMSElog: 9.7767 grad_loss: 11472.0010 normal_loss: 0.9329
[epoch 44][iter  420] loss: 148136.9375 RMSElog: 9.9557 grad_loss: 14802.7871 normal_loss: 0.9498
[epoch 44][iter  430] loss: 195671.7031 RMSElog: 10.1080 grad_loss: 19556.1133 normal_loss: 0.9494
[epoch 44][iter  440] loss: 104340.6328 RMSElog: 9.7692 grad_loss: 10423.3701 normal_loss: 0.9234
[epoch 44][iter  450] loss: 127776.8359 RMSElog: 10.0947 grad_loss: 12766.6396 normal_loss: 0.9490
[epoch 44][iter  460] loss: 165976.7500 RMSElog: 9.9385 grad_loss: 16586.7715 normal_loss: 0.9645
[epoch 44][iter  470] loss: 137296.6562 RMSElog: 9.9750 grad_loss: 13718.7412 normal_loss: 0.9495
[epoch 44][iter  480] loss: 184016.5469 RMSElog: 9.9468 grad_loss: 18390.7363 normal_loss: 0.9712
[epoch 44][iter  490] loss: 154609.8281 RMSElog: 9.8745 grad_loss: 15450.1855 normal_loss: 0.9220
[epoch 44][iter  500] loss: 160743.4062 RMSElog: 9.7965 grad_loss: 16063.6025 normal_loss: 0.9413
[epoch 44][iter  510] loss: 233395.7188 RMSElog: 10.2368 grad_loss: 23328.3906 normal_loss: 0.9452
[epoch 44][iter  520] loss: 142696.6875 RMSElog: 9.9612 grad_loss: 14258.7588 normal_loss: 0.9493
[epoch 44][iter  530] loss: 109148.7344 RMSElog: 10.1172 grad_loss: 10903.8262 normal_loss: 0.9294
[epoch 44][iter  540] loss: 198036.3750 RMSElog: 10.2619 grad_loss: 19792.4219 normal_loss: 0.9529
[epoch 44][iter  550] loss: 176914.4531 RMSElog: 10.3802 grad_loss: 17680.1094 normal_loss: 0.9546
[epoch 44][iter  560] loss: 119429.4609 RMSElog: 9.6388 grad_loss: 11932.3740 normal_loss: 0.9339
[epoch 44][iter  570] loss: 105877.8750 RMSElog: 9.7638 grad_loss: 10577.0986 normal_loss: 0.9249
[epoch 44][iter  580] loss: 179740.0625 RMSElog: 10.1206 grad_loss: 17962.9453 normal_loss: 0.9397
[epoch 44][iter  590] loss: 137252.6406 RMSElog: 9.3538 grad_loss: 13714.9785 normal_loss: 0.9319
[epoch 45][iter    0] loss: 161633.2656 RMSElog: 10.0107 grad_loss: 16152.3643 normal_loss: 0.9518
[epoch 45][iter   10] loss: 207919.9844 RMSElog: 10.2783 grad_loss: 20780.7637 normal_loss: 0.9578
[epoch 45][iter   20] loss: 103955.0000 RMSElog: 9.6256 grad_loss: 10384.9521 normal_loss: 0.9222
[epoch 45][iter   30] loss: 111914.6406 RMSElog: 9.3004 grad_loss: 11181.2412 normal_loss: 0.9218
[epoch 45][iter   40] loss: 197520.7500 RMSElog: 10.1663 grad_loss: 19740.9414 normal_loss: 0.9662
[epoch 45][iter   50] loss: 195412.9844 RMSElog: 10.0493 grad_loss: 19530.2969 normal_loss: 0.9539
[epoch 45][iter   60] loss: 167412.9531 RMSElog: 9.7311 grad_loss: 16730.6387 normal_loss: 0.9260
[epoch 45][iter   70] loss: 154029.7812 RMSElog: 9.8645 grad_loss: 15392.1729 normal_loss: 0.9405
[epoch 45][iter   80] loss: 177063.6250 RMSElog: 10.1532 grad_loss: 17695.2383 normal_loss: 0.9719
[epoch 45][iter   90] loss: 150153.5000 RMSElog: 10.0554 grad_loss: 15004.3457 normal_loss: 0.9478
[epoch 45][iter  100] loss: 167008.4531 RMSElog: 10.0165 grad_loss: 16689.8750 normal_loss: 0.9557
[epoch 45][iter  110] loss: 154609.8281 RMSElog: 9.8745 grad_loss: 15450.1855 normal_loss: 0.9220
[epoch 45][iter  120] loss: 176914.4531 RMSElog: 10.3802 grad_loss: 17680.1094 normal_loss: 0.9546
[epoch 45][iter  130] loss: 168688.8125 RMSElog: 9.8493 grad_loss: 16858.0781 normal_loss: 0.9524
[epoch 45][iter  140] loss: 153447.6875 RMSElog: 9.9051 grad_loss: 15333.9209 normal_loss: 0.9425
[epoch 45][iter  150] loss: 176260.7500 RMSElog: 10.0638 grad_loss: 17615.0703 normal_loss: 0.9395
[epoch 45][iter  160] loss: 147371.3906 RMSElog: 9.6635 grad_loss: 14726.5488 normal_loss: 0.9280
[epoch 45][iter  170] loss: 146465.6875 RMSElog: 10.2283 grad_loss: 14635.4062 normal_loss: 0.9337
[epoch 45][iter  180] loss: 232982.0469 RMSElog: 10.2845 grad_loss: 23286.9707 normal_loss: 0.9500
[epoch 45][iter  190] loss: 154165.2969 RMSElog: 10.0187 grad_loss: 15405.5605 normal_loss: 0.9498
[epoch 45][iter  200] loss: 141865.3906 RMSElog: 9.7199 grad_loss: 14175.8896 normal_loss: 0.9298
[epoch 45][iter  210] loss: 208770.3906 RMSElog: 10.1964 grad_loss: 20865.8984 normal_loss: 0.9442
[epoch 45][iter  220] loss: 156985.6250 RMSElog: 9.5765 grad_loss: 15688.0479 normal_loss: 0.9385
[epoch 45][iter  230] loss: 193024.5000 RMSElog: 9.9639 grad_loss: 19291.5117 normal_loss: 0.9740
[epoch 45][iter  240] loss: 109441.5234 RMSElog: 9.7466 grad_loss: 10933.4756 normal_loss: 0.9300
[epoch 45][iter  250] loss: 177814.3125 RMSElog: 10.0854 grad_loss: 17770.3887 normal_loss: 0.9571
[epoch 45][iter  260] loss: 134677.6719 RMSElog: 9.8776 grad_loss: 13456.9443 normal_loss: 0.9441
[epoch 45][iter  270] loss: 149369.9219 RMSElog: 10.0930 grad_loss: 14925.9766 normal_loss: 0.9225
[epoch 45][iter  280] loss: 106769.7500 RMSElog: 9.8688 grad_loss: 10666.1914 normal_loss: 0.9140
[epoch 45][iter  290] loss: 196437.8125 RMSElog: 10.2319 grad_loss: 19632.5898 normal_loss: 0.9584
[epoch 45][iter  300] loss: 179740.0625 RMSElog: 10.1206 grad_loss: 17962.9453 normal_loss: 0.9397
[epoch 45][iter  310] loss: 199070.3125 RMSElog: 9.9947 grad_loss: 19896.0605 normal_loss: 0.9757
[epoch 45][iter  320] loss: 104446.5625 RMSElog: 10.1932 grad_loss: 10433.5342 normal_loss: 0.9288
[epoch 45][iter  330] loss: 211925.8438 RMSElog: 10.1863 grad_loss: 21181.4551 normal_loss: 0.9436
[epoch 45][iter  340] loss: 167345.2500 RMSElog: 9.8843 grad_loss: 16723.6777 normal_loss: 0.9620
[epoch 45][iter  350] loss: 185902.4219 RMSElog: 9.8998 grad_loss: 18579.3711 normal_loss: 0.9705
[epoch 45][iter  360] loss: 185772.2500 RMSElog: 9.9702 grad_loss: 18566.2969 normal_loss: 0.9567
[epoch 45][iter  370] loss: 105816.5000 RMSElog: 9.4266 grad_loss: 10571.3066 normal_loss: 0.9171
[epoch 45][iter  380] loss: 162522.4531 RMSElog: 9.9860 grad_loss: 16241.3066 normal_loss: 0.9518
[epoch 45][iter  390] loss: 193978.6250 RMSElog: 10.3530 grad_loss: 19386.5586 normal_loss: 0.9518
[epoch 45][iter  400] loss: 178593.9688 RMSElog: 10.0081 grad_loss: 17848.4395 normal_loss: 0.9490
[epoch 45][iter  410] loss: 224077.6719 RMSElog: 10.3869 grad_loss: 22396.4219 normal_loss: 0.9584
[epoch 45][iter  420] loss: 164843.6719 RMSElog: 9.7972 grad_loss: 16473.6348 normal_loss: 0.9360
[epoch 45][iter  430] loss: 166542.4219 RMSElog: 9.8913 grad_loss: 16643.4043 normal_loss: 0.9482
[epoch 45][iter  440] loss: 151723.8750 RMSElog: 9.9680 grad_loss: 15161.4746 normal_loss: 0.9449
[epoch 45][iter  450] loss: 113426.9688 RMSElog: 9.7682 grad_loss: 11331.9922 normal_loss: 0.9362
[epoch 45][iter  460] loss: 185495.4062 RMSElog: 9.9310 grad_loss: 18538.6641 normal_loss: 0.9455
[epoch 45][iter  470] loss: 160074.2031 RMSElog: 9.7506 grad_loss: 15996.7432 normal_loss: 0.9257
[epoch 45][iter  480] loss: 135114.8750 RMSElog: 9.9265 grad_loss: 13500.6055 normal_loss: 0.9548
[epoch 45][iter  490] loss: 157706.0781 RMSElog: 9.9981 grad_loss: 15759.6602 normal_loss: 0.9499
[epoch 45][iter  500] loss: 206083.5156 RMSElog: 10.2501 grad_loss: 20597.1445 normal_loss: 0.9562
[epoch 45][iter  510] loss: 175122.9531 RMSElog: 9.9588 grad_loss: 17501.3789 normal_loss: 0.9573
[epoch 45][iter  520] loss: 169013.5312 RMSElog: 9.5880 grad_loss: 16890.8242 normal_loss: 0.9419
[epoch 45][iter  530] loss: 110603.0625 RMSElog: 9.8046 grad_loss: 11049.5654 normal_loss: 0.9364
[epoch 45][iter  540] loss: 148057.2500 RMSElog: 9.9895 grad_loss: 14794.7812 normal_loss: 0.9555
[epoch 45][iter  550] loss: 119655.7969 RMSElog: 9.6110 grad_loss: 11955.0312 normal_loss: 0.9378
[epoch 45][iter  560] loss: 137296.6406 RMSElog: 9.9750 grad_loss: 13718.7402 normal_loss: 0.9495
[epoch 45][iter  570] loss: 104749.2812 RMSElog: 9.7512 grad_loss: 10464.2490 normal_loss: 0.9281
[epoch 45][iter  580] loss: 201162.4062 RMSElog: 9.9531 grad_loss: 20105.3164 normal_loss: 0.9706
[epoch 45][iter  590] loss: 138950.6406 RMSElog: 9.9777 grad_loss: 13884.1387 normal_loss: 0.9482
[epoch 46][iter    0] loss: 167730.9531 RMSElog: 9.9663 grad_loss: 16762.1797 normal_loss: 0.9501
[epoch 46][iter   10] loss: 121959.7344 RMSElog: 9.7378 grad_loss: 12185.3047 normal_loss: 0.9314
[epoch 46][iter   20] loss: 115452.5781 RMSElog: 9.2301 grad_loss: 11535.1230 normal_loss: 0.9042
[epoch 46][iter   30] loss: 117693.8984 RMSElog: 9.6527 grad_loss: 11758.8105 normal_loss: 0.9267
[epoch 46][iter   40] loss: 157810.1406 RMSElog: 9.9254 grad_loss: 15770.1621 normal_loss: 0.9253
[epoch 46][iter   50] loss: 190938.1250 RMSElog: 10.3028 grad_loss: 19082.5566 normal_loss: 0.9541
[epoch 46][iter   60] loss: 178784.1250 RMSElog: 9.9460 grad_loss: 17867.5078 normal_loss: 0.9592
[epoch 46][iter   70] loss: 203973.6562 RMSElog: 10.4423 grad_loss: 20385.9727 normal_loss: 0.9505
[epoch 46][iter   80] loss: 196437.8125 RMSElog: 10.2319 grad_loss: 19632.5898 normal_loss: 0.9584
[epoch 46][iter   90] loss: 144440.4531 RMSElog: 9.8843 grad_loss: 14433.2314 normal_loss: 0.9296
[epoch 46][iter  100] loss: 198293.9219 RMSElog: 10.2275 grad_loss: 19818.2148 normal_loss: 0.9521
[epoch 46][iter  110] loss: 129743.7969 RMSElog: 9.9523 grad_loss: 12963.4785 normal_loss: 0.9492
[epoch 46][iter  120] loss: 182005.7031 RMSElog: 10.0998 grad_loss: 18189.5195 normal_loss: 0.9507
[epoch 46][iter  130] loss: 149387.5469 RMSElog: 9.9550 grad_loss: 14927.8516 normal_loss: 0.9483
[epoch 46][iter  140] loss: 110874.4688 RMSElog: 9.2829 grad_loss: 11077.2607 normal_loss: 0.9034
[epoch 46][iter  150] loss: 122331.3906 RMSElog: 9.7180 grad_loss: 12222.4795 normal_loss: 0.9418
[epoch 46][iter  160] loss: 184016.5469 RMSElog: 9.9468 grad_loss: 18390.7363 normal_loss: 0.9712
[epoch 46][iter  170] loss: 160825.5938 RMSElog: 9.9290 grad_loss: 16071.6816 normal_loss: 0.9491
[epoch 46][iter  180] loss: 211640.2344 RMSElog: 10.2414 grad_loss: 21152.8145 normal_loss: 0.9675
[epoch 46][iter  190] loss: 216509.5625 RMSElog: 10.1942 grad_loss: 21639.8223 normal_loss: 0.9420
[epoch 46][iter  200] loss: 164181.7344 RMSElog: 10.0104 grad_loss: 16407.1973 normal_loss: 0.9660
[epoch 46][iter  210] loss: 110808.0469 RMSElog: 9.7171 grad_loss: 11070.1562 normal_loss: 0.9312
[epoch 46][iter  220] loss: 214775.1719 RMSElog: 10.1719 grad_loss: 21466.3848 normal_loss: 0.9613
[epoch 46][iter  230] loss: 137296.6406 RMSElog: 9.9750 grad_loss: 13718.7402 normal_loss: 0.9495
[epoch 46][iter  240] loss: 224077.6719 RMSElog: 10.3869 grad_loss: 22396.4219 normal_loss: 0.9584
[epoch 46][iter  250] loss: 175008.6562 RMSElog: 9.9496 grad_loss: 17489.9434 normal_loss: 0.9734
[epoch 46][iter  260] loss: 158875.5312 RMSElog: 9.5853 grad_loss: 15877.0430 normal_loss: 0.9260
[epoch 46][iter  270] loss: 143904.7812 RMSElog: 10.0263 grad_loss: 14379.5068 normal_loss: 0.9457
[epoch 46][iter  280] loss: 119895.5078 RMSElog: 9.3187 grad_loss: 11979.3164 normal_loss: 0.9157
[epoch 46][iter  290] loss: 181465.8125 RMSElog: 10.1136 grad_loss: 18135.5117 normal_loss: 0.9572
[epoch 46][iter  300] loss: 161633.2656 RMSElog: 10.0107 grad_loss: 16152.3643 normal_loss: 0.9518
[epoch 46][iter  310] loss: 154609.8281 RMSElog: 9.8745 grad_loss: 15450.1855 normal_loss: 0.9220
[epoch 46][iter  320] loss: 111518.2734 RMSElog: 9.6257 grad_loss: 11141.2676 normal_loss: 0.9335
[epoch 46][iter  330] loss: 206764.1875 RMSElog: 10.2849 grad_loss: 20665.1777 normal_loss: 0.9549
[epoch 46][iter  340] loss: 223018.5625 RMSElog: 10.3592 grad_loss: 22290.5508 normal_loss: 0.9462
[epoch 46][iter  350] loss: 167211.6250 RMSElog: 10.0285 grad_loss: 16710.1777 normal_loss: 0.9542
[epoch 46][iter  360] loss: 135215.1250 RMSElog: 9.9407 grad_loss: 13510.6152 normal_loss: 0.9572
[epoch 46][iter  370] loss: 114924.9844 RMSElog: 9.6925 grad_loss: 11481.8691 normal_loss: 0.9360
[epoch 46][iter  380] loss: 136869.7500 RMSElog: 9.5771 grad_loss: 13676.4668 normal_loss: 0.9315
[epoch 46][iter  390] loss: 216232.0312 RMSElog: 10.2147 grad_loss: 21612.0234 normal_loss: 0.9640
[epoch 46][iter  400] loss: 207885.1875 RMSElog: 10.4422 grad_loss: 20777.1328 normal_loss: 0.9460
[epoch 46][iter  410] loss: 175359.1875 RMSElog: 10.1476 grad_loss: 17524.8125 normal_loss: 0.9571
[epoch 46][iter  420] loss: 162835.5312 RMSElog: 9.9145 grad_loss: 16272.6904 normal_loss: 0.9492
[epoch 46][iter  430] loss: 203023.3438 RMSElog: 10.3036 grad_loss: 20291.0723 normal_loss: 0.9583
[epoch 46][iter  440] loss: 135699.4688 RMSElog: 9.6754 grad_loss: 13559.3340 normal_loss: 0.9364
[epoch 46][iter  450] loss: 188717.5625 RMSElog: 10.0336 grad_loss: 18860.7715 normal_loss: 0.9509
[epoch 46][iter  460] loss: 198743.4219 RMSElog: 10.0842 grad_loss: 19863.2930 normal_loss: 0.9643
[epoch 46][iter  470] loss: 189507.2500 RMSElog: 10.2157 grad_loss: 18939.5586 normal_loss: 0.9510
[epoch 46][iter  480] loss: 189543.6719 RMSElog: 10.1592 grad_loss: 18943.2598 normal_loss: 0.9474
[epoch 46][iter  490] loss: 108409.6484 RMSElog: 9.2105 grad_loss: 10830.8506 normal_loss: 0.9033
[epoch 46][iter  500] loss: 220587.5781 RMSElog: 10.2995 grad_loss: 22047.5117 normal_loss: 0.9466
[epoch 46][iter  510] loss: 162522.4531 RMSElog: 9.9860 grad_loss: 16241.3066 normal_loss: 0.9518
[epoch 46][iter  520] loss: 147544.5000 RMSElog: 9.8913 grad_loss: 14743.6045 normal_loss: 0.9534
[epoch 46][iter  530] loss: 145332.8438 RMSElog: 10.0385 grad_loss: 14522.2842 normal_loss: 0.9619
[epoch 46][iter  540] loss: 188502.1875 RMSElog: 10.1669 grad_loss: 18839.0801 normal_loss: 0.9722
[epoch 46][iter  550] loss: 124246.3594 RMSElog: 9.7165 grad_loss: 12413.9834 normal_loss: 0.9360
[epoch 46][iter  560] loss: 170542.9531 RMSElog: 9.7855 grad_loss: 17043.5410 normal_loss: 0.9688
[epoch 46][iter  570] loss: 197520.7500 RMSElog: 10.1663 grad_loss: 19740.9414 normal_loss: 0.9662
[epoch 46][iter  580] loss: 103955.0000 RMSElog: 9.6256 grad_loss: 10384.9521 normal_loss: 0.9222
[epoch 46][iter  590] loss: 99107.7500 RMSElog: 9.5230 grad_loss: 9900.3428 normal_loss: 0.9094
[epoch 47][iter    0] loss: 140645.6875 RMSElog: 10.1118 grad_loss: 14053.4873 normal_loss: 0.9695
[epoch 47][iter   10] loss: 115534.3750 RMSElog: 9.7196 grad_loss: 11542.7852 normal_loss: 0.9325
[epoch 47][iter   20] loss: 151723.8750 RMSElog: 9.9680 grad_loss: 15161.4746 normal_loss: 0.9449
[epoch 47][iter   30] loss: 148057.2500 RMSElog: 9.9895 grad_loss: 14794.7812 normal_loss: 0.9555
[epoch 47][iter   40] loss: 182315.9688 RMSElog: 10.1291 grad_loss: 18220.5039 normal_loss: 0.9652
[epoch 47][iter   50] loss: 172755.6875 RMSElog: 10.0251 grad_loss: 17264.6016 normal_loss: 0.9419
[epoch 47][iter   60] loss: 164767.1875 RMSElog: 9.8398 grad_loss: 16465.9102 normal_loss: 0.9691
[epoch 47][iter   70] loss: 145409.6719 RMSElog: 9.8330 grad_loss: 14530.1904 normal_loss: 0.9445
[epoch 47][iter   80] loss: 141228.9219 RMSElog: 9.9926 grad_loss: 14111.9531 normal_loss: 0.9477
[epoch 47][iter   90] loss: 160074.2031 RMSElog: 9.7506 grad_loss: 15996.7432 normal_loss: 0.9257
[epoch 47][iter  100] loss: 120765.1875 RMSElog: 9.5441 grad_loss: 12066.0400 normal_loss: 0.9348
[epoch 47][iter  110] loss: 206400.7812 RMSElog: 10.3027 grad_loss: 20628.8281 normal_loss: 0.9467
[epoch 47][iter  120] loss: 147765.9219 RMSElog: 10.0607 grad_loss: 14765.6104 normal_loss: 0.9219
[epoch 47][iter  130] loss: 161285.2188 RMSElog: 9.8339 grad_loss: 16117.7402 normal_loss: 0.9476
[epoch 47][iter  140] loss: 110808.0469 RMSElog: 9.7171 grad_loss: 11070.1562 normal_loss: 0.9312
[epoch 47][iter  150] loss: 138606.5781 RMSElog: 9.6389 grad_loss: 13850.0977 normal_loss: 0.9205
[epoch 47][iter  160] loss: 161088.9688 RMSElog: 9.9258 grad_loss: 16098.0254 normal_loss: 0.9455
[epoch 47][iter  170] loss: 148184.5469 RMSElog: 10.0681 grad_loss: 14807.4355 normal_loss: 0.9504
[epoch 47][iter  180] loss: 146161.1562 RMSElog: 10.0682 grad_loss: 14605.1289 normal_loss: 0.9190
[epoch 47][iter  190] loss: 223018.5625 RMSElog: 10.3592 grad_loss: 22290.5508 normal_loss: 0.9462
[epoch 47][iter  200] loss: 150364.6250 RMSElog: 9.9123 grad_loss: 15025.6094 normal_loss: 0.9418
[epoch 47][iter  210] loss: 132707.3594 RMSElog: 9.7842 grad_loss: 13260.0049 normal_loss: 0.9472
[epoch 47][iter  220] loss: 110603.0625 RMSElog: 9.8046 grad_loss: 11049.5654 normal_loss: 0.9364
[epoch 47][iter  230] loss: 150323.7656 RMSElog: 10.0510 grad_loss: 15021.3848 normal_loss: 0.9410
[epoch 47][iter  240] loss: 129743.7969 RMSElog: 9.9523 grad_loss: 12963.4785 normal_loss: 0.9492
[epoch 47][iter  250] loss: 233395.7188 RMSElog: 10.2368 grad_loss: 23328.3906 normal_loss: 0.9452
[epoch 47][iter  260] loss: 182859.0469 RMSElog: 9.8110 grad_loss: 18275.1250 normal_loss: 0.9678
[epoch 47][iter  270] loss: 218625.8750 RMSElog: 10.2839 grad_loss: 21851.3457 normal_loss: 0.9592
[epoch 47][iter  280] loss: 138057.5156 RMSElog: 9.5188 grad_loss: 13795.3184 normal_loss: 0.9138
[epoch 47][iter  290] loss: 181942.5625 RMSElog: 10.0291 grad_loss: 18183.2793 normal_loss: 0.9464
[epoch 47][iter  300] loss: 127463.1875 RMSElog: 9.4952 grad_loss: 12735.9082 normal_loss: 0.9153
[epoch 47][iter  310] loss: 114827.1094 RMSElog: 9.7767 grad_loss: 11472.0020 normal_loss: 0.9329
[epoch 47][iter  320] loss: 137249.3438 RMSElog: 9.7342 grad_loss: 13714.2764 normal_loss: 0.9239
[epoch 47][iter  330] loss: 104340.6328 RMSElog: 9.7692 grad_loss: 10423.3701 normal_loss: 0.9234
[epoch 47][iter  340] loss: 120590.1719 RMSElog: 9.9054 grad_loss: 12048.1719 normal_loss: 0.9408
[epoch 47][iter  350] loss: 114509.4531 RMSElog: 9.7409 grad_loss: 11440.2627 normal_loss: 0.9413
[epoch 47][iter  360] loss: 146960.1406 RMSElog: 10.0979 grad_loss: 14684.9941 normal_loss: 0.9220
[epoch 47][iter  370] loss: 144657.9844 RMSElog: 9.9197 grad_loss: 14454.9258 normal_loss: 0.9518
[epoch 47][iter  380] loss: 193024.5000 RMSElog: 9.9639 grad_loss: 19291.5117 normal_loss: 0.9740
[epoch 47][iter  390] loss: 196277.7188 RMSElog: 10.2453 grad_loss: 19616.5664 normal_loss: 0.9595
[epoch 47][iter  400] loss: 162556.3906 RMSElog: 9.9091 grad_loss: 16244.7939 normal_loss: 0.9368
[epoch 47][iter  410] loss: 168688.8125 RMSElog: 9.8493 grad_loss: 16858.0781 normal_loss: 0.9524
[epoch 47][iter  420] loss: 137252.6406 RMSElog: 9.3538 grad_loss: 13714.9785 normal_loss: 0.9319
[epoch 47][iter  430] loss: 105828.7812 RMSElog: 10.1032 grad_loss: 10571.8545 normal_loss: 0.9195
[epoch 47][iter  440] loss: 161646.4688 RMSElog: 10.1105 grad_loss: 16153.5762 normal_loss: 0.9608
[epoch 47][iter  450] loss: 218949.8281 RMSElog: 10.4462 grad_loss: 21883.5918 normal_loss: 0.9447
[epoch 47][iter  460] loss: 165020.2344 RMSElog: 9.7791 grad_loss: 16491.2910 normal_loss: 0.9530
[epoch 47][iter  470] loss: 195999.5625 RMSElog: 10.1749 grad_loss: 19588.8047 normal_loss: 0.9756
[epoch 47][iter  480] loss: 106516.8047 RMSElog: 9.6966 grad_loss: 10641.0557 normal_loss: 0.9285
[epoch 47][iter  490] loss: 218457.8281 RMSElog: 10.1255 grad_loss: 21834.7168 normal_loss: 0.9418
[epoch 47][iter  500] loss: 142016.3906 RMSElog: 9.9191 grad_loss: 14190.7676 normal_loss: 0.9526
[epoch 47][iter  510] loss: 127499.3281 RMSElog: 9.2769 grad_loss: 12739.7178 normal_loss: 0.9374
[epoch 47][iter  520] loss: 114149.3750 RMSElog: 9.2655 grad_loss: 11404.7646 normal_loss: 0.9073
[epoch 47][iter  530] loss: 224077.7188 RMSElog: 10.3869 grad_loss: 22396.4258 normal_loss: 0.9584
[epoch 47][iter  540] loss: 215542.7500 RMSElog: 10.4358 grad_loss: 21542.8887 normal_loss: 0.9514
[epoch 47][iter  550] loss: 112535.5625 RMSElog: 9.5823 grad_loss: 11243.0459 normal_loss: 0.9289
[epoch 47][iter  560] loss: 142005.0781 RMSElog: 9.9168 grad_loss: 14189.6533 normal_loss: 0.9371
[epoch 47][iter  570] loss: 169013.5312 RMSElog: 9.5880 grad_loss: 16890.8242 normal_loss: 0.9419
[epoch 47][iter  580] loss: 188717.5625 RMSElog: 10.0336 grad_loss: 18860.7715 normal_loss: 0.9509
[epoch 47][iter  590] loss: 144191.5938 RMSElog: 9.6650 grad_loss: 14408.5654 normal_loss: 0.9284
[epoch 48][iter    0] loss: 162363.8906 RMSElog: 9.7464 grad_loss: 16225.6777 normal_loss: 0.9661
[epoch 48][iter   10] loss: 142016.3906 RMSElog: 9.9191 grad_loss: 14190.7676 normal_loss: 0.9526
[epoch 48][iter   20] loss: 141118.5312 RMSElog: 10.0520 grad_loss: 14100.8545 normal_loss: 0.9477
[epoch 48][iter   30] loss: 154609.8281 RMSElog: 9.8745 grad_loss: 15450.1855 normal_loss: 0.9220
[epoch 48][iter   40] loss: 159798.6094 RMSElog: 9.8075 grad_loss: 15969.1260 normal_loss: 0.9263
[epoch 48][iter   50] loss: 172248.5156 RMSElog: 9.5166 grad_loss: 17214.3730 normal_loss: 0.9619
[epoch 48][iter   60] loss: 133024.8594 RMSElog: 9.6937 grad_loss: 13291.8662 normal_loss: 0.9261
[epoch 48][iter   70] loss: 153723.9844 RMSElog: 9.9609 grad_loss: 15361.4902 normal_loss: 0.9475
[epoch 48][iter   80] loss: 180425.2344 RMSElog: 10.0354 grad_loss: 18031.5332 normal_loss: 0.9552
[epoch 48][iter   90] loss: 148793.9688 RMSElog: 10.1047 grad_loss: 14868.3438 normal_loss: 0.9480
[epoch 48][iter  100] loss: 112535.5625 RMSElog: 9.5823 grad_loss: 11243.0459 normal_loss: 0.9289
[epoch 48][iter  110] loss: 178468.8906 RMSElog: 9.9039 grad_loss: 17836.0469 normal_loss: 0.9380
[epoch 48][iter  120] loss: 129205.9375 RMSElog: 9.8851 grad_loss: 12909.7734 normal_loss: 0.9356
[epoch 48][iter  130] loss: 158034.0625 RMSElog: 9.9527 grad_loss: 15792.5088 normal_loss: 0.9445
[epoch 48][iter  140] loss: 239768.0938 RMSElog: 10.2366 grad_loss: 23965.6250 normal_loss: 0.9466
[epoch 48][iter  150] loss: 146942.0781 RMSElog: 9.4227 grad_loss: 14683.8535 normal_loss: 0.9314
[epoch 48][iter  160] loss: 120440.5938 RMSElog: 9.1890 grad_loss: 12033.9463 normal_loss: 0.9240
[epoch 48][iter  170] loss: 177936.2812 RMSElog: 10.0381 grad_loss: 17782.6348 normal_loss: 0.9545
[epoch 48][iter  180] loss: 136869.7500 RMSElog: 9.5771 grad_loss: 13676.4668 normal_loss: 0.9315
[epoch 48][iter  190] loss: 154165.2969 RMSElog: 10.0187 grad_loss: 15405.5605 normal_loss: 0.9498
[epoch 48][iter  200] loss: 218949.8281 RMSElog: 10.4462 grad_loss: 21883.5918 normal_loss: 0.9447
[epoch 48][iter  210] loss: 160782.5156 RMSElog: 9.7710 grad_loss: 16067.5547 normal_loss: 0.9244
[epoch 48][iter  220] loss: 111712.9062 RMSElog: 9.7826 grad_loss: 11160.5801 normal_loss: 0.9290
[epoch 48][iter  230] loss: 195671.7031 RMSElog: 10.1080 grad_loss: 19556.1133 normal_loss: 0.9494
[epoch 48][iter  240] loss: 160667.2500 RMSElog: 9.9014 grad_loss: 16055.8711 normal_loss: 0.9534
[epoch 48][iter  250] loss: 188502.1875 RMSElog: 10.1669 grad_loss: 18839.0801 normal_loss: 0.9722
[epoch 48][iter  260] loss: 141442.1250 RMSElog: 9.6437 grad_loss: 14133.6191 normal_loss: 0.9495
[epoch 48][iter  270] loss: 221213.4219 RMSElog: 10.1740 grad_loss: 22110.2266 normal_loss: 0.9415
[epoch 48][iter  280] loss: 162711.9688 RMSElog: 9.6160 grad_loss: 16260.6387 normal_loss: 0.9427
[epoch 48][iter  290] loss: 155482.2031 RMSElog: 10.0818 grad_loss: 15537.1826 normal_loss: 0.9552
[epoch 48][iter  300] loss: 175486.0625 RMSElog: 10.0393 grad_loss: 17537.6094 normal_loss: 0.9563
[epoch 48][iter  310] loss: 128883.6328 RMSElog: 9.8798 grad_loss: 12877.5244 normal_loss: 0.9593
[epoch 48][iter  320] loss: 110808.0469 RMSElog: 9.7171 grad_loss: 11070.1562 normal_loss: 0.9312
[epoch 48][iter  330] loss: 165954.0938 RMSElog: 9.8997 grad_loss: 16584.5449 normal_loss: 0.9646
[epoch 48][iter  340] loss: 100749.6875 RMSElog: 9.9914 grad_loss: 10064.0400 normal_loss: 0.9377
[epoch 48][iter  350] loss: 139061.3750 RMSElog: 9.6201 grad_loss: 13895.5771 normal_loss: 0.9401
[epoch 48][iter  360] loss: 153594.7031 RMSElog: 9.8426 grad_loss: 15348.6826 normal_loss: 0.9453
[epoch 48][iter  370] loss: 113895.6328 RMSElog: 9.2606 grad_loss: 11379.3926 normal_loss: 0.9106
[epoch 48][iter  380] loss: 151401.7031 RMSElog: 9.4414 grad_loss: 15129.7930 normal_loss: 0.9361
[epoch 48][iter  390] loss: 102096.6484 RMSElog: 9.8437 grad_loss: 10198.8926 normal_loss: 0.9290
[epoch 48][iter  400] loss: 181983.5312 RMSElog: 10.0741 grad_loss: 18187.3320 normal_loss: 0.9464
[epoch 48][iter  410] loss: 122386.0234 RMSElog: 9.6688 grad_loss: 12227.9971 normal_loss: 0.9366
[epoch 48][iter  420] loss: 177063.6250 RMSElog: 10.1532 grad_loss: 17695.2383 normal_loss: 0.9719
[epoch 48][iter  430] loss: 189507.2500 RMSElog: 10.2157 grad_loss: 18939.5586 normal_loss: 0.9510
[epoch 48][iter  440] loss: 80097.4062 RMSElog: 9.2433 grad_loss: 7999.5806 normal_loss: 0.9166
[epoch 48][iter  450] loss: 204650.3281 RMSElog: 10.1934 grad_loss: 20453.8633 normal_loss: 0.9763
[epoch 48][iter  460] loss: 163253.2500 RMSElog: 9.8740 grad_loss: 16314.5010 normal_loss: 0.9499
[epoch 48][iter  470] loss: 194578.5781 RMSElog: 10.2312 grad_loss: 19446.6680 normal_loss: 0.9596
[epoch 48][iter  480] loss: 134406.1875 RMSElog: 9.9402 grad_loss: 13429.7246 normal_loss: 0.9539
[epoch 48][iter  490] loss: 172966.9375 RMSElog: 9.9430 grad_loss: 17285.8008 normal_loss: 0.9485
[epoch 48][iter  500] loss: 163272.7188 RMSElog: 10.0261 grad_loss: 16316.3047 normal_loss: 0.9400
[epoch 48][iter  510] loss: 148057.2500 RMSElog: 9.9895 grad_loss: 14794.7812 normal_loss: 0.9555
[epoch 48][iter  520] loss: 214493.2188 RMSElog: 10.3525 grad_loss: 21438.0078 normal_loss: 0.9611
[epoch 48][iter  530] loss: 178076.4688 RMSElog: 9.8778 grad_loss: 17796.8457 normal_loss: 0.9233
[epoch 48][iter  540] loss: 197714.9844 RMSElog: 10.4355 grad_loss: 19760.1094 normal_loss: 0.9532
[epoch 48][iter  550] loss: 105896.9375 RMSElog: 9.7662 grad_loss: 10578.9961 normal_loss: 0.9310
[epoch 48][iter  560] loss: 128488.4375 RMSElog: 9.7402 grad_loss: 12838.1719 normal_loss: 0.9319
[epoch 48][iter  570] loss: 160743.4062 RMSElog: 9.7965 grad_loss: 16063.6025 normal_loss: 0.9413
[epoch 48][iter  580] loss: 167730.9531 RMSElog: 9.9663 grad_loss: 16762.1797 normal_loss: 0.9501
[epoch 48][iter  590] loss: 162433.1875 RMSElog: 9.8183 grad_loss: 16232.5488 normal_loss: 0.9510
[epoch 49][iter    0] loss: 149036.1562 RMSElog: 9.7728 grad_loss: 14892.8887 normal_loss: 0.9547
[epoch 49][iter   10] loss: 226949.5156 RMSElog: 10.2029 grad_loss: 22683.8047 normal_loss: 0.9430
[epoch 49][iter   20] loss: 125315.5625 RMSElog: 9.7124 grad_loss: 12520.9131 normal_loss: 0.9309
[epoch 49][iter   30] loss: 196842.3125 RMSElog: 10.1249 grad_loss: 19673.1523 normal_loss: 0.9534
[epoch 49][iter   40] loss: 128883.6406 RMSElog: 9.8798 grad_loss: 12877.5254 normal_loss: 0.9593
[epoch 49][iter   50] loss: 149135.0000 RMSElog: 10.0932 grad_loss: 14902.4570 normal_loss: 0.9500
[epoch 49][iter   60] loss: 197520.7500 RMSElog: 10.1663 grad_loss: 19740.9414 normal_loss: 0.9662
[epoch 49][iter   70] loss: 195413.0312 RMSElog: 10.0493 grad_loss: 19530.3008 normal_loss: 0.9539
[epoch 49][iter   80] loss: 158230.6094 RMSElog: 9.9234 grad_loss: 15812.1895 normal_loss: 0.9483
[epoch 49][iter   90] loss: 177195.4844 RMSElog: 10.1516 grad_loss: 17708.4512 normal_loss: 0.9458
[epoch 49][iter  100] loss: 109148.7344 RMSElog: 10.1172 grad_loss: 10903.8262 normal_loss: 0.9294
[epoch 49][iter  110] loss: 153997.7188 RMSElog: 9.7620 grad_loss: 15389.0713 normal_loss: 0.9383
[epoch 49][iter  120] loss: 131942.8750 RMSElog: 9.8414 grad_loss: 13183.5000 normal_loss: 0.9453
[epoch 49][iter  130] loss: 177814.3125 RMSElog: 10.0854 grad_loss: 17770.3887 normal_loss: 0.9571
[epoch 49][iter  140] loss: 147371.3906 RMSElog: 9.6635 grad_loss: 14726.5488 normal_loss: 0.9280
[epoch 49][iter  150] loss: 199070.3125 RMSElog: 9.9947 grad_loss: 19896.0605 normal_loss: 0.9757
[epoch 49][iter  160] loss: 130527.6094 RMSElog: 9.8914 grad_loss: 13041.9180 normal_loss: 0.9509
[epoch 49][iter  170] loss: 80097.4062 RMSElog: 9.2433 grad_loss: 7999.5806 normal_loss: 0.9166
[epoch 49][iter  180] loss: 124246.3594 RMSElog: 9.7165 grad_loss: 12413.9834 normal_loss: 0.9360
[epoch 49][iter  190] loss: 140938.8594 RMSElog: 9.7483 grad_loss: 14083.1953 normal_loss: 0.9424
[epoch 49][iter  200] loss: 115534.3750 RMSElog: 9.7196 grad_loss: 11542.7852 normal_loss: 0.9325
[epoch 49][iter  210] loss: 205362.7344 RMSElog: 10.1686 grad_loss: 20525.1582 normal_loss: 0.9475
[epoch 49][iter  220] loss: 169013.5625 RMSElog: 9.5880 grad_loss: 16890.8262 normal_loss: 0.9419
[epoch 49][iter  230] loss: 167794.0469 RMSElog: 9.8335 grad_loss: 16768.6211 normal_loss: 0.9500
[epoch 49][iter  240] loss: 150153.5000 RMSElog: 10.0554 grad_loss: 15004.3457 normal_loss: 0.9478
[epoch 49][iter  250] loss: 144440.4531 RMSElog: 9.8843 grad_loss: 14433.2314 normal_loss: 0.9296
[epoch 49][iter  260] loss: 154165.2969 RMSElog: 10.0187 grad_loss: 15405.5605 normal_loss: 0.9498
[epoch 49][iter  270] loss: 123525.9062 RMSElog: 9.3524 grad_loss: 12342.2930 normal_loss: 0.9450
[epoch 49][iter  280] loss: 217174.7812 RMSElog: 10.2254 grad_loss: 21706.2910 normal_loss: 0.9635
[epoch 49][iter  290] loss: 103305.2422 RMSElog: 9.5543 grad_loss: 10320.0674 normal_loss: 0.9024
[epoch 49][iter  300] loss: 154310.0938 RMSElog: 9.9148 grad_loss: 15420.1445 normal_loss: 0.9495
[epoch 49][iter  310] loss: 178784.0938 RMSElog: 9.9460 grad_loss: 17867.5059 normal_loss: 0.9592
[epoch 49][iter  320] loss: 121866.5156 RMSElog: 9.8422 grad_loss: 12175.8965 normal_loss: 0.9131
[epoch 49][iter  330] loss: 170542.9531 RMSElog: 9.7855 grad_loss: 17043.5410 normal_loss: 0.9688
[epoch 49][iter  340] loss: 183095.1562 RMSElog: 9.9762 grad_loss: 18298.5703 normal_loss: 0.9696
[epoch 49][iter  350] loss: 107956.6094 RMSElog: 10.0900 grad_loss: 10784.6504 normal_loss: 0.9209
[epoch 49][iter  360] loss: 87697.6250 RMSElog: 9.9487 grad_loss: 8758.8750 normal_loss: 0.9395
[epoch 49][iter  370] loss: 142544.5156 RMSElog: 10.1645 grad_loss: 14243.3457 normal_loss: 0.9425
[epoch 49][iter  380] loss: 138606.5781 RMSElog: 9.6389 grad_loss: 13850.0977 normal_loss: 0.9205
[epoch 49][iter  390] loss: 182318.8125 RMSElog: 10.1008 grad_loss: 18220.8418 normal_loss: 0.9384
[epoch 49][iter  400] loss: 104127.8047 RMSElog: 9.6242 grad_loss: 10402.2324 normal_loss: 0.9237
[epoch 49][iter  410] loss: 110579.5391 RMSElog: 9.3019 grad_loss: 11047.7109 normal_loss: 0.9413
[epoch 49][iter  420] loss: 119655.7969 RMSElog: 9.6110 grad_loss: 11955.0312 normal_loss: 0.9378
[epoch 49][iter  430] loss: 132012.6719 RMSElog: 9.6802 grad_loss: 13190.6582 normal_loss: 0.9291
[epoch 49][iter  440] loss: 191559.9688 RMSElog: 9.9749 grad_loss: 19145.0742 normal_loss: 0.9477
[epoch 49][iter  450] loss: 150323.7656 RMSElog: 10.0510 grad_loss: 15021.3848 normal_loss: 0.9410
[epoch 49][iter  460] loss: 154290.2500 RMSElog: 9.8471 grad_loss: 15418.2334 normal_loss: 0.9458
[epoch 49][iter  470] loss: 191825.0781 RMSElog: 10.3012 grad_loss: 19171.2500 normal_loss: 0.9570
[epoch 49][iter  480] loss: 150321.2812 RMSElog: 9.9248 grad_loss: 15021.2656 normal_loss: 0.9385
[epoch 49][iter  490] loss: 139433.9219 RMSElog: 9.8139 grad_loss: 13932.6191 normal_loss: 0.9591
[epoch 49][iter  500] loss: 208770.3906 RMSElog: 10.1964 grad_loss: 20865.8984 normal_loss: 0.9442
[epoch 49][iter  510] loss: 146960.1250 RMSElog: 10.0979 grad_loss: 14684.9922 normal_loss: 0.9220
[epoch 49][iter  520] loss: 180425.2344 RMSElog: 10.0354 grad_loss: 18031.5332 normal_loss: 0.9552
[epoch 49][iter  530] loss: 135042.2031 RMSElog: 9.6763 grad_loss: 13493.6074 normal_loss: 0.9366
[epoch 49][iter  540] loss: 165954.0938 RMSElog: 9.8997 grad_loss: 16584.5449 normal_loss: 0.9646
[epoch 49][iter  550] loss: 223875.4062 RMSElog: 10.3929 grad_loss: 22376.1953 normal_loss: 0.9536
[epoch 49][iter  560] loss: 103509.2188 RMSElog: 9.9787 grad_loss: 10340.0186 normal_loss: 0.9251
[epoch 49][iter  570] loss: 152593.5000 RMSElog: 9.8773 grad_loss: 15248.5449 normal_loss: 0.9283
[epoch 49][iter  580] loss: 228053.0312 RMSElog: 10.2288 grad_loss: 22794.1328 normal_loss: 0.9422
[epoch 49][iter  590] loss: 139878.0625 RMSElog: 9.8515 grad_loss: 13977.0225 normal_loss: 0.9327
[epoch 50][iter    0] loss: 153997.7188 RMSElog: 9.7620 grad_loss: 15389.0713 normal_loss: 0.9383
[epoch 50][iter   10] loss: 154626.7656 RMSElog: 9.9098 grad_loss: 15451.8066 normal_loss: 0.9596
[epoch 50][iter   20] loss: 160743.5156 RMSElog: 9.9654 grad_loss: 16063.4521 normal_loss: 0.9340
[epoch 50][iter   30] loss: 143534.0938 RMSElog: 9.7662 grad_loss: 14342.7061 normal_loss: 0.9374
[epoch 50][iter   40] loss: 202119.8125 RMSElog: 10.2544 grad_loss: 20200.7715 normal_loss: 0.9549
[epoch 50][iter   50] loss: 159824.3594 RMSElog: 9.8973 grad_loss: 15971.5811 normal_loss: 0.9569
[epoch 50][iter   60] loss: 135814.7969 RMSElog: 9.5386 grad_loss: 13571.0137 normal_loss: 0.9281
[epoch 50][iter   70] loss: 139433.9219 RMSElog: 9.8139 grad_loss: 13932.6191 normal_loss: 0.9591
[epoch 50][iter   80] loss: 100980.7812 RMSElog: 9.6528 grad_loss: 10087.5176 normal_loss: 0.9085
[epoch 50][iter   90] loss: 122912.0625 RMSElog: 9.7328 grad_loss: 12280.5225 normal_loss: 0.9509
[epoch 50][iter  100] loss: 146942.0781 RMSElog: 9.4227 grad_loss: 14683.8535 normal_loss: 0.9314
[epoch 50][iter  110] loss: 119655.7969 RMSElog: 9.6110 grad_loss: 11955.0312 normal_loss: 0.9378
[epoch 50][iter  120] loss: 181293.3438 RMSElog: 10.0009 grad_loss: 18118.3809 normal_loss: 0.9531
[epoch 50][iter  130] loss: 140333.8125 RMSElog: 9.8261 grad_loss: 14022.6191 normal_loss: 0.9368
[epoch 50][iter  140] loss: 140282.6250 RMSElog: 9.9822 grad_loss: 14017.3145 normal_loss: 0.9650
[epoch 50][iter  150] loss: 114173.8984 RMSElog: 9.3600 grad_loss: 11407.1230 normal_loss: 0.9061
[epoch 50][iter  160] loss: 153594.7031 RMSElog: 9.8426 grad_loss: 15348.6826 normal_loss: 0.9453
[epoch 50][iter  170] loss: 103815.4375 RMSElog: 9.7764 grad_loss: 10370.8340 normal_loss: 0.9338
[epoch 50][iter  180] loss: 100749.6875 RMSElog: 9.9914 grad_loss: 10064.0400 normal_loss: 0.9377
[epoch 50][iter  190] loss: 107073.5156 RMSElog: 9.7289 grad_loss: 10696.6914 normal_loss: 0.9313
[epoch 50][iter  200] loss: 135042.2031 RMSElog: 9.6763 grad_loss: 13493.6074 normal_loss: 0.9366
[epoch 50][iter  210] loss: 166492.4688 RMSElog: 9.8376 grad_loss: 16638.4531 normal_loss: 0.9558
[epoch 50][iter  220] loss: 203023.3438 RMSElog: 10.3036 grad_loss: 20291.0723 normal_loss: 0.9583
[epoch 50][iter  230] loss: 231696.5312 RMSElog: 10.3202 grad_loss: 23158.3867 normal_loss: 0.9457
[epoch 50][iter  240] loss: 160934.5156 RMSElog: 9.8035 grad_loss: 16082.6846 normal_loss: 0.9629
[epoch 50][iter  250] loss: 132012.6719 RMSElog: 9.6802 grad_loss: 13190.6582 normal_loss: 0.9291
[epoch 50][iter  260] loss: 132425.4375 RMSElog: 10.0155 grad_loss: 13231.5801 normal_loss: 0.9468
[epoch 50][iter  270] loss: 136533.4688 RMSElog: 9.6756 grad_loss: 13642.7363 normal_loss: 0.9359
[epoch 50][iter  280] loss: 119895.5078 RMSElog: 9.3187 grad_loss: 11979.3164 normal_loss: 0.9157
[epoch 50][iter  290] loss: 163786.3906 RMSElog: 9.7419 grad_loss: 16367.9502 normal_loss: 0.9470
[epoch 50][iter  300] loss: 195412.9844 RMSElog: 10.0493 grad_loss: 19530.2969 normal_loss: 0.9539
[epoch 50][iter  310] loss: 130739.4062 RMSElog: 9.5471 grad_loss: 13063.4434 normal_loss: 0.9498
[epoch 50][iter  320] loss: 109260.8984 RMSElog: 9.7819 grad_loss: 10915.3799 normal_loss: 0.9279
[epoch 50][iter  330] loss: 147915.7812 RMSElog: 10.0131 grad_loss: 14780.6104 normal_loss: 0.9547
[epoch 50][iter  340] loss: 215955.5312 RMSElog: 10.2675 grad_loss: 21584.3203 normal_loss: 0.9654
[epoch 50][iter  350] loss: 130145.6250 RMSElog: 9.7206 grad_loss: 13003.9072 normal_loss: 0.9344
[epoch 50][iter  360] loss: 160667.2500 RMSElog: 9.9014 grad_loss: 16055.8711 normal_loss: 0.9534
[epoch 50][iter  370] loss: 135966.2969 RMSElog: 9.6504 grad_loss: 13586.0273 normal_loss: 0.9522
[epoch 50][iter  380] loss: 167740.4688 RMSElog: 9.8540 grad_loss: 16763.2344 normal_loss: 0.9591
[epoch 50][iter  390] loss: 124932.5703 RMSElog: 9.4215 grad_loss: 12482.8789 normal_loss: 0.9558
[epoch 50][iter  400] loss: 226865.1406 RMSElog: 10.2887 grad_loss: 22675.2832 normal_loss: 0.9418
[epoch 50][iter  410] loss: 218547.0156 RMSElog: 10.4419 grad_loss: 21843.3086 normal_loss: 0.9515
[epoch 50][iter  420] loss: 179740.0625 RMSElog: 10.1206 grad_loss: 17962.9453 normal_loss: 0.9397
[epoch 50][iter  430] loss: 109101.1250 RMSElog: 9.7363 grad_loss: 10899.4590 normal_loss: 0.9167
[epoch 50][iter  440] loss: 197715.0000 RMSElog: 10.4355 grad_loss: 19760.1113 normal_loss: 0.9532
[epoch 50][iter  450] loss: 172683.9219 RMSElog: 9.9061 grad_loss: 17257.5312 normal_loss: 0.9550
[epoch 50][iter  460] loss: 124246.3594 RMSElog: 9.7165 grad_loss: 12413.9834 normal_loss: 0.9360
[epoch 50][iter  470] loss: 100663.8984 RMSElog: 9.9598 grad_loss: 10055.5020 normal_loss: 0.9277
[epoch 50][iter  480] loss: 182476.7344 RMSElog: 9.9672 grad_loss: 18236.7656 normal_loss: 0.9407
[epoch 50][iter  490] loss: 224257.9688 RMSElog: 10.2842 grad_loss: 22414.5605 normal_loss: 0.9521
[epoch 50][iter  500] loss: 179901.4844 RMSElog: 9.8267 grad_loss: 17979.3633 normal_loss: 0.9589
[epoch 50][iter  510] loss: 148239.1406 RMSElog: 9.8055 grad_loss: 14813.1689 normal_loss: 0.9395
[epoch 50][iter  520] loss: 137249.3438 RMSElog: 9.7342 grad_loss: 13714.2764 normal_loss: 0.9239
[epoch 50][iter  530] loss: 104749.2812 RMSElog: 9.7512 grad_loss: 10464.2490 normal_loss: 0.9281
[epoch 50][iter  540] loss: 158230.6094 RMSElog: 9.9234 grad_loss: 15812.1895 normal_loss: 0.9483
[epoch 50][iter  550] loss: 115500.4766 RMSElog: 9.9924 grad_loss: 11539.1230 normal_loss: 0.9328
[epoch 50][iter  560] loss: 162976.7812 RMSElog: 9.6016 grad_loss: 16287.1572 normal_loss: 0.9201
[epoch 50][iter  570] loss: 114893.3984 RMSElog: 9.2562 grad_loss: 11479.1777 normal_loss: 0.9067
[epoch 50][iter  580] loss: 162556.3906 RMSElog: 9.9091 grad_loss: 16244.7939 normal_loss: 0.9368
[epoch 50][iter  590] loss: 134677.6719 RMSElog: 9.8776 grad_loss: 13456.9443 normal_loss: 0.9441
[epoch 51][iter    0] loss: 142138.5156 RMSElog: 9.7967 grad_loss: 14203.1260 normal_loss: 0.9291
[epoch 51][iter   10] loss: 141442.1250 RMSElog: 9.6437 grad_loss: 14133.6191 normal_loss: 0.9495
[epoch 51][iter   20] loss: 154310.0938 RMSElog: 9.9148 grad_loss: 15420.1445 normal_loss: 0.9495
[epoch 51][iter   30] loss: 201373.6719 RMSElog: 10.3870 grad_loss: 20126.0312 normal_loss: 0.9491
[epoch 51][iter   40] loss: 150153.5000 RMSElog: 10.0554 grad_loss: 15004.3457 normal_loss: 0.9478
[epoch 51][iter   50] loss: 165020.2344 RMSElog: 9.7791 grad_loss: 16491.2910 normal_loss: 0.9530
[epoch 51][iter   60] loss: 167084.9219 RMSElog: 9.8660 grad_loss: 16697.6582 normal_loss: 0.9689
[epoch 51][iter   70] loss: 135966.2969 RMSElog: 9.6504 grad_loss: 13586.0273 normal_loss: 0.9522
[epoch 51][iter   80] loss: 156253.7656 RMSElog: 9.8231 grad_loss: 15614.6211 normal_loss: 0.9316
[epoch 51][iter   90] loss: 177664.5469 RMSElog: 9.7808 grad_loss: 17755.7363 normal_loss: 0.9369
[epoch 51][iter  100] loss: 188645.4531 RMSElog: 10.1590 grad_loss: 18853.4082 normal_loss: 0.9794
[epoch 51][iter  110] loss: 153099.1094 RMSElog: 9.8355 grad_loss: 15299.1445 normal_loss: 0.9304
[epoch 51][iter  120] loss: 163272.7188 RMSElog: 10.0261 grad_loss: 16316.3047 normal_loss: 0.9400
[epoch 51][iter  130] loss: 185772.2500 RMSElog: 9.9702 grad_loss: 18566.2969 normal_loss: 0.9567
[epoch 51][iter  140] loss: 156173.3594 RMSElog: 9.9681 grad_loss: 15606.4199 normal_loss: 0.9482
[epoch 51][iter  150] loss: 162522.4531 RMSElog: 9.9860 grad_loss: 16241.3066 normal_loss: 0.9518
[epoch 51][iter  160] loss: 139154.8594 RMSElog: 9.5986 grad_loss: 13904.9707 normal_loss: 0.9164
[epoch 51][iter  170] loss: 214808.7500 RMSElog: 10.1872 grad_loss: 21469.7461 normal_loss: 0.9423
[epoch 51][iter  180] loss: 106769.7500 RMSElog: 9.8688 grad_loss: 10666.1914 normal_loss: 0.9140
[epoch 51][iter  190] loss: 161285.2188 RMSElog: 9.8339 grad_loss: 16117.7402 normal_loss: 0.9476
[epoch 51][iter  200] loss: 189507.2500 RMSElog: 10.2157 grad_loss: 18939.5586 normal_loss: 0.9510
[epoch 51][iter  210] loss: 117693.8984 RMSElog: 9.6527 grad_loss: 11758.8105 normal_loss: 0.9267
[epoch 51][iter  220] loss: 161633.2812 RMSElog: 10.0107 grad_loss: 16152.3652 normal_loss: 0.9518
[epoch 51][iter  230] loss: 223018.5625 RMSElog: 10.3592 grad_loss: 22290.5508 normal_loss: 0.9462
[epoch 51][iter  240] loss: 179649.7188 RMSElog: 10.2020 grad_loss: 17953.8125 normal_loss: 0.9593
[epoch 51][iter  250] loss: 132623.8750 RMSElog: 9.3325 grad_loss: 13252.1152 normal_loss: 0.9391
[epoch 51][iter  260] loss: 181983.5312 RMSElog: 10.0741 grad_loss: 18187.3320 normal_loss: 0.9464
[epoch 51][iter  270] loss: 132707.3594 RMSElog: 9.7842 grad_loss: 13260.0049 normal_loss: 0.9472
[epoch 51][iter  280] loss: 218757.0312 RMSElog: 10.1228 grad_loss: 21864.6270 normal_loss: 0.9524
[epoch 51][iter  290] loss: 103948.1719 RMSElog: 9.7608 grad_loss: 10384.1299 normal_loss: 0.9264
[epoch 51][iter  300] loss: 170056.2812 RMSElog: 9.7423 grad_loss: 16994.9395 normal_loss: 0.9472
[epoch 51][iter  310] loss: 147371.3906 RMSElog: 9.6635 grad_loss: 14726.5488 normal_loss: 0.9280
[epoch 51][iter  320] loss: 136796.3594 RMSElog: 9.6524 grad_loss: 13669.0283 normal_loss: 0.9549
[epoch 51][iter  330] loss: 226949.5156 RMSElog: 10.2029 grad_loss: 22683.8047 normal_loss: 0.9430
[epoch 51][iter  340] loss: 143440.0000 RMSElog: 9.6404 grad_loss: 14333.4336 normal_loss: 0.9261
[epoch 51][iter  350] loss: 104749.2812 RMSElog: 9.7512 grad_loss: 10464.2490 normal_loss: 0.9281
[epoch 51][iter  360] loss: 130739.4062 RMSElog: 9.5471 grad_loss: 13063.4434 normal_loss: 0.9498
[epoch 51][iter  370] loss: 152593.5000 RMSElog: 9.8773 grad_loss: 15248.5449 normal_loss: 0.9283
[epoch 51][iter  380] loss: 119923.2031 RMSElog: 9.5699 grad_loss: 11981.8154 normal_loss: 0.9342
[epoch 51][iter  390] loss: 221213.4531 RMSElog: 10.1740 grad_loss: 22110.2305 normal_loss: 0.9415
[epoch 51][iter  400] loss: 156709.9531 RMSElog: 9.8741 grad_loss: 15660.1758 normal_loss: 0.9453
[epoch 51][iter  410] loss: 145332.8438 RMSElog: 10.0385 grad_loss: 14522.2842 normal_loss: 0.9619
[epoch 51][iter  420] loss: 104456.8438 RMSElog: 10.0170 grad_loss: 10434.7461 normal_loss: 0.9214
[epoch 51][iter  430] loss: 197715.0000 RMSElog: 10.4355 grad_loss: 19760.1113 normal_loss: 0.9532
[epoch 51][iter  440] loss: 152540.7500 RMSElog: 9.9175 grad_loss: 15243.2090 normal_loss: 0.9474
[epoch 51][iter  450] loss: 182315.9688 RMSElog: 10.1291 grad_loss: 18220.5039 normal_loss: 0.9652
[epoch 51][iter  460] loss: 127499.3281 RMSElog: 9.2769 grad_loss: 12739.7178 normal_loss: 0.9374
[epoch 51][iter  470] loss: 180208.2188 RMSElog: 10.0490 grad_loss: 18009.8223 normal_loss: 0.9517
[epoch 51][iter  480] loss: 174565.0781 RMSElog: 9.7759 grad_loss: 17445.7832 normal_loss: 0.9484
[epoch 51][iter  490] loss: 208699.7031 RMSElog: 10.3419 grad_loss: 20858.6719 normal_loss: 0.9568
[epoch 51][iter  500] loss: 167740.4688 RMSElog: 9.8540 grad_loss: 16763.2344 normal_loss: 0.9591
[epoch 51][iter  510] loss: 138934.7188 RMSElog: 9.8450 grad_loss: 13882.6904 normal_loss: 0.9370
[epoch 51][iter  520] loss: 134406.1875 RMSElog: 9.9402 grad_loss: 13429.7246 normal_loss: 0.9539
[epoch 51][iter  530] loss: 163171.1562 RMSElog: 9.7905 grad_loss: 16306.3594 normal_loss: 0.9652
[epoch 51][iter  540] loss: 167784.2500 RMSElog: 9.9330 grad_loss: 16767.5391 normal_loss: 0.9539
[epoch 51][iter  550] loss: 154609.8281 RMSElog: 9.8745 grad_loss: 15450.1855 normal_loss: 0.9220
[epoch 51][iter  560] loss: 125281.0156 RMSElog: 9.5401 grad_loss: 12517.6484 normal_loss: 0.9128
[epoch 51][iter  570] loss: 137249.3438 RMSElog: 9.7342 grad_loss: 13714.2764 normal_loss: 0.9239
[epoch 51][iter  580] loss: 102998.8984 RMSElog: 9.8140 grad_loss: 10289.1318 normal_loss: 0.9431
[epoch 51][iter  590] loss: 202119.8125 RMSElog: 10.2544 grad_loss: 20200.7715 normal_loss: 0.9549
[epoch 52][iter    0] loss: 137764.4219 RMSElog: 9.8227 grad_loss: 13765.6758 normal_loss: 0.9442
[epoch 52][iter   10] loss: 188662.5156 RMSElog: 10.2592 grad_loss: 18855.0352 normal_loss: 0.9574
[epoch 52][iter   20] loss: 208699.7031 RMSElog: 10.3419 grad_loss: 20858.6719 normal_loss: 0.9568
[epoch 52][iter   30] loss: 140333.8438 RMSElog: 9.8261 grad_loss: 14022.6211 normal_loss: 0.9368
[epoch 52][iter   40] loss: 180318.6719 RMSElog: 9.9530 grad_loss: 18020.9785 normal_loss: 0.9357
[epoch 52][iter   50] loss: 218757.0312 RMSElog: 10.1228 grad_loss: 21864.6270 normal_loss: 0.9524
[epoch 52][iter   60] loss: 159422.2656 RMSElog: 9.7010 grad_loss: 15931.5957 normal_loss: 0.9299
[epoch 52][iter   70] loss: 119470.5156 RMSElog: 9.6196 grad_loss: 11936.5020 normal_loss: 0.9297
[epoch 52][iter   80] loss: 165954.0938 RMSElog: 9.8997 grad_loss: 16584.5449 normal_loss: 0.9646
[epoch 52][iter   90] loss: 105877.8750 RMSElog: 9.7638 grad_loss: 10577.0986 normal_loss: 0.9249
[epoch 52][iter  100] loss: 232158.2188 RMSElog: 10.1898 grad_loss: 23204.6875 normal_loss: 0.9455
[epoch 52][iter  110] loss: 204650.3281 RMSElog: 10.1934 grad_loss: 20453.8633 normal_loss: 0.9763
[epoch 52][iter  120] loss: 114924.9844 RMSElog: 9.6925 grad_loss: 11481.8691 normal_loss: 0.9360
[epoch 52][iter  130] loss: 100749.6875 RMSElog: 9.9914 grad_loss: 10064.0400 normal_loss: 0.9377
[epoch 52][iter  140] loss: 128277.9766 RMSElog: 9.7565 grad_loss: 12817.0889 normal_loss: 0.9518
[epoch 52][iter  150] loss: 215955.5312 RMSElog: 10.2675 grad_loss: 21584.3203 normal_loss: 0.9654
[epoch 52][iter  160] loss: 113426.9531 RMSElog: 9.7682 grad_loss: 11331.9902 normal_loss: 0.9362
[epoch 52][iter  170] loss: 195440.9062 RMSElog: 9.7403 grad_loss: 19533.3926 normal_loss: 0.9572
[epoch 52][iter  180] loss: 104127.7969 RMSElog: 9.6242 grad_loss: 10402.2314 normal_loss: 0.9237
[epoch 52][iter  190] loss: 184016.5469 RMSElog: 9.9468 grad_loss: 18390.7363 normal_loss: 0.9712
[epoch 52][iter  200] loss: 196277.7188 RMSElog: 10.2453 grad_loss: 19616.5664 normal_loss: 0.9595
[epoch 52][iter  210] loss: 162433.1875 RMSElog: 9.8183 grad_loss: 16232.5488 normal_loss: 0.9510
[epoch 52][iter  220] loss: 191825.0781 RMSElog: 10.3012 grad_loss: 19171.2500 normal_loss: 0.9570
[epoch 52][iter  230] loss: 189017.5781 RMSElog: 10.1390 grad_loss: 18890.6602 normal_loss: 0.9589
[epoch 52][iter  240] loss: 176798.0469 RMSElog: 9.8176 grad_loss: 17669.0312 normal_loss: 0.9559
[epoch 52][iter  250] loss: 198036.3750 RMSElog: 10.2619 grad_loss: 19792.4219 normal_loss: 0.9529
[epoch 52][iter  260] loss: 135114.8750 RMSElog: 9.9265 grad_loss: 13500.6055 normal_loss: 0.9548
[epoch 52][iter  270] loss: 126821.0234 RMSElog: 10.1398 grad_loss: 12671.0410 normal_loss: 0.9215
[epoch 52][iter  280] loss: 176914.4531 RMSElog: 10.3802 grad_loss: 17680.1094 normal_loss: 0.9546
[epoch 52][iter  290] loss: 158230.6094 RMSElog: 9.9234 grad_loss: 15812.1895 normal_loss: 0.9483
[epoch 52][iter  300] loss: 103955.0000 RMSElog: 9.6256 grad_loss: 10384.9521 normal_loss: 0.9222
[epoch 52][iter  310] loss: 179298.9062 RMSElog: 9.7220 grad_loss: 17919.2227 normal_loss: 0.9453
[epoch 52][iter  320] loss: 136507.7344 RMSElog: 10.0223 grad_loss: 13639.7900 normal_loss: 0.9608
[epoch 52][iter  330] loss: 152149.7500 RMSElog: 9.9062 grad_loss: 15204.1221 normal_loss: 0.9462
[epoch 52][iter  340] loss: 109260.8906 RMSElog: 9.7819 grad_loss: 10915.3789 normal_loss: 0.9279
[epoch 52][iter  350] loss: 182318.8125 RMSElog: 10.1008 grad_loss: 18220.8418 normal_loss: 0.9384
[epoch 52][iter  360] loss: 151368.4219 RMSElog: 9.7883 grad_loss: 15126.1289 normal_loss: 0.9251
[epoch 52][iter  370] loss: 145332.8438 RMSElog: 10.0385 grad_loss: 14522.2842 normal_loss: 0.9619
[epoch 52][iter  380] loss: 185772.2500 RMSElog: 9.9702 grad_loss: 18566.2969 normal_loss: 0.9567
[epoch 52][iter  390] loss: 196437.8125 RMSElog: 10.2319 grad_loss: 19632.5898 normal_loss: 0.9584
[epoch 52][iter  400] loss: 152517.4844 RMSElog: 9.8233 grad_loss: 15241.0010 normal_loss: 0.9249
[epoch 52][iter  410] loss: 130641.2188 RMSElog: 9.7184 grad_loss: 13053.4688 normal_loss: 0.9344
[epoch 52][iter  420] loss: 197714.9844 RMSElog: 10.4355 grad_loss: 19760.1094 normal_loss: 0.9532
[epoch 52][iter  430] loss: 203973.6562 RMSElog: 10.4423 grad_loss: 20385.9727 normal_loss: 0.9505
[epoch 52][iter  440] loss: 143904.7812 RMSElog: 10.0263 grad_loss: 14379.5068 normal_loss: 0.9457
[epoch 52][iter  450] loss: 228338.7500 RMSElog: 10.2936 grad_loss: 22822.6348 normal_loss: 0.9477
[epoch 52][iter  460] loss: 198743.4219 RMSElog: 10.0842 grad_loss: 19863.2930 normal_loss: 0.9643
[epoch 52][iter  470] loss: 214067.1875 RMSElog: 10.0580 grad_loss: 21395.7207 normal_loss: 0.9394
[epoch 52][iter  480] loss: 186049.0312 RMSElog: 10.0165 grad_loss: 18593.9277 normal_loss: 0.9585
[epoch 52][iter  490] loss: 111655.4766 RMSElog: 9.2050 grad_loss: 11155.4395 normal_loss: 0.9029
[epoch 52][iter  500] loss: 110579.5391 RMSElog: 9.3019 grad_loss: 11047.7109 normal_loss: 0.9413
[epoch 52][iter  510] loss: 206526.4062 RMSElog: 10.1531 grad_loss: 20641.5449 normal_loss: 0.9425
[epoch 52][iter  520] loss: 141442.1250 RMSElog: 9.6437 grad_loss: 14133.6191 normal_loss: 0.9495
[epoch 52][iter  530] loss: 150364.6250 RMSElog: 9.9123 grad_loss: 15025.6094 normal_loss: 0.9418
[epoch 52][iter  540] loss: 126023.2344 RMSElog: 9.6219 grad_loss: 12591.7656 normal_loss: 0.9360
[epoch 52][iter  550] loss: 221089.3594 RMSElog: 10.3457 grad_loss: 22097.6465 normal_loss: 0.9439
[epoch 52][iter  560] loss: 190938.1250 RMSElog: 10.3028 grad_loss: 19082.5566 normal_loss: 0.9541
[epoch 52][iter  570] loss: 197520.7500 RMSElog: 10.1663 grad_loss: 19740.9414 normal_loss: 0.9662
[epoch 52][iter  580] loss: 158534.6406 RMSElog: 9.6735 grad_loss: 15842.8672 normal_loss: 0.9232
[epoch 52][iter  590] loss: 141228.9219 RMSElog: 9.9926 grad_loss: 14111.9531 normal_loss: 0.9477
[epoch 53][iter    0] loss: 121866.5156 RMSElog: 9.8422 grad_loss: 12175.8965 normal_loss: 0.9131
[epoch 53][iter   10] loss: 161813.2500 RMSElog: 9.9875 grad_loss: 16170.3857 normal_loss: 0.9508
[epoch 53][iter   20] loss: 201264.6250 RMSElog: 10.1098 grad_loss: 20115.4023 normal_loss: 0.9505
[epoch 53][iter   30] loss: 166207.1094 RMSElog: 9.7947 grad_loss: 16609.9668 normal_loss: 0.9492
[epoch 53][iter   40] loss: 154776.6094 RMSElog: 9.8981 grad_loss: 15466.8418 normal_loss: 0.9209
[epoch 53][iter   50] loss: 173574.0781 RMSElog: 10.1780 grad_loss: 17346.2734 normal_loss: 0.9575
[epoch 53][iter   60] loss: 114893.3984 RMSElog: 9.2562 grad_loss: 11479.1777 normal_loss: 0.9067
[epoch 53][iter   70] loss: 189507.2500 RMSElog: 10.2157 grad_loss: 18939.5586 normal_loss: 0.9510
[epoch 53][iter   80] loss: 203973.6562 RMSElog: 10.4423 grad_loss: 20385.9727 normal_loss: 0.9505
[epoch 53][iter   90] loss: 158630.9688 RMSElog: 9.8321 grad_loss: 15852.3232 normal_loss: 0.9428
[epoch 53][iter  100] loss: 100663.8984 RMSElog: 9.9598 grad_loss: 10055.5020 normal_loss: 0.9277
[epoch 53][iter  110] loss: 114509.4531 RMSElog: 9.7409 grad_loss: 11440.2627 normal_loss: 0.9413
[epoch 53][iter  120] loss: 105706.7031 RMSElog: 9.7501 grad_loss: 10559.9883 normal_loss: 0.9321
[epoch 53][iter  130] loss: 136869.7500 RMSElog: 9.5771 grad_loss: 13676.4668 normal_loss: 0.9315
[epoch 53][iter  140] loss: 123289.4609 RMSElog: 9.4106 grad_loss: 12318.5752 normal_loss: 0.9606
[epoch 53][iter  150] loss: 139154.8594 RMSElog: 9.5986 grad_loss: 13904.9707 normal_loss: 0.9164
[epoch 53][iter  160] loss: 179649.7188 RMSElog: 10.2020 grad_loss: 17953.8125 normal_loss: 0.9593
[epoch 53][iter  170] loss: 153099.1094 RMSElog: 9.8355 grad_loss: 15299.1445 normal_loss: 0.9304
[epoch 53][iter  180] loss: 125486.8672 RMSElog: 9.7758 grad_loss: 12537.9834 normal_loss: 0.9281
[epoch 53][iter  190] loss: 115534.3672 RMSElog: 9.7196 grad_loss: 11542.7842 normal_loss: 0.9325
[epoch 53][iter  200] loss: 204037.7500 RMSElog: 10.1754 grad_loss: 20392.6582 normal_loss: 0.9420
[epoch 53][iter  210] loss: 135966.2969 RMSElog: 9.6504 grad_loss: 13586.0273 normal_loss: 0.9522
[epoch 53][iter  220] loss: 141442.1250 RMSElog: 9.6437 grad_loss: 14133.6191 normal_loss: 0.9495
[epoch 53][iter  230] loss: 144440.4531 RMSElog: 9.8843 grad_loss: 14433.2314 normal_loss: 0.9296
[epoch 53][iter  240] loss: 185772.2500 RMSElog: 9.9702 grad_loss: 18566.2969 normal_loss: 0.9567
[epoch 53][iter  250] loss: 175524.1250 RMSElog: 10.2056 grad_loss: 17541.2441 normal_loss: 0.9625
[epoch 53][iter  260] loss: 160642.9375 RMSElog: 9.9306 grad_loss: 16053.4121 normal_loss: 0.9504
[epoch 53][iter  270] loss: 108830.7344 RMSElog: 9.7359 grad_loss: 10872.4062 normal_loss: 0.9307
[epoch 53][iter  280] loss: 101101.5547 RMSElog: 9.9274 grad_loss: 10099.2979 normal_loss: 0.9297
[epoch 53][iter  290] loss: 163252.6875 RMSElog: 9.8236 grad_loss: 16314.5117 normal_loss: 0.9348
[epoch 53][iter  300] loss: 139098.6406 RMSElog: 9.8619 grad_loss: 13899.0508 normal_loss: 0.9510
[epoch 53][iter  310] loss: 195999.5625 RMSElog: 10.1749 grad_loss: 19588.8047 normal_loss: 0.9756
[epoch 53][iter  320] loss: 194987.6250 RMSElog: 9.8416 grad_loss: 19487.9453 normal_loss: 0.9748
[epoch 53][iter  330] loss: 114924.9844 RMSElog: 9.6925 grad_loss: 11481.8691 normal_loss: 0.9360
[epoch 53][iter  340] loss: 131942.8750 RMSElog: 9.8414 grad_loss: 13183.5000 normal_loss: 0.9453
[epoch 53][iter  350] loss: 167740.4688 RMSElog: 9.8540 grad_loss: 16763.2344 normal_loss: 0.9591
[epoch 53][iter  360] loss: 159824.3594 RMSElog: 9.8973 grad_loss: 15971.5811 normal_loss: 0.9569
[epoch 53][iter  370] loss: 148184.5469 RMSElog: 10.0681 grad_loss: 14807.4355 normal_loss: 0.9504
[epoch 53][iter  380] loss: 188645.4531 RMSElog: 10.1590 grad_loss: 18853.4082 normal_loss: 0.9794
[epoch 53][iter  390] loss: 157119.7500 RMSElog: 9.8959 grad_loss: 15701.1318 normal_loss: 0.9480
[epoch 53][iter  400] loss: 212239.7031 RMSElog: 10.2690 grad_loss: 21212.7383 normal_loss: 0.9638
[epoch 53][iter  410] loss: 142016.3906 RMSElog: 9.9191 grad_loss: 14190.7676 normal_loss: 0.9526
[epoch 53][iter  420] loss: 178468.8906 RMSElog: 9.9039 grad_loss: 17836.0469 normal_loss: 0.9380
[epoch 53][iter  430] loss: 117693.8984 RMSElog: 9.6527 grad_loss: 11758.8105 normal_loss: 0.9267
[epoch 53][iter  440] loss: 130641.2188 RMSElog: 9.7184 grad_loss: 13053.4688 normal_loss: 0.9344
[epoch 53][iter  450] loss: 142544.5156 RMSElog: 10.1645 grad_loss: 14243.3457 normal_loss: 0.9425
[epoch 53][iter  460] loss: 103815.4375 RMSElog: 9.7764 grad_loss: 10370.8340 normal_loss: 0.9338
[epoch 53][iter  470] loss: 105098.6875 RMSElog: 9.7426 grad_loss: 10499.2012 normal_loss: 0.9257
[epoch 53][iter  480] loss: 181293.3438 RMSElog: 10.0009 grad_loss: 18118.3809 normal_loss: 0.9531
[epoch 53][iter  490] loss: 191371.4375 RMSElog: 10.1918 grad_loss: 19126.0020 normal_loss: 0.9504
[epoch 53][iter  500] loss: 216509.5938 RMSElog: 10.1942 grad_loss: 21639.8242 normal_loss: 0.9420
[epoch 53][iter  510] loss: 159284.2188 RMSElog: 9.6028 grad_loss: 15917.8828 normal_loss: 0.9361
[epoch 53][iter  520] loss: 155968.4062 RMSElog: 9.9200 grad_loss: 15585.9980 normal_loss: 0.9232
[epoch 53][iter  530] loss: 137249.3438 RMSElog: 9.7342 grad_loss: 13714.2764 normal_loss: 0.9239
[epoch 53][iter  540] loss: 164843.6719 RMSElog: 9.7972 grad_loss: 16473.6348 normal_loss: 0.9360
[epoch 53][iter  550] loss: 96006.2500 RMSElog: 9.5560 grad_loss: 9590.1396 normal_loss: 0.9294
[epoch 53][iter  560] loss: 154165.2969 RMSElog: 10.0187 grad_loss: 15405.5605 normal_loss: 0.9498
[epoch 53][iter  570] loss: 182187.7188 RMSElog: 10.0835 grad_loss: 18207.7461 normal_loss: 0.9413
[epoch 53][iter  580] loss: 223875.4062 RMSElog: 10.3929 grad_loss: 22376.1953 normal_loss: 0.9536
[epoch 53][iter  590] loss: 177664.5469 RMSElog: 9.7808 grad_loss: 17755.7363 normal_loss: 0.9369
[epoch 54][iter    0] loss: 157425.7188 RMSElog: 9.9235 grad_loss: 15731.7012 normal_loss: 0.9477
[epoch 54][iter   10] loss: 111518.2734 RMSElog: 9.6257 grad_loss: 11141.2676 normal_loss: 0.9335
[epoch 54][iter   20] loss: 196677.7812 RMSElog: 10.0486 grad_loss: 19656.7715 normal_loss: 0.9571
[epoch 54][iter   30] loss: 161813.2500 RMSElog: 9.9875 grad_loss: 16170.3857 normal_loss: 0.9508
[epoch 54][iter   40] loss: 110874.4688 RMSElog: 9.2829 grad_loss: 11077.2607 normal_loss: 0.9034
[epoch 54][iter   50] loss: 122359.4531 RMSElog: 9.7683 grad_loss: 12225.2402 normal_loss: 0.9361
[epoch 54][iter   60] loss: 127463.1875 RMSElog: 9.4952 grad_loss: 12735.9082 normal_loss: 0.9153
[epoch 54][iter   70] loss: 140645.6875 RMSElog: 10.1118 grad_loss: 14053.4873 normal_loss: 0.9695
[epoch 54][iter   80] loss: 123289.4609 RMSElog: 9.4106 grad_loss: 12318.5752 normal_loss: 0.9606
[epoch 54][iter   90] loss: 204650.3281 RMSElog: 10.1934 grad_loss: 20453.8633 normal_loss: 0.9762
[epoch 54][iter  100] loss: 153099.1094 RMSElog: 9.8355 grad_loss: 15299.1445 normal_loss: 0.9304
[epoch 54][iter  110] loss: 148184.5469 RMSElog: 10.0681 grad_loss: 14807.4355 normal_loss: 0.9504
[epoch 54][iter  120] loss: 189582.6719 RMSElog: 9.9918 grad_loss: 18947.3223 normal_loss: 0.9523
[epoch 54][iter  130] loss: 119895.5000 RMSElog: 9.3187 grad_loss: 11979.3154 normal_loss: 0.9157
[epoch 54][iter  140] loss: 170357.3281 RMSElog: 9.9687 grad_loss: 17024.7988 normal_loss: 0.9644
[epoch 54][iter  150] loss: 203973.6562 RMSElog: 10.4423 grad_loss: 20385.9727 normal_loss: 0.9505
[epoch 54][iter  160] loss: 104086.5234 RMSElog: 9.7787 grad_loss: 10397.9453 normal_loss: 0.9289
[epoch 54][iter  170] loss: 111712.9062 RMSElog: 9.7826 grad_loss: 11160.5801 normal_loss: 0.9290
[epoch 54][iter  180] loss: 214067.1875 RMSElog: 10.0580 grad_loss: 21395.7207 normal_loss: 0.9394
[epoch 54][iter  190] loss: 135042.2031 RMSElog: 9.6763 grad_loss: 13493.6074 normal_loss: 0.9366
[epoch 54][iter  200] loss: 195777.0469 RMSElog: 10.1280 grad_loss: 19566.6035 normal_loss: 0.9726
[epoch 54][iter  210] loss: 175299.8438 RMSElog: 10.1975 grad_loss: 17518.8301 normal_loss: 0.9576
[epoch 54][iter  220] loss: 105794.8906 RMSElog: 9.7676 grad_loss: 10568.7949 normal_loss: 0.9267
[epoch 54][iter  230] loss: 195507.4062 RMSElog: 10.1854 grad_loss: 19539.6016 normal_loss: 0.9533
[epoch 54][iter  240] loss: 142544.5156 RMSElog: 10.1645 grad_loss: 14243.3457 normal_loss: 0.9425
[epoch 54][iter  250] loss: 184374.1875 RMSElog: 10.2641 grad_loss: 18426.1953 normal_loss: 0.9596
[epoch 54][iter  260] loss: 216509.5625 RMSElog: 10.1942 grad_loss: 21639.8223 normal_loss: 0.9420
[epoch 54][iter  270] loss: 100650.2734 RMSElog: 9.7441 grad_loss: 10054.3555 normal_loss: 0.9279
[epoch 54][iter  280] loss: 178593.9688 RMSElog: 10.0081 grad_loss: 17848.4395 normal_loss: 0.9490
[epoch 54][iter  290] loss: 168545.0938 RMSElog: 9.6168 grad_loss: 16843.9434 normal_loss: 0.9495
[epoch 54][iter  300] loss: 160788.1406 RMSElog: 9.7539 grad_loss: 16068.1289 normal_loss: 0.9318
[epoch 54][iter  310] loss: 220587.5781 RMSElog: 10.2995 grad_loss: 22047.5117 normal_loss: 0.9466
[epoch 54][iter  320] loss: 146465.6875 RMSElog: 10.2283 grad_loss: 14635.4062 normal_loss: 0.9337
[epoch 54][iter  330] loss: 113921.7891 RMSElog: 9.7251 grad_loss: 11381.5186 normal_loss: 0.9357
[epoch 54][iter  340] loss: 221213.4531 RMSElog: 10.1740 grad_loss: 22110.2305 normal_loss: 0.9415
[epoch 54][iter  350] loss: 235211.2344 RMSElog: 10.3371 grad_loss: 23509.8398 normal_loss: 0.9452
[epoch 54][iter  360] loss: 183095.1562 RMSElog: 9.9762 grad_loss: 18298.5703 normal_loss: 0.9696
[epoch 54][iter  370] loss: 205235.7188 RMSElog: 10.2875 grad_loss: 20512.3281 normal_loss: 0.9564
[epoch 54][iter  380] loss: 167784.2500 RMSElog: 9.9330 grad_loss: 16767.5391 normal_loss: 0.9539
[epoch 54][iter  390] loss: 142005.0781 RMSElog: 9.9168 grad_loss: 14189.6533 normal_loss: 0.9371
[epoch 54][iter  400] loss: 212239.7031 RMSElog: 10.2690 grad_loss: 21212.7383 normal_loss: 0.9638
[epoch 54][iter  410] loss: 146161.1562 RMSElog: 10.0682 grad_loss: 14605.1279 normal_loss: 0.9190
[epoch 54][iter  420] loss: 189543.6719 RMSElog: 10.1592 grad_loss: 18943.2598 normal_loss: 0.9474
[epoch 54][iter  430] loss: 149369.9062 RMSElog: 10.0930 grad_loss: 14925.9756 normal_loss: 0.9225
[epoch 54][iter  440] loss: 124563.3906 RMSElog: 9.9514 grad_loss: 12445.4473 normal_loss: 0.9400
[epoch 54][iter  450] loss: 167455.8750 RMSElog: 9.7229 grad_loss: 16734.9395 normal_loss: 0.9256
[epoch 54][iter  460] loss: 185495.4062 RMSElog: 9.9310 grad_loss: 18538.6641 normal_loss: 0.9455
[epoch 54][iter  470] loss: 181465.8125 RMSElog: 10.1136 grad_loss: 18135.5117 normal_loss: 0.9572
[epoch 54][iter  480] loss: 213401.8906 RMSElog: 10.0568 grad_loss: 21329.1582 normal_loss: 0.9749
[epoch 54][iter  490] loss: 104446.5625 RMSElog: 10.1932 grad_loss: 10433.5342 normal_loss: 0.9288
[epoch 54][iter  500] loss: 128277.9766 RMSElog: 9.7565 grad_loss: 12817.0889 normal_loss: 0.9518
[epoch 54][iter  510] loss: 195440.9062 RMSElog: 9.7403 grad_loss: 19533.3926 normal_loss: 0.9572
[epoch 54][iter  520] loss: 152149.7500 RMSElog: 9.9062 grad_loss: 15204.1221 normal_loss: 0.9462
[epoch 54][iter  530] loss: 160466.8750 RMSElog: 9.9085 grad_loss: 16035.8252 normal_loss: 0.9540
[epoch 54][iter  540] loss: 214493.2031 RMSElog: 10.3525 grad_loss: 21438.0059 normal_loss: 0.9611
[epoch 54][iter  550] loss: 185071.7031 RMSElog: 10.0456 grad_loss: 18496.1699 normal_loss: 0.9557
[epoch 54][iter  560] loss: 122446.5469 RMSElog: 9.6438 grad_loss: 12234.0742 normal_loss: 0.9366
[epoch 54][iter  570] loss: 165453.0938 RMSElog: 9.9262 grad_loss: 16534.4160 normal_loss: 0.9660
[epoch 54][iter  580] loss: 157706.0781 RMSElog: 9.9981 grad_loss: 15759.6602 normal_loss: 0.9499
[epoch 54][iter  590] loss: 152562.3906 RMSElog: 9.8216 grad_loss: 15245.4785 normal_loss: 0.9391
[epoch 55][iter    0] loss: 152562.3750 RMSElog: 9.8216 grad_loss: 15245.4775 normal_loss: 0.9391
[epoch 55][iter   10] loss: 183095.1562 RMSElog: 9.9762 grad_loss: 18298.5703 normal_loss: 0.9696
[epoch 55][iter   20] loss: 157061.8906 RMSElog: 9.9892 grad_loss: 15695.2461 normal_loss: 0.9531
[epoch 55][iter   30] loss: 113426.9531 RMSElog: 9.7682 grad_loss: 11331.9902 normal_loss: 0.9362
[epoch 55][iter   40] loss: 140938.8594 RMSElog: 9.7483 grad_loss: 14083.1953 normal_loss: 0.9424
[epoch 55][iter   50] loss: 90482.6875 RMSElog: 9.3127 grad_loss: 9038.0186 normal_loss: 0.9378
[epoch 55][iter   60] loss: 163389.5469 RMSElog: 9.7702 grad_loss: 16328.2637 normal_loss: 0.9208
[epoch 55][iter   70] loss: 199607.1562 RMSElog: 10.1504 grad_loss: 19949.6133 normal_loss: 0.9517
[epoch 55][iter   80] loss: 100749.6875 RMSElog: 9.9914 grad_loss: 10064.0400 normal_loss: 0.9377
[epoch 55][iter   90] loss: 165954.0938 RMSElog: 9.8997 grad_loss: 16584.5449 normal_loss: 0.9646
[epoch 55][iter  100] loss: 155482.2031 RMSElog: 10.0818 grad_loss: 15537.1826 normal_loss: 0.9552
[epoch 55][iter  110] loss: 160051.0156 RMSElog: 9.8405 grad_loss: 15994.3105 normal_loss: 0.9506
[epoch 55][iter  120] loss: 149807.1562 RMSElog: 9.9060 grad_loss: 14969.8613 normal_loss: 0.9481
[epoch 55][iter  130] loss: 164181.7344 RMSElog: 10.0104 grad_loss: 16407.1973 normal_loss: 0.9660
[epoch 55][iter  140] loss: 115658.3125 RMSElog: 9.1856 grad_loss: 11555.7031 normal_loss: 0.9424
[epoch 55][iter  150] loss: 196677.7812 RMSElog: 10.0486 grad_loss: 19656.7715 normal_loss: 0.9571
[epoch 55][iter  160] loss: 114509.4531 RMSElog: 9.7409 grad_loss: 11440.2627 normal_loss: 0.9413
[epoch 55][iter  170] loss: 233395.7188 RMSElog: 10.2368 grad_loss: 23328.3906 normal_loss: 0.9452
[epoch 55][iter  180] loss: 223018.5625 RMSElog: 10.3592 grad_loss: 22290.5508 normal_loss: 0.9462
[epoch 55][iter  190] loss: 107073.5156 RMSElog: 9.7289 grad_loss: 10696.6914 normal_loss: 0.9313
[epoch 55][iter  200] loss: 155762.3438 RMSElog: 9.8487 grad_loss: 15565.4404 normal_loss: 0.9448
[epoch 55][iter  210] loss: 111518.2734 RMSElog: 9.6257 grad_loss: 11141.2676 normal_loss: 0.9335
[epoch 55][iter  220] loss: 151354.1562 RMSElog: 9.8837 grad_loss: 15124.5889 normal_loss: 0.9428
[epoch 55][iter  230] loss: 106952.9219 RMSElog: 10.0294 grad_loss: 10684.3398 normal_loss: 0.9231
[epoch 55][iter  240] loss: 156173.3594 RMSElog: 9.9681 grad_loss: 15606.4199 normal_loss: 0.9482
[epoch 55][iter  250] loss: 185902.4219 RMSElog: 9.8998 grad_loss: 18579.3711 normal_loss: 0.9705
[epoch 55][iter  260] loss: 150459.7656 RMSElog: 9.9429 grad_loss: 15035.0986 normal_loss: 0.9347
[epoch 55][iter  270] loss: 134406.2031 RMSElog: 9.9402 grad_loss: 13429.7256 normal_loss: 0.9539
[epoch 55][iter  280] loss: 121838.3672 RMSElog: 9.7453 grad_loss: 12173.1494 normal_loss: 0.9425
[epoch 55][iter  290] loss: 147915.7812 RMSElog: 10.0131 grad_loss: 14780.6104 normal_loss: 0.9547
[epoch 55][iter  300] loss: 177934.3750 RMSElog: 9.8386 grad_loss: 17782.6445 normal_loss: 0.9542
[epoch 55][iter  310] loss: 112851.7266 RMSElog: 9.4205 grad_loss: 11274.8320 normal_loss: 0.9199
[epoch 55][iter  320] loss: 202119.8125 RMSElog: 10.2544 grad_loss: 20200.7715 normal_loss: 0.9549
[epoch 55][iter  330] loss: 96006.2500 RMSElog: 9.5560 grad_loss: 9590.1396 normal_loss: 0.9294
[epoch 55][iter  340] loss: 239768.0938 RMSElog: 10.2366 grad_loss: 23965.6250 normal_loss: 0.9466
[epoch 55][iter  350] loss: 131213.2969 RMSElog: 9.6860 grad_loss: 13110.7217 normal_loss: 0.9217
[epoch 55][iter  360] loss: 140282.6250 RMSElog: 9.9822 grad_loss: 14017.3145 normal_loss: 0.9650
[epoch 55][iter  370] loss: 119470.5156 RMSElog: 9.6196 grad_loss: 11936.5020 normal_loss: 0.9297
[epoch 55][iter  380] loss: 167745.5625 RMSElog: 9.8817 grad_loss: 16763.7129 normal_loss: 0.9621
[epoch 55][iter  390] loss: 148136.9375 RMSElog: 9.9557 grad_loss: 14802.7871 normal_loss: 0.9498
[epoch 55][iter  400] loss: 175299.8438 RMSElog: 10.1975 grad_loss: 17518.8301 normal_loss: 0.9576
[epoch 55][iter  410] loss: 157274.4688 RMSElog: 9.9277 grad_loss: 15716.5723 normal_loss: 0.9461
[epoch 55][iter  420] loss: 132425.4375 RMSElog: 10.0155 grad_loss: 13231.5801 normal_loss: 0.9468
[epoch 55][iter  430] loss: 167455.8750 RMSElog: 9.7229 grad_loss: 16734.9395 normal_loss: 0.9256
[epoch 55][iter  440] loss: 152149.7500 RMSElog: 9.9062 grad_loss: 15204.1221 normal_loss: 0.9462
[epoch 55][iter  450] loss: 175547.8906 RMSElog: 10.1409 grad_loss: 17543.7148 normal_loss: 0.9342
[epoch 55][iter  460] loss: 148184.5469 RMSElog: 10.0681 grad_loss: 14807.4355 normal_loss: 0.9504
[epoch 55][iter  470] loss: 166492.4375 RMSElog: 9.8376 grad_loss: 16638.4512 normal_loss: 0.9558
[epoch 55][iter  480] loss: 160024.9375 RMSElog: 9.8489 grad_loss: 15991.7236 normal_loss: 0.9215
[epoch 55][iter  490] loss: 147765.9219 RMSElog: 10.0607 grad_loss: 14765.6104 normal_loss: 0.9219
[epoch 55][iter  500] loss: 110579.5391 RMSElog: 9.3019 grad_loss: 11047.7109 normal_loss: 0.9413
[epoch 55][iter  510] loss: 118746.3359 RMSElog: 9.7278 grad_loss: 11863.9629 normal_loss: 0.9433
[epoch 55][iter  520] loss: 129708.8906 RMSElog: 10.0691 grad_loss: 12959.8711 normal_loss: 0.9481
[epoch 55][iter  530] loss: 164766.2812 RMSElog: 9.6928 grad_loss: 16465.9941 normal_loss: 0.9411
[epoch 55][iter  540] loss: 191825.0781 RMSElog: 10.3012 grad_loss: 19171.2500 normal_loss: 0.9570
[epoch 55][iter  550] loss: 106769.7500 RMSElog: 9.8688 grad_loss: 10666.1914 normal_loss: 0.9140
[epoch 55][iter  560] loss: 129743.7969 RMSElog: 9.9523 grad_loss: 12963.4785 normal_loss: 0.9492
[epoch 55][iter  570] loss: 136869.7500 RMSElog: 9.5771 grad_loss: 13676.4668 normal_loss: 0.9315
[epoch 55][iter  580] loss: 139830.9062 RMSElog: 9.7552 grad_loss: 13972.3770 normal_loss: 0.9585
[epoch 55][iter  590] loss: 136507.7344 RMSElog: 10.0223 grad_loss: 13639.7900 normal_loss: 0.9608
[epoch 56][iter    0] loss: 114173.8984 RMSElog: 9.3600 grad_loss: 11407.1230 normal_loss: 0.9061
[epoch 56][iter   10] loss: 231973.6719 RMSElog: 10.2099 grad_loss: 23186.2090 normal_loss: 0.9490
[epoch 56][iter   20] loss: 132012.6719 RMSElog: 9.6802 grad_loss: 13190.6582 normal_loss: 0.9291
[epoch 56][iter   30] loss: 227080.5938 RMSElog: 10.4127 grad_loss: 22696.6992 normal_loss: 0.9482
[epoch 56][iter   40] loss: 158230.6094 RMSElog: 9.9234 grad_loss: 15812.1895 normal_loss: 0.9483
[epoch 56][iter   50] loss: 189543.6719 RMSElog: 10.1592 grad_loss: 18943.2598 normal_loss: 0.9474
[epoch 56][iter   60] loss: 102096.6484 RMSElog: 9.8437 grad_loss: 10198.8926 normal_loss: 0.9290
[epoch 56][iter   70] loss: 126821.0234 RMSElog: 10.1398 grad_loss: 12671.0410 normal_loss: 0.9215
[epoch 56][iter   80] loss: 162892.2969 RMSElog: 9.8252 grad_loss: 16278.4717 normal_loss: 0.9331
[epoch 56][iter   90] loss: 136533.4688 RMSElog: 9.6756 grad_loss: 13642.7363 normal_loss: 0.9359
[epoch 56][iter  100] loss: 128883.6406 RMSElog: 9.8798 grad_loss: 12877.5254 normal_loss: 0.9593
[epoch 56][iter  110] loss: 173721.0156 RMSElog: 10.0355 grad_loss: 17361.1191 normal_loss: 0.9468
[epoch 56][iter  120] loss: 164843.6875 RMSElog: 9.7972 grad_loss: 16473.6367 normal_loss: 0.9360
[epoch 56][iter  130] loss: 153724.0000 RMSElog: 9.9609 grad_loss: 15361.4922 normal_loss: 0.9475
[epoch 56][iter  140] loss: 103514.7812 RMSElog: 9.3451 grad_loss: 10341.2109 normal_loss: 0.9229
[epoch 56][iter  150] loss: 121983.8203 RMSElog: 9.5225 grad_loss: 12187.9473 normal_loss: 0.9124
[epoch 56][iter  160] loss: 157706.0781 RMSElog: 9.9981 grad_loss: 15759.6602 normal_loss: 0.9499
[epoch 56][iter  170] loss: 126023.2344 RMSElog: 9.6219 grad_loss: 12591.7656 normal_loss: 0.9360
[epoch 56][iter  180] loss: 87697.6250 RMSElog: 9.9487 grad_loss: 8758.8750 normal_loss: 0.9395
[epoch 56][iter  190] loss: 139098.6406 RMSElog: 9.8619 grad_loss: 13899.0508 normal_loss: 0.9510
[epoch 56][iter  200] loss: 188502.1875 RMSElog: 10.1669 grad_loss: 18839.0801 normal_loss: 0.9722
[epoch 56][iter  210] loss: 147598.9844 RMSElog: 9.8981 grad_loss: 14749.0645 normal_loss: 0.9355
[epoch 56][iter  220] loss: 170690.9375 RMSElog: 9.9328 grad_loss: 17058.2129 normal_loss: 0.9470
[epoch 56][iter  230] loss: 72289.2656 RMSElog: 9.4677 grad_loss: 7218.5376 normal_loss: 0.9211
[epoch 56][iter  240] loss: 176914.4531 RMSElog: 10.3802 grad_loss: 17680.1094 normal_loss: 0.9546
[epoch 56][iter  250] loss: 170056.2812 RMSElog: 9.7423 grad_loss: 16994.9395 normal_loss: 0.9472
[epoch 56][iter  260] loss: 165954.0938 RMSElog: 9.8997 grad_loss: 16584.5449 normal_loss: 0.9646
[epoch 56][iter  270] loss: 177292.4062 RMSElog: 10.1195 grad_loss: 17718.1602 normal_loss: 0.9615
[epoch 56][iter  280] loss: 167740.4688 RMSElog: 9.8540 grad_loss: 16763.2344 normal_loss: 0.9591
[epoch 56][iter  290] loss: 207919.9844 RMSElog: 10.2783 grad_loss: 20780.7637 normal_loss: 0.9578
[epoch 56][iter  300] loss: 119923.2031 RMSElog: 9.5699 grad_loss: 11981.8154 normal_loss: 0.9342
[epoch 56][iter  310] loss: 177195.4844 RMSElog: 10.1516 grad_loss: 17708.4512 normal_loss: 0.9458
[epoch 56][iter  320] loss: 106516.8047 RMSElog: 9.6966 grad_loss: 10641.0557 normal_loss: 0.9285
[epoch 56][iter  330] loss: 148136.9375 RMSElog: 9.9557 grad_loss: 14802.7871 normal_loss: 0.9498
[epoch 56][iter  340] loss: 146781.9531 RMSElog: 9.8688 grad_loss: 14667.3818 normal_loss: 0.9440
[epoch 56][iter  350] loss: 148184.5469 RMSElog: 10.0681 grad_loss: 14807.4355 normal_loss: 0.9504
[epoch 56][iter  360] loss: 152593.5000 RMSElog: 9.8773 grad_loss: 15248.5449 normal_loss: 0.9283
[epoch 56][iter  370] loss: 178076.4688 RMSElog: 9.8778 grad_loss: 17796.8457 normal_loss: 0.9233
[epoch 56][iter  380] loss: 130527.6172 RMSElog: 9.8914 grad_loss: 13041.9189 normal_loss: 0.9509
[epoch 56][iter  390] loss: 158924.9062 RMSElog: 9.8027 grad_loss: 15881.7363 normal_loss: 0.9524
[epoch 56][iter  400] loss: 108830.7344 RMSElog: 9.7359 grad_loss: 10872.4062 normal_loss: 0.9307
[epoch 56][iter  410] loss: 127075.1875 RMSElog: 10.1217 grad_loss: 12696.4443 normal_loss: 0.9524
[epoch 56][iter  420] loss: 153997.7188 RMSElog: 9.7620 grad_loss: 15389.0713 normal_loss: 0.9383
[epoch 56][iter  430] loss: 175301.7344 RMSElog: 9.8017 grad_loss: 17519.4180 normal_loss: 0.9555
[epoch 56][iter  440] loss: 176217.8125 RMSElog: 10.1087 grad_loss: 17610.7246 normal_loss: 0.9466
[epoch 56][iter  450] loss: 100650.2734 RMSElog: 9.7441 grad_loss: 10054.3555 normal_loss: 0.9279
[epoch 56][iter  460] loss: 203973.6562 RMSElog: 10.4423 grad_loss: 20385.9727 normal_loss: 0.9505
[epoch 56][iter  470] loss: 156982.7188 RMSElog: 10.0093 grad_loss: 15687.3193 normal_loss: 0.9438
[epoch 56][iter  480] loss: 156557.3594 RMSElog: 9.6970 grad_loss: 15645.1025 normal_loss: 0.9363
[epoch 56][iter  490] loss: 144191.5938 RMSElog: 9.6650 grad_loss: 14408.5654 normal_loss: 0.9284
[epoch 56][iter  500] loss: 110874.4688 RMSElog: 9.2829 grad_loss: 11077.2607 normal_loss: 0.9034
[epoch 56][iter  510] loss: 155482.1875 RMSElog: 10.0818 grad_loss: 15537.1816 normal_loss: 0.9552
[epoch 56][iter  520] loss: 176223.4531 RMSElog: 9.9872 grad_loss: 17611.4141 normal_loss: 0.9446
[epoch 56][iter  530] loss: 182187.7188 RMSElog: 10.0835 grad_loss: 18207.7461 normal_loss: 0.9413
[epoch 56][iter  540] loss: 198743.4219 RMSElog: 10.0842 grad_loss: 19863.2930 normal_loss: 0.9643
[epoch 56][iter  550] loss: 122912.0625 RMSElog: 9.7328 grad_loss: 12280.5225 normal_loss: 0.9509
[epoch 56][iter  560] loss: 112697.1016 RMSElog: 9.7295 grad_loss: 11259.0449 normal_loss: 0.9352
[epoch 56][iter  570] loss: 175524.1250 RMSElog: 10.2056 grad_loss: 17541.2441 normal_loss: 0.9625
[epoch 56][iter  580] loss: 162542.0469 RMSElog: 9.9393 grad_loss: 16243.3115 normal_loss: 0.9527
[epoch 56][iter  590] loss: 150364.6250 RMSElog: 9.9123 grad_loss: 15025.6094 normal_loss: 0.9418
[epoch 57][iter    0] loss: 162892.2969 RMSElog: 9.8252 grad_loss: 16278.4717 normal_loss: 0.9331
[epoch 57][iter   10] loss: 156253.7656 RMSElog: 9.8231 grad_loss: 15614.6211 normal_loss: 0.9316
[epoch 57][iter   20] loss: 165306.9531 RMSElog: 9.8881 grad_loss: 16519.8438 normal_loss: 0.9638
[epoch 57][iter   30] loss: 212429.2969 RMSElog: 10.2794 grad_loss: 21231.6719 normal_loss: 0.9784
[epoch 57][iter   40] loss: 159824.3594 RMSElog: 9.8973 grad_loss: 15971.5811 normal_loss: 0.9569
[epoch 57][iter   50] loss: 111655.4766 RMSElog: 9.2050 grad_loss: 11155.4395 normal_loss: 0.9029
[epoch 57][iter   60] loss: 188662.5156 RMSElog: 10.2592 grad_loss: 18855.0352 normal_loss: 0.9574
[epoch 57][iter   70] loss: 87697.6250 RMSElog: 9.9487 grad_loss: 8758.8750 normal_loss: 0.9395
[epoch 57][iter   80] loss: 121983.8203 RMSElog: 9.5225 grad_loss: 12187.9473 normal_loss: 0.9124
[epoch 57][iter   90] loss: 152540.7500 RMSElog: 9.9175 grad_loss: 15243.2090 normal_loss: 0.9474
[epoch 57][iter  100] loss: 214067.1875 RMSElog: 10.0580 grad_loss: 21395.7207 normal_loss: 0.9394
[epoch 57][iter  110] loss: 103509.2188 RMSElog: 9.9787 grad_loss: 10340.0186 normal_loss: 0.9251
[epoch 57][iter  120] loss: 179634.8125 RMSElog: 10.0930 grad_loss: 17952.4258 normal_loss: 0.9607
[epoch 57][iter  130] loss: 121866.5156 RMSElog: 9.8422 grad_loss: 12175.8965 normal_loss: 0.9131
[epoch 57][iter  140] loss: 143534.0938 RMSElog: 9.7662 grad_loss: 14342.7061 normal_loss: 0.9374
[epoch 57][iter  150] loss: 194987.6250 RMSElog: 9.8416 grad_loss: 19487.9453 normal_loss: 0.9748
[epoch 57][iter  160] loss: 206764.2031 RMSElog: 10.2849 grad_loss: 20665.1797 normal_loss: 0.9549
[epoch 57][iter  170] loss: 130739.4062 RMSElog: 9.5471 grad_loss: 13063.4434 normal_loss: 0.9498
[epoch 57][iter  180] loss: 127075.1953 RMSElog: 10.1217 grad_loss: 12696.4453 normal_loss: 0.9524
[epoch 57][iter  190] loss: 100990.1562 RMSElog: 9.7350 grad_loss: 10088.3584 normal_loss: 0.9222
[epoch 57][iter  200] loss: 106516.8047 RMSElog: 9.6966 grad_loss: 10641.0557 normal_loss: 0.9285
[epoch 57][iter  210] loss: 166492.4688 RMSElog: 9.8376 grad_loss: 16638.4531 normal_loss: 0.9558
[epoch 57][iter  220] loss: 152149.7500 RMSElog: 9.9062 grad_loss: 15204.1230 normal_loss: 0.9462
[epoch 57][iter  230] loss: 127110.9375 RMSElog: 9.5273 grad_loss: 12700.6064 normal_loss: 0.9604
[epoch 57][iter  240] loss: 181293.3438 RMSElog: 10.0009 grad_loss: 18118.3809 normal_loss: 0.9531
[epoch 57][iter  250] loss: 149807.1562 RMSElog: 9.9060 grad_loss: 14969.8613 normal_loss: 0.9481
[epoch 57][iter  260] loss: 122670.8984 RMSElog: 9.4365 grad_loss: 12256.7207 normal_loss: 0.9331
[epoch 57][iter  270] loss: 106182.2031 RMSElog: 9.7575 grad_loss: 10607.5332 normal_loss: 0.9294
[epoch 57][iter  280] loss: 107948.0156 RMSElog: 9.7377 grad_loss: 10784.1299 normal_loss: 0.9342
[epoch 57][iter  290] loss: 130145.6250 RMSElog: 9.7206 grad_loss: 13003.9072 normal_loss: 0.9344
[epoch 57][iter  300] loss: 182476.7344 RMSElog: 9.9672 grad_loss: 18236.7656 normal_loss: 0.9407
[epoch 57][iter  310] loss: 151368.4219 RMSElog: 9.7883 grad_loss: 15126.1289 normal_loss: 0.9251
[epoch 57][iter  320] loss: 196842.3125 RMSElog: 10.1249 grad_loss: 19673.1523 normal_loss: 0.9534
[epoch 57][iter  330] loss: 158924.9062 RMSElog: 9.8027 grad_loss: 15881.7363 normal_loss: 0.9524
[epoch 57][iter  340] loss: 113195.8125 RMSElog: 9.1761 grad_loss: 11309.5039 normal_loss: 0.9012
[epoch 57][iter  350] loss: 105628.3672 RMSElog: 10.0899 grad_loss: 10551.8262 normal_loss: 0.9213
[epoch 57][iter  360] loss: 224943.8906 RMSElog: 10.1206 grad_loss: 22483.3125 normal_loss: 0.9551
[epoch 57][iter  370] loss: 196437.8125 RMSElog: 10.2319 grad_loss: 19632.5898 normal_loss: 0.9584
[epoch 57][iter  380] loss: 103948.1719 RMSElog: 9.7608 grad_loss: 10384.1299 normal_loss: 0.9264
[epoch 57][iter  390] loss: 127061.7500 RMSElog: 9.6726 grad_loss: 12695.5654 normal_loss: 0.9369
[epoch 57][iter  400] loss: 203973.6562 RMSElog: 10.4423 grad_loss: 20385.9727 normal_loss: 0.9505
[epoch 57][iter  410] loss: 178468.8906 RMSElog: 9.9039 grad_loss: 17836.0469 normal_loss: 0.9380
[epoch 57][iter  420] loss: 189743.5625 RMSElog: 10.1862 grad_loss: 18963.2207 normal_loss: 0.9490
[epoch 57][iter  430] loss: 160825.5938 RMSElog: 9.9290 grad_loss: 16071.6816 normal_loss: 0.9491
[epoch 57][iter  440] loss: 128883.6406 RMSElog: 9.8798 grad_loss: 12877.5254 normal_loss: 0.9593
[epoch 57][iter  450] loss: 100980.7812 RMSElog: 9.6528 grad_loss: 10087.5176 normal_loss: 0.9085
[epoch 57][iter  460] loss: 178593.9688 RMSElog: 10.0081 grad_loss: 17848.4395 normal_loss: 0.9490
[epoch 57][iter  470] loss: 175004.6719 RMSElog: 10.1774 grad_loss: 17489.3320 normal_loss: 0.9561
[epoch 57][iter  480] loss: 212239.7031 RMSElog: 10.2690 grad_loss: 21212.7383 normal_loss: 0.9638
[epoch 57][iter  490] loss: 129205.9375 RMSElog: 9.8851 grad_loss: 12909.7734 normal_loss: 0.9356
[epoch 57][iter  500] loss: 175122.9531 RMSElog: 9.9588 grad_loss: 17501.3789 normal_loss: 0.9573
[epoch 57][iter  510] loss: 140057.2812 RMSElog: 9.9766 grad_loss: 13994.7988 normal_loss: 0.9526
[epoch 57][iter  520] loss: 120292.3438 RMSElog: 9.8042 grad_loss: 12018.4863 normal_loss: 0.9430
[epoch 57][iter  530] loss: 115500.4766 RMSElog: 9.9924 grad_loss: 11539.1230 normal_loss: 0.9328
[epoch 57][iter  540] loss: 137764.4062 RMSElog: 9.8227 grad_loss: 13765.6748 normal_loss: 0.9442
[epoch 57][iter  550] loss: 200470.3906 RMSElog: 10.2414 grad_loss: 20035.8379 normal_loss: 0.9585
[epoch 57][iter  560] loss: 135203.5938 RMSElog: 9.4545 grad_loss: 13509.9697 normal_loss: 0.9356
[epoch 57][iter  570] loss: 134406.1875 RMSElog: 9.9402 grad_loss: 13429.7246 normal_loss: 0.9539
[epoch 57][iter  580] loss: 135042.2031 RMSElog: 9.6763 grad_loss: 13493.6074 normal_loss: 0.9366
[epoch 57][iter  590] loss: 120745.8203 RMSElog: 9.7706 grad_loss: 12063.8691 normal_loss: 0.9428
[epoch 58][iter    0] loss: 153447.6875 RMSElog: 9.9051 grad_loss: 15333.9209 normal_loss: 0.9425
[epoch 58][iter   10] loss: 154855.9062 RMSElog: 9.8054 grad_loss: 15474.8340 normal_loss: 0.9510
[epoch 58][iter   20] loss: 216509.5625 RMSElog: 10.1942 grad_loss: 21639.8223 normal_loss: 0.9420
[epoch 58][iter   30] loss: 118746.3359 RMSElog: 9.7278 grad_loss: 11863.9629 normal_loss: 0.9433
[epoch 58][iter   40] loss: 156253.7656 RMSElog: 9.8231 grad_loss: 15614.6211 normal_loss: 0.9316
[epoch 58][iter   50] loss: 164766.2812 RMSElog: 9.6928 grad_loss: 16465.9941 normal_loss: 0.9411
[epoch 58][iter   60] loss: 104446.5625 RMSElog: 10.1932 grad_loss: 10433.5342 normal_loss: 0.9288
[epoch 58][iter   70] loss: 146444.9062 RMSElog: 10.1139 grad_loss: 14633.4365 normal_loss: 0.9403
[epoch 58][iter   80] loss: 121866.5156 RMSElog: 9.8422 grad_loss: 12175.8965 normal_loss: 0.9131
[epoch 58][iter   90] loss: 233395.7188 RMSElog: 10.2368 grad_loss: 23328.3906 normal_loss: 0.9452
[epoch 58][iter  100] loss: 108830.7344 RMSElog: 9.7359 grad_loss: 10872.4062 normal_loss: 0.9307
[epoch 58][iter  110] loss: 212239.7031 RMSElog: 10.2690 grad_loss: 21212.7383 normal_loss: 0.9638
[epoch 58][iter  120] loss: 161633.2656 RMSElog: 10.0107 grad_loss: 16152.3643 normal_loss: 0.9518
[epoch 58][iter  130] loss: 157119.7500 RMSElog: 9.8959 grad_loss: 15701.1318 normal_loss: 0.9480
[epoch 58][iter  140] loss: 111712.9062 RMSElog: 9.7826 grad_loss: 11160.5801 normal_loss: 0.9290
[epoch 58][iter  150] loss: 138714.8906 RMSElog: 10.1491 grad_loss: 13860.4043 normal_loss: 0.9360
[epoch 58][iter  160] loss: 176223.4531 RMSElog: 9.9872 grad_loss: 17611.4141 normal_loss: 0.9446
[epoch 58][iter  170] loss: 196437.7969 RMSElog: 10.2319 grad_loss: 19632.5879 normal_loss: 0.9584
[epoch 58][iter  180] loss: 190972.3281 RMSElog: 10.1152 grad_loss: 19086.1523 normal_loss: 0.9647
[epoch 58][iter  190] loss: 142596.2344 RMSElog: 9.8922 grad_loss: 14248.7803 normal_loss: 0.9508
[epoch 58][iter  200] loss: 135699.4688 RMSElog: 9.6754 grad_loss: 13559.3340 normal_loss: 0.9364
[epoch 58][iter  210] loss: 109148.7344 RMSElog: 10.1172 grad_loss: 10903.8262 normal_loss: 0.9294
[epoch 58][iter  220] loss: 165750.3750 RMSElog: 9.8971 grad_loss: 16564.1895 normal_loss: 0.9518
[epoch 58][iter  230] loss: 160051.0156 RMSElog: 9.8405 grad_loss: 15994.3105 normal_loss: 0.9506
[epoch 58][iter  240] loss: 172248.5156 RMSElog: 9.5166 grad_loss: 17214.3730 normal_loss: 0.9619
[epoch 58][iter  250] loss: 218457.8281 RMSElog: 10.1255 grad_loss: 21834.7168 normal_loss: 0.9418
[epoch 58][iter  260] loss: 125315.5625 RMSElog: 9.7124 grad_loss: 12520.9131 normal_loss: 0.9309
[epoch 58][iter  270] loss: 146161.1562 RMSElog: 10.0682 grad_loss: 14605.1289 normal_loss: 0.9190
[epoch 58][iter  280] loss: 173574.0781 RMSElog: 10.1780 grad_loss: 17346.2734 normal_loss: 0.9575
[epoch 58][iter  290] loss: 224257.9688 RMSElog: 10.2842 grad_loss: 22414.5605 normal_loss: 0.9521
[epoch 58][iter  300] loss: 112715.8125 RMSElog: 9.6867 grad_loss: 11260.9570 normal_loss: 0.9375
[epoch 58][iter  310] loss: 186960.2969 RMSElog: 10.2468 grad_loss: 18684.8457 normal_loss: 0.9380
[epoch 58][iter  320] loss: 201503.0625 RMSElog: 10.3149 grad_loss: 20139.0410 normal_loss: 0.9514
[epoch 58][iter  330] loss: 217174.7656 RMSElog: 10.2254 grad_loss: 21706.2891 normal_loss: 0.9635
[epoch 58][iter  340] loss: 179740.0625 RMSElog: 10.1206 grad_loss: 17962.9453 normal_loss: 0.9397
[epoch 58][iter  350] loss: 147544.5000 RMSElog: 9.8913 grad_loss: 14743.6045 normal_loss: 0.9534
[epoch 58][iter  360] loss: 150608.8906 RMSElog: 10.0680 grad_loss: 15049.8750 normal_loss: 0.9460
[epoch 58][iter  370] loss: 111518.2734 RMSElog: 9.6257 grad_loss: 11141.2676 normal_loss: 0.9335
[epoch 58][iter  380] loss: 175934.2344 RMSElog: 10.1481 grad_loss: 17582.3086 normal_loss: 0.9674
[epoch 58][iter  390] loss: 147915.7812 RMSElog: 10.0131 grad_loss: 14780.6104 normal_loss: 0.9547
[epoch 58][iter  400] loss: 196677.7812 RMSElog: 10.0486 grad_loss: 19656.7715 normal_loss: 0.9571
[epoch 58][iter  410] loss: 121983.8203 RMSElog: 9.5225 grad_loss: 12187.9473 normal_loss: 0.9124
[epoch 58][iter  420] loss: 103514.7812 RMSElog: 9.3451 grad_loss: 10341.2109 normal_loss: 0.9229
[epoch 58][iter  430] loss: 114893.3984 RMSElog: 9.2562 grad_loss: 11479.1777 normal_loss: 0.9067
[epoch 58][iter  440] loss: 141865.3906 RMSElog: 9.7199 grad_loss: 14175.8896 normal_loss: 0.9298
[epoch 58][iter  450] loss: 148184.5469 RMSElog: 10.0681 grad_loss: 14807.4355 normal_loss: 0.9504
[epoch 58][iter  460] loss: 117693.8984 RMSElog: 9.6527 grad_loss: 11758.8105 normal_loss: 0.9267
[epoch 58][iter  470] loss: 200953.4062 RMSElog: 10.1163 grad_loss: 20084.2598 normal_loss: 0.9623
[epoch 58][iter  480] loss: 100116.4922 RMSElog: 9.9235 grad_loss: 10000.7998 normal_loss: 0.9258
[epoch 58][iter  490] loss: 103815.4375 RMSElog: 9.7764 grad_loss: 10370.8340 normal_loss: 0.9338
[epoch 58][iter  500] loss: 159798.6094 RMSElog: 9.8075 grad_loss: 15969.1260 normal_loss: 0.9263
[epoch 58][iter  510] loss: 176798.0312 RMSElog: 9.8176 grad_loss: 17669.0293 normal_loss: 0.9559
[epoch 58][iter  520] loss: 95221.2109 RMSElog: 9.1894 grad_loss: 9512.0205 normal_loss: 0.9115
[epoch 58][iter  530] loss: 124246.3594 RMSElog: 9.7165 grad_loss: 12413.9834 normal_loss: 0.9360
[epoch 58][iter  540] loss: 132935.3906 RMSElog: 9.9640 grad_loss: 13282.6172 normal_loss: 0.9581
[epoch 58][iter  550] loss: 104086.5234 RMSElog: 9.7787 grad_loss: 10397.9453 normal_loss: 0.9289
[epoch 58][iter  560] loss: 144589.9531 RMSElog: 9.7438 grad_loss: 14448.3340 normal_loss: 0.9167
[epoch 58][iter  570] loss: 199607.1562 RMSElog: 10.1504 grad_loss: 19949.6133 normal_loss: 0.9517
[epoch 58][iter  580] loss: 181942.5625 RMSElog: 10.0291 grad_loss: 18183.2793 normal_loss: 0.9464
[epoch 58][iter  590] loss: 129708.8906 RMSElog: 10.0691 grad_loss: 12959.8711 normal_loss: 0.9481
[epoch 59][iter    0] loss: 177664.5312 RMSElog: 9.7808 grad_loss: 17755.7344 normal_loss: 0.9369
[epoch 59][iter   10] loss: 116073.5938 RMSElog: 9.6844 grad_loss: 11596.7324 normal_loss: 0.9429
[epoch 59][iter   20] loss: 168409.1562 RMSElog: 9.7031 grad_loss: 16830.2910 normal_loss: 0.9228
[epoch 59][iter   30] loss: 142138.5312 RMSElog: 9.7967 grad_loss: 14203.1270 normal_loss: 0.9291
[epoch 59][iter   40] loss: 176914.4531 RMSElog: 10.3802 grad_loss: 17680.1094 normal_loss: 0.9546
[epoch 59][iter   50] loss: 108830.7344 RMSElog: 9.7359 grad_loss: 10872.4062 normal_loss: 0.9307
[epoch 59][iter   60] loss: 170542.9531 RMSElog: 9.7855 grad_loss: 17043.5410 normal_loss: 0.9688
[epoch 59][iter   70] loss: 232771.8594 RMSElog: 10.2938 grad_loss: 23265.9453 normal_loss: 0.9464
[epoch 59][iter   80] loss: 182476.7344 RMSElog: 9.9672 grad_loss: 18236.7656 normal_loss: 0.9407
[epoch 59][iter   90] loss: 122446.5469 RMSElog: 9.6438 grad_loss: 12234.0742 normal_loss: 0.9366
[epoch 59][iter  100] loss: 105098.6875 RMSElog: 9.7426 grad_loss: 10499.2012 normal_loss: 0.9257
[epoch 59][iter  110] loss: 166093.7344 RMSElog: 10.4480 grad_loss: 16597.9766 normal_loss: 0.9489
[epoch 59][iter  120] loss: 127463.1875 RMSElog: 9.4952 grad_loss: 12735.9082 normal_loss: 0.9153
[epoch 59][iter  130] loss: 179901.4844 RMSElog: 9.8267 grad_loss: 17979.3633 normal_loss: 0.9589
[epoch 59][iter  140] loss: 172868.6875 RMSElog: 10.0539 grad_loss: 17275.8652 normal_loss: 0.9500
[epoch 59][iter  150] loss: 179649.7188 RMSElog: 10.2020 grad_loss: 17953.8125 normal_loss: 0.9593
[epoch 59][iter  160] loss: 203023.3438 RMSElog: 10.3036 grad_loss: 20291.0723 normal_loss: 0.9583
[epoch 59][iter  170] loss: 114374.2344 RMSElog: 9.1913 grad_loss: 11427.3242 normal_loss: 0.9087
[epoch 59][iter  180] loss: 184374.1875 RMSElog: 10.2641 grad_loss: 18426.1953 normal_loss: 0.9596
[epoch 59][iter  190] loss: 103798.3281 RMSElog: 9.6193 grad_loss: 10369.2812 normal_loss: 0.9331
[epoch 59][iter  200] loss: 100650.2734 RMSElog: 9.7441 grad_loss: 10054.3555 normal_loss: 0.9279
[epoch 59][iter  210] loss: 172115.2812 RMSElog: 9.7682 grad_loss: 17200.8242 normal_loss: 0.9358
[epoch 59][iter  220] loss: 198036.3750 RMSElog: 10.2619 grad_loss: 19792.4219 normal_loss: 0.9529
[epoch 59][iter  230] loss: 105877.8750 RMSElog: 9.7638 grad_loss: 10577.0986 normal_loss: 0.9249
[epoch 59][iter  240] loss: 175002.7344 RMSElog: 9.8652 grad_loss: 17489.4648 normal_loss: 0.9436
[epoch 59][iter  250] loss: 232158.2188 RMSElog: 10.1898 grad_loss: 23204.6875 normal_loss: 0.9455
[epoch 59][iter  260] loss: 208184.3125 RMSElog: 10.4061 grad_loss: 20807.0781 normal_loss: 0.9472
[epoch 59][iter  270] loss: 159798.6094 RMSElog: 9.8075 grad_loss: 15969.1260 normal_loss: 0.9263
[epoch 59][iter  280] loss: 146960.1250 RMSElog: 10.0979 grad_loss: 14684.9922 normal_loss: 0.9220
[epoch 59][iter  290] loss: 199607.1562 RMSElog: 10.1504 grad_loss: 19949.6133 normal_loss: 0.9517
[epoch 59][iter  300] loss: 156982.7188 RMSElog: 10.0093 grad_loss: 15687.3193 normal_loss: 0.9438
[epoch 59][iter  310] loss: 207885.1875 RMSElog: 10.4422 grad_loss: 20777.1328 normal_loss: 0.9460
[epoch 59][iter  320] loss: 185071.7188 RMSElog: 10.0456 grad_loss: 18496.1719 normal_loss: 0.9557
[epoch 59][iter  330] loss: 177699.9688 RMSElog: 9.4851 grad_loss: 17759.5488 normal_loss: 0.9621
[epoch 59][iter  340] loss: 136507.7344 RMSElog: 10.0223 grad_loss: 13639.7900 normal_loss: 0.9608
[epoch 59][iter  350] loss: 188645.4531 RMSElog: 10.1590 grad_loss: 18853.4082 normal_loss: 0.9794
[epoch 59][iter  360] loss: 181293.3438 RMSElog: 10.0009 grad_loss: 18118.3809 normal_loss: 0.9531
[epoch 59][iter  370] loss: 125695.5391 RMSElog: 9.9422 grad_loss: 12558.6836 normal_loss: 0.9278
[epoch 59][iter  380] loss: 214067.1875 RMSElog: 10.0580 grad_loss: 21395.7207 normal_loss: 0.9394
[epoch 59][iter  390] loss: 156253.7656 RMSElog: 9.8231 grad_loss: 15614.6211 normal_loss: 0.9316
[epoch 59][iter  400] loss: 120590.1719 RMSElog: 9.9054 grad_loss: 12048.1719 normal_loss: 0.9408
[epoch 59][iter  410] loss: 152149.7500 RMSElog: 9.9062 grad_loss: 15204.1221 normal_loss: 0.9462
[epoch 59][iter  420] loss: 160825.5938 RMSElog: 9.9290 grad_loss: 16071.6816 normal_loss: 0.9491
[epoch 59][iter  430] loss: 174565.0781 RMSElog: 9.7759 grad_loss: 17445.7832 normal_loss: 0.9484
[epoch 59][iter  440] loss: 104086.5234 RMSElog: 9.7787 grad_loss: 10397.9453 normal_loss: 0.9289
[epoch 59][iter  450] loss: 112715.8125 RMSElog: 9.6867 grad_loss: 11260.9570 normal_loss: 0.9375
[epoch 59][iter  460] loss: 124246.3594 RMSElog: 9.7165 grad_loss: 12413.9834 normal_loss: 0.9360
[epoch 59][iter  470] loss: 230519.3906 RMSElog: 10.3105 grad_loss: 23040.6836 normal_loss: 0.9459
[epoch 59][iter  480] loss: 125486.8672 RMSElog: 9.7758 grad_loss: 12537.9834 normal_loss: 0.9281
[epoch 59][iter  490] loss: 182005.7031 RMSElog: 10.0998 grad_loss: 18189.5195 normal_loss: 0.9507
[epoch 59][iter  500] loss: 119429.4609 RMSElog: 9.6388 grad_loss: 11932.3740 normal_loss: 0.9339
[epoch 59][iter  510] loss: 106516.8047 RMSElog: 9.6966 grad_loss: 10641.0557 normal_loss: 0.9285
[epoch 59][iter  520] loss: 102096.6484 RMSElog: 9.8437 grad_loss: 10198.8926 normal_loss: 0.9290
[epoch 59][iter  530] loss: 205362.7188 RMSElog: 10.1686 grad_loss: 20525.1562 normal_loss: 0.9475
[epoch 59][iter  540] loss: 160743.4062 RMSElog: 9.7965 grad_loss: 16063.6025 normal_loss: 0.9413
[epoch 59][iter  550] loss: 160788.1406 RMSElog: 9.7539 grad_loss: 16068.1289 normal_loss: 0.9318
[epoch 59][iter  560] loss: 132623.8750 RMSElog: 9.3325 grad_loss: 13252.1152 normal_loss: 0.9391
[epoch 59][iter  570] loss: 104456.8438 RMSElog: 10.0170 grad_loss: 10434.7461 normal_loss: 0.9214
[epoch 59][iter  580] loss: 148184.5469 RMSElog: 10.0681 grad_loss: 14807.4355 normal_loss: 0.9504
[epoch 59][iter  590] loss: 221213.4219 RMSElog: 10.1740 grad_loss: 22110.2266 normal_loss: 0.9415
[epoch 60][iter    0] loss: 109441.5234 RMSElog: 9.7466 grad_loss: 10933.4756 normal_loss: 0.9300
[epoch 60][iter   10] loss: 194408.7812 RMSElog: 10.2924 grad_loss: 19429.6328 normal_loss: 0.9537
[epoch 60][iter   20] loss: 221089.3594 RMSElog: 10.3457 grad_loss: 22097.6465 normal_loss: 0.9439
[epoch 60][iter   30] loss: 159824.3594 RMSElog: 9.8973 grad_loss: 15971.5811 normal_loss: 0.9569
[epoch 60][iter   40] loss: 143460.8594 RMSElog: 9.7520 grad_loss: 14335.3848 normal_loss: 0.9489
[epoch 60][iter   50] loss: 153594.7031 RMSElog: 9.8426 grad_loss: 15348.6826 normal_loss: 0.9453
[epoch 60][iter   60] loss: 182005.7031 RMSElog: 10.0998 grad_loss: 18189.5195 normal_loss: 0.9507
[epoch 60][iter   70] loss: 175299.8438 RMSElog: 10.1975 grad_loss: 17518.8301 normal_loss: 0.9576
[epoch 60][iter   80] loss: 149036.1562 RMSElog: 9.7728 grad_loss: 14892.8887 normal_loss: 0.9547
[epoch 60][iter   90] loss: 218757.0312 RMSElog: 10.1228 grad_loss: 21864.6270 normal_loss: 0.9524
[epoch 60][iter  100] loss: 191971.7188 RMSElog: 10.0873 grad_loss: 19186.1172 normal_loss: 0.9662
[epoch 60][iter  110] loss: 164767.1875 RMSElog: 9.8398 grad_loss: 16465.9102 normal_loss: 0.9691
[epoch 60][iter  120] loss: 115495.2188 RMSElog: 9.6084 grad_loss: 11538.9688 normal_loss: 0.9442
[epoch 60][iter  130] loss: 167784.2500 RMSElog: 9.9330 grad_loss: 16767.5391 normal_loss: 0.9539
[epoch 60][iter  140] loss: 107472.2578 RMSElog: 9.0654 grad_loss: 10737.2480 normal_loss: 0.9124
[epoch 60][iter  150] loss: 199607.1562 RMSElog: 10.1504 grad_loss: 19949.6133 normal_loss: 0.9517
[epoch 60][iter  160] loss: 177664.5469 RMSElog: 9.7808 grad_loss: 17755.7363 normal_loss: 0.9369
[epoch 60][iter  170] loss: 144440.4531 RMSElog: 9.8843 grad_loss: 14433.2314 normal_loss: 0.9296
[epoch 60][iter  180] loss: 124323.5156 RMSElog: 9.7543 grad_loss: 12421.6650 normal_loss: 0.9322
[epoch 60][iter  190] loss: 149128.2188 RMSElog: 9.7262 grad_loss: 14902.1533 normal_loss: 0.9419
[epoch 60][iter  200] loss: 170542.9531 RMSElog: 9.7855 grad_loss: 17043.5410 normal_loss: 0.9688
[epoch 60][iter  210] loss: 113895.6328 RMSElog: 9.2606 grad_loss: 11379.3926 normal_loss: 0.9106
[epoch 60][iter  220] loss: 140282.6250 RMSElog: 9.9822 grad_loss: 14017.3145 normal_loss: 0.9650
[epoch 60][iter  230] loss: 156557.3594 RMSElog: 9.6970 grad_loss: 15645.1025 normal_loss: 0.9363
[epoch 60][iter  240] loss: 218457.8281 RMSElog: 10.1255 grad_loss: 21834.7168 normal_loss: 0.9418
[epoch 60][iter  250] loss: 138934.7188 RMSElog: 9.8450 grad_loss: 13882.6904 normal_loss: 0.9370
[epoch 60][iter  260] loss: 100980.7812 RMSElog: 9.6528 grad_loss: 10087.5176 normal_loss: 0.9085
[epoch 60][iter  270] loss: 206083.5312 RMSElog: 10.2501 grad_loss: 20597.1465 normal_loss: 0.9562
[epoch 60][iter  280] loss: 166207.1094 RMSElog: 9.7947 grad_loss: 16609.9668 normal_loss: 0.9492
[epoch 60][iter  290] loss: 206400.7812 RMSElog: 10.3027 grad_loss: 20628.8281 normal_loss: 0.9467
[epoch 60][iter  300] loss: 172755.6875 RMSElog: 10.0251 grad_loss: 17264.6016 normal_loss: 0.9419
[epoch 60][iter  310] loss: 141962.4844 RMSElog: 9.7918 grad_loss: 14185.5273 normal_loss: 0.9289
[epoch 60][iter  320] loss: 167794.0469 RMSElog: 9.8335 grad_loss: 16768.6211 normal_loss: 0.9500
[epoch 60][iter  330] loss: 160788.1406 RMSElog: 9.7539 grad_loss: 16068.1289 normal_loss: 0.9318
[epoch 60][iter  340] loss: 103514.7734 RMSElog: 9.3451 grad_loss: 10341.2100 normal_loss: 0.9229
[epoch 60][iter  350] loss: 107948.0312 RMSElog: 9.7377 grad_loss: 10784.1309 normal_loss: 0.9342
[epoch 60][iter  360] loss: 146465.6875 RMSElog: 10.2283 grad_loss: 14635.4062 normal_loss: 0.9337
[epoch 60][iter  370] loss: 129743.7969 RMSElog: 9.9523 grad_loss: 12963.4785 normal_loss: 0.9492
[epoch 60][iter  380] loss: 177936.2812 RMSElog: 10.0381 grad_loss: 17782.6348 normal_loss: 0.9545
[epoch 60][iter  390] loss: 106516.8047 RMSElog: 9.6966 grad_loss: 10641.0557 normal_loss: 0.9285
[epoch 60][iter  400] loss: 161813.2500 RMSElog: 9.9875 grad_loss: 16170.3857 normal_loss: 0.9508
[epoch 60][iter  410] loss: 205235.7188 RMSElog: 10.2875 grad_loss: 20512.3281 normal_loss: 0.9564
[epoch 60][iter  420] loss: 108800.2422 RMSElog: 9.7538 grad_loss: 10869.3369 normal_loss: 0.9334
[epoch 60][iter  430] loss: 103305.2422 RMSElog: 9.5543 grad_loss: 10320.0674 normal_loss: 0.9024
[epoch 60][iter  440] loss: 197520.7500 RMSElog: 10.1663 grad_loss: 19740.9414 normal_loss: 0.9662
[epoch 60][iter  450] loss: 189543.6719 RMSElog: 10.1592 grad_loss: 18943.2598 normal_loss: 0.9474
[epoch 60][iter  460] loss: 210654.5312 RMSElog: 10.4982 grad_loss: 21054.0039 normal_loss: 0.9517
[epoch 60][iter  470] loss: 143613.7656 RMSElog: 9.8882 grad_loss: 14350.5566 normal_loss: 0.9315
[epoch 60][iter  480] loss: 150323.7656 RMSElog: 10.0510 grad_loss: 15021.3848 normal_loss: 0.9410
[epoch 60][iter  490] loss: 148057.2500 RMSElog: 9.9895 grad_loss: 14794.7803 normal_loss: 0.9555
[epoch 60][iter  500] loss: 109148.7344 RMSElog: 10.1172 grad_loss: 10903.8262 normal_loss: 0.9294
[epoch 60][iter  510] loss: 160879.7969 RMSElog: 9.7521 grad_loss: 16077.2764 normal_loss: 0.9513
[epoch 60][iter  520] loss: 105816.5000 RMSElog: 9.4266 grad_loss: 10571.3066 normal_loss: 0.9171
[epoch 60][iter  530] loss: 148239.1406 RMSElog: 9.8055 grad_loss: 14813.1689 normal_loss: 0.9395
[epoch 60][iter  540] loss: 145389.9375 RMSElog: 9.8784 grad_loss: 14528.1660 normal_loss: 0.9489
[epoch 60][iter  550] loss: 204037.7500 RMSElog: 10.1754 grad_loss: 20392.6582 normal_loss: 0.9420
[epoch 60][iter  560] loss: 144589.9531 RMSElog: 9.7438 grad_loss: 14448.3340 normal_loss: 0.9167
[epoch 60][iter  570] loss: 139433.9219 RMSElog: 9.8139 grad_loss: 13932.6191 normal_loss: 0.9591
[epoch 60][iter  580] loss: 227323.8281 RMSElog: 10.4159 grad_loss: 22721.0195 normal_loss: 0.9477
[epoch 60][iter  590] loss: 175524.1250 RMSElog: 10.2056 grad_loss: 17541.2441 normal_loss: 0.9625
[epoch 61][iter    0] loss: 110808.0469 RMSElog: 9.7171 grad_loss: 11070.1562 normal_loss: 0.9312
[epoch 61][iter   10] loss: 189507.2500 RMSElog: 10.2157 grad_loss: 18939.5586 normal_loss: 0.9510
[epoch 61][iter   20] loss: 228338.7500 RMSElog: 10.2936 grad_loss: 22822.6348 normal_loss: 0.9477
[epoch 61][iter   30] loss: 175524.1250 RMSElog: 10.2056 grad_loss: 17541.2441 normal_loss: 0.9625
[epoch 61][iter   40] loss: 113895.6328 RMSElog: 9.2606 grad_loss: 11379.3926 normal_loss: 0.9106
[epoch 61][iter   50] loss: 194987.6250 RMSElog: 9.8416 grad_loss: 19487.9453 normal_loss: 0.9748
[epoch 61][iter   60] loss: 105628.3672 RMSElog: 10.0899 grad_loss: 10551.8262 normal_loss: 0.9213
[epoch 61][iter   70] loss: 189543.6719 RMSElog: 10.1592 grad_loss: 18943.2598 normal_loss: 0.9474
[epoch 61][iter   80] loss: 154290.2500 RMSElog: 9.8471 grad_loss: 15418.2334 normal_loss: 0.9458
[epoch 61][iter   90] loss: 224257.9844 RMSElog: 10.2842 grad_loss: 22414.5625 normal_loss: 0.9521
[epoch 61][iter  100] loss: 114374.2344 RMSElog: 9.1913 grad_loss: 11427.3242 normal_loss: 0.9087
[epoch 61][iter  110] loss: 147598.9844 RMSElog: 9.8981 grad_loss: 14749.0645 normal_loss: 0.9355
[epoch 61][iter  120] loss: 194578.5781 RMSElog: 10.2312 grad_loss: 19446.6680 normal_loss: 0.9596
[epoch 61][iter  130] loss: 103955.0000 RMSElog: 9.6256 grad_loss: 10384.9521 normal_loss: 0.9222
[epoch 61][iter  140] loss: 221213.4219 RMSElog: 10.1740 grad_loss: 22110.2266 normal_loss: 0.9415
[epoch 61][iter  150] loss: 176223.4531 RMSElog: 9.9872 grad_loss: 17611.4141 normal_loss: 0.9446
[epoch 61][iter  160] loss: 105098.6875 RMSElog: 9.7426 grad_loss: 10499.2012 normal_loss: 0.9257
[epoch 61][iter  170] loss: 177936.2812 RMSElog: 10.0381 grad_loss: 17782.6348 normal_loss: 0.9545
[epoch 61][iter  180] loss: 102096.6484 RMSElog: 9.8437 grad_loss: 10198.8926 normal_loss: 0.9290
[epoch 61][iter  190] loss: 100990.1562 RMSElog: 9.7350 grad_loss: 10088.3584 normal_loss: 0.9222
[epoch 61][iter  200] loss: 218949.8281 RMSElog: 10.4462 grad_loss: 21883.5918 normal_loss: 0.9447
[epoch 61][iter  210] loss: 147207.9844 RMSElog: 9.9779 grad_loss: 14709.8789 normal_loss: 0.9413
[epoch 61][iter  220] loss: 103534.6562 RMSElog: 9.7670 grad_loss: 10342.7715 normal_loss: 0.9280
[epoch 61][iter  230] loss: 213401.8906 RMSElog: 10.0568 grad_loss: 21329.1582 normal_loss: 0.9749
[epoch 61][iter  240] loss: 233395.7188 RMSElog: 10.2368 grad_loss: 23328.3906 normal_loss: 0.9452
[epoch 61][iter  250] loss: 160825.5938 RMSElog: 9.9290 grad_loss: 16071.6816 normal_loss: 0.9491
[epoch 61][iter  260] loss: 163268.1250 RMSElog: 9.9218 grad_loss: 16315.9277 normal_loss: 0.9630
[epoch 61][iter  270] loss: 109148.7344 RMSElog: 10.1172 grad_loss: 10903.8262 normal_loss: 0.9294
[epoch 61][iter  280] loss: 177664.5469 RMSElog: 9.7808 grad_loss: 17755.7363 normal_loss: 0.9369
[epoch 61][iter  290] loss: 105706.7031 RMSElog: 9.7501 grad_loss: 10559.9883 normal_loss: 0.9321
[epoch 61][iter  300] loss: 218547.0156 RMSElog: 10.4419 grad_loss: 21843.3086 normal_loss: 0.9515
[epoch 61][iter  310] loss: 214775.1719 RMSElog: 10.1719 grad_loss: 21466.3848 normal_loss: 0.9613
[epoch 61][iter  320] loss: 115500.4766 RMSElog: 9.9924 grad_loss: 11539.1230 normal_loss: 0.9328
[epoch 61][iter  330] loss: 153594.7031 RMSElog: 9.8426 grad_loss: 15348.6826 normal_loss: 0.9453
[epoch 61][iter  340] loss: 200470.3906 RMSElog: 10.2414 grad_loss: 20035.8379 normal_loss: 0.9585
[epoch 61][iter  350] loss: 231973.6719 RMSElog: 10.2099 grad_loss: 23186.2090 normal_loss: 0.9490
[epoch 61][iter  360] loss: 115534.3750 RMSElog: 9.7196 grad_loss: 11542.7852 normal_loss: 0.9325
[epoch 61][iter  370] loss: 138934.7188 RMSElog: 9.8450 grad_loss: 13882.6904 normal_loss: 0.9370
[epoch 61][iter  380] loss: 115452.5781 RMSElog: 9.2301 grad_loss: 11535.1230 normal_loss: 0.9042
[epoch 61][iter  390] loss: 178593.9688 RMSElog: 10.0081 grad_loss: 17848.4395 normal_loss: 0.9490
[epoch 61][iter  400] loss: 153501.7031 RMSElog: 10.1127 grad_loss: 15339.1152 normal_loss: 0.9429
[epoch 61][iter  410] loss: 119655.7969 RMSElog: 9.6110 grad_loss: 11955.0312 normal_loss: 0.9378
[epoch 61][iter  420] loss: 175002.7344 RMSElog: 9.8652 grad_loss: 17489.4648 normal_loss: 0.9436
[epoch 61][iter  430] loss: 175301.7344 RMSElog: 9.8017 grad_loss: 17519.4180 normal_loss: 0.9555
[epoch 61][iter  440] loss: 130739.4062 RMSElog: 9.5471 grad_loss: 13063.4434 normal_loss: 0.9498
[epoch 61][iter  450] loss: 234945.1719 RMSElog: 10.0015 grad_loss: 23483.5664 normal_loss: 0.9500
[epoch 61][iter  460] loss: 129163.5156 RMSElog: 9.7674 grad_loss: 12905.6660 normal_loss: 0.9175
[epoch 61][iter  470] loss: 109441.5234 RMSElog: 9.7466 grad_loss: 10933.4756 normal_loss: 0.9300
[epoch 61][iter  480] loss: 125281.0156 RMSElog: 9.5401 grad_loss: 12517.6484 normal_loss: 0.9128
[epoch 61][iter  490] loss: 196025.7031 RMSElog: 10.2650 grad_loss: 19591.3496 normal_loss: 0.9559
[epoch 61][iter  500] loss: 145409.6719 RMSElog: 9.8330 grad_loss: 14530.1904 normal_loss: 0.9445
[epoch 61][iter  510] loss: 195999.5625 RMSElog: 10.1749 grad_loss: 19588.8047 normal_loss: 0.9756
[epoch 61][iter  520] loss: 164766.2812 RMSElog: 9.6928 grad_loss: 16465.9941 normal_loss: 0.9411
[epoch 61][iter  530] loss: 182476.7344 RMSElog: 9.9672 grad_loss: 18236.7656 normal_loss: 0.9407
[epoch 61][iter  540] loss: 176914.4531 RMSElog: 10.3802 grad_loss: 17680.1094 normal_loss: 0.9546
[epoch 61][iter  550] loss: 173574.0781 RMSElog: 10.1780 grad_loss: 17346.2734 normal_loss: 0.9575
[epoch 61][iter  560] loss: 177063.6250 RMSElog: 10.1532 grad_loss: 17695.2383 normal_loss: 0.9719
[epoch 61][iter  570] loss: 128883.6406 RMSElog: 9.8798 grad_loss: 12877.5254 normal_loss: 0.9593
[epoch 61][iter  580] loss: 120292.3438 RMSElog: 9.8042 grad_loss: 12018.4863 normal_loss: 0.9430
[epoch 61][iter  590] loss: 214808.7500 RMSElog: 10.1872 grad_loss: 21469.7461 normal_loss: 0.9423
[epoch 62][iter    0] loss: 218625.8750 RMSElog: 10.2839 grad_loss: 21851.3457 normal_loss: 0.9592
[epoch 62][iter   10] loss: 198293.9219 RMSElog: 10.2275 grad_loss: 19818.2148 normal_loss: 0.9521
[epoch 62][iter   20] loss: 174375.7031 RMSElog: 10.0512 grad_loss: 17426.5703 normal_loss: 0.9483
[epoch 62][iter   30] loss: 133464.0312 RMSElog: 9.6242 grad_loss: 13335.8584 normal_loss: 0.9201
[epoch 62][iter   40] loss: 107073.5156 RMSElog: 9.7289 grad_loss: 10696.6914 normal_loss: 0.9313
[epoch 62][iter   50] loss: 103509.2188 RMSElog: 9.9787 grad_loss: 10340.0186 normal_loss: 0.9251
[epoch 62][iter   60] loss: 206083.5156 RMSElog: 10.2501 grad_loss: 20597.1445 normal_loss: 0.9562
[epoch 62][iter   70] loss: 103955.0000 RMSElog: 9.6256 grad_loss: 10384.9521 normal_loss: 0.9222
[epoch 62][iter   80] loss: 123525.9062 RMSElog: 9.3524 grad_loss: 12342.2930 normal_loss: 0.9450
[epoch 62][iter   90] loss: 227080.5938 RMSElog: 10.4127 grad_loss: 22696.6992 normal_loss: 0.9482
[epoch 62][iter  100] loss: 143534.0938 RMSElog: 9.7662 grad_loss: 14342.7061 normal_loss: 0.9374
[epoch 62][iter  110] loss: 152593.5000 RMSElog: 9.8773 grad_loss: 15248.5449 normal_loss: 0.9283
[epoch 62][iter  120] loss: 130641.2188 RMSElog: 9.7184 grad_loss: 13053.4688 normal_loss: 0.9344
[epoch 62][iter  130] loss: 136796.3438 RMSElog: 9.6524 grad_loss: 13669.0273 normal_loss: 0.9549
[epoch 62][iter  140] loss: 159824.3594 RMSElog: 9.8973 grad_loss: 15971.5811 normal_loss: 0.9569
[epoch 62][iter  150] loss: 175002.7344 RMSElog: 9.8652 grad_loss: 17489.4648 normal_loss: 0.9436
[epoch 62][iter  160] loss: 201503.0625 RMSElog: 10.3149 grad_loss: 20139.0410 normal_loss: 0.9514
[epoch 62][iter  170] loss: 149369.9219 RMSElog: 10.0930 grad_loss: 14925.9766 normal_loss: 0.9225
[epoch 62][iter  180] loss: 175301.7344 RMSElog: 9.8017 grad_loss: 17519.4180 normal_loss: 0.9555
[epoch 62][iter  190] loss: 173855.4375 RMSElog: 10.0769 grad_loss: 17374.5098 normal_loss: 0.9566
[epoch 62][iter  200] loss: 188717.5625 RMSElog: 10.0336 grad_loss: 18860.7715 normal_loss: 0.9509
[epoch 62][iter  210] loss: 148057.2500 RMSElog: 9.9895 grad_loss: 14794.7803 normal_loss: 0.9555
[epoch 62][iter  220] loss: 163171.1562 RMSElog: 9.7905 grad_loss: 16306.3594 normal_loss: 0.9652
[epoch 62][iter  230] loss: 105742.0469 RMSElog: 9.7582 grad_loss: 10563.5244 normal_loss: 0.9230
[epoch 62][iter  240] loss: 95221.2109 RMSElog: 9.1894 grad_loss: 9512.0205 normal_loss: 0.9115
[epoch 62][iter  250] loss: 190972.3281 RMSElog: 10.1152 grad_loss: 19086.1523 normal_loss: 0.9647
[epoch 62][iter  260] loss: 122331.3906 RMSElog: 9.7180 grad_loss: 12222.4795 normal_loss: 0.9418
[epoch 62][iter  270] loss: 128277.9766 RMSElog: 9.7565 grad_loss: 12817.0889 normal_loss: 0.9518
[epoch 62][iter  280] loss: 138057.5156 RMSElog: 9.5188 grad_loss: 13795.3184 normal_loss: 0.9138
[epoch 62][iter  290] loss: 162711.9688 RMSElog: 9.6160 grad_loss: 16260.6387 normal_loss: 0.9427
[epoch 62][iter  300] loss: 152549.8750 RMSElog: 9.9928 grad_loss: 15244.0293 normal_loss: 0.9654
[epoch 62][iter  310] loss: 103534.6562 RMSElog: 9.7670 grad_loss: 10342.7715 normal_loss: 0.9280
[epoch 62][iter  320] loss: 203023.3438 RMSElog: 10.3036 grad_loss: 20291.0723 normal_loss: 0.9583
[epoch 62][iter  330] loss: 113895.6328 RMSElog: 9.2606 grad_loss: 11379.3926 normal_loss: 0.9106
[epoch 62][iter  340] loss: 101101.5781 RMSElog: 9.9274 grad_loss: 10099.3008 normal_loss: 0.9297
[epoch 62][iter  350] loss: 176798.0469 RMSElog: 9.8176 grad_loss: 17669.0312 normal_loss: 0.9559
[epoch 62][iter  360] loss: 152772.4062 RMSElog: 9.8249 grad_loss: 15266.4727 normal_loss: 0.9438
[epoch 62][iter  370] loss: 106952.9219 RMSElog: 10.0294 grad_loss: 10684.3398 normal_loss: 0.9231
[epoch 62][iter  380] loss: 143613.7656 RMSElog: 9.8882 grad_loss: 14350.5566 normal_loss: 0.9315
[epoch 62][iter  390] loss: 153724.0000 RMSElog: 9.9609 grad_loss: 15361.4922 normal_loss: 0.9475
[epoch 62][iter  400] loss: 143904.7812 RMSElog: 10.0263 grad_loss: 14379.5068 normal_loss: 0.9457
[epoch 62][iter  410] loss: 185902.4219 RMSElog: 9.8998 grad_loss: 18579.3711 normal_loss: 0.9705
[epoch 62][iter  420] loss: 157061.8906 RMSElog: 9.9892 grad_loss: 15695.2461 normal_loss: 0.9531
[epoch 62][iter  430] loss: 195412.9844 RMSElog: 10.0493 grad_loss: 19530.2969 normal_loss: 0.9539
[epoch 62][iter  440] loss: 153099.1094 RMSElog: 9.8355 grad_loss: 15299.1445 normal_loss: 0.9304
[epoch 62][iter  450] loss: 162433.1875 RMSElog: 9.8183 grad_loss: 16232.5488 normal_loss: 0.9510
[epoch 62][iter  460] loss: 150321.2812 RMSElog: 9.9248 grad_loss: 15021.2656 normal_loss: 0.9385
[epoch 62][iter  470] loss: 135215.1250 RMSElog: 9.9407 grad_loss: 13510.6152 normal_loss: 0.9572
[epoch 62][iter  480] loss: 195777.0469 RMSElog: 10.1280 grad_loss: 19566.6035 normal_loss: 0.9726
[epoch 62][iter  490] loss: 127110.9375 RMSElog: 9.5273 grad_loss: 12700.6064 normal_loss: 0.9604
[epoch 62][iter  500] loss: 109260.8906 RMSElog: 9.7819 grad_loss: 10915.3789 normal_loss: 0.9279
[epoch 62][iter  510] loss: 160642.9375 RMSElog: 9.9306 grad_loss: 16053.4121 normal_loss: 0.9504
[epoch 62][iter  520] loss: 161084.5469 RMSElog: 9.9606 grad_loss: 16097.5430 normal_loss: 0.9512
[epoch 62][iter  530] loss: 170690.9375 RMSElog: 9.9328 grad_loss: 17058.2129 normal_loss: 0.9470
[epoch 62][iter  540] loss: 115658.3125 RMSElog: 9.1856 grad_loss: 11555.7031 normal_loss: 0.9424
[epoch 62][iter  550] loss: 174565.0781 RMSElog: 9.7759 grad_loss: 17445.7832 normal_loss: 0.9484
[epoch 62][iter  560] loss: 156982.7188 RMSElog: 10.0093 grad_loss: 15687.3193 normal_loss: 0.9438
[epoch 62][iter  570] loss: 140282.6250 RMSElog: 9.9822 grad_loss: 14017.3145 normal_loss: 0.9650
[epoch 62][iter  580] loss: 180982.8438 RMSElog: 10.2711 grad_loss: 18087.0605 normal_loss: 0.9528
[epoch 62][iter  590] loss: 112851.7266 RMSElog: 9.4205 grad_loss: 11274.8320 normal_loss: 0.9199
[epoch 63][iter    0] loss: 189507.2500 RMSElog: 10.2157 grad_loss: 18939.5586 normal_loss: 0.9510
[epoch 63][iter   10] loss: 184374.2188 RMSElog: 10.2641 grad_loss: 18426.1992 normal_loss: 0.9596
[epoch 63][iter   20] loss: 234945.1719 RMSElog: 10.0015 grad_loss: 23483.5664 normal_loss: 0.9500
[epoch 63][iter   30] loss: 106195.4219 RMSElog: 9.2531 grad_loss: 10609.3711 normal_loss: 0.9179
[epoch 63][iter   40] loss: 161813.2500 RMSElog: 9.9875 grad_loss: 16170.3857 normal_loss: 0.9508
[epoch 63][iter   50] loss: 139154.8594 RMSElog: 9.5986 grad_loss: 13904.9707 normal_loss: 0.9164
[epoch 63][iter   60] loss: 206083.5156 RMSElog: 10.2501 grad_loss: 20597.1445 normal_loss: 0.9562
[epoch 63][iter   70] loss: 106952.9219 RMSElog: 10.0294 grad_loss: 10684.3398 normal_loss: 0.9231
[epoch 63][iter   80] loss: 144440.4531 RMSElog: 9.8843 grad_loss: 14433.2314 normal_loss: 0.9296
[epoch 63][iter   90] loss: 122359.4531 RMSElog: 9.7683 grad_loss: 12225.2402 normal_loss: 0.9361
[epoch 63][iter  100] loss: 176914.4531 RMSElog: 10.3802 grad_loss: 17680.1094 normal_loss: 0.9546
[epoch 63][iter  110] loss: 154165.2969 RMSElog: 10.0187 grad_loss: 15405.5605 normal_loss: 0.9498
[epoch 63][iter  120] loss: 166010.9531 RMSElog: 9.9249 grad_loss: 16590.2227 normal_loss: 0.9481
[epoch 63][iter  130] loss: 162372.7500 RMSElog: 9.8226 grad_loss: 16226.4922 normal_loss: 0.9603
[epoch 63][iter  140] loss: 215542.7500 RMSElog: 10.4358 grad_loss: 21542.8887 normal_loss: 0.9514
[epoch 63][iter  150] loss: 133045.7969 RMSElog: 9.8765 grad_loss: 13293.7490 normal_loss: 0.9538
[epoch 63][iter  160] loss: 191320.0938 RMSElog: 10.0978 grad_loss: 19120.9492 normal_loss: 0.9627
[epoch 63][iter  170] loss: 197714.9844 RMSElog: 10.4355 grad_loss: 19760.1094 normal_loss: 0.9532
[epoch 63][iter  180] loss: 165750.3750 RMSElog: 9.8971 grad_loss: 16564.1895 normal_loss: 0.9518
[epoch 63][iter  190] loss: 163389.5625 RMSElog: 9.7702 grad_loss: 16328.2656 normal_loss: 0.9208
[epoch 63][iter  200] loss: 172115.2812 RMSElog: 9.7682 grad_loss: 17200.8242 normal_loss: 0.9358
[epoch 63][iter  210] loss: 126023.2344 RMSElog: 9.6219 grad_loss: 12591.7656 normal_loss: 0.9360
[epoch 63][iter  220] loss: 139830.9062 RMSElog: 9.7552 grad_loss: 13972.3770 normal_loss: 0.9585
[epoch 63][iter  230] loss: 138950.6406 RMSElog: 9.9777 grad_loss: 13884.1387 normal_loss: 0.9482
[epoch 63][iter  240] loss: 212239.7031 RMSElog: 10.2690 grad_loss: 21212.7383 normal_loss: 0.9638
[epoch 63][iter  250] loss: 102096.6484 RMSElog: 9.8437 grad_loss: 10198.8926 normal_loss: 0.9290
[epoch 63][iter  260] loss: 186049.0312 RMSElog: 10.0165 grad_loss: 18593.9277 normal_loss: 0.9585
[epoch 63][iter  270] loss: 151401.7188 RMSElog: 9.4414 grad_loss: 15129.7939 normal_loss: 0.9361
[epoch 63][iter  280] loss: 143904.7812 RMSElog: 10.0263 grad_loss: 14379.5068 normal_loss: 0.9457
[epoch 63][iter  290] loss: 189582.6719 RMSElog: 9.9918 grad_loss: 18947.3223 normal_loss: 0.9523
[epoch 63][iter  300] loss: 130641.2188 RMSElog: 9.7184 grad_loss: 13053.4688 normal_loss: 0.9344
[epoch 63][iter  310] loss: 100663.8984 RMSElog: 9.9598 grad_loss: 10055.5020 normal_loss: 0.9277
[epoch 63][iter  320] loss: 103305.2500 RMSElog: 9.5543 grad_loss: 10320.0684 normal_loss: 0.9024
[epoch 63][iter  330] loss: 127075.1875 RMSElog: 10.1217 grad_loss: 12696.4443 normal_loss: 0.9524
[epoch 63][iter  340] loss: 175002.7344 RMSElog: 9.8652 grad_loss: 17489.4648 normal_loss: 0.9436
[epoch 63][iter  350] loss: 214493.2188 RMSElog: 10.3525 grad_loss: 21438.0078 normal_loss: 0.9611
[epoch 63][iter  360] loss: 157425.7188 RMSElog: 9.9235 grad_loss: 15731.7012 normal_loss: 0.9477
[epoch 63][iter  370] loss: 159340.4219 RMSElog: 9.9507 grad_loss: 15923.1406 normal_loss: 0.9500
[epoch 63][iter  380] loss: 103798.3281 RMSElog: 9.6193 grad_loss: 10369.2812 normal_loss: 0.9331
[epoch 63][iter  390] loss: 196437.8125 RMSElog: 10.2319 grad_loss: 19632.5898 normal_loss: 0.9584
[epoch 63][iter  400] loss: 159824.3594 RMSElog: 9.8973 grad_loss: 15971.5811 normal_loss: 0.9569
[epoch 63][iter  410] loss: 146781.9531 RMSElog: 9.8688 grad_loss: 14667.3818 normal_loss: 0.9440
[epoch 63][iter  420] loss: 179649.7188 RMSElog: 10.2020 grad_loss: 17953.8125 normal_loss: 0.9593
[epoch 63][iter  430] loss: 131213.2969 RMSElog: 9.6860 grad_loss: 13110.7217 normal_loss: 0.9217
[epoch 63][iter  440] loss: 154310.0938 RMSElog: 9.9148 grad_loss: 15420.1445 normal_loss: 0.9495
[epoch 63][iter  450] loss: 141228.9219 RMSElog: 9.9926 grad_loss: 14111.9531 normal_loss: 0.9477
[epoch 63][iter  460] loss: 144589.9531 RMSElog: 9.7438 grad_loss: 14448.3340 normal_loss: 0.9167
[epoch 63][iter  470] loss: 121983.8203 RMSElog: 9.5225 grad_loss: 12187.9473 normal_loss: 0.9124
[epoch 63][iter  480] loss: 145409.6719 RMSElog: 9.8330 grad_loss: 14530.1904 normal_loss: 0.9445
[epoch 63][iter  490] loss: 125683.7969 RMSElog: 9.8131 grad_loss: 12557.6123 normal_loss: 0.9537
[epoch 63][iter  500] loss: 142016.3906 RMSElog: 9.9191 grad_loss: 14190.7676 normal_loss: 0.9526
[epoch 63][iter  510] loss: 183480.3438 RMSElog: 10.0091 grad_loss: 18337.0801 normal_loss: 0.9458
[epoch 63][iter  520] loss: 218949.8281 RMSElog: 10.4462 grad_loss: 21883.5918 normal_loss: 0.9447
[epoch 63][iter  530] loss: 232771.8594 RMSElog: 10.2938 grad_loss: 23265.9453 normal_loss: 0.9464
[epoch 63][iter  540] loss: 125281.0078 RMSElog: 9.5401 grad_loss: 12517.6475 normal_loss: 0.9128
[epoch 63][iter  550] loss: 188116.2812 RMSElog: 10.0888 grad_loss: 18800.5703 normal_loss: 0.9711
[epoch 63][iter  560] loss: 158534.6406 RMSElog: 9.6735 grad_loss: 15842.8672 normal_loss: 0.9232
[epoch 63][iter  570] loss: 100980.7812 RMSElog: 9.6528 grad_loss: 10087.5176 normal_loss: 0.9085
[epoch 63][iter  580] loss: 197520.7500 RMSElog: 10.1663 grad_loss: 19740.9414 normal_loss: 0.9662
[epoch 63][iter  590] loss: 188662.5156 RMSElog: 10.2592 grad_loss: 18855.0352 normal_loss: 0.9574
[epoch 64][iter    0] loss: 115534.3750 RMSElog: 9.7196 grad_loss: 11542.7852 normal_loss: 0.9325
[epoch 64][iter   10] loss: 114374.2344 RMSElog: 9.1913 grad_loss: 11427.3242 normal_loss: 0.9087
[epoch 64][iter   20] loss: 176223.4531 RMSElog: 9.9872 grad_loss: 17611.4141 normal_loss: 0.9446
[epoch 64][iter   30] loss: 164415.3438 RMSElog: 10.0829 grad_loss: 16430.4961 normal_loss: 0.9571
[epoch 64][iter   40] loss: 218949.8281 RMSElog: 10.4462 grad_loss: 21883.5918 normal_loss: 0.9447
[epoch 64][iter   50] loss: 128319.4141 RMSElog: 9.6733 grad_loss: 12821.3398 normal_loss: 0.9287
[epoch 64][iter   60] loss: 174375.6875 RMSElog: 10.0512 grad_loss: 17426.5684 normal_loss: 0.9483
[epoch 64][iter   70] loss: 211640.2344 RMSElog: 10.2414 grad_loss: 21152.8145 normal_loss: 0.9675
[epoch 64][iter   80] loss: 142696.6875 RMSElog: 9.9612 grad_loss: 14258.7588 normal_loss: 0.9493
[epoch 64][iter   90] loss: 133045.7969 RMSElog: 9.8765 grad_loss: 13293.7480 normal_loss: 0.9538
[epoch 64][iter  100] loss: 153724.0000 RMSElog: 9.9609 grad_loss: 15361.4922 normal_loss: 0.9475
[epoch 64][iter  110] loss: 153594.7031 RMSElog: 9.8426 grad_loss: 15348.6826 normal_loss: 0.9453
[epoch 64][iter  120] loss: 142138.5156 RMSElog: 9.7967 grad_loss: 14203.1260 normal_loss: 0.9291
[epoch 64][iter  130] loss: 175486.0625 RMSElog: 10.0393 grad_loss: 17537.6094 normal_loss: 0.9563
[epoch 64][iter  140] loss: 160051.0156 RMSElog: 9.8405 grad_loss: 15994.3105 normal_loss: 0.9506
[epoch 64][iter  150] loss: 110603.0781 RMSElog: 9.8046 grad_loss: 11049.5664 normal_loss: 0.9364
[epoch 64][iter  160] loss: 163171.1562 RMSElog: 9.7905 grad_loss: 16306.3594 normal_loss: 0.9652
[epoch 64][iter  170] loss: 129743.7969 RMSElog: 9.9523 grad_loss: 12963.4785 normal_loss: 0.9492
[epoch 64][iter  180] loss: 195413.0312 RMSElog: 10.0493 grad_loss: 19530.3008 normal_loss: 0.9539
[epoch 64][iter  190] loss: 125281.0078 RMSElog: 9.5401 grad_loss: 12517.6475 normal_loss: 0.9128
[epoch 64][iter  200] loss: 166093.7344 RMSElog: 10.4480 grad_loss: 16597.9766 normal_loss: 0.9489
[epoch 64][iter  210] loss: 105896.6094 RMSElog: 9.6983 grad_loss: 10579.0293 normal_loss: 0.9335
[epoch 64][iter  220] loss: 140938.8594 RMSElog: 9.7483 grad_loss: 14083.1953 normal_loss: 0.9424
[epoch 64][iter  230] loss: 178076.4688 RMSElog: 9.8778 grad_loss: 17796.8457 normal_loss: 0.9233
[epoch 64][iter  240] loss: 180929.5938 RMSElog: 10.0745 grad_loss: 18081.9297 normal_loss: 0.9546
[epoch 64][iter  250] loss: 199607.0938 RMSElog: 10.0729 grad_loss: 19949.6758 normal_loss: 0.9604
[epoch 64][iter  260] loss: 160074.2031 RMSElog: 9.7506 grad_loss: 15996.7432 normal_loss: 0.9257
[epoch 64][iter  270] loss: 170357.3281 RMSElog: 9.9687 grad_loss: 17024.7988 normal_loss: 0.9644
[epoch 64][iter  280] loss: 154776.6250 RMSElog: 9.8981 grad_loss: 15466.8438 normal_loss: 0.9209
[epoch 64][iter  290] loss: 104340.6328 RMSElog: 9.7692 grad_loss: 10423.3701 normal_loss: 0.9234
[epoch 64][iter  300] loss: 107956.6094 RMSElog: 10.0900 grad_loss: 10784.6504 normal_loss: 0.9209
[epoch 64][iter  310] loss: 128488.4375 RMSElog: 9.7402 grad_loss: 12838.1719 normal_loss: 0.9319
[epoch 64][iter  320] loss: 135203.5938 RMSElog: 9.4545 grad_loss: 13509.9697 normal_loss: 0.9356
[epoch 64][iter  330] loss: 134406.1875 RMSElog: 9.9402 grad_loss: 13429.7246 normal_loss: 0.9539
[epoch 64][iter  340] loss: 135042.2031 RMSElog: 9.6763 grad_loss: 13493.6074 normal_loss: 0.9366
[epoch 64][iter  350] loss: 157425.7188 RMSElog: 9.9235 grad_loss: 15731.7012 normal_loss: 0.9477
[epoch 64][iter  360] loss: 152549.8750 RMSElog: 9.9928 grad_loss: 15244.0293 normal_loss: 0.9654
[epoch 64][iter  370] loss: 122670.8984 RMSElog: 9.4365 grad_loss: 12256.7207 normal_loss: 0.9331
[epoch 64][iter  380] loss: 152149.7500 RMSElog: 9.9062 grad_loss: 15204.1221 normal_loss: 0.9462
[epoch 64][iter  390] loss: 103514.7734 RMSElog: 9.3451 grad_loss: 10341.2100 normal_loss: 0.9229
[epoch 64][iter  400] loss: 136507.7344 RMSElog: 10.0223 grad_loss: 13639.7900 normal_loss: 0.9608
[epoch 64][iter  410] loss: 135699.4688 RMSElog: 9.6754 grad_loss: 13559.3340 normal_loss: 0.9364
[epoch 64][iter  420] loss: 129166.5312 RMSElog: 9.7364 grad_loss: 12905.9775 normal_loss: 0.9391
[epoch 64][iter  430] loss: 151401.7031 RMSElog: 9.4414 grad_loss: 15129.7930 normal_loss: 0.9361
[epoch 64][iter  440] loss: 160466.8750 RMSElog: 9.9086 grad_loss: 16035.8252 normal_loss: 0.9540
[epoch 64][iter  450] loss: 143534.0938 RMSElog: 9.7662 grad_loss: 14342.7061 normal_loss: 0.9374
[epoch 64][iter  460] loss: 161084.5469 RMSElog: 9.9606 grad_loss: 16097.5430 normal_loss: 0.9512
[epoch 64][iter  470] loss: 188645.4531 RMSElog: 10.1590 grad_loss: 18853.4082 normal_loss: 0.9794
[epoch 64][iter  480] loss: 220587.5781 RMSElog: 10.2995 grad_loss: 22047.5117 normal_loss: 0.9466
[epoch 64][iter  490] loss: 161646.4688 RMSElog: 10.1105 grad_loss: 16153.5762 normal_loss: 0.9608
[epoch 64][iter  500] loss: 180318.6719 RMSElog: 9.9530 grad_loss: 18020.9785 normal_loss: 0.9357
[epoch 64][iter  510] loss: 90482.6875 RMSElog: 9.3127 grad_loss: 9038.0186 normal_loss: 0.9378
[epoch 64][iter  520] loss: 116582.6172 RMSElog: 9.7871 grad_loss: 11647.5430 normal_loss: 0.9314
[epoch 64][iter  530] loss: 100980.7812 RMSElog: 9.6528 grad_loss: 10087.5176 normal_loss: 0.9085
[epoch 64][iter  540] loss: 191825.0781 RMSElog: 10.3012 grad_loss: 19171.2500 normal_loss: 0.9570
[epoch 64][iter  550] loss: 139154.8594 RMSElog: 9.5986 grad_loss: 13904.9707 normal_loss: 0.9164
[epoch 64][iter  560] loss: 130527.6172 RMSElog: 9.8914 grad_loss: 13041.9189 normal_loss: 0.9509
[epoch 64][iter  570] loss: 201373.6875 RMSElog: 10.3870 grad_loss: 20126.0332 normal_loss: 0.9491
[epoch 64][iter  580] loss: 127463.1875 RMSElog: 9.4952 grad_loss: 12735.9082 normal_loss: 0.9153
[epoch 64][iter  590] loss: 195777.0156 RMSElog: 10.1280 grad_loss: 19566.5996 normal_loss: 0.9726
[epoch 65][iter    0] loss: 109260.8906 RMSElog: 9.7819 grad_loss: 10915.3789 normal_loss: 0.9279
[epoch 65][iter   10] loss: 114893.3984 RMSElog: 9.2562 grad_loss: 11479.1777 normal_loss: 0.9067
[epoch 65][iter   20] loss: 119470.5156 RMSElog: 9.6196 grad_loss: 11936.5020 normal_loss: 0.9297
[epoch 65][iter   30] loss: 205362.7188 RMSElog: 10.1686 grad_loss: 20525.1562 normal_loss: 0.9475
[epoch 65][iter   40] loss: 108800.2422 RMSElog: 9.7538 grad_loss: 10869.3369 normal_loss: 0.9334
[epoch 65][iter   50] loss: 201162.4062 RMSElog: 9.9531 grad_loss: 20105.3164 normal_loss: 0.9706
[epoch 65][iter   60] loss: 113895.6328 RMSElog: 9.2606 grad_loss: 11379.3926 normal_loss: 0.9106
[epoch 65][iter   70] loss: 102223.4531 RMSElog: 9.4278 grad_loss: 10212.0010 normal_loss: 0.9169
[epoch 65][iter   80] loss: 154705.2344 RMSElog: 9.9142 grad_loss: 15459.6562 normal_loss: 0.9527
[epoch 65][iter   90] loss: 122450.9844 RMSElog: 9.6977 grad_loss: 12234.4727 normal_loss: 0.9292
[epoch 65][iter  100] loss: 221089.3594 RMSElog: 10.3457 grad_loss: 22097.6465 normal_loss: 0.9439
[epoch 65][iter  110] loss: 157274.4688 RMSElog: 9.9277 grad_loss: 15716.5723 normal_loss: 0.9461
[epoch 65][iter  120] loss: 138057.5156 RMSElog: 9.5188 grad_loss: 13795.3184 normal_loss: 0.9138
[epoch 65][iter  130] loss: 151368.4219 RMSElog: 9.7883 grad_loss: 15126.1289 normal_loss: 0.9251
[epoch 65][iter  140] loss: 171588.0938 RMSElog: 9.8325 grad_loss: 17148.0527 normal_loss: 0.9237
[epoch 65][iter  150] loss: 104340.6328 RMSElog: 9.7692 grad_loss: 10423.3701 normal_loss: 0.9234
[epoch 65][iter  160] loss: 108830.7344 RMSElog: 9.7359 grad_loss: 10872.4062 normal_loss: 0.9307
[epoch 65][iter  170] loss: 167784.2500 RMSElog: 9.9330 grad_loss: 16767.5391 normal_loss: 0.9539
[epoch 65][iter  180] loss: 186960.2969 RMSElog: 10.2468 grad_loss: 18684.8457 normal_loss: 0.9380
[epoch 65][iter  190] loss: 170056.2812 RMSElog: 9.7423 grad_loss: 16994.9395 normal_loss: 0.9472
[epoch 65][iter  200] loss: 148184.5469 RMSElog: 10.0681 grad_loss: 14807.4355 normal_loss: 0.9504
[epoch 65][iter  210] loss: 115495.2188 RMSElog: 9.6084 grad_loss: 11538.9688 normal_loss: 0.9442
[epoch 65][iter  220] loss: 176914.4531 RMSElog: 10.3802 grad_loss: 17680.1094 normal_loss: 0.9546
[epoch 65][iter  230] loss: 185902.4219 RMSElog: 9.8998 grad_loss: 18579.3711 normal_loss: 0.9705
[epoch 65][iter  240] loss: 193024.5000 RMSElog: 9.9639 grad_loss: 19291.5117 normal_loss: 0.9740
[epoch 65][iter  250] loss: 206083.5312 RMSElog: 10.2501 grad_loss: 20597.1465 normal_loss: 0.9562
[epoch 65][iter  260] loss: 162372.7500 RMSElog: 9.8226 grad_loss: 16226.4922 normal_loss: 0.9603
[epoch 65][iter  270] loss: 125829.2891 RMSElog: 9.7844 grad_loss: 12572.2148 normal_loss: 0.9297
[epoch 65][iter  280] loss: 149036.1562 RMSElog: 9.7728 grad_loss: 14892.8887 normal_loss: 0.9547
[epoch 65][iter  290] loss: 166093.7344 RMSElog: 10.4480 grad_loss: 16597.9766 normal_loss: 0.9489
[epoch 65][iter  300] loss: 114827.1016 RMSElog: 9.7767 grad_loss: 11472.0010 normal_loss: 0.9329
[epoch 65][iter  310] loss: 232982.0469 RMSElog: 10.2845 grad_loss: 23286.9707 normal_loss: 0.9500
[epoch 65][iter  320] loss: 147598.9844 RMSElog: 9.8981 grad_loss: 14749.0645 normal_loss: 0.9355
[epoch 65][iter  330] loss: 147544.5000 RMSElog: 9.8913 grad_loss: 14743.6045 normal_loss: 0.9534
[epoch 65][iter  340] loss: 127776.8359 RMSElog: 10.0947 grad_loss: 12766.6396 normal_loss: 0.9490
[epoch 65][iter  350] loss: 133013.4531 RMSElog: 9.5635 grad_loss: 13290.8457 normal_loss: 0.9356
[epoch 65][iter  360] loss: 164843.6875 RMSElog: 9.7972 grad_loss: 16473.6367 normal_loss: 0.9360
[epoch 65][iter  370] loss: 123289.4609 RMSElog: 9.4106 grad_loss: 12318.5752 normal_loss: 0.9606
[epoch 65][iter  380] loss: 134677.6719 RMSElog: 9.8776 grad_loss: 13456.9443 normal_loss: 0.9441
[epoch 65][iter  390] loss: 160743.4062 RMSElog: 9.7965 grad_loss: 16063.6025 normal_loss: 0.9413
[epoch 65][iter  400] loss: 114509.4531 RMSElog: 9.7409 grad_loss: 11440.2627 normal_loss: 0.9413
[epoch 65][iter  410] loss: 217174.7656 RMSElog: 10.2254 grad_loss: 21706.2891 normal_loss: 0.9635
[epoch 65][iter  420] loss: 137252.6406 RMSElog: 9.3538 grad_loss: 13714.9785 normal_loss: 0.9319
[epoch 65][iter  430] loss: 167745.5625 RMSElog: 9.8817 grad_loss: 16763.7129 normal_loss: 0.9621
[epoch 65][iter  440] loss: 146960.1406 RMSElog: 10.0979 grad_loss: 14684.9941 normal_loss: 0.9220
[epoch 65][iter  450] loss: 157810.1406 RMSElog: 9.9254 grad_loss: 15770.1621 normal_loss: 0.9253
[epoch 65][iter  460] loss: 190938.1250 RMSElog: 10.3028 grad_loss: 19082.5566 normal_loss: 0.9541
[epoch 65][iter  470] loss: 189743.5625 RMSElog: 10.1862 grad_loss: 18963.2207 normal_loss: 0.9490
[epoch 65][iter  480] loss: 160438.2812 RMSElog: 9.9714 grad_loss: 16032.9082 normal_loss: 0.9487
[epoch 65][iter  490] loss: 72289.2656 RMSElog: 9.4677 grad_loss: 7218.5376 normal_loss: 0.9211
[epoch 65][iter  500] loss: 108409.6484 RMSElog: 9.2105 grad_loss: 10830.8506 normal_loss: 0.9033
[epoch 65][iter  510] loss: 175547.9062 RMSElog: 10.1409 grad_loss: 17543.7168 normal_loss: 0.9342
[epoch 65][iter  520] loss: 130145.6250 RMSElog: 9.7206 grad_loss: 13003.9072 normal_loss: 0.9344
[epoch 65][iter  530] loss: 160024.9375 RMSElog: 9.8489 grad_loss: 15991.7236 normal_loss: 0.9215
[epoch 65][iter  540] loss: 175008.6562 RMSElog: 9.9496 grad_loss: 17489.9434 normal_loss: 0.9734
[epoch 65][iter  550] loss: 122331.3906 RMSElog: 9.7180 grad_loss: 12222.4795 normal_loss: 0.9418
[epoch 65][iter  560] loss: 103955.0000 RMSElog: 9.6256 grad_loss: 10384.9521 normal_loss: 0.9222
[epoch 65][iter  570] loss: 158630.9688 RMSElog: 9.8321 grad_loss: 15852.3232 normal_loss: 0.9428
[epoch 65][iter  580] loss: 106182.2188 RMSElog: 9.7575 grad_loss: 10607.5342 normal_loss: 0.9294
[epoch 65][iter  590] loss: 87697.6250 RMSElog: 9.9487 grad_loss: 8758.8750 normal_loss: 0.9395
[epoch 66][iter    0] loss: 147765.9219 RMSElog: 10.0607 grad_loss: 14765.6104 normal_loss: 0.9219
[epoch 66][iter   10] loss: 154310.0938 RMSElog: 9.9148 grad_loss: 15420.1445 normal_loss: 0.9495
[epoch 66][iter   20] loss: 149036.1562 RMSElog: 9.7728 grad_loss: 14892.8887 normal_loss: 0.9547
[epoch 66][iter   30] loss: 105706.7109 RMSElog: 9.7501 grad_loss: 10559.9893 normal_loss: 0.9321
[epoch 66][iter   40] loss: 224077.6719 RMSElog: 10.3869 grad_loss: 22396.4219 normal_loss: 0.9584
[epoch 66][iter   50] loss: 180663.0469 RMSElog: 10.2319 grad_loss: 18055.1094 normal_loss: 0.9630
[epoch 66][iter   60] loss: 102223.4531 RMSElog: 9.4278 grad_loss: 10212.0010 normal_loss: 0.9169
[epoch 66][iter   70] loss: 159422.2500 RMSElog: 9.7010 grad_loss: 15931.5938 normal_loss: 0.9299
[epoch 66][iter   80] loss: 153997.7188 RMSElog: 9.7620 grad_loss: 15389.0713 normal_loss: 0.9383
[epoch 66][iter   90] loss: 229081.2812 RMSElog: 10.2778 grad_loss: 22896.9102 normal_loss: 0.9420
[epoch 66][iter  100] loss: 112697.1016 RMSElog: 9.7295 grad_loss: 11259.0449 normal_loss: 0.9352
[epoch 66][iter  110] loss: 149135.0000 RMSElog: 10.0932 grad_loss: 14902.4570 normal_loss: 0.9500
[epoch 66][iter  120] loss: 115500.4766 RMSElog: 9.9924 grad_loss: 11539.1230 normal_loss: 0.9328
[epoch 66][iter  130] loss: 160743.4062 RMSElog: 9.7965 grad_loss: 16063.6025 normal_loss: 0.9413
[epoch 66][iter  140] loss: 195999.5625 RMSElog: 10.1749 grad_loss: 19588.8047 normal_loss: 0.9756
[epoch 66][iter  150] loss: 103305.2422 RMSElog: 9.5543 grad_loss: 10320.0674 normal_loss: 0.9024
[epoch 66][iter  160] loss: 132012.6719 RMSElog: 9.6802 grad_loss: 13190.6582 normal_loss: 0.9291
[epoch 66][iter  170] loss: 166207.1094 RMSElog: 9.7947 grad_loss: 16609.9668 normal_loss: 0.9492
[epoch 66][iter  180] loss: 115534.3672 RMSElog: 9.7196 grad_loss: 11542.7842 normal_loss: 0.9325
[epoch 66][iter  190] loss: 153447.6875 RMSElog: 9.9051 grad_loss: 15333.9209 normal_loss: 0.9425
[epoch 66][iter  200] loss: 124563.3906 RMSElog: 9.9514 grad_loss: 12445.4473 normal_loss: 0.9400
[epoch 66][iter  210] loss: 177292.4062 RMSElog: 10.1195 grad_loss: 17718.1602 normal_loss: 0.9615
[epoch 66][iter  220] loss: 190938.1094 RMSElog: 10.3028 grad_loss: 19082.5547 normal_loss: 0.9541
[epoch 66][iter  230] loss: 163253.2500 RMSElog: 9.8740 grad_loss: 16314.5010 normal_loss: 0.9499
[epoch 66][iter  240] loss: 140282.6250 RMSElog: 9.9822 grad_loss: 14017.3145 normal_loss: 0.9650
[epoch 66][iter  250] loss: 227080.5938 RMSElog: 10.4127 grad_loss: 22696.6992 normal_loss: 0.9482
[epoch 66][iter  260] loss: 180425.2344 RMSElog: 10.0354 grad_loss: 18031.5332 normal_loss: 0.9552
[epoch 66][iter  270] loss: 228338.7500 RMSElog: 10.2936 grad_loss: 22822.6348 normal_loss: 0.9477
[epoch 66][iter  280] loss: 141865.3906 RMSElog: 9.7199 grad_loss: 14175.8896 normal_loss: 0.9298
[epoch 66][iter  290] loss: 95221.2109 RMSElog: 9.1894 grad_loss: 9512.0205 normal_loss: 0.9115
[epoch 66][iter  300] loss: 197715.0000 RMSElog: 10.4355 grad_loss: 19760.1113 normal_loss: 0.9532
[epoch 66][iter  310] loss: 204037.7500 RMSElog: 10.1754 grad_loss: 20392.6582 normal_loss: 0.9420
[epoch 66][iter  320] loss: 115651.6406 RMSElog: 9.1877 grad_loss: 11555.0664 normal_loss: 0.9106
[epoch 66][iter  330] loss: 159798.6094 RMSElog: 9.8075 grad_loss: 15969.1260 normal_loss: 0.9263
[epoch 66][iter  340] loss: 185495.4062 RMSElog: 9.9310 grad_loss: 18538.6641 normal_loss: 0.9455
[epoch 66][iter  350] loss: 179649.7188 RMSElog: 10.2020 grad_loss: 17953.8125 normal_loss: 0.9593
[epoch 66][iter  360] loss: 182187.7188 RMSElog: 10.0835 grad_loss: 18207.7461 normal_loss: 0.9413
[epoch 66][iter  370] loss: 166069.7812 RMSElog: 9.6813 grad_loss: 16596.3574 normal_loss: 0.9393
[epoch 66][iter  380] loss: 189543.6719 RMSElog: 10.1592 grad_loss: 18943.2598 normal_loss: 0.9474
[epoch 66][iter  390] loss: 195671.7031 RMSElog: 10.1080 grad_loss: 19556.1133 normal_loss: 0.9494
[epoch 66][iter  400] loss: 163695.2656 RMSElog: 9.7037 grad_loss: 16358.8877 normal_loss: 0.9345
[epoch 66][iter  410] loss: 178593.9688 RMSElog: 10.0081 grad_loss: 17848.4395 normal_loss: 0.9490
[epoch 66][iter  420] loss: 104456.8438 RMSElog: 10.0170 grad_loss: 10434.7461 normal_loss: 0.9214
[epoch 66][iter  430] loss: 147915.7812 RMSElog: 10.0131 grad_loss: 14780.6104 normal_loss: 0.9547
[epoch 66][iter  440] loss: 175299.8438 RMSElog: 10.1975 grad_loss: 17518.8301 normal_loss: 0.9576
[epoch 66][iter  450] loss: 158630.9688 RMSElog: 9.8321 grad_loss: 15852.3232 normal_loss: 0.9428
[epoch 66][iter  460] loss: 160788.1406 RMSElog: 9.7539 grad_loss: 16068.1289 normal_loss: 0.9318
[epoch 66][iter  470] loss: 116073.6016 RMSElog: 9.6844 grad_loss: 11596.7334 normal_loss: 0.9429
[epoch 66][iter  480] loss: 135699.4688 RMSElog: 9.6754 grad_loss: 13559.3340 normal_loss: 0.9364
[epoch 66][iter  490] loss: 161285.2188 RMSElog: 9.8339 grad_loss: 16117.7402 normal_loss: 0.9476
[epoch 66][iter  500] loss: 133464.0312 RMSElog: 9.6242 grad_loss: 13335.8584 normal_loss: 0.9201
[epoch 66][iter  510] loss: 223018.5625 RMSElog: 10.3592 grad_loss: 22290.5508 normal_loss: 0.9462
[epoch 66][iter  520] loss: 100980.7812 RMSElog: 9.6528 grad_loss: 10087.5176 normal_loss: 0.9085
[epoch 66][iter  530] loss: 114149.3672 RMSElog: 9.2655 grad_loss: 11404.7637 normal_loss: 0.9073
[epoch 66][iter  540] loss: 105896.9375 RMSElog: 9.7662 grad_loss: 10578.9961 normal_loss: 0.9310
[epoch 66][iter  550] loss: 143904.7812 RMSElog: 10.0263 grad_loss: 14379.5068 normal_loss: 0.9457
[epoch 66][iter  560] loss: 217174.7656 RMSElog: 10.2254 grad_loss: 21706.2891 normal_loss: 0.9635
[epoch 66][iter  570] loss: 136796.3438 RMSElog: 9.6524 grad_loss: 13669.0273 normal_loss: 0.9549
[epoch 66][iter  580] loss: 100749.6875 RMSElog: 9.9914 grad_loss: 10064.0400 normal_loss: 0.9377
[epoch 66][iter  590] loss: 167784.2500 RMSElog: 9.9330 grad_loss: 16767.5391 normal_loss: 0.9539
[epoch 67][iter    0] loss: 121959.7344 RMSElog: 9.7378 grad_loss: 12185.3047 normal_loss: 0.9314
[epoch 67][iter   10] loss: 135114.8750 RMSElog: 9.9265 grad_loss: 13500.6055 normal_loss: 0.9548
[epoch 67][iter   20] loss: 105706.7109 RMSElog: 9.7501 grad_loss: 10559.9893 normal_loss: 0.9321
[epoch 67][iter   30] loss: 151354.1562 RMSElog: 9.8837 grad_loss: 15124.5889 normal_loss: 0.9428
[epoch 67][iter   40] loss: 128979.7266 RMSElog: 9.7049 grad_loss: 12887.3350 normal_loss: 0.9327
[epoch 67][iter   50] loss: 160825.5938 RMSElog: 9.9290 grad_loss: 16071.6816 normal_loss: 0.9491
[epoch 67][iter   60] loss: 148239.1406 RMSElog: 9.8055 grad_loss: 14813.1689 normal_loss: 0.9395
[epoch 67][iter   70] loss: 202630.0781 RMSElog: 10.0417 grad_loss: 20252.0078 normal_loss: 0.9589
[epoch 67][iter   80] loss: 134406.1875 RMSElog: 9.9402 grad_loss: 13429.7246 normal_loss: 0.9539
[epoch 67][iter   90] loss: 146465.6875 RMSElog: 10.2283 grad_loss: 14635.4062 normal_loss: 0.9337
[epoch 67][iter  100] loss: 186960.2969 RMSElog: 10.2468 grad_loss: 18684.8457 normal_loss: 0.9380
[epoch 67][iter  110] loss: 173339.3750 RMSElog: 9.8230 grad_loss: 17323.1934 normal_loss: 0.9212
[epoch 67][iter  120] loss: 110235.3906 RMSElog: 9.7805 grad_loss: 11012.8271 normal_loss: 0.9312
[epoch 67][iter  130] loss: 162433.1875 RMSElog: 9.8183 grad_loss: 16232.5488 normal_loss: 0.9510
[epoch 67][iter  140] loss: 139878.0625 RMSElog: 9.8515 grad_loss: 13977.0225 normal_loss: 0.9327
[epoch 67][iter  150] loss: 103509.2188 RMSElog: 9.9787 grad_loss: 10340.0186 normal_loss: 0.9251
[epoch 67][iter  160] loss: 121866.5156 RMSElog: 9.8422 grad_loss: 12175.8965 normal_loss: 0.9131
[epoch 67][iter  170] loss: 161088.9688 RMSElog: 9.9258 grad_loss: 16098.0254 normal_loss: 0.9455
[epoch 67][iter  180] loss: 221089.3594 RMSElog: 10.3457 grad_loss: 22097.6465 normal_loss: 0.9439
[epoch 67][iter  190] loss: 195999.5625 RMSElog: 10.1749 grad_loss: 19588.8047 normal_loss: 0.9756
[epoch 67][iter  200] loss: 120765.1875 RMSElog: 9.5441 grad_loss: 12066.0400 normal_loss: 0.9348
[epoch 67][iter  210] loss: 105896.9375 RMSElog: 9.7662 grad_loss: 10578.9961 normal_loss: 0.9310
[epoch 67][iter  220] loss: 165750.3750 RMSElog: 9.8971 grad_loss: 16564.1895 normal_loss: 0.9518
[epoch 67][iter  230] loss: 111655.4766 RMSElog: 9.2050 grad_loss: 11155.4395 normal_loss: 0.9029
[epoch 67][iter  240] loss: 185772.2500 RMSElog: 9.9702 grad_loss: 18566.2969 normal_loss: 0.9567
[epoch 67][iter  250] loss: 147479.0312 RMSElog: 9.6781 grad_loss: 14737.2812 normal_loss: 0.9446
[epoch 67][iter  260] loss: 167455.8750 RMSElog: 9.7229 grad_loss: 16734.9395 normal_loss: 0.9256
[epoch 67][iter  270] loss: 193978.6562 RMSElog: 10.3530 grad_loss: 19386.5605 normal_loss: 0.9518
[epoch 67][iter  280] loss: 154705.2188 RMSElog: 9.9142 grad_loss: 15459.6543 normal_loss: 0.9527
[epoch 67][iter  290] loss: 105828.7812 RMSElog: 10.1032 grad_loss: 10571.8545 normal_loss: 0.9195
[epoch 67][iter  300] loss: 211640.2344 RMSElog: 10.2414 grad_loss: 21152.8145 normal_loss: 0.9675
[epoch 67][iter  310] loss: 167211.6250 RMSElog: 10.0285 grad_loss: 16710.1777 normal_loss: 0.9542
[epoch 67][iter  320] loss: 177934.3750 RMSElog: 9.8386 grad_loss: 17782.6445 normal_loss: 0.9542
[epoch 67][iter  330] loss: 115452.5781 RMSElog: 9.2301 grad_loss: 11535.1230 normal_loss: 0.9042
[epoch 67][iter  340] loss: 190972.3281 RMSElog: 10.1152 grad_loss: 19086.1523 normal_loss: 0.9647
[epoch 67][iter  350] loss: 141962.4844 RMSElog: 9.7918 grad_loss: 14185.5273 normal_loss: 0.9289
[epoch 67][iter  360] loss: 231973.6719 RMSElog: 10.2099 grad_loss: 23186.2090 normal_loss: 0.9490
[epoch 67][iter  370] loss: 141228.9219 RMSElog: 9.9926 grad_loss: 14111.9531 normal_loss: 0.9477
[epoch 67][iter  380] loss: 117061.6250 RMSElog: 9.2141 grad_loss: 11696.0439 normal_loss: 0.9046
[epoch 67][iter  390] loss: 180425.2344 RMSElog: 10.0354 grad_loss: 18031.5332 normal_loss: 0.9552
[epoch 67][iter  400] loss: 204650.3281 RMSElog: 10.1934 grad_loss: 20453.8633 normal_loss: 0.9763
[epoch 67][iter  410] loss: 142005.0781 RMSElog: 9.9168 grad_loss: 14189.6533 normal_loss: 0.9371
[epoch 67][iter  420] loss: 218547.0156 RMSElog: 10.4419 grad_loss: 21843.3086 normal_loss: 0.9515
[epoch 67][iter  430] loss: 129205.9375 RMSElog: 9.8851 grad_loss: 12909.7734 normal_loss: 0.9356
[epoch 67][iter  440] loss: 110579.5391 RMSElog: 9.3019 grad_loss: 11047.7109 normal_loss: 0.9413
[epoch 67][iter  450] loss: 216232.0312 RMSElog: 10.2147 grad_loss: 21612.0234 normal_loss: 0.9640
[epoch 67][iter  460] loss: 177814.3125 RMSElog: 10.0854 grad_loss: 17770.3887 normal_loss: 0.9571
[epoch 67][iter  470] loss: 100663.8984 RMSElog: 9.9598 grad_loss: 10055.5020 normal_loss: 0.9277
[epoch 67][iter  480] loss: 161084.5469 RMSElog: 9.9606 grad_loss: 16097.5430 normal_loss: 0.9512
[epoch 67][iter  490] loss: 188662.5156 RMSElog: 10.2592 grad_loss: 18855.0352 normal_loss: 0.9574
[epoch 67][iter  500] loss: 150323.7656 RMSElog: 10.0510 grad_loss: 15021.3848 normal_loss: 0.9410
[epoch 67][iter  510] loss: 109441.5234 RMSElog: 9.7466 grad_loss: 10933.4756 normal_loss: 0.9300
[epoch 67][iter  520] loss: 117693.8984 RMSElog: 9.6527 grad_loss: 11758.8105 normal_loss: 0.9267
[epoch 67][iter  530] loss: 232771.8594 RMSElog: 10.2938 grad_loss: 23265.9453 normal_loss: 0.9464
[epoch 67][iter  540] loss: 186049.0312 RMSElog: 10.0165 grad_loss: 18593.9277 normal_loss: 0.9585
[epoch 67][iter  550] loss: 104456.8438 RMSElog: 10.0170 grad_loss: 10434.7461 normal_loss: 0.9214
[epoch 67][iter  560] loss: 162892.2969 RMSElog: 9.8252 grad_loss: 16278.4717 normal_loss: 0.9331
[epoch 67][iter  570] loss: 181465.8125 RMSElog: 10.1136 grad_loss: 18135.5117 normal_loss: 0.9572
[epoch 67][iter  580] loss: 239768.0938 RMSElog: 10.2366 grad_loss: 23965.6250 normal_loss: 0.9466
[epoch 67][iter  590] loss: 212239.7031 RMSElog: 10.2690 grad_loss: 21212.7383 normal_loss: 0.9638
[epoch 68][iter    0] loss: 133024.8594 RMSElog: 9.6937 grad_loss: 13291.8662 normal_loss: 0.9261
[epoch 68][iter   10] loss: 109148.7344 RMSElog: 10.1172 grad_loss: 10903.8262 normal_loss: 0.9294
[epoch 68][iter   20] loss: 208184.3125 RMSElog: 10.4061 grad_loss: 20807.0781 normal_loss: 0.9472
[epoch 68][iter   30] loss: 139154.8594 RMSElog: 9.5986 grad_loss: 13904.9707 normal_loss: 0.9164
[epoch 68][iter   40] loss: 211925.8438 RMSElog: 10.1863 grad_loss: 21181.4551 normal_loss: 0.9436
[epoch 68][iter   50] loss: 185902.4219 RMSElog: 9.8998 grad_loss: 18579.3711 normal_loss: 0.9705
[epoch 68][iter   60] loss: 166542.4219 RMSElog: 9.8913 grad_loss: 16643.4043 normal_loss: 0.9482
[epoch 68][iter   70] loss: 115738.1953 RMSElog: 9.6968 grad_loss: 11563.1836 normal_loss: 0.9382
[epoch 68][iter   80] loss: 129166.5312 RMSElog: 9.7364 grad_loss: 12905.9775 normal_loss: 0.9391
[epoch 68][iter   90] loss: 103355.8203 RMSElog: 9.7106 grad_loss: 10324.9463 normal_loss: 0.9244
[epoch 68][iter  100] loss: 158924.9062 RMSElog: 9.8027 grad_loss: 15881.7363 normal_loss: 0.9524
[epoch 68][iter  110] loss: 196437.8125 RMSElog: 10.2319 grad_loss: 19632.5898 normal_loss: 0.9584
[epoch 68][iter  120] loss: 100749.6875 RMSElog: 9.9914 grad_loss: 10064.0400 normal_loss: 0.9377
[epoch 68][iter  130] loss: 148793.9688 RMSElog: 10.1047 grad_loss: 14868.3447 normal_loss: 0.9480
[epoch 68][iter  140] loss: 186053.2969 RMSElog: 10.1470 grad_loss: 18594.2324 normal_loss: 0.9504
[epoch 68][iter  150] loss: 221089.3594 RMSElog: 10.3457 grad_loss: 22097.6465 normal_loss: 0.9439
[epoch 68][iter  160] loss: 149128.2188 RMSElog: 9.7262 grad_loss: 14902.1533 normal_loss: 0.9419
[epoch 68][iter  170] loss: 161646.4688 RMSElog: 10.1105 grad_loss: 16153.5762 normal_loss: 0.9608
[epoch 68][iter  180] loss: 213401.8906 RMSElog: 10.0568 grad_loss: 21329.1582 normal_loss: 0.9749
[epoch 68][iter  190] loss: 185495.4062 RMSElog: 9.9310 grad_loss: 18538.6641 normal_loss: 0.9455
[epoch 68][iter  200] loss: 144657.9844 RMSElog: 9.9197 grad_loss: 14454.9258 normal_loss: 0.9518
[epoch 68][iter  210] loss: 167794.0469 RMSElog: 9.8335 grad_loss: 16768.6211 normal_loss: 0.9500
[epoch 68][iter  220] loss: 128488.4375 RMSElog: 9.7402 grad_loss: 12838.1719 normal_loss: 0.9319
[epoch 68][iter  230] loss: 144987.0938 RMSElog: 9.8653 grad_loss: 14487.9062 normal_loss: 0.9378
[epoch 68][iter  240] loss: 203093.2969 RMSElog: 10.1238 grad_loss: 20298.2598 normal_loss: 0.9478
[epoch 68][iter  250] loss: 140282.6250 RMSElog: 9.9822 grad_loss: 14017.3145 normal_loss: 0.9650
[epoch 68][iter  260] loss: 136507.7344 RMSElog: 10.0223 grad_loss: 13639.7900 normal_loss: 0.9608
[epoch 68][iter  270] loss: 241971.8750 RMSElog: 10.2390 grad_loss: 24186.0000 normal_loss: 0.9488
[epoch 68][iter  280] loss: 150153.5000 RMSElog: 10.0554 grad_loss: 15004.3457 normal_loss: 0.9478
[epoch 68][iter  290] loss: 154626.7656 RMSElog: 9.9098 grad_loss: 15451.8066 normal_loss: 0.9596
[epoch 68][iter  300] loss: 174629.2812 RMSElog: 9.9405 grad_loss: 17452.0625 normal_loss: 0.9229
[epoch 68][iter  310] loss: 146576.9219 RMSElog: 9.6520 grad_loss: 14647.1045 normal_loss: 0.9356
[epoch 68][iter  320] loss: 103305.2500 RMSElog: 9.5543 grad_loss: 10320.0684 normal_loss: 0.9024
[epoch 68][iter  330] loss: 129205.9375 RMSElog: 9.8851 grad_loss: 12909.7734 normal_loss: 0.9356
[epoch 68][iter  340] loss: 154705.2344 RMSElog: 9.9142 grad_loss: 15459.6562 normal_loss: 0.9527
[epoch 68][iter  350] loss: 141962.4844 RMSElog: 9.7918 grad_loss: 14185.5273 normal_loss: 0.9289
[epoch 68][iter  360] loss: 165326.5625 RMSElog: 9.8832 grad_loss: 16521.8086 normal_loss: 0.9647
[epoch 68][iter  370] loss: 142138.5156 RMSElog: 9.7967 grad_loss: 14203.1260 normal_loss: 0.9291
[epoch 68][iter  380] loss: 228338.7500 RMSElog: 10.2936 grad_loss: 22822.6348 normal_loss: 0.9477
[epoch 68][iter  390] loss: 139061.3750 RMSElog: 9.6201 grad_loss: 13895.5771 normal_loss: 0.9401
[epoch 68][iter  400] loss: 115500.4766 RMSElog: 9.9924 grad_loss: 11539.1230 normal_loss: 0.9328
[epoch 68][iter  410] loss: 110235.3906 RMSElog: 9.7805 grad_loss: 11012.8271 normal_loss: 0.9312
[epoch 68][iter  420] loss: 154290.2500 RMSElog: 9.8471 grad_loss: 15418.2334 normal_loss: 0.9458
[epoch 68][iter  430] loss: 137296.6406 RMSElog: 9.9750 grad_loss: 13718.7402 normal_loss: 0.9495
[epoch 68][iter  440] loss: 182005.7031 RMSElog: 10.0998 grad_loss: 18189.5195 normal_loss: 0.9507
[epoch 68][iter  450] loss: 100116.4922 RMSElog: 9.9235 grad_loss: 10000.7998 normal_loss: 0.9258
[epoch 68][iter  460] loss: 103514.7734 RMSElog: 9.3451 grad_loss: 10341.2100 normal_loss: 0.9229
[epoch 68][iter  470] loss: 132012.6719 RMSElog: 9.6802 grad_loss: 13190.6582 normal_loss: 0.9291
[epoch 68][iter  480] loss: 223018.5625 RMSElog: 10.3592 grad_loss: 22290.5508 normal_loss: 0.9462
[epoch 68][iter  490] loss: 206083.5156 RMSElog: 10.2501 grad_loss: 20597.1445 normal_loss: 0.9562
[epoch 68][iter  500] loss: 190938.1250 RMSElog: 10.3028 grad_loss: 19082.5566 normal_loss: 0.9541
[epoch 68][iter  510] loss: 196277.7188 RMSElog: 10.2453 grad_loss: 19616.5664 normal_loss: 0.9595
[epoch 68][iter  520] loss: 193978.6250 RMSElog: 10.3530 grad_loss: 19386.5586 normal_loss: 0.9518
[epoch 68][iter  530] loss: 197714.9844 RMSElog: 10.4355 grad_loss: 19760.1094 normal_loss: 0.9532
[epoch 68][iter  540] loss: 149387.5625 RMSElog: 9.9550 grad_loss: 14927.8525 normal_loss: 0.9483
[epoch 68][iter  550] loss: 127463.1875 RMSElog: 9.4952 grad_loss: 12735.9082 normal_loss: 0.9153
[epoch 68][iter  560] loss: 176217.8125 RMSElog: 10.1087 grad_loss: 17610.7246 normal_loss: 0.9466
[epoch 68][iter  570] loss: 175122.9531 RMSElog: 9.9588 grad_loss: 17501.3789 normal_loss: 0.9573
[epoch 68][iter  580] loss: 149540.3750 RMSElog: 9.7408 grad_loss: 14943.3740 normal_loss: 0.9229
[epoch 68][iter  590] loss: 152593.5000 RMSElog: 9.8773 grad_loss: 15248.5439 normal_loss: 0.9283
[epoch 69][iter    0] loss: 105877.8750 RMSElog: 9.7638 grad_loss: 10577.0986 normal_loss: 0.9249
[epoch 69][iter   10] loss: 181983.5312 RMSElog: 10.0741 grad_loss: 18187.3320 normal_loss: 0.9464
[epoch 69][iter   20] loss: 173339.3594 RMSElog: 9.8230 grad_loss: 17323.1914 normal_loss: 0.9212
[epoch 69][iter   30] loss: 179298.9062 RMSElog: 9.7220 grad_loss: 17919.2227 normal_loss: 0.9453
[epoch 69][iter   40] loss: 130527.6172 RMSElog: 9.8914 grad_loss: 13041.9189 normal_loss: 0.9509
[epoch 69][iter   50] loss: 125486.8672 RMSElog: 9.7758 grad_loss: 12537.9834 normal_loss: 0.9281
[epoch 69][iter   60] loss: 167008.4531 RMSElog: 10.0165 grad_loss: 16689.8750 normal_loss: 0.9557
[epoch 69][iter   70] loss: 176217.8125 RMSElog: 10.1087 grad_loss: 17610.7246 normal_loss: 0.9466
[epoch 69][iter   80] loss: 160788.1406 RMSElog: 9.7539 grad_loss: 16068.1289 normal_loss: 0.9318
[epoch 69][iter   90] loss: 152149.7500 RMSElog: 9.9062 grad_loss: 15204.1230 normal_loss: 0.9462
[epoch 69][iter  100] loss: 191371.4375 RMSElog: 10.1918 grad_loss: 19126.0020 normal_loss: 0.9504
[epoch 69][iter  110] loss: 161088.9688 RMSElog: 9.9258 grad_loss: 16098.0254 normal_loss: 0.9455
[epoch 69][iter  120] loss: 173574.0781 RMSElog: 10.1780 grad_loss: 17346.2734 normal_loss: 0.9575
[epoch 69][iter  130] loss: 174761.2344 RMSElog: 9.7333 grad_loss: 17465.4512 normal_loss: 0.9393
[epoch 69][iter  140] loss: 105742.0391 RMSElog: 9.7582 grad_loss: 10563.5234 normal_loss: 0.9230
[epoch 69][iter  150] loss: 152332.5469 RMSElog: 9.9813 grad_loss: 15222.3486 normal_loss: 0.9245
[epoch 69][iter  160] loss: 218949.8281 RMSElog: 10.4462 grad_loss: 21883.5918 normal_loss: 0.9447
[epoch 69][iter  170] loss: 130739.4062 RMSElog: 9.5471 grad_loss: 13063.4434 normal_loss: 0.9498
[epoch 69][iter  180] loss: 143460.8594 RMSElog: 9.7520 grad_loss: 14335.3848 normal_loss: 0.9489
[epoch 69][iter  190] loss: 152549.8750 RMSElog: 9.9928 grad_loss: 15244.0293 normal_loss: 0.9654
[epoch 69][iter  200] loss: 160743.4062 RMSElog: 9.7965 grad_loss: 16063.6025 normal_loss: 0.9413
[epoch 69][iter  210] loss: 155762.3438 RMSElog: 9.8487 grad_loss: 15565.4404 normal_loss: 0.9448
[epoch 69][iter  220] loss: 121983.8203 RMSElog: 9.5225 grad_loss: 12187.9473 normal_loss: 0.9124
[epoch 69][iter  230] loss: 200470.3906 RMSElog: 10.2414 grad_loss: 20035.8379 normal_loss: 0.9585
[epoch 69][iter  240] loss: 231973.6719 RMSElog: 10.2099 grad_loss: 23186.2090 normal_loss: 0.9490
[epoch 69][iter  250] loss: 119923.2031 RMSElog: 9.5699 grad_loss: 11981.8154 normal_loss: 0.9342
[epoch 69][iter  260] loss: 196677.7812 RMSElog: 10.0486 grad_loss: 19656.7715 normal_loss: 0.9571
[epoch 69][iter  270] loss: 99107.7500 RMSElog: 9.5230 grad_loss: 9900.3428 normal_loss: 0.9094
[epoch 69][iter  280] loss: 115495.2188 RMSElog: 9.6084 grad_loss: 11538.9688 normal_loss: 0.9442
[epoch 69][iter  290] loss: 119429.4609 RMSElog: 9.6388 grad_loss: 11932.3740 normal_loss: 0.9339
[epoch 69][iter  300] loss: 102096.6484 RMSElog: 9.8437 grad_loss: 10198.8926 normal_loss: 0.9290
[epoch 69][iter  310] loss: 163272.7031 RMSElog: 10.0261 grad_loss: 16316.3037 normal_loss: 0.9400
[epoch 69][iter  320] loss: 218757.0312 RMSElog: 10.1228 grad_loss: 21864.6270 normal_loss: 0.9524
[epoch 69][iter  330] loss: 130145.6250 RMSElog: 9.7206 grad_loss: 13003.9072 normal_loss: 0.9344
[epoch 69][iter  340] loss: 147915.7812 RMSElog: 10.0131 grad_loss: 14780.6104 normal_loss: 0.9547
[epoch 69][iter  350] loss: 114310.3047 RMSElog: 9.4026 grad_loss: 11420.7217 normal_loss: 0.9063
[epoch 69][iter  360] loss: 100650.2734 RMSElog: 9.7441 grad_loss: 10054.3555 normal_loss: 0.9279
[epoch 69][iter  370] loss: 100980.7812 RMSElog: 9.6528 grad_loss: 10087.5176 normal_loss: 0.9085
[epoch 69][iter  380] loss: 175301.7344 RMSElog: 9.8017 grad_loss: 17519.4180 normal_loss: 0.9555
[epoch 69][iter  390] loss: 153099.1094 RMSElog: 9.8355 grad_loss: 15299.1445 normal_loss: 0.9304
[epoch 69][iter  400] loss: 196920.5938 RMSElog: 10.2474 grad_loss: 19680.8340 normal_loss: 0.9760
[epoch 69][iter  410] loss: 122386.0234 RMSElog: 9.6688 grad_loss: 12227.9971 normal_loss: 0.9366
[epoch 69][iter  420] loss: 147207.9844 RMSElog: 9.9779 grad_loss: 14709.8789 normal_loss: 0.9413
[epoch 69][iter  430] loss: 114924.9844 RMSElog: 9.6925 grad_loss: 11481.8691 normal_loss: 0.9360
[epoch 69][iter  440] loss: 176223.4531 RMSElog: 9.9872 grad_loss: 17611.4141 normal_loss: 0.9446
[epoch 69][iter  450] loss: 201162.4062 RMSElog: 9.9531 grad_loss: 20105.3164 normal_loss: 0.9706
[epoch 69][iter  460] loss: 114827.1016 RMSElog: 9.7767 grad_loss: 11472.0010 normal_loss: 0.9329
[epoch 69][iter  470] loss: 128979.7188 RMSElog: 9.7049 grad_loss: 12887.3340 normal_loss: 0.9327
[epoch 69][iter  480] loss: 120745.8203 RMSElog: 9.7706 grad_loss: 12063.8691 normal_loss: 0.9428
[epoch 69][iter  490] loss: 224077.6719 RMSElog: 10.3869 grad_loss: 22396.4219 normal_loss: 0.9584
[epoch 69][iter  500] loss: 196842.3281 RMSElog: 10.1249 grad_loss: 19673.1543 normal_loss: 0.9534
[epoch 69][iter  510] loss: 122331.3906 RMSElog: 9.7180 grad_loss: 12222.4795 normal_loss: 0.9418
[epoch 69][iter  520] loss: 218457.8281 RMSElog: 10.1255 grad_loss: 21834.7168 normal_loss: 0.9418
[epoch 69][iter  530] loss: 103355.8203 RMSElog: 9.7106 grad_loss: 10324.9463 normal_loss: 0.9244
[epoch 69][iter  540] loss: 224257.9844 RMSElog: 10.2842 grad_loss: 22414.5625 normal_loss: 0.9521
[epoch 69][iter  550] loss: 125695.5391 RMSElog: 9.9422 grad_loss: 12558.6836 normal_loss: 0.9278
[epoch 69][iter  560] loss: 162711.9688 RMSElog: 9.6160 grad_loss: 16260.6387 normal_loss: 0.9427
[epoch 69][iter  570] loss: 228053.0312 RMSElog: 10.2288 grad_loss: 22794.1328 normal_loss: 0.9422
[epoch 69][iter  580] loss: 160825.5938 RMSElog: 9.9290 grad_loss: 16071.6816 normal_loss: 0.9491
[epoch 69][iter  590] loss: 208821.7500 RMSElog: 10.1565 grad_loss: 20871.0762 normal_loss: 0.9431
[epoch 70][iter    0] loss: 182315.9688 RMSElog: 10.1291 grad_loss: 18220.5039 normal_loss: 0.9652
[epoch 70][iter   10] loss: 113426.9688 RMSElog: 9.7682 grad_loss: 11331.9922 normal_loss: 0.9362
[epoch 70][iter   20] loss: 214493.2031 RMSElog: 10.3525 grad_loss: 21438.0059 normal_loss: 0.9611
[epoch 70][iter   30] loss: 117693.8984 RMSElog: 9.6527 grad_loss: 11758.8105 normal_loss: 0.9267
[epoch 70][iter   40] loss: 215542.7500 RMSElog: 10.4358 grad_loss: 21542.8887 normal_loss: 0.9514
[epoch 70][iter   50] loss: 149369.9062 RMSElog: 10.0930 grad_loss: 14925.9756 normal_loss: 0.9225
[epoch 70][iter   60] loss: 163389.5469 RMSElog: 9.7702 grad_loss: 16328.2637 normal_loss: 0.9208
[epoch 70][iter   70] loss: 149128.2188 RMSElog: 9.7262 grad_loss: 14902.1533 normal_loss: 0.9419
[epoch 70][iter   80] loss: 116582.6172 RMSElog: 9.7871 grad_loss: 11647.5430 normal_loss: 0.9314
[epoch 70][iter   90] loss: 183095.1562 RMSElog: 9.9762 grad_loss: 18298.5703 normal_loss: 0.9696
[epoch 70][iter  100] loss: 150296.1562 RMSElog: 9.8188 grad_loss: 15018.8564 normal_loss: 0.9407
[epoch 70][iter  110] loss: 143460.8594 RMSElog: 9.7520 grad_loss: 14335.3848 normal_loss: 0.9489
[epoch 70][iter  120] loss: 206764.1875 RMSElog: 10.2849 grad_loss: 20665.1777 normal_loss: 0.9549
[epoch 70][iter  130] loss: 147544.5000 RMSElog: 9.8913 grad_loss: 14743.6045 normal_loss: 0.9534
[epoch 70][iter  140] loss: 156985.6250 RMSElog: 9.5765 grad_loss: 15688.0479 normal_loss: 0.9385
[epoch 70][iter  150] loss: 142696.6875 RMSElog: 9.9612 grad_loss: 14258.7588 normal_loss: 0.9493
[epoch 70][iter  160] loss: 213417.0312 RMSElog: 10.3416 grad_loss: 21330.4082 normal_loss: 0.9536
[epoch 70][iter  170] loss: 160074.2031 RMSElog: 9.7506 grad_loss: 15996.7432 normal_loss: 0.9257
[epoch 70][iter  180] loss: 131213.2969 RMSElog: 9.6860 grad_loss: 13110.7217 normal_loss: 0.9217
[epoch 70][iter  190] loss: 156253.7812 RMSElog: 9.8231 grad_loss: 15614.6230 normal_loss: 0.9316
[epoch 70][iter  200] loss: 165750.3750 RMSElog: 9.8971 grad_loss: 16564.1895 normal_loss: 0.9518
[epoch 70][iter  210] loss: 166010.9531 RMSElog: 9.9249 grad_loss: 16590.2227 normal_loss: 0.9481
[epoch 70][iter  220] loss: 155762.3594 RMSElog: 9.8487 grad_loss: 15565.4414 normal_loss: 0.9448
[epoch 70][iter  230] loss: 130739.4062 RMSElog: 9.5471 grad_loss: 13063.4434 normal_loss: 0.9498
[epoch 70][iter  240] loss: 174565.0781 RMSElog: 9.7759 grad_loss: 17445.7832 normal_loss: 0.9484
[epoch 70][iter  250] loss: 105628.3594 RMSElog: 10.0899 grad_loss: 10551.8252 normal_loss: 0.9213
[epoch 70][iter  260] loss: 157274.4688 RMSElog: 9.9277 grad_loss: 15716.5723 normal_loss: 0.9461
[epoch 70][iter  270] loss: 184374.1875 RMSElog: 10.2641 grad_loss: 18426.1953 normal_loss: 0.9596
[epoch 70][iter  280] loss: 175486.0156 RMSElog: 10.0393 grad_loss: 17537.6055 normal_loss: 0.9563
[epoch 70][iter  290] loss: 191320.1250 RMSElog: 10.0978 grad_loss: 19120.9512 normal_loss: 0.9627
[epoch 70][iter  300] loss: 154626.7656 RMSElog: 9.9098 grad_loss: 15451.8066 normal_loss: 0.9596
[epoch 70][iter  310] loss: 151401.7188 RMSElog: 9.4414 grad_loss: 15129.7939 normal_loss: 0.9361
[epoch 70][iter  320] loss: 142138.5312 RMSElog: 9.7967 grad_loss: 14203.1270 normal_loss: 0.9291
[epoch 70][iter  330] loss: 134471.5938 RMSElog: 9.9200 grad_loss: 13436.3281 normal_loss: 0.9111
[epoch 70][iter  340] loss: 217174.7656 RMSElog: 10.2254 grad_loss: 21706.2891 normal_loss: 0.9635
[epoch 70][iter  350] loss: 139878.0781 RMSElog: 9.8515 grad_loss: 13977.0234 normal_loss: 0.9327
[epoch 70][iter  360] loss: 167084.9219 RMSElog: 9.8660 grad_loss: 16697.6582 normal_loss: 0.9689
[epoch 70][iter  370] loss: 127061.7500 RMSElog: 9.6726 grad_loss: 12695.5654 normal_loss: 0.9369
[epoch 70][iter  380] loss: 142544.5156 RMSElog: 10.1645 grad_loss: 14243.3457 normal_loss: 0.9425
[epoch 70][iter  390] loss: 227080.5938 RMSElog: 10.4127 grad_loss: 22696.6992 normal_loss: 0.9482
[epoch 70][iter  400] loss: 99107.7500 RMSElog: 9.5230 grad_loss: 9900.3428 normal_loss: 0.9094
[epoch 70][iter  410] loss: 135114.8750 RMSElog: 9.9265 grad_loss: 13500.6055 normal_loss: 0.9548
[epoch 70][iter  420] loss: 195440.9062 RMSElog: 9.7403 grad_loss: 19533.3926 normal_loss: 0.9572
[epoch 70][iter  430] loss: 153997.7188 RMSElog: 9.7620 grad_loss: 15389.0713 normal_loss: 0.9383
[epoch 70][iter  440] loss: 152517.4844 RMSElog: 9.8233 grad_loss: 15241.0010 normal_loss: 0.9249
[epoch 70][iter  450] loss: 115651.6406 RMSElog: 9.1877 grad_loss: 11555.0664 normal_loss: 0.9106
[epoch 70][iter  460] loss: 152593.5000 RMSElog: 9.8773 grad_loss: 15248.5449 normal_loss: 0.9283
[epoch 70][iter  470] loss: 207919.9844 RMSElog: 10.2783 grad_loss: 20780.7637 normal_loss: 0.9578
[epoch 70][iter  480] loss: 141118.5312 RMSElog: 10.0520 grad_loss: 14100.8545 normal_loss: 0.9477
[epoch 70][iter  490] loss: 139061.3750 RMSElog: 9.6201 grad_loss: 13895.5771 normal_loss: 0.9401
[epoch 70][iter  500] loss: 115534.3750 RMSElog: 9.7196 grad_loss: 11542.7852 normal_loss: 0.9325
[epoch 70][iter  510] loss: 177936.2812 RMSElog: 10.0381 grad_loss: 17782.6348 normal_loss: 0.9545
[epoch 70][iter  520] loss: 176260.7500 RMSElog: 10.0638 grad_loss: 17615.0703 normal_loss: 0.9395
[epoch 70][iter  530] loss: 144987.0938 RMSElog: 9.8653 grad_loss: 14487.9062 normal_loss: 0.9378
[epoch 70][iter  540] loss: 156982.7188 RMSElog: 10.0093 grad_loss: 15687.3193 normal_loss: 0.9438
[epoch 70][iter  550] loss: 185495.4062 RMSElog: 9.9310 grad_loss: 18538.6641 normal_loss: 0.9455
[epoch 70][iter  560] loss: 177063.6250 RMSElog: 10.1532 grad_loss: 17695.2383 normal_loss: 0.9719
[epoch 70][iter  570] loss: 128979.7266 RMSElog: 9.7049 grad_loss: 12887.3350 normal_loss: 0.9327
[epoch 70][iter  580] loss: 107073.5156 RMSElog: 9.7289 grad_loss: 10696.6914 normal_loss: 0.9313
[epoch 70][iter  590] loss: 241971.8750 RMSElog: 10.2390 grad_loss: 24186.0000 normal_loss: 0.9488
[epoch 71][iter    0] loss: 127463.1875 RMSElog: 9.4952 grad_loss: 12735.9082 normal_loss: 0.9153
[epoch 71][iter   10] loss: 160782.5156 RMSElog: 9.7710 grad_loss: 16067.5547 normal_loss: 0.9244
[epoch 71][iter   20] loss: 165326.5625 RMSElog: 9.8832 grad_loss: 16521.8086 normal_loss: 0.9647
[epoch 71][iter   30] loss: 130739.4062 RMSElog: 9.5471 grad_loss: 13063.4434 normal_loss: 0.9498
[epoch 71][iter   40] loss: 164767.1875 RMSElog: 9.8398 grad_loss: 16465.9102 normal_loss: 0.9691
[epoch 71][iter   50] loss: 150153.5000 RMSElog: 10.0554 grad_loss: 15004.3457 normal_loss: 0.9478
[epoch 71][iter   60] loss: 105896.9375 RMSElog: 9.7662 grad_loss: 10578.9961 normal_loss: 0.9310
[epoch 71][iter   70] loss: 127776.8359 RMSElog: 10.0947 grad_loss: 12766.6396 normal_loss: 0.9490
[epoch 71][iter   80] loss: 177664.5312 RMSElog: 9.7808 grad_loss: 17755.7344 normal_loss: 0.9369
[epoch 71][iter   90] loss: 147371.3906 RMSElog: 9.6635 grad_loss: 14726.5488 normal_loss: 0.9280
[epoch 71][iter  100] loss: 157119.7500 RMSElog: 9.8959 grad_loss: 15701.1318 normal_loss: 0.9480
[epoch 71][iter  110] loss: 190034.7188 RMSElog: 9.9619 grad_loss: 18992.5625 normal_loss: 0.9485
[epoch 71][iter  120] loss: 176217.8125 RMSElog: 10.1087 grad_loss: 17610.7246 normal_loss: 0.9466
[epoch 71][iter  130] loss: 122331.3906 RMSElog: 9.7180 grad_loss: 12222.4795 normal_loss: 0.9418
[epoch 71][iter  140] loss: 218457.8281 RMSElog: 10.1255 grad_loss: 21834.7168 normal_loss: 0.9418
[epoch 71][iter  150] loss: 106516.8047 RMSElog: 9.6966 grad_loss: 10641.0557 normal_loss: 0.9285
[epoch 71][iter  160] loss: 136533.4688 RMSElog: 9.6756 grad_loss: 13642.7363 normal_loss: 0.9359
[epoch 71][iter  170] loss: 162363.8906 RMSElog: 9.7464 grad_loss: 16225.6777 normal_loss: 0.9661
[epoch 71][iter  180] loss: 122386.0156 RMSElog: 9.6688 grad_loss: 12227.9961 normal_loss: 0.9366
[epoch 71][iter  190] loss: 154855.9062 RMSElog: 9.8054 grad_loss: 15474.8340 normal_loss: 0.9510
[epoch 71][iter  200] loss: 106952.9219 RMSElog: 10.0294 grad_loss: 10684.3398 normal_loss: 0.9231
[epoch 71][iter  210] loss: 140645.6875 RMSElog: 10.1118 grad_loss: 14053.4873 normal_loss: 0.9695
[epoch 71][iter  220] loss: 208184.3125 RMSElog: 10.4061 grad_loss: 20807.0781 normal_loss: 0.9472
[epoch 71][iter  230] loss: 173574.0781 RMSElog: 10.1780 grad_loss: 17346.2734 normal_loss: 0.9575
[epoch 71][iter  240] loss: 182476.7344 RMSElog: 9.9672 grad_loss: 18236.7656 normal_loss: 0.9407
[epoch 71][iter  250] loss: 138057.5156 RMSElog: 9.5188 grad_loss: 13795.3184 normal_loss: 0.9138
[epoch 71][iter  260] loss: 160024.9375 RMSElog: 9.8489 grad_loss: 15991.7236 normal_loss: 0.9215
[epoch 71][iter  270] loss: 100749.6875 RMSElog: 9.9914 grad_loss: 10064.0400 normal_loss: 0.9377
[epoch 71][iter  280] loss: 208770.3906 RMSElog: 10.1964 grad_loss: 20865.8984 normal_loss: 0.9442
[epoch 71][iter  290] loss: 154290.2500 RMSElog: 9.8471 grad_loss: 15418.2334 normal_loss: 0.9458
[epoch 71][iter  300] loss: 224077.6719 RMSElog: 10.3869 grad_loss: 22396.4219 normal_loss: 0.9584
[epoch 71][iter  310] loss: 194578.5781 RMSElog: 10.2312 grad_loss: 19446.6680 normal_loss: 0.9596
[epoch 71][iter  320] loss: 132935.3906 RMSElog: 9.9640 grad_loss: 13282.6172 normal_loss: 0.9581
[epoch 71][iter  330] loss: 161285.2188 RMSElog: 9.8339 grad_loss: 16117.7402 normal_loss: 0.9476
[epoch 71][iter  340] loss: 162976.7812 RMSElog: 9.6016 grad_loss: 16287.1572 normal_loss: 0.9201
[epoch 71][iter  350] loss: 174629.2812 RMSElog: 9.9405 grad_loss: 17452.0625 normal_loss: 0.9229
[epoch 71][iter  360] loss: 154626.7656 RMSElog: 9.9098 grad_loss: 15451.8066 normal_loss: 0.9596
[epoch 71][iter  370] loss: 216232.0312 RMSElog: 10.2147 grad_loss: 21612.0234 normal_loss: 0.9640
[epoch 71][iter  380] loss: 107956.6094 RMSElog: 10.0900 grad_loss: 10784.6504 normal_loss: 0.9209
[epoch 71][iter  390] loss: 122359.4531 RMSElog: 9.7683 grad_loss: 12225.2402 normal_loss: 0.9361
[epoch 71][iter  400] loss: 228053.0312 RMSElog: 10.2288 grad_loss: 22794.1328 normal_loss: 0.9422
[epoch 71][iter  410] loss: 142138.5156 RMSElog: 9.7967 grad_loss: 14203.1260 normal_loss: 0.9291
[epoch 71][iter  420] loss: 167730.9688 RMSElog: 9.9663 grad_loss: 16762.1816 normal_loss: 0.9501
[epoch 71][iter  430] loss: 141442.1250 RMSElog: 9.6437 grad_loss: 14133.6191 normal_loss: 0.9495
[epoch 71][iter  440] loss: 166069.7812 RMSElog: 9.6813 grad_loss: 16596.3574 normal_loss: 0.9393
[epoch 71][iter  450] loss: 150699.3750 RMSElog: 9.8567 grad_loss: 15059.1328 normal_loss: 0.9486
[epoch 71][iter  460] loss: 153594.7031 RMSElog: 9.8426 grad_loss: 15348.6826 normal_loss: 0.9453
[epoch 71][iter  470] loss: 155762.3594 RMSElog: 9.8487 grad_loss: 15565.4414 normal_loss: 0.9448
[epoch 71][iter  480] loss: 132425.4375 RMSElog: 10.0155 grad_loss: 13231.5801 normal_loss: 0.9468
[epoch 71][iter  490] loss: 175524.1250 RMSElog: 10.2056 grad_loss: 17541.2441 normal_loss: 0.9625
[epoch 71][iter  500] loss: 234945.1719 RMSElog: 10.0015 grad_loss: 23483.5664 normal_loss: 0.9500
[epoch 71][iter  510] loss: 165453.0938 RMSElog: 9.9262 grad_loss: 16534.4160 normal_loss: 0.9660
[epoch 71][iter  520] loss: 150459.7656 RMSElog: 9.9429 grad_loss: 15035.0986 normal_loss: 0.9347
[epoch 71][iter  530] loss: 212429.2969 RMSElog: 10.2794 grad_loss: 21231.6719 normal_loss: 0.9784
[epoch 71][iter  540] loss: 167211.6250 RMSElog: 10.0285 grad_loss: 16710.1777 normal_loss: 0.9542
[epoch 71][iter  550] loss: 133024.8594 RMSElog: 9.6937 grad_loss: 13291.8662 normal_loss: 0.9261
[epoch 71][iter  560] loss: 226949.5156 RMSElog: 10.2029 grad_loss: 22683.8047 normal_loss: 0.9430
[epoch 71][iter  570] loss: 185071.7031 RMSElog: 10.0456 grad_loss: 18496.1699 normal_loss: 0.9557
[epoch 71][iter  580] loss: 102223.4531 RMSElog: 9.4278 grad_loss: 10212.0010 normal_loss: 0.9169
[epoch 71][iter  590] loss: 179901.4844 RMSElog: 9.8267 grad_loss: 17979.3633 normal_loss: 0.9589
[epoch 72][iter    0] loss: 176914.4531 RMSElog: 10.3802 grad_loss: 17680.1094 normal_loss: 0.9546
[epoch 72][iter   10] loss: 201503.0625 RMSElog: 10.3149 grad_loss: 20139.0410 normal_loss: 0.9514
[epoch 72][iter   20] loss: 107948.0156 RMSElog: 9.7377 grad_loss: 10784.1299 normal_loss: 0.9342
[epoch 72][iter   30] loss: 158630.9688 RMSElog: 9.8321 grad_loss: 15852.3232 normal_loss: 0.9428
[epoch 72][iter   40] loss: 133045.7969 RMSElog: 9.8765 grad_loss: 13293.7490 normal_loss: 0.9538
[epoch 72][iter   50] loss: 135203.5938 RMSElog: 9.4545 grad_loss: 13509.9697 normal_loss: 0.9356
[epoch 72][iter   60] loss: 114924.9844 RMSElog: 9.6925 grad_loss: 11481.8691 normal_loss: 0.9360
[epoch 72][iter   70] loss: 213417.0312 RMSElog: 10.3416 grad_loss: 21330.4082 normal_loss: 0.9536
[epoch 72][iter   80] loss: 180425.2344 RMSElog: 10.0354 grad_loss: 18031.5332 normal_loss: 0.9552
[epoch 72][iter   90] loss: 182859.0469 RMSElog: 9.8110 grad_loss: 18275.1250 normal_loss: 0.9678
[epoch 72][iter  100] loss: 113921.7891 RMSElog: 9.7251 grad_loss: 11381.5186 normal_loss: 0.9357
[epoch 72][iter  110] loss: 105794.8828 RMSElog: 9.7676 grad_loss: 10568.7939 normal_loss: 0.9267
[epoch 72][iter  120] loss: 127061.7500 RMSElog: 9.6726 grad_loss: 12695.5654 normal_loss: 0.9369
[epoch 72][iter  130] loss: 197520.7500 RMSElog: 10.1663 grad_loss: 19740.9414 normal_loss: 0.9662
[epoch 72][iter  140] loss: 175002.7344 RMSElog: 9.8652 grad_loss: 17489.4648 normal_loss: 0.9436
[epoch 72][iter  150] loss: 149135.0000 RMSElog: 10.0932 grad_loss: 14902.4570 normal_loss: 0.9500
[epoch 72][iter  160] loss: 194578.5781 RMSElog: 10.2312 grad_loss: 19446.6680 normal_loss: 0.9596
[epoch 72][iter  170] loss: 181942.5625 RMSElog: 10.0291 grad_loss: 18183.2793 normal_loss: 0.9464
[epoch 72][iter  180] loss: 138606.5781 RMSElog: 9.6389 grad_loss: 13850.0977 normal_loss: 0.9205
[epoch 72][iter  190] loss: 149387.5625 RMSElog: 9.9550 grad_loss: 14927.8525 normal_loss: 0.9483
[epoch 72][iter  200] loss: 152540.7500 RMSElog: 9.9175 grad_loss: 15243.2090 normal_loss: 0.9474
[epoch 72][iter  210] loss: 138934.7188 RMSElog: 9.8450 grad_loss: 13882.6904 normal_loss: 0.9370
[epoch 72][iter  220] loss: 137764.4062 RMSElog: 9.8227 grad_loss: 13765.6748 normal_loss: 0.9442
[epoch 72][iter  230] loss: 177292.4062 RMSElog: 10.1195 grad_loss: 17718.1602 normal_loss: 0.9615
[epoch 72][iter  240] loss: 168545.0938 RMSElog: 9.6168 grad_loss: 16843.9434 normal_loss: 0.9495
[epoch 72][iter  250] loss: 106952.9219 RMSElog: 10.0294 grad_loss: 10684.3398 normal_loss: 0.9231
[epoch 72][iter  260] loss: 103534.6562 RMSElog: 9.7670 grad_loss: 10342.7715 normal_loss: 0.9280
[epoch 72][iter  270] loss: 223018.5625 RMSElog: 10.3592 grad_loss: 22290.5508 normal_loss: 0.9462
[epoch 72][iter  280] loss: 160743.5156 RMSElog: 9.9654 grad_loss: 16063.4521 normal_loss: 0.9340
[epoch 72][iter  290] loss: 235211.2344 RMSElog: 10.3371 grad_loss: 23509.8398 normal_loss: 0.9452
[epoch 72][iter  300] loss: 228338.7500 RMSElog: 10.2936 grad_loss: 22822.6348 normal_loss: 0.9477
[epoch 72][iter  310] loss: 150699.3750 RMSElog: 9.8567 grad_loss: 15059.1328 normal_loss: 0.9486
[epoch 72][iter  320] loss: 182318.7812 RMSElog: 10.1008 grad_loss: 18220.8398 normal_loss: 0.9384
[epoch 72][iter  330] loss: 188662.5156 RMSElog: 10.2592 grad_loss: 18855.0352 normal_loss: 0.9574
[epoch 72][iter  340] loss: 162372.7500 RMSElog: 9.8226 grad_loss: 16226.4922 normal_loss: 0.9603
[epoch 72][iter  350] loss: 136796.3438 RMSElog: 9.6524 grad_loss: 13669.0273 normal_loss: 0.9549
[epoch 72][iter  360] loss: 154626.7656 RMSElog: 9.9098 grad_loss: 15451.8066 normal_loss: 0.9596
[epoch 72][iter  370] loss: 160782.5156 RMSElog: 9.7710 grad_loss: 16067.5547 normal_loss: 0.9244
[epoch 72][iter  380] loss: 204037.7500 RMSElog: 10.1754 grad_loss: 20392.6582 normal_loss: 0.9420
[epoch 72][iter  390] loss: 214067.1875 RMSElog: 10.0580 grad_loss: 21395.7207 normal_loss: 0.9394
[epoch 72][iter  400] loss: 160743.4062 RMSElog: 9.7965 grad_loss: 16063.6025 normal_loss: 0.9413
[epoch 72][iter  410] loss: 158875.5625 RMSElog: 9.5853 grad_loss: 15877.0449 normal_loss: 0.9260
[epoch 72][iter  420] loss: 100980.7812 RMSElog: 9.6528 grad_loss: 10087.5176 normal_loss: 0.9085
[epoch 72][iter  430] loss: 188717.5625 RMSElog: 10.0336 grad_loss: 18860.7715 normal_loss: 0.9509
[epoch 72][iter  440] loss: 139433.9219 RMSElog: 9.8139 grad_loss: 13932.6191 normal_loss: 0.9591
[epoch 72][iter  450] loss: 208184.3438 RMSElog: 10.4061 grad_loss: 20807.0801 normal_loss: 0.9472
[epoch 72][iter  460] loss: 123525.9062 RMSElog: 9.3524 grad_loss: 12342.2930 normal_loss: 0.9450
[epoch 72][iter  470] loss: 109260.8984 RMSElog: 9.7819 grad_loss: 10915.3799 normal_loss: 0.9279
[epoch 72][iter  480] loss: 188116.2812 RMSElog: 10.0888 grad_loss: 18800.5703 normal_loss: 0.9711
[epoch 72][iter  490] loss: 132623.8750 RMSElog: 9.3325 grad_loss: 13252.1152 normal_loss: 0.9391
[epoch 72][iter  500] loss: 110603.0781 RMSElog: 9.8046 grad_loss: 11049.5664 normal_loss: 0.9364
[epoch 72][iter  510] loss: 129743.7969 RMSElog: 9.9523 grad_loss: 12963.4785 normal_loss: 0.9492
[epoch 72][iter  520] loss: 114509.4531 RMSElog: 9.7409 grad_loss: 11440.2627 normal_loss: 0.9413
[epoch 72][iter  530] loss: 149807.1562 RMSElog: 9.9060 grad_loss: 14969.8613 normal_loss: 0.9481
[epoch 72][iter  540] loss: 167745.5625 RMSElog: 9.8817 grad_loss: 16763.7129 normal_loss: 0.9621
[epoch 72][iter  550] loss: 214808.7344 RMSElog: 10.1872 grad_loss: 21469.7441 normal_loss: 0.9423
[epoch 72][iter  560] loss: 159340.4219 RMSElog: 9.9507 grad_loss: 15923.1406 normal_loss: 0.9500
[epoch 72][iter  570] loss: 128277.9766 RMSElog: 9.7565 grad_loss: 12817.0889 normal_loss: 0.9518
[epoch 72][iter  580] loss: 168688.8125 RMSElog: 9.8493 grad_loss: 16858.0781 normal_loss: 0.9524
[epoch 72][iter  590] loss: 150608.8906 RMSElog: 10.0680 grad_loss: 15049.8750 normal_loss: 0.9460
[epoch 73][iter    0] loss: 194408.7812 RMSElog: 10.2924 grad_loss: 19429.6328 normal_loss: 0.9537
[epoch 73][iter   10] loss: 142016.3906 RMSElog: 9.9191 grad_loss: 14190.7676 normal_loss: 0.9526
[epoch 73][iter   20] loss: 218757.0312 RMSElog: 10.1228 grad_loss: 21864.6270 normal_loss: 0.9524
[epoch 73][iter   30] loss: 147544.5000 RMSElog: 9.8913 grad_loss: 14743.6045 normal_loss: 0.9534
[epoch 73][iter   40] loss: 173086.7031 RMSElog: 10.4992 grad_loss: 17297.2227 normal_loss: 0.9480
[epoch 73][iter   50] loss: 109101.1250 RMSElog: 9.7363 grad_loss: 10899.4590 normal_loss: 0.9167
[epoch 73][iter   60] loss: 153997.7188 RMSElog: 9.7620 grad_loss: 15389.0713 normal_loss: 0.9383
[epoch 73][iter   70] loss: 144657.9844 RMSElog: 9.9197 grad_loss: 14454.9258 normal_loss: 0.9518
[epoch 73][iter   80] loss: 143613.7656 RMSElog: 9.8882 grad_loss: 14350.5566 normal_loss: 0.9315
[epoch 73][iter   90] loss: 137249.3438 RMSElog: 9.7342 grad_loss: 13714.2764 normal_loss: 0.9239
[epoch 73][iter  100] loss: 110235.3906 RMSElog: 9.7805 grad_loss: 11012.8271 normal_loss: 0.9312
[epoch 73][iter  110] loss: 119655.7969 RMSElog: 9.6110 grad_loss: 11955.0312 normal_loss: 0.9378
[epoch 73][iter  120] loss: 113195.8125 RMSElog: 9.1761 grad_loss: 11309.5039 normal_loss: 0.9012
[epoch 73][iter  130] loss: 131213.2969 RMSElog: 9.6860 grad_loss: 13110.7217 normal_loss: 0.9217
[epoch 73][iter  140] loss: 204037.7500 RMSElog: 10.1754 grad_loss: 20392.6582 normal_loss: 0.9420
[epoch 73][iter  150] loss: 139016.5781 RMSElog: 9.6276 grad_loss: 13891.1045 normal_loss: 0.9261
[epoch 73][iter  160] loss: 115534.3672 RMSElog: 9.7196 grad_loss: 11542.7842 normal_loss: 0.9325
[epoch 73][iter  170] loss: 150153.5000 RMSElog: 10.0554 grad_loss: 15004.3457 normal_loss: 0.9478
[epoch 73][iter  180] loss: 105896.9375 RMSElog: 9.7662 grad_loss: 10578.9961 normal_loss: 0.9310
[epoch 73][iter  190] loss: 176217.8125 RMSElog: 10.1087 grad_loss: 17610.7246 normal_loss: 0.9466
[epoch 73][iter  200] loss: 109148.7344 RMSElog: 10.1172 grad_loss: 10903.8262 normal_loss: 0.9294
[epoch 73][iter  210] loss: 154310.0938 RMSElog: 9.9148 grad_loss: 15420.1445 normal_loss: 0.9495
[epoch 73][iter  220] loss: 168688.7812 RMSElog: 9.8493 grad_loss: 16858.0762 normal_loss: 0.9524
[epoch 73][iter  230] loss: 108409.6484 RMSElog: 9.2105 grad_loss: 10830.8506 normal_loss: 0.9033
[epoch 73][iter  240] loss: 201162.4062 RMSElog: 9.9531 grad_loss: 20105.3164 normal_loss: 0.9706
[epoch 73][iter  250] loss: 107948.0156 RMSElog: 9.7377 grad_loss: 10784.1299 normal_loss: 0.9342
[epoch 73][iter  260] loss: 160024.9375 RMSElog: 9.8489 grad_loss: 15991.7236 normal_loss: 0.9215
[epoch 73][iter  270] loss: 130527.6094 RMSElog: 9.8914 grad_loss: 13041.9180 normal_loss: 0.9509
[epoch 73][iter  280] loss: 172966.9375 RMSElog: 9.9430 grad_loss: 17285.8008 normal_loss: 0.9485
[epoch 73][iter  290] loss: 175299.8438 RMSElog: 10.1975 grad_loss: 17518.8301 normal_loss: 0.9576
[epoch 73][iter  300] loss: 170357.3438 RMSElog: 9.9687 grad_loss: 17024.8008 normal_loss: 0.9644
[epoch 73][iter  310] loss: 132425.4375 RMSElog: 10.0155 grad_loss: 13231.5801 normal_loss: 0.9468
[epoch 73][iter  320] loss: 182318.7812 RMSElog: 10.1008 grad_loss: 18220.8398 normal_loss: 0.9384
[epoch 73][iter  330] loss: 149807.1562 RMSElog: 9.9060 grad_loss: 14969.8613 normal_loss: 0.9481
[epoch 73][iter  340] loss: 115651.6406 RMSElog: 9.1877 grad_loss: 11555.0664 normal_loss: 0.9106
[epoch 73][iter  350] loss: 120765.1875 RMSElog: 9.5441 grad_loss: 12066.0400 normal_loss: 0.9348
[epoch 73][iter  360] loss: 154705.2344 RMSElog: 9.9142 grad_loss: 15459.6562 normal_loss: 0.9527
[epoch 73][iter  370] loss: 148057.2500 RMSElog: 9.9895 grad_loss: 14794.7803 normal_loss: 0.9555
[epoch 73][iter  380] loss: 153099.1094 RMSElog: 9.8355 grad_loss: 15299.1445 normal_loss: 0.9304
[epoch 73][iter  390] loss: 140282.6250 RMSElog: 9.9822 grad_loss: 14017.3145 normal_loss: 0.9650
[epoch 73][iter  400] loss: 132707.3594 RMSElog: 9.7842 grad_loss: 13260.0049 normal_loss: 0.9472
[epoch 73][iter  410] loss: 116073.5938 RMSElog: 9.6844 grad_loss: 11596.7324 normal_loss: 0.9429
[epoch 73][iter  420] loss: 146444.9062 RMSElog: 10.1139 grad_loss: 14633.4365 normal_loss: 0.9403
[epoch 73][iter  430] loss: 224257.9688 RMSElog: 10.2842 grad_loss: 22414.5605 normal_loss: 0.9521
[epoch 73][iter  440] loss: 118746.3359 RMSElog: 9.7278 grad_loss: 11863.9629 normal_loss: 0.9433
[epoch 73][iter  450] loss: 136796.3438 RMSElog: 9.6524 grad_loss: 13669.0273 normal_loss: 0.9549
[epoch 73][iter  460] loss: 195999.5625 RMSElog: 10.1749 grad_loss: 19588.8047 normal_loss: 0.9756
[epoch 73][iter  470] loss: 156173.3594 RMSElog: 9.9681 grad_loss: 15606.4199 normal_loss: 0.9482
[epoch 73][iter  480] loss: 224077.6719 RMSElog: 10.3869 grad_loss: 22396.4219 normal_loss: 0.9584
[epoch 73][iter  490] loss: 110579.5391 RMSElog: 9.3019 grad_loss: 11047.7109 normal_loss: 0.9413
[epoch 73][iter  500] loss: 100980.7812 RMSElog: 9.6528 grad_loss: 10087.5176 normal_loss: 0.9085
[epoch 73][iter  510] loss: 149128.2188 RMSElog: 9.7262 grad_loss: 14902.1533 normal_loss: 0.9419
[epoch 73][iter  520] loss: 206526.4062 RMSElog: 10.1531 grad_loss: 20641.5449 normal_loss: 0.9425
[epoch 73][iter  530] loss: 145615.4531 RMSElog: 9.8847 grad_loss: 14550.7207 normal_loss: 0.9406
[epoch 73][iter  540] loss: 135814.7656 RMSElog: 9.5386 grad_loss: 13571.0107 normal_loss: 0.9281
[epoch 73][iter  550] loss: 177699.9688 RMSElog: 9.4851 grad_loss: 17759.5488 normal_loss: 0.9621
[epoch 73][iter  560] loss: 130739.4062 RMSElog: 9.5471 grad_loss: 13063.4434 normal_loss: 0.9498
[epoch 73][iter  570] loss: 227080.5938 RMSElog: 10.4127 grad_loss: 22696.6992 normal_loss: 0.9482
[epoch 73][iter  580] loss: 133045.7969 RMSElog: 9.8765 grad_loss: 13293.7490 normal_loss: 0.9538
[epoch 73][iter  590] loss: 115495.2188 RMSElog: 9.6084 grad_loss: 11538.9688 normal_loss: 0.9442
[epoch 74][iter    0] loss: 119655.7969 RMSElog: 9.6110 grad_loss: 11955.0312 normal_loss: 0.9378
[epoch 74][iter   10] loss: 162556.3906 RMSElog: 9.9091 grad_loss: 16244.7939 normal_loss: 0.9368
[epoch 74][iter   20] loss: 116073.6016 RMSElog: 9.6844 grad_loss: 11596.7334 normal_loss: 0.9429
[epoch 74][iter   30] loss: 201162.3750 RMSElog: 9.9531 grad_loss: 20105.3145 normal_loss: 0.9706
[epoch 74][iter   40] loss: 114509.4531 RMSElog: 9.7409 grad_loss: 11440.2627 normal_loss: 0.9413
[epoch 74][iter   50] loss: 175486.0625 RMSElog: 10.0393 grad_loss: 17537.6094 normal_loss: 0.9563
[epoch 74][iter   60] loss: 158875.5625 RMSElog: 9.5853 grad_loss: 15877.0449 normal_loss: 0.9260
[epoch 74][iter   70] loss: 196277.7188 RMSElog: 10.2453 grad_loss: 19616.5664 normal_loss: 0.9595
[epoch 74][iter   80] loss: 223875.4062 RMSElog: 10.3929 grad_loss: 22376.1953 normal_loss: 0.9536
[epoch 74][iter   90] loss: 191371.4375 RMSElog: 10.1918 grad_loss: 19126.0020 normal_loss: 0.9504
[epoch 74][iter  100] loss: 107073.5156 RMSElog: 9.7289 grad_loss: 10696.6914 normal_loss: 0.9313
[epoch 74][iter  110] loss: 100990.1562 RMSElog: 9.7350 grad_loss: 10088.3584 normal_loss: 0.9222
[epoch 74][iter  120] loss: 122670.8984 RMSElog: 9.4365 grad_loss: 12256.7207 normal_loss: 0.9331
[epoch 74][iter  130] loss: 167794.0469 RMSElog: 9.8335 grad_loss: 16768.6211 normal_loss: 0.9500
[epoch 74][iter  140] loss: 155762.3594 RMSElog: 9.8487 grad_loss: 15565.4414 normal_loss: 0.9448
[epoch 74][iter  150] loss: 206400.7812 RMSElog: 10.3027 grad_loss: 20628.8281 normal_loss: 0.9467
[epoch 74][iter  160] loss: 142261.5781 RMSElog: 9.7627 grad_loss: 14215.4639 normal_loss: 0.9304
[epoch 74][iter  170] loss: 126821.0234 RMSElog: 10.1398 grad_loss: 12671.0410 normal_loss: 0.9215
[epoch 74][iter  180] loss: 112851.7266 RMSElog: 9.4205 grad_loss: 11274.8320 normal_loss: 0.9199
[epoch 74][iter  190] loss: 233395.7188 RMSElog: 10.2368 grad_loss: 23328.3906 normal_loss: 0.9452
[epoch 74][iter  200] loss: 115766.5312 RMSElog: 9.7467 grad_loss: 11565.9717 normal_loss: 0.9344
[epoch 74][iter  210] loss: 180318.6719 RMSElog: 9.9530 grad_loss: 18020.9785 normal_loss: 0.9357
[epoch 74][iter  220] loss: 140282.6250 RMSElog: 9.9822 grad_loss: 14017.3145 normal_loss: 0.9650
[epoch 74][iter  230] loss: 167008.4531 RMSElog: 10.0165 grad_loss: 16689.8750 normal_loss: 0.9557
[epoch 74][iter  240] loss: 151723.8750 RMSElog: 9.9680 grad_loss: 15161.4746 normal_loss: 0.9449
[epoch 74][iter  250] loss: 183095.1562 RMSElog: 9.9762 grad_loss: 18298.5703 normal_loss: 0.9696
[epoch 74][iter  260] loss: 168688.7812 RMSElog: 9.8493 grad_loss: 16858.0762 normal_loss: 0.9524
[epoch 74][iter  270] loss: 191320.0938 RMSElog: 10.0978 grad_loss: 19120.9492 normal_loss: 0.9627
[epoch 74][iter  280] loss: 206083.5312 RMSElog: 10.2501 grad_loss: 20597.1465 normal_loss: 0.9562
[epoch 74][iter  290] loss: 138606.5781 RMSElog: 9.6389 grad_loss: 13850.0977 normal_loss: 0.9205
[epoch 74][iter  300] loss: 220695.7969 RMSElog: 10.2208 grad_loss: 22058.4082 normal_loss: 0.9520
[epoch 74][iter  310] loss: 232982.0469 RMSElog: 10.2845 grad_loss: 23286.9707 normal_loss: 0.9500
[epoch 74][iter  320] loss: 162976.7812 RMSElog: 9.6016 grad_loss: 16287.1572 normal_loss: 0.9201
[epoch 74][iter  330] loss: 177292.4062 RMSElog: 10.1195 grad_loss: 17718.1602 normal_loss: 0.9615
[epoch 74][iter  340] loss: 145389.9375 RMSElog: 9.8784 grad_loss: 14528.1660 normal_loss: 0.9489
[epoch 74][iter  350] loss: 154310.0938 RMSElog: 9.9148 grad_loss: 15420.1445 normal_loss: 0.9495
[epoch 74][iter  360] loss: 177664.5312 RMSElog: 9.7808 grad_loss: 17755.7344 normal_loss: 0.9369
[epoch 74][iter  370] loss: 104340.6328 RMSElog: 9.7692 grad_loss: 10423.3701 normal_loss: 0.9234
[epoch 74][iter  380] loss: 103355.8125 RMSElog: 9.7106 grad_loss: 10324.9453 normal_loss: 0.9244
[epoch 74][iter  390] loss: 138950.6406 RMSElog: 9.9777 grad_loss: 13884.1387 normal_loss: 0.9482
[epoch 74][iter  400] loss: 156557.3594 RMSElog: 9.6970 grad_loss: 15645.1025 normal_loss: 0.9363
[epoch 74][iter  410] loss: 142596.2344 RMSElog: 9.8922 grad_loss: 14248.7793 normal_loss: 0.9508
[epoch 74][iter  420] loss: 107472.2578 RMSElog: 9.0654 grad_loss: 10737.2480 normal_loss: 0.9124
[epoch 74][iter  430] loss: 212429.2969 RMSElog: 10.2794 grad_loss: 21231.6719 normal_loss: 0.9784
[epoch 74][iter  440] loss: 190034.7188 RMSElog: 9.9619 grad_loss: 18992.5625 normal_loss: 0.9485
[epoch 74][iter  450] loss: 102998.8984 RMSElog: 9.8140 grad_loss: 10289.1318 normal_loss: 0.9431
[epoch 74][iter  460] loss: 231973.6719 RMSElog: 10.2099 grad_loss: 23186.2090 normal_loss: 0.9490
[epoch 74][iter  470] loss: 218757.0312 RMSElog: 10.1228 grad_loss: 21864.6270 normal_loss: 0.9524
[epoch 74][iter  480] loss: 180208.2188 RMSElog: 10.0490 grad_loss: 18009.8223 normal_loss: 0.9517
[epoch 74][iter  490] loss: 176798.0469 RMSElog: 9.8176 grad_loss: 17669.0312 normal_loss: 0.9559
[epoch 74][iter  500] loss: 146576.9219 RMSElog: 9.6520 grad_loss: 14647.1045 normal_loss: 0.9356
[epoch 74][iter  510] loss: 153447.6875 RMSElog: 9.9051 grad_loss: 15333.9209 normal_loss: 0.9425
[epoch 74][iter  520] loss: 125683.7969 RMSElog: 9.8131 grad_loss: 12557.6123 normal_loss: 0.9537
[epoch 74][iter  530] loss: 214493.2031 RMSElog: 10.3525 grad_loss: 21438.0059 normal_loss: 0.9611
[epoch 74][iter  540] loss: 122359.4531 RMSElog: 9.7683 grad_loss: 12225.2402 normal_loss: 0.9361
[epoch 74][iter  550] loss: 135042.2031 RMSElog: 9.6763 grad_loss: 13493.6074 normal_loss: 0.9366
[epoch 74][iter  560] loss: 125695.5391 RMSElog: 9.9422 grad_loss: 12558.6836 normal_loss: 0.9278
[epoch 74][iter  570] loss: 122446.5469 RMSElog: 9.6438 grad_loss: 12234.0742 normal_loss: 0.9366
[epoch 74][iter  580] loss: 152562.3750 RMSElog: 9.8216 grad_loss: 15245.4775 normal_loss: 0.9391
[epoch 74][iter  590] loss: 143534.0938 RMSElog: 9.7662 grad_loss: 14342.7061 normal_loss: 0.9374
[epoch 75][iter    0] loss: 198036.3750 RMSElog: 10.2619 grad_loss: 19792.4219 normal_loss: 0.9529
[epoch 75][iter   10] loss: 189017.5781 RMSElog: 10.1390 grad_loss: 18890.6602 normal_loss: 0.9589
[epoch 75][iter   20] loss: 167794.0469 RMSElog: 9.8335 grad_loss: 16768.6211 normal_loss: 0.9500
[epoch 75][iter   30] loss: 175004.6719 RMSElog: 10.1774 grad_loss: 17489.3320 normal_loss: 0.9561
[epoch 75][iter   40] loss: 155482.2031 RMSElog: 10.0818 grad_loss: 15537.1826 normal_loss: 0.9552
[epoch 75][iter   50] loss: 105896.6094 RMSElog: 9.6983 grad_loss: 10579.0293 normal_loss: 0.9335
[epoch 75][iter   60] loss: 138934.7188 RMSElog: 9.8450 grad_loss: 13882.6904 normal_loss: 0.9370
[epoch 75][iter   70] loss: 158924.9062 RMSElog: 9.8027 grad_loss: 15881.7363 normal_loss: 0.9524
[epoch 75][iter   80] loss: 146465.6875 RMSElog: 10.2283 grad_loss: 14635.4062 normal_loss: 0.9337
[epoch 75][iter   90] loss: 135215.1250 RMSElog: 9.9407 grad_loss: 13510.6152 normal_loss: 0.9572
[epoch 75][iter  100] loss: 115534.3750 RMSElog: 9.7196 grad_loss: 11542.7852 normal_loss: 0.9325
[epoch 75][iter  110] loss: 218625.8750 RMSElog: 10.2839 grad_loss: 21851.3457 normal_loss: 0.9592
[epoch 75][iter  120] loss: 118746.3359 RMSElog: 9.7278 grad_loss: 11863.9629 normal_loss: 0.9433
[epoch 75][iter  130] loss: 211925.8438 RMSElog: 10.1863 grad_loss: 21181.4551 normal_loss: 0.9436
[epoch 75][iter  140] loss: 144657.9844 RMSElog: 9.9197 grad_loss: 14454.9258 normal_loss: 0.9518
[epoch 75][iter  150] loss: 195777.0469 RMSElog: 10.1280 grad_loss: 19566.6035 normal_loss: 0.9726
[epoch 75][iter  160] loss: 191825.0781 RMSElog: 10.3012 grad_loss: 19171.2500 normal_loss: 0.9570
[epoch 75][iter  170] loss: 142138.5312 RMSElog: 9.7967 grad_loss: 14203.1270 normal_loss: 0.9291
[epoch 75][iter  180] loss: 167084.9219 RMSElog: 9.8660 grad_loss: 16697.6582 normal_loss: 0.9689
[epoch 75][iter  190] loss: 138469.0469 RMSElog: 9.4787 grad_loss: 13836.4834 normal_loss: 0.9438
[epoch 75][iter  200] loss: 224943.8906 RMSElog: 10.1206 grad_loss: 22483.3125 normal_loss: 0.9551
[epoch 75][iter  210] loss: 190938.1250 RMSElog: 10.3028 grad_loss: 19082.5566 normal_loss: 0.9541
[epoch 75][iter  220] loss: 194578.5781 RMSElog: 10.2312 grad_loss: 19446.6680 normal_loss: 0.9596
[epoch 75][iter  230] loss: 157061.8906 RMSElog: 9.9892 grad_loss: 15695.2461 normal_loss: 0.9531
[epoch 75][iter  240] loss: 142596.2344 RMSElog: 9.8922 grad_loss: 14248.7793 normal_loss: 0.9508
[epoch 75][iter  250] loss: 152562.3906 RMSElog: 9.8216 grad_loss: 15245.4785 normal_loss: 0.9391
[epoch 75][iter  260] loss: 161813.2500 RMSElog: 9.9875 grad_loss: 16170.3857 normal_loss: 0.9508
[epoch 75][iter  270] loss: 197714.9844 RMSElog: 10.4355 grad_loss: 19760.1094 normal_loss: 0.9532
[epoch 75][iter  280] loss: 140645.6875 RMSElog: 10.1118 grad_loss: 14053.4873 normal_loss: 0.9695
[epoch 75][iter  290] loss: 168409.1562 RMSElog: 9.7031 grad_loss: 16830.2910 normal_loss: 0.9228
[epoch 75][iter  300] loss: 165020.2344 RMSElog: 9.7791 grad_loss: 16491.2910 normal_loss: 0.9530
[epoch 75][iter  310] loss: 141865.3906 RMSElog: 9.7199 grad_loss: 14175.8896 normal_loss: 0.9298
[epoch 75][iter  320] loss: 158630.9688 RMSElog: 9.8321 grad_loss: 15852.3232 normal_loss: 0.9428
[epoch 75][iter  330] loss: 124323.5234 RMSElog: 9.7543 grad_loss: 12421.6660 normal_loss: 0.9322
[epoch 75][iter  340] loss: 109441.5234 RMSElog: 9.7466 grad_loss: 10933.4756 normal_loss: 0.9300
[epoch 75][iter  350] loss: 179298.9062 RMSElog: 9.7220 grad_loss: 17919.2227 normal_loss: 0.9453
[epoch 75][iter  360] loss: 161646.4688 RMSElog: 10.1105 grad_loss: 16153.5762 normal_loss: 0.9608
[epoch 75][iter  370] loss: 137252.6406 RMSElog: 9.3538 grad_loss: 13714.9795 normal_loss: 0.9319
[epoch 75][iter  380] loss: 157274.4688 RMSElog: 9.9277 grad_loss: 15716.5723 normal_loss: 0.9461
[epoch 75][iter  390] loss: 157810.1406 RMSElog: 9.9254 grad_loss: 15770.1621 normal_loss: 0.9253
[epoch 75][iter  400] loss: 217845.5000 RMSElog: 10.3597 grad_loss: 21773.2148 normal_loss: 0.9774
[epoch 75][iter  410] loss: 203023.3438 RMSElog: 10.3036 grad_loss: 20291.0723 normal_loss: 0.9583
[epoch 75][iter  420] loss: 132425.4375 RMSElog: 10.0155 grad_loss: 13231.5801 normal_loss: 0.9468
[epoch 75][iter  430] loss: 186053.2812 RMSElog: 10.1470 grad_loss: 18594.2305 normal_loss: 0.9504
[epoch 75][iter  440] loss: 103355.8125 RMSElog: 9.7106 grad_loss: 10324.9453 normal_loss: 0.9244
[epoch 75][iter  450] loss: 189743.5625 RMSElog: 10.1862 grad_loss: 18963.2207 normal_loss: 0.9490
[epoch 75][iter  460] loss: 153724.0000 RMSElog: 9.9609 grad_loss: 15361.4922 normal_loss: 0.9475
[epoch 75][iter  470] loss: 227323.8281 RMSElog: 10.4159 grad_loss: 22721.0195 normal_loss: 0.9477
[epoch 75][iter  480] loss: 114173.8984 RMSElog: 9.3600 grad_loss: 11407.1230 normal_loss: 0.9061
[epoch 75][iter  490] loss: 139098.6406 RMSElog: 9.8619 grad_loss: 13899.0508 normal_loss: 0.9510
[epoch 75][iter  500] loss: 186960.2969 RMSElog: 10.2468 grad_loss: 18684.8457 normal_loss: 0.9380
[epoch 75][iter  510] loss: 207919.9844 RMSElog: 10.2783 grad_loss: 20780.7637 normal_loss: 0.9578
[epoch 75][iter  520] loss: 154029.7812 RMSElog: 9.8645 grad_loss: 15392.1729 normal_loss: 0.9405
[epoch 75][iter  530] loss: 200953.4062 RMSElog: 10.1163 grad_loss: 20084.2598 normal_loss: 0.9623
[epoch 75][iter  540] loss: 220695.8125 RMSElog: 10.2208 grad_loss: 22058.4102 normal_loss: 0.9520
[epoch 75][iter  550] loss: 179901.4844 RMSElog: 9.8267 grad_loss: 17979.3633 normal_loss: 0.9589
[epoch 75][iter  560] loss: 127776.8359 RMSElog: 10.0947 grad_loss: 12766.6396 normal_loss: 0.9490
[epoch 75][iter  570] loss: 189582.6719 RMSElog: 9.9918 grad_loss: 18947.3223 normal_loss: 0.9523
[epoch 75][iter  580] loss: 170542.9531 RMSElog: 9.7855 grad_loss: 17043.5410 normal_loss: 0.9688
[epoch 75][iter  590] loss: 120440.5938 RMSElog: 9.1890 grad_loss: 12033.9463 normal_loss: 0.9240
[epoch 76][iter    0] loss: 184374.2188 RMSElog: 10.2641 grad_loss: 18426.1992 normal_loss: 0.9596
[epoch 76][iter   10] loss: 205362.7344 RMSElog: 10.1686 grad_loss: 20525.1582 normal_loss: 0.9475
[epoch 76][iter   20] loss: 198036.3750 RMSElog: 10.2619 grad_loss: 19792.4219 normal_loss: 0.9529
[epoch 76][iter   30] loss: 217174.7812 RMSElog: 10.2254 grad_loss: 21706.2910 normal_loss: 0.9635
[epoch 76][iter   40] loss: 173855.4375 RMSElog: 10.0769 grad_loss: 17374.5098 normal_loss: 0.9566
[epoch 76][iter   50] loss: 160051.0156 RMSElog: 9.8405 grad_loss: 15994.3105 normal_loss: 0.9506
[epoch 76][iter   60] loss: 149036.1562 RMSElog: 9.7728 grad_loss: 14892.8887 normal_loss: 0.9547
[epoch 76][iter   70] loss: 164181.7344 RMSElog: 10.0104 grad_loss: 16407.1973 normal_loss: 0.9660
[epoch 76][iter   80] loss: 157810.1406 RMSElog: 9.9254 grad_loss: 15770.1621 normal_loss: 0.9253
[epoch 76][iter   90] loss: 140645.6875 RMSElog: 10.1118 grad_loss: 14053.4873 normal_loss: 0.9695
[epoch 76][iter  100] loss: 165020.2344 RMSElog: 9.7791 grad_loss: 16491.2910 normal_loss: 0.9530
[epoch 76][iter  110] loss: 160667.2500 RMSElog: 9.9014 grad_loss: 16055.8711 normal_loss: 0.9534
[epoch 76][iter  120] loss: 188717.5625 RMSElog: 10.0336 grad_loss: 18860.7715 normal_loss: 0.9509
[epoch 76][iter  130] loss: 162892.2969 RMSElog: 9.8252 grad_loss: 16278.4717 normal_loss: 0.9331
[epoch 76][iter  140] loss: 127110.9297 RMSElog: 9.5273 grad_loss: 12700.6055 normal_loss: 0.9604
[epoch 76][iter  150] loss: 163253.2500 RMSElog: 9.8740 grad_loss: 16314.5010 normal_loss: 0.9499
[epoch 76][iter  160] loss: 155482.1875 RMSElog: 10.0818 grad_loss: 15537.1816 normal_loss: 0.9552
[epoch 76][iter  170] loss: 102096.6484 RMSElog: 9.8437 grad_loss: 10198.8926 normal_loss: 0.9290
[epoch 76][iter  180] loss: 125315.5625 RMSElog: 9.7124 grad_loss: 12520.9131 normal_loss: 0.9309
[epoch 76][iter  190] loss: 176223.4531 RMSElog: 9.9872 grad_loss: 17611.4141 normal_loss: 0.9446
[epoch 76][iter  200] loss: 125695.5391 RMSElog: 9.9422 grad_loss: 12558.6836 normal_loss: 0.9278
[epoch 76][iter  210] loss: 122359.4531 RMSElog: 9.7683 grad_loss: 12225.2402 normal_loss: 0.9361
[epoch 76][iter  220] loss: 108800.2422 RMSElog: 9.7538 grad_loss: 10869.3369 normal_loss: 0.9334
[epoch 76][iter  230] loss: 133013.4531 RMSElog: 9.5635 grad_loss: 13290.8457 normal_loss: 0.9356
[epoch 76][iter  240] loss: 165453.0938 RMSElog: 9.9262 grad_loss: 16534.4160 normal_loss: 0.9660
[epoch 76][iter  250] loss: 162556.3906 RMSElog: 9.9091 grad_loss: 16244.7939 normal_loss: 0.9368
[epoch 76][iter  260] loss: 166093.7344 RMSElog: 10.4480 grad_loss: 16597.9766 normal_loss: 0.9489
[epoch 76][iter  270] loss: 121838.3672 RMSElog: 9.7453 grad_loss: 12173.1494 normal_loss: 0.9425
[epoch 76][iter  280] loss: 117693.8984 RMSElog: 9.6527 grad_loss: 11758.8105 normal_loss: 0.9267
[epoch 76][iter  290] loss: 138469.0469 RMSElog: 9.4787 grad_loss: 13836.4834 normal_loss: 0.9438
[epoch 76][iter  300] loss: 124246.3594 RMSElog: 9.7165 grad_loss: 12413.9834 normal_loss: 0.9360
[epoch 76][iter  310] loss: 137764.4062 RMSElog: 9.8227 grad_loss: 13765.6748 normal_loss: 0.9442
[epoch 76][iter  320] loss: 147479.0312 RMSElog: 9.6781 grad_loss: 14737.2812 normal_loss: 0.9446
[epoch 76][iter  330] loss: 189543.6719 RMSElog: 10.1592 grad_loss: 18943.2598 normal_loss: 0.9474
[epoch 76][iter  340] loss: 121866.5156 RMSElog: 9.8422 grad_loss: 12175.8965 normal_loss: 0.9131
[epoch 76][iter  350] loss: 232158.2188 RMSElog: 10.1898 grad_loss: 23204.6875 normal_loss: 0.9455
[epoch 76][iter  360] loss: 160782.5156 RMSElog: 9.7710 grad_loss: 16067.5547 normal_loss: 0.9244
[epoch 76][iter  370] loss: 128277.9766 RMSElog: 9.7565 grad_loss: 12817.0889 normal_loss: 0.9518
[epoch 76][iter  380] loss: 163272.7188 RMSElog: 10.0261 grad_loss: 16316.3047 normal_loss: 0.9400
[epoch 76][iter  390] loss: 158630.9688 RMSElog: 9.8321 grad_loss: 15852.3232 normal_loss: 0.9428
[epoch 76][iter  400] loss: 105816.5000 RMSElog: 9.4266 grad_loss: 10571.3066 normal_loss: 0.9171
[epoch 76][iter  410] loss: 167455.8750 RMSElog: 9.7229 grad_loss: 16734.9395 normal_loss: 0.9256
[epoch 76][iter  420] loss: 110603.0625 RMSElog: 9.8046 grad_loss: 11049.5654 normal_loss: 0.9364
[epoch 76][iter  430] loss: 167730.9531 RMSElog: 9.9663 grad_loss: 16762.1797 normal_loss: 0.9501
[epoch 76][iter  440] loss: 183095.1562 RMSElog: 9.9762 grad_loss: 18298.5703 normal_loss: 0.9696
[epoch 76][iter  450] loss: 145321.5469 RMSElog: 9.7332 grad_loss: 14521.4590 normal_loss: 0.9632
[epoch 76][iter  460] loss: 231973.6719 RMSElog: 10.2099 grad_loss: 23186.2090 normal_loss: 0.9490
[epoch 76][iter  470] loss: 135699.4688 RMSElog: 9.6754 grad_loss: 13559.3340 normal_loss: 0.9364
[epoch 76][iter  480] loss: 154029.7812 RMSElog: 9.8645 grad_loss: 15392.1729 normal_loss: 0.9405
[epoch 76][iter  490] loss: 128319.4141 RMSElog: 9.6733 grad_loss: 12821.3398 normal_loss: 0.9287
[epoch 76][iter  500] loss: 220695.7969 RMSElog: 10.2208 grad_loss: 22058.4082 normal_loss: 0.9520
[epoch 76][iter  510] loss: 119470.5156 RMSElog: 9.6196 grad_loss: 11936.5020 normal_loss: 0.9297
[epoch 76][iter  520] loss: 167784.2500 RMSElog: 9.9330 grad_loss: 16767.5391 normal_loss: 0.9539
[epoch 76][iter  530] loss: 122670.8984 RMSElog: 9.4365 grad_loss: 12256.7207 normal_loss: 0.9331
[epoch 76][iter  540] loss: 111712.9062 RMSElog: 9.7826 grad_loss: 11160.5801 normal_loss: 0.9290
[epoch 76][iter  550] loss: 161813.2500 RMSElog: 9.9875 grad_loss: 16170.3857 normal_loss: 0.9508
[epoch 76][iter  560] loss: 109148.7344 RMSElog: 10.1172 grad_loss: 10903.8262 normal_loss: 0.9294
[epoch 76][iter  570] loss: 235211.2344 RMSElog: 10.3371 grad_loss: 23509.8398 normal_loss: 0.9452
[epoch 76][iter  580] loss: 173086.7031 RMSElog: 10.4992 grad_loss: 17297.2227 normal_loss: 0.9480
[epoch 76][iter  590] loss: 110874.4688 RMSElog: 9.2829 grad_loss: 11077.2607 normal_loss: 0.9034
[epoch 77][iter    0] loss: 146444.9062 RMSElog: 10.1139 grad_loss: 14633.4365 normal_loss: 0.9403
[epoch 77][iter   10] loss: 157810.1406 RMSElog: 9.9254 grad_loss: 15770.1621 normal_loss: 0.9253
[epoch 77][iter   20] loss: 138469.0469 RMSElog: 9.4787 grad_loss: 13836.4834 normal_loss: 0.9438
[epoch 77][iter   30] loss: 163268.1250 RMSElog: 9.9218 grad_loss: 16315.9277 normal_loss: 0.9630
[epoch 77][iter   40] loss: 226865.1406 RMSElog: 10.2887 grad_loss: 22675.2832 normal_loss: 0.9418
[epoch 77][iter   50] loss: 103305.2422 RMSElog: 9.5543 grad_loss: 10320.0674 normal_loss: 0.9024
[epoch 77][iter   60] loss: 127499.3281 RMSElog: 9.2769 grad_loss: 12739.7178 normal_loss: 0.9374
[epoch 77][iter   70] loss: 124563.3672 RMSElog: 9.9514 grad_loss: 12445.4453 normal_loss: 0.9400
[epoch 77][iter   80] loss: 109441.5234 RMSElog: 9.7466 grad_loss: 10933.4756 normal_loss: 0.9300
[epoch 77][iter   90] loss: 204650.3281 RMSElog: 10.1934 grad_loss: 20453.8633 normal_loss: 0.9762
[epoch 77][iter  100] loss: 158630.9688 RMSElog: 9.8321 grad_loss: 15852.3232 normal_loss: 0.9428
[epoch 77][iter  110] loss: 144440.4531 RMSElog: 9.8843 grad_loss: 14433.2314 normal_loss: 0.9296
[epoch 77][iter  120] loss: 154310.0938 RMSElog: 9.9148 grad_loss: 15420.1445 normal_loss: 0.9495
[epoch 77][iter  130] loss: 163389.5469 RMSElog: 9.7702 grad_loss: 16328.2637 normal_loss: 0.9208
[epoch 77][iter  140] loss: 145389.9375 RMSElog: 9.8784 grad_loss: 14528.1660 normal_loss: 0.9489
[epoch 77][iter  150] loss: 161285.2188 RMSElog: 9.8339 grad_loss: 16117.7402 normal_loss: 0.9476
[epoch 77][iter  160] loss: 164843.6719 RMSElog: 9.7972 grad_loss: 16473.6348 normal_loss: 0.9360
[epoch 77][iter  170] loss: 167412.9688 RMSElog: 9.7311 grad_loss: 16730.6406 normal_loss: 0.9260
[epoch 77][iter  180] loss: 153447.6875 RMSElog: 9.9051 grad_loss: 15333.9209 normal_loss: 0.9425
[epoch 77][iter  190] loss: 100980.7812 RMSElog: 9.6528 grad_loss: 10087.5176 normal_loss: 0.9085
[epoch 77][iter  200] loss: 213417.0312 RMSElog: 10.3416 grad_loss: 21330.4082 normal_loss: 0.9536
[epoch 77][iter  210] loss: 202119.8125 RMSElog: 10.2544 grad_loss: 20200.7715 normal_loss: 0.9549
[epoch 77][iter  220] loss: 159340.4375 RMSElog: 9.9507 grad_loss: 15923.1426 normal_loss: 0.9500
[epoch 77][iter  230] loss: 175122.9531 RMSElog: 9.9588 grad_loss: 17501.3789 normal_loss: 0.9573
[epoch 77][iter  240] loss: 217174.7656 RMSElog: 10.2254 grad_loss: 21706.2891 normal_loss: 0.9635
[epoch 77][iter  250] loss: 90482.6875 RMSElog: 9.3127 grad_loss: 9038.0186 normal_loss: 0.9378
[epoch 77][iter  260] loss: 172683.9219 RMSElog: 9.9061 grad_loss: 17257.5312 normal_loss: 0.9550
[epoch 77][iter  270] loss: 143534.0938 RMSElog: 9.7662 grad_loss: 14342.7061 normal_loss: 0.9374
[epoch 77][iter  280] loss: 141118.5312 RMSElog: 10.0520 grad_loss: 14100.8545 normal_loss: 0.9477
[epoch 77][iter  290] loss: 179740.0625 RMSElog: 10.1206 grad_loss: 17962.9453 normal_loss: 0.9397
[epoch 77][iter  300] loss: 175008.6562 RMSElog: 9.9496 grad_loss: 17489.9434 normal_loss: 0.9734
[epoch 77][iter  310] loss: 195671.7031 RMSElog: 10.1080 grad_loss: 19556.1133 normal_loss: 0.9494
[epoch 77][iter  320] loss: 156985.6250 RMSElog: 9.5765 grad_loss: 15688.0479 normal_loss: 0.9385
[epoch 77][iter  330] loss: 172868.6875 RMSElog: 10.0539 grad_loss: 17275.8652 normal_loss: 0.9500
[epoch 77][iter  340] loss: 172966.9375 RMSElog: 9.9430 grad_loss: 17285.8008 normal_loss: 0.9485
[epoch 77][iter  350] loss: 115452.5781 RMSElog: 9.2301 grad_loss: 11535.1230 normal_loss: 0.9042
[epoch 77][iter  360] loss: 180318.6719 RMSElog: 9.9530 grad_loss: 18020.9785 normal_loss: 0.9357
[epoch 77][iter  370] loss: 194987.6250 RMSElog: 9.8416 grad_loss: 19487.9453 normal_loss: 0.9748
[epoch 77][iter  380] loss: 103955.0000 RMSElog: 9.6256 grad_loss: 10384.9521 normal_loss: 0.9222
[epoch 77][iter  390] loss: 188502.1875 RMSElog: 10.1669 grad_loss: 18839.0801 normal_loss: 0.9722
[epoch 77][iter  400] loss: 161813.2500 RMSElog: 9.9875 grad_loss: 16170.3857 normal_loss: 0.9508
[epoch 77][iter  410] loss: 153724.0000 RMSElog: 9.9609 grad_loss: 15361.4922 normal_loss: 0.9475
[epoch 77][iter  420] loss: 150699.3750 RMSElog: 9.8567 grad_loss: 15059.1328 normal_loss: 0.9486
[epoch 77][iter  430] loss: 144589.9531 RMSElog: 9.7438 grad_loss: 14448.3340 normal_loss: 0.9167
[epoch 77][iter  440] loss: 96006.2500 RMSElog: 9.5560 grad_loss: 9590.1396 normal_loss: 0.9294
[epoch 77][iter  450] loss: 184016.5469 RMSElog: 9.9468 grad_loss: 18390.7363 normal_loss: 0.9712
[epoch 77][iter  460] loss: 142596.2344 RMSElog: 9.8922 grad_loss: 14248.7793 normal_loss: 0.9508
[epoch 77][iter  470] loss: 154609.8281 RMSElog: 9.8745 grad_loss: 15450.1855 normal_loss: 0.9220
[epoch 77][iter  480] loss: 195507.4062 RMSElog: 10.1854 grad_loss: 19539.6016 normal_loss: 0.9533
[epoch 77][iter  490] loss: 199070.3125 RMSElog: 9.9947 grad_loss: 19896.0605 normal_loss: 0.9757
[epoch 77][iter  500] loss: 135215.1250 RMSElog: 9.9407 grad_loss: 13510.6152 normal_loss: 0.9572
[epoch 77][iter  510] loss: 122912.0625 RMSElog: 9.7328 grad_loss: 12280.5225 normal_loss: 0.9509
[epoch 77][iter  520] loss: 146161.1562 RMSElog: 10.0682 grad_loss: 14605.1279 normal_loss: 0.9190
[epoch 77][iter  530] loss: 160825.5938 RMSElog: 9.9290 grad_loss: 16071.6816 normal_loss: 0.9491
[epoch 77][iter  540] loss: 194408.7812 RMSElog: 10.2924 grad_loss: 19429.6328 normal_loss: 0.9537
[epoch 77][iter  550] loss: 113921.7891 RMSElog: 9.7251 grad_loss: 11381.5186 normal_loss: 0.9357
[epoch 77][iter  560] loss: 149036.1562 RMSElog: 9.7728 grad_loss: 14892.8887 normal_loss: 0.9547
[epoch 77][iter  570] loss: 112851.7109 RMSElog: 9.4205 grad_loss: 11274.8301 normal_loss: 0.9199
[epoch 77][iter  580] loss: 114893.3984 RMSElog: 9.2562 grad_loss: 11479.1777 normal_loss: 0.9067
[epoch 77][iter  590] loss: 149807.1562 RMSElog: 9.9060 grad_loss: 14969.8613 normal_loss: 0.9481
[epoch 78][iter    0] loss: 127061.7500 RMSElog: 9.6726 grad_loss: 12695.5654 normal_loss: 0.9369
[epoch 78][iter   10] loss: 154290.2500 RMSElog: 9.8471 grad_loss: 15418.2334 normal_loss: 0.9458
[epoch 78][iter   20] loss: 143534.0938 RMSElog: 9.7662 grad_loss: 14342.7061 normal_loss: 0.9374
[epoch 78][iter   30] loss: 115500.4766 RMSElog: 9.9924 grad_loss: 11539.1230 normal_loss: 0.9328
[epoch 78][iter   40] loss: 149128.2188 RMSElog: 9.7262 grad_loss: 14902.1533 normal_loss: 0.9419
[epoch 78][iter   50] loss: 233395.7188 RMSElog: 10.2368 grad_loss: 23328.3906 normal_loss: 0.9452
[epoch 78][iter   60] loss: 140282.6250 RMSElog: 9.9822 grad_loss: 14017.3145 normal_loss: 0.9650
[epoch 78][iter   70] loss: 176260.7500 RMSElog: 10.0638 grad_loss: 17615.0703 normal_loss: 0.9395
[epoch 78][iter   80] loss: 139098.6406 RMSElog: 9.8619 grad_loss: 13899.0508 normal_loss: 0.9510
[epoch 78][iter   90] loss: 162711.9688 RMSElog: 9.6160 grad_loss: 16260.6387 normal_loss: 0.9427
[epoch 78][iter  100] loss: 177195.4844 RMSElog: 10.1516 grad_loss: 17708.4512 normal_loss: 0.9458
[epoch 78][iter  110] loss: 196025.7031 RMSElog: 10.2650 grad_loss: 19591.3496 normal_loss: 0.9559
[epoch 78][iter  120] loss: 178076.4688 RMSElog: 9.8778 grad_loss: 17796.8457 normal_loss: 0.9233
[epoch 78][iter  130] loss: 199607.1562 RMSElog: 10.1504 grad_loss: 19949.6133 normal_loss: 0.9517
[epoch 78][iter  140] loss: 165453.0938 RMSElog: 9.9262 grad_loss: 16534.4160 normal_loss: 0.9660
[epoch 78][iter  150] loss: 114924.9844 RMSElog: 9.6925 grad_loss: 11481.8691 normal_loss: 0.9360
[epoch 78][iter  160] loss: 129205.9375 RMSElog: 9.8851 grad_loss: 12909.7734 normal_loss: 0.9356
[epoch 78][iter  170] loss: 158230.6094 RMSElog: 9.9234 grad_loss: 15812.1895 normal_loss: 0.9483
[epoch 78][iter  180] loss: 119655.7969 RMSElog: 9.6110 grad_loss: 11955.0312 normal_loss: 0.9378
[epoch 78][iter  190] loss: 174629.2812 RMSElog: 9.9405 grad_loss: 17452.0625 normal_loss: 0.9229
[epoch 78][iter  200] loss: 114374.2344 RMSElog: 9.1913 grad_loss: 11427.3242 normal_loss: 0.9087
[epoch 78][iter  210] loss: 146960.1406 RMSElog: 10.0979 grad_loss: 14684.9941 normal_loss: 0.9220
[epoch 78][iter  220] loss: 157274.4688 RMSElog: 9.9277 grad_loss: 15716.5723 normal_loss: 0.9461
[epoch 78][iter  230] loss: 175359.1875 RMSElog: 10.1476 grad_loss: 17524.8125 normal_loss: 0.9571
[epoch 78][iter  240] loss: 120745.8203 RMSElog: 9.7706 grad_loss: 12063.8691 normal_loss: 0.9428
[epoch 78][iter  250] loss: 121838.3672 RMSElog: 9.7453 grad_loss: 12173.1494 normal_loss: 0.9425
[epoch 78][iter  260] loss: 111914.6406 RMSElog: 9.3004 grad_loss: 11181.2412 normal_loss: 0.9218
[epoch 78][iter  270] loss: 186960.2969 RMSElog: 10.2468 grad_loss: 18684.8457 normal_loss: 0.9380
[epoch 78][iter  280] loss: 154855.9062 RMSElog: 9.8054 grad_loss: 15474.8340 normal_loss: 0.9510
[epoch 78][iter  290] loss: 125486.8672 RMSElog: 9.7758 grad_loss: 12537.9834 normal_loss: 0.9281
[epoch 78][iter  300] loss: 227323.8281 RMSElog: 10.4159 grad_loss: 22721.0195 normal_loss: 0.9477
[epoch 78][iter  310] loss: 203023.3438 RMSElog: 10.3036 grad_loss: 20291.0723 normal_loss: 0.9583
[epoch 78][iter  320] loss: 115495.2188 RMSElog: 9.6084 grad_loss: 11538.9688 normal_loss: 0.9442
[epoch 78][iter  330] loss: 167345.2500 RMSElog: 9.8843 grad_loss: 16723.6777 normal_loss: 0.9620
[epoch 78][iter  340] loss: 204650.3281 RMSElog: 10.1934 grad_loss: 20453.8633 normal_loss: 0.9763
[epoch 78][iter  350] loss: 114893.3984 RMSElog: 9.2562 grad_loss: 11479.1777 normal_loss: 0.9067
[epoch 78][iter  360] loss: 156173.3594 RMSElog: 9.9681 grad_loss: 15606.4199 normal_loss: 0.9482
[epoch 78][iter  370] loss: 175122.9531 RMSElog: 9.9588 grad_loss: 17501.3789 normal_loss: 0.9573
[epoch 78][iter  380] loss: 157061.8906 RMSElog: 9.9892 grad_loss: 15695.2461 normal_loss: 0.9531
[epoch 78][iter  390] loss: 151354.1562 RMSElog: 9.8837 grad_loss: 15124.5889 normal_loss: 0.9428
[epoch 78][iter  400] loss: 125683.7969 RMSElog: 9.8131 grad_loss: 12557.6123 normal_loss: 0.9537
[epoch 78][iter  410] loss: 188502.1875 RMSElog: 10.1669 grad_loss: 18839.0801 normal_loss: 0.9722
[epoch 78][iter  420] loss: 104446.5625 RMSElog: 10.1932 grad_loss: 10433.5342 normal_loss: 0.9288
[epoch 78][iter  430] loss: 114173.8984 RMSElog: 9.3600 grad_loss: 11407.1230 normal_loss: 0.9061
[epoch 78][iter  440] loss: 117061.6250 RMSElog: 9.2141 grad_loss: 11696.0439 normal_loss: 0.9046
[epoch 78][iter  450] loss: 159284.2188 RMSElog: 9.6028 grad_loss: 15917.8828 normal_loss: 0.9361
[epoch 78][iter  460] loss: 162363.8906 RMSElog: 9.7464 grad_loss: 16225.6777 normal_loss: 0.9661
[epoch 78][iter  470] loss: 105816.5000 RMSElog: 9.4266 grad_loss: 10571.3066 normal_loss: 0.9171
[epoch 78][iter  480] loss: 148057.2500 RMSElog: 9.9895 grad_loss: 14794.7803 normal_loss: 0.9555
[epoch 78][iter  490] loss: 142261.5781 RMSElog: 9.7627 grad_loss: 14215.4639 normal_loss: 0.9304
[epoch 78][iter  500] loss: 119895.5000 RMSElog: 9.3187 grad_loss: 11979.3154 normal_loss: 0.9157
[epoch 78][iter  510] loss: 214808.7500 RMSElog: 10.1872 grad_loss: 21469.7461 normal_loss: 0.9423
[epoch 78][iter  520] loss: 189743.5625 RMSElog: 10.1862 grad_loss: 18963.2207 normal_loss: 0.9490
[epoch 78][iter  530] loss: 155911.2344 RMSElog: 9.7863 grad_loss: 15580.3965 normal_loss: 0.9403
[epoch 78][iter  540] loss: 173574.0781 RMSElog: 10.1780 grad_loss: 17346.2734 normal_loss: 0.9575
[epoch 78][iter  550] loss: 179901.4844 RMSElog: 9.8267 grad_loss: 17979.3633 normal_loss: 0.9589
[epoch 78][iter  560] loss: 160466.8750 RMSElog: 9.9086 grad_loss: 16035.8252 normal_loss: 0.9540
[epoch 78][iter  570] loss: 100116.4922 RMSElog: 9.9235 grad_loss: 10000.7998 normal_loss: 0.9258
[epoch 78][iter  580] loss: 107956.6094 RMSElog: 10.0900 grad_loss: 10784.6504 normal_loss: 0.9209
[epoch 78][iter  590] loss: 211640.2344 RMSElog: 10.2414 grad_loss: 21152.8145 normal_loss: 0.9675
[epoch 79][iter    0] loss: 121983.8203 RMSElog: 9.5225 grad_loss: 12187.9473 normal_loss: 0.9124
[epoch 79][iter   10] loss: 115348.2734 RMSElog: 9.7235 grad_loss: 11524.1562 normal_loss: 0.9470
[epoch 79][iter   20] loss: 232158.2188 RMSElog: 10.1898 grad_loss: 23204.6875 normal_loss: 0.9455
[epoch 79][iter   30] loss: 135966.2969 RMSElog: 9.6504 grad_loss: 13586.0273 normal_loss: 0.9522
[epoch 79][iter   40] loss: 124246.3594 RMSElog: 9.7165 grad_loss: 12413.9834 normal_loss: 0.9360
[epoch 79][iter   50] loss: 90482.6875 RMSElog: 9.3127 grad_loss: 9038.0186 normal_loss: 0.9378
[epoch 79][iter   60] loss: 196437.8125 RMSElog: 10.2319 grad_loss: 19632.5898 normal_loss: 0.9584
[epoch 79][iter   70] loss: 189507.2500 RMSElog: 10.2157 grad_loss: 18939.5586 normal_loss: 0.9510
[epoch 79][iter   80] loss: 137249.3438 RMSElog: 9.7342 grad_loss: 13714.2764 normal_loss: 0.9239
[epoch 79][iter   90] loss: 135814.7656 RMSElog: 9.5386 grad_loss: 13571.0107 normal_loss: 0.9281
[epoch 79][iter  100] loss: 125695.5391 RMSElog: 9.9422 grad_loss: 12558.6836 normal_loss: 0.9278
[epoch 79][iter  110] loss: 182005.7031 RMSElog: 10.0998 grad_loss: 18189.5195 normal_loss: 0.9507
[epoch 79][iter  120] loss: 234945.1719 RMSElog: 10.0015 grad_loss: 23483.5664 normal_loss: 0.9500
[epoch 79][iter  130] loss: 103514.7734 RMSElog: 9.3451 grad_loss: 10341.2100 normal_loss: 0.9229
[epoch 79][iter  140] loss: 162711.9688 RMSElog: 9.6160 grad_loss: 16260.6387 normal_loss: 0.9427
[epoch 79][iter  150] loss: 164597.1094 RMSElog: 9.8631 grad_loss: 16448.8984 normal_loss: 0.9502
[epoch 79][iter  160] loss: 152332.5469 RMSElog: 9.9813 grad_loss: 15222.3486 normal_loss: 0.9245
[epoch 79][iter  170] loss: 103305.2500 RMSElog: 9.5543 grad_loss: 10320.0684 normal_loss: 0.9024
[epoch 79][iter  180] loss: 163171.1562 RMSElog: 9.7905 grad_loss: 16306.3594 normal_loss: 0.9652
[epoch 79][iter  190] loss: 165453.0938 RMSElog: 9.9262 grad_loss: 16534.4160 normal_loss: 0.9660
[epoch 79][iter  200] loss: 215542.7500 RMSElog: 10.4358 grad_loss: 21542.8887 normal_loss: 0.9514
[epoch 79][iter  210] loss: 198293.9219 RMSElog: 10.2275 grad_loss: 19818.2148 normal_loss: 0.9521
[epoch 79][iter  220] loss: 158230.6094 RMSElog: 9.9234 grad_loss: 15812.1895 normal_loss: 0.9483
[epoch 79][iter  230] loss: 177934.3750 RMSElog: 9.8386 grad_loss: 17782.6445 normal_loss: 0.9542
[epoch 79][iter  240] loss: 167211.6250 RMSElog: 10.0285 grad_loss: 16710.1777 normal_loss: 0.9542
[epoch 79][iter  250] loss: 159340.4375 RMSElog: 9.9507 grad_loss: 15923.1426 normal_loss: 0.9500
[epoch 79][iter  260] loss: 104456.8438 RMSElog: 10.0170 grad_loss: 10434.7461 normal_loss: 0.9214
[epoch 79][iter  270] loss: 143904.7812 RMSElog: 10.0263 grad_loss: 14379.5068 normal_loss: 0.9457
[epoch 79][iter  280] loss: 227323.8281 RMSElog: 10.4159 grad_loss: 22721.0195 normal_loss: 0.9477
[epoch 79][iter  290] loss: 167784.2500 RMSElog: 9.9330 grad_loss: 16767.5391 normal_loss: 0.9539
[epoch 79][iter  300] loss: 191320.1250 RMSElog: 10.0978 grad_loss: 19120.9512 normal_loss: 0.9627
[epoch 79][iter  310] loss: 128488.4375 RMSElog: 9.7402 grad_loss: 12838.1719 normal_loss: 0.9319
[epoch 79][iter  320] loss: 169013.5625 RMSElog: 9.5880 grad_loss: 16890.8262 normal_loss: 0.9419
[epoch 79][iter  330] loss: 181293.3438 RMSElog: 10.0009 grad_loss: 18118.3809 normal_loss: 0.9531
[epoch 79][iter  340] loss: 217914.6094 RMSElog: 10.2786 grad_loss: 21780.2246 normal_loss: 0.9575
[epoch 79][iter  350] loss: 172755.6875 RMSElog: 10.0251 grad_loss: 17264.6016 normal_loss: 0.9419
[epoch 79][iter  360] loss: 165326.5625 RMSElog: 9.8832 grad_loss: 16521.8086 normal_loss: 0.9647
[epoch 79][iter  370] loss: 135042.2031 RMSElog: 9.6763 grad_loss: 13493.6074 normal_loss: 0.9366
[epoch 79][iter  380] loss: 213417.0312 RMSElog: 10.3416 grad_loss: 21330.4082 normal_loss: 0.9536
[epoch 79][iter  390] loss: 157425.7188 RMSElog: 9.9235 grad_loss: 15731.7012 normal_loss: 0.9477
[epoch 79][iter  400] loss: 127075.1875 RMSElog: 10.1217 grad_loss: 12696.4443 normal_loss: 0.9524
[epoch 79][iter  410] loss: 198743.4219 RMSElog: 10.0842 grad_loss: 19863.2930 normal_loss: 0.9643
[epoch 79][iter  420] loss: 132425.4375 RMSElog: 10.0155 grad_loss: 13231.5801 normal_loss: 0.9468
[epoch 79][iter  430] loss: 214493.2188 RMSElog: 10.3525 grad_loss: 21438.0078 normal_loss: 0.9611
[epoch 79][iter  440] loss: 160788.1406 RMSElog: 9.7539 grad_loss: 16068.1289 normal_loss: 0.9318
[epoch 79][iter  450] loss: 180929.5938 RMSElog: 10.0745 grad_loss: 18081.9297 normal_loss: 0.9546
[epoch 79][iter  460] loss: 160743.5156 RMSElog: 9.9654 grad_loss: 16063.4521 normal_loss: 0.9340
[epoch 79][iter  470] loss: 224077.7188 RMSElog: 10.3869 grad_loss: 22396.4258 normal_loss: 0.9584
[epoch 79][iter  480] loss: 165976.7500 RMSElog: 9.9385 grad_loss: 16586.7715 normal_loss: 0.9645
[epoch 79][iter  490] loss: 120745.8203 RMSElog: 9.7706 grad_loss: 12063.8691 normal_loss: 0.9428
[epoch 79][iter  500] loss: 174761.2344 RMSElog: 9.7333 grad_loss: 17465.4512 normal_loss: 0.9393
[epoch 79][iter  510] loss: 112697.1016 RMSElog: 9.7295 grad_loss: 11259.0449 normal_loss: 0.9352
[epoch 79][iter  520] loss: 149135.0000 RMSElog: 10.0932 grad_loss: 14902.4570 normal_loss: 0.9500
[epoch 79][iter  530] loss: 221089.3594 RMSElog: 10.3457 grad_loss: 22097.6465 normal_loss: 0.9439
[epoch 79][iter  540] loss: 188717.5625 RMSElog: 10.0336 grad_loss: 18860.7715 normal_loss: 0.9509
[epoch 79][iter  550] loss: 109260.8906 RMSElog: 9.7819 grad_loss: 10915.3789 normal_loss: 0.9279
[epoch 79][iter  560] loss: 189543.6719 RMSElog: 10.1592 grad_loss: 18943.2598 normal_loss: 0.9474
[epoch 79][iter  570] loss: 144191.5938 RMSElog: 9.6650 grad_loss: 14408.5654 normal_loss: 0.9284
[epoch 79][iter  580] loss: 183480.3438 RMSElog: 10.0091 grad_loss: 18337.0801 normal_loss: 0.9458
[epoch 79][iter  590] loss: 72289.2656 RMSElog: 9.4677 grad_loss: 7218.5376 normal_loss: 0.9211
#############################################################
#epochs=13 different visualization method, only RMSElog loss#
#############################################################
[epoch  0][iter    0] loss: 11.4853 RMSElog: 11.4853
[epoch  0][iter   10] loss: 11.3043 RMSElog: 11.3043
[epoch  0][iter   20] loss: 11.1224 RMSElog: 11.1224
[epoch  0][iter   30] loss: 12.6791 RMSElog: 12.6791
[epoch  0][iter   40] loss: 10.5954 RMSElog: 10.5954
[epoch  0][iter   50] loss: 10.9723 RMSElog: 10.9723
[epoch  0][iter   60] loss: 10.2830 RMSElog: 10.2830
[epoch  0][iter   70] loss: 10.8010 RMSElog: 10.8010
[epoch  0][iter   80] loss: 11.5373 RMSElog: 11.5373
[epoch  0][iter   90] loss: 10.5749 RMSElog: 10.5749
[epoch  0][iter  100] loss: 12.3009 RMSElog: 12.3009
[epoch  0][iter  110] loss: 12.1549 RMSElog: 12.1549
[epoch  0][iter  120] loss: 10.7651 RMSElog: 10.7651
[epoch  0][iter  130] loss: 11.7406 RMSElog: 11.7406
[epoch  0][iter  140] loss: 10.7011 RMSElog: 10.7011
[epoch  0][iter  150] loss: 10.2836 RMSElog: 10.2836
[epoch  0][iter  160] loss: 10.2678 RMSElog: 10.2678
[epoch  0][iter  170] loss: 11.9932 RMSElog: 11.9932
[epoch  0][iter  180] loss: 11.6059 RMSElog: 11.6059
[epoch  0][iter  190] loss: 10.5166 RMSElog: 10.5166
[epoch  0][iter  200] loss: 12.2506 RMSElog: 12.2506
[epoch  0][iter  210] loss: 10.4998 RMSElog: 10.4998
[epoch  0][iter  220] loss: 10.8689 RMSElog: 10.8689
[epoch  0][iter  230] loss: 11.5967 RMSElog: 11.5967
[epoch  0][iter  240] loss: 11.4904 RMSElog: 11.4904
[epoch  0][iter  250] loss: 11.8704 RMSElog: 11.8704
[epoch  0][iter  260] loss: 11.8001 RMSElog: 11.8001
[epoch  0][iter  270] loss: 11.3304 RMSElog: 11.3304
[epoch  0][iter  280] loss: 11.3018 RMSElog: 11.3018
[epoch  0][iter  290] loss: 10.6542 RMSElog: 10.6542
[epoch  0][iter  300] loss: 10.6113 RMSElog: 10.6113
[epoch  0][iter  310] loss: 12.1147 RMSElog: 12.1147
[epoch  0][iter  320] loss: 12.4301 RMSElog: 12.4301
[epoch  0][iter  330] loss: 10.1968 RMSElog: 10.1968
[epoch  0][iter  340] loss: 11.2843 RMSElog: 11.2843
[epoch  0][iter  350] loss: 9.1019 RMSElog: 9.1019
[epoch  0][iter  360] loss: 10.8784 RMSElog: 10.8784
[epoch  0][iter  370] loss: 10.2700 RMSElog: 10.2700
[epoch  0][iter  380] loss: 11.0483 RMSElog: 11.0483
[epoch  0][iter  390] loss: 9.5149 RMSElog: 9.5149
[epoch  0][iter  400] loss: 11.8728 RMSElog: 11.8728
[epoch  0][iter  410] loss: 11.0061 RMSElog: 11.0061
[epoch  0][iter  420] loss: 11.2051 RMSElog: 11.2051
[epoch  0][iter  430] loss: 9.8529 RMSElog: 9.8529
[epoch  0][iter  440] loss: 10.2610 RMSElog: 10.2610
[epoch  0][iter  450] loss: 9.2554 RMSElog: 9.2554
[epoch  0][iter  460] loss: 10.7817 RMSElog: 10.7817
[epoch  0][iter  470] loss: 10.7397 RMSElog: 10.7397
[epoch  0][iter  480] loss: 10.4942 RMSElog: 10.4942
[epoch  0][iter  490] loss: 10.3983 RMSElog: 10.3983
[epoch  0][iter  500] loss: 10.6885 RMSElog: 10.6885
[epoch  0][iter  510] loss: 10.4262 RMSElog: 10.4262
[epoch  0][iter  520] loss: 12.1243 RMSElog: 12.1243
[epoch  0][iter  530] loss: 11.6290 RMSElog: 11.6290
[epoch  0][iter  540] loss: 9.8771 RMSElog: 9.8771
[epoch  0][iter  550] loss: 8.7852 RMSElog: 8.7852
[epoch  0][iter  560] loss: 10.2603 RMSElog: 10.2603
[epoch  0][iter  570] loss: 11.0731 RMSElog: 11.0731
[epoch  0][iter  580] loss: 10.5142 RMSElog: 10.5142
[epoch  0][iter  590] loss: 11.3436 RMSElog: 11.3436
[epoch  1][iter    0] loss: 11.1677 RMSElog: 11.1677
[epoch  1][iter   10] loss: 10.8231 RMSElog: 10.8231
[epoch  1][iter   20] loss: 10.6920 RMSElog: 10.6920
[epoch  1][iter   30] loss: 10.5555 RMSElog: 10.5555
[epoch  1][iter   40] loss: 10.9428 RMSElog: 10.9428
[epoch  1][iter   50] loss: 10.7747 RMSElog: 10.7747
[epoch  1][iter   60] loss: 11.7336 RMSElog: 11.7336
[epoch  1][iter   70] loss: 12.0463 RMSElog: 12.0463
[epoch  1][iter   80] loss: 10.5285 RMSElog: 10.5285
[epoch  1][iter   90] loss: 10.2210 RMSElog: 10.2210
[epoch  1][iter  100] loss: 8.8868 RMSElog: 8.8868
[epoch  1][iter  110] loss: 9.0043 RMSElog: 9.0043
[epoch  1][iter  120] loss: 11.2648 RMSElog: 11.2648
[epoch  1][iter  130] loss: 8.2009 RMSElog: 8.2009
[epoch  1][iter  140] loss: 9.2312 RMSElog: 9.2312
[epoch  1][iter  150] loss: 8.7642 RMSElog: 8.7642
[epoch  1][iter  160] loss: 9.2945 RMSElog: 9.2945
[epoch  1][iter  170] loss: 8.8702 RMSElog: 8.8702
[epoch  1][iter  180] loss: 9.5380 RMSElog: 9.5380
[epoch  1][iter  190] loss: 9.8026 RMSElog: 9.8026
[epoch  1][iter  200] loss: 7.8633 RMSElog: 7.8633
[epoch  1][iter  210] loss: 8.7638 RMSElog: 8.7638
[epoch  1][iter  220] loss: 7.5957 RMSElog: 7.5957
[epoch  1][iter  230] loss: 7.9361 RMSElog: 7.9361
[epoch  1][iter  240] loss: 9.7189 RMSElog: 9.7189
[epoch  1][iter  250] loss: 8.5704 RMSElog: 8.5704
[epoch  1][iter  260] loss: 7.5942 RMSElog: 7.5942
[epoch  1][iter  270] loss: 8.8199 RMSElog: 8.8199
[epoch  1][iter  280] loss: 9.7647 RMSElog: 9.7647
[epoch  1][iter  290] loss: 8.0695 RMSElog: 8.0695
[epoch  1][iter  300] loss: 9.3242 RMSElog: 9.3242
[epoch  1][iter  310] loss: 7.9987 RMSElog: 7.9987
[epoch  1][iter  320] loss: 8.6399 RMSElog: 8.6399
[epoch  1][iter  330] loss: 8.2092 RMSElog: 8.2092
[epoch  1][iter  340] loss: 9.2668 RMSElog: 9.2668
[epoch  1][iter  350] loss: 9.1257 RMSElog: 9.1257
[epoch  1][iter  360] loss: 7.3083 RMSElog: 7.3083
[epoch  1][iter  370] loss: 8.2468 RMSElog: 8.2468
[epoch  1][iter  380] loss: 8.1645 RMSElog: 8.1645
[epoch  1][iter  390] loss: 8.5666 RMSElog: 8.5666
[epoch  1][iter  400] loss: 7.4912 RMSElog: 7.4912
[epoch  1][iter  410] loss: 7.9566 RMSElog: 7.9566
[epoch  1][iter  420] loss: 7.9012 RMSElog: 7.9012
[epoch  1][iter  430] loss: 8.4932 RMSElog: 8.4932
[epoch  1][iter  440] loss: 7.4724 RMSElog: 7.4724
[epoch  1][iter  450] loss: 7.3397 RMSElog: 7.3397
[epoch  1][iter  460] loss: 8.1010 RMSElog: 8.1010
[epoch  1][iter  470] loss: 8.0158 RMSElog: 8.0158
[epoch  1][iter  480] loss: 7.6132 RMSElog: 7.6132
[epoch  1][iter  490] loss: 7.9198 RMSElog: 7.9198
[epoch  1][iter  500] loss: 8.0229 RMSElog: 8.0229
[epoch  1][iter  510] loss: 8.4781 RMSElog: 8.4781
[epoch  1][iter  520] loss: 8.4783 RMSElog: 8.4783
[epoch  1][iter  530] loss: 7.6145 RMSElog: 7.6145
[epoch  1][iter  540] loss: 7.2659 RMSElog: 7.2659
[epoch  1][iter  550] loss: 7.7402 RMSElog: 7.7402
[epoch  1][iter  560] loss: 7.8244 RMSElog: 7.8244
[epoch  1][iter  570] loss: 6.9314 RMSElog: 6.9314
[epoch  1][iter  580] loss: 6.8539 RMSElog: 6.8539
[epoch  1][iter  590] loss: 6.7607 RMSElog: 6.7607
[epoch  2][iter    0] loss: 7.3059 RMSElog: 7.3059
[epoch  2][iter   10] loss: 7.2381 RMSElog: 7.2381
[epoch  2][iter   20] loss: 8.0902 RMSElog: 8.0902
[epoch  2][iter   30] loss: 6.5403 RMSElog: 6.5403
[epoch  2][iter   40] loss: 6.9556 RMSElog: 6.9556
[epoch  2][iter   50] loss: 6.6547 RMSElog: 6.6547
[epoch  2][iter   60] loss: 7.0110 RMSElog: 7.0110
[epoch  2][iter   70] loss: 6.8127 RMSElog: 6.8127
[epoch  2][iter   80] loss: 8.0122 RMSElog: 8.0122
[epoch  2][iter   90] loss: 7.5896 RMSElog: 7.5896
[epoch  2][iter  100] loss: 7.3592 RMSElog: 7.3592
[epoch  2][iter  110] loss: 8.2809 RMSElog: 8.2809
[epoch  2][iter  120] loss: 8.0174 RMSElog: 8.0174
[epoch  2][iter  130] loss: 7.5284 RMSElog: 7.5284
[epoch  2][iter  140] loss: 7.2700 RMSElog: 7.2700
[epoch  2][iter  150] loss: 7.5596 RMSElog: 7.5596
[epoch  2][iter  160] loss: 6.9298 RMSElog: 6.9298
[epoch  2][iter  170] loss: 6.8665 RMSElog: 6.8665
[epoch  2][iter  180] loss: 7.0219 RMSElog: 7.0219
[epoch  2][iter  190] loss: 7.3646 RMSElog: 7.3646
[epoch  2][iter  200] loss: 6.8607 RMSElog: 6.8607
[epoch  2][iter  210] loss: 6.6196 RMSElog: 6.6196
[epoch  2][iter  220] loss: 7.0149 RMSElog: 7.0149
[epoch  2][iter  230] loss: 6.3849 RMSElog: 6.3849
[epoch  2][iter  240] loss: 6.6355 RMSElog: 6.6355
[epoch  2][iter  250] loss: 7.1866 RMSElog: 7.1866
[epoch  2][iter  260] loss: 6.8189 RMSElog: 6.8189
[epoch  2][iter  270] loss: 6.4762 RMSElog: 6.4762
[epoch  2][iter  280] loss: 6.2244 RMSElog: 6.2244
[epoch  2][iter  290] loss: 6.9740 RMSElog: 6.9740
[epoch  2][iter  300] loss: 6.1504 RMSElog: 6.1504
[epoch  2][iter  310] loss: 6.5336 RMSElog: 6.5336
[epoch  2][iter  320] loss: 5.6391 RMSElog: 5.6391
[epoch  2][iter  330] loss: 6.6238 RMSElog: 6.6238
[epoch  2][iter  340] loss: 6.1708 RMSElog: 6.1708
[epoch  2][iter  350] loss: 7.2006 RMSElog: 7.2006
[epoch  2][iter  360] loss: 6.2708 RMSElog: 6.2708
[epoch  2][iter  370] loss: 6.6475 RMSElog: 6.6475
[epoch  2][iter  380] loss: 6.6933 RMSElog: 6.6933
[epoch  2][iter  390] loss: 6.1026 RMSElog: 6.1026
[epoch  2][iter  400] loss: 6.2618 RMSElog: 6.2618
[epoch  2][iter  410] loss: 5.5076 RMSElog: 5.5076
[epoch  2][iter  420] loss: 6.4582 RMSElog: 6.4582
[epoch  2][iter  430] loss: 6.1373 RMSElog: 6.1373
[epoch  2][iter  440] loss: 6.4417 RMSElog: 6.4417
[epoch  2][iter  450] loss: 5.8935 RMSElog: 5.8935
[epoch  2][iter  460] loss: 6.4228 RMSElog: 6.4228
[epoch  2][iter  470] loss: 5.9333 RMSElog: 5.9333
[epoch  2][iter  480] loss: 7.2804 RMSElog: 7.2804
[epoch  2][iter  490] loss: 6.5133 RMSElog: 6.5133
[epoch  2][iter  500] loss: 6.4393 RMSElog: 6.4393
[epoch  2][iter  510] loss: 6.7900 RMSElog: 6.7900
[epoch  2][iter  520] loss: 6.5888 RMSElog: 6.5888
[epoch  2][iter  530] loss: 5.4792 RMSElog: 5.4792
[epoch  2][iter  540] loss: 5.9153 RMSElog: 5.9153
[epoch  2][iter  550] loss: 6.0061 RMSElog: 6.0061
[epoch  2][iter  560] loss: 6.9479 RMSElog: 6.9479
[epoch  2][iter  570] loss: 6.0260 RMSElog: 6.0260
[epoch  2][iter  580] loss: 5.7551 RMSElog: 5.7551
[epoch  2][iter  590] loss: 6.4477 RMSElog: 6.4477
[epoch  3][iter    0] loss: 6.2052 RMSElog: 6.2052
[epoch  3][iter   10] loss: 6.6836 RMSElog: 6.6836
[epoch  3][iter   20] loss: 5.7827 RMSElog: 5.7827
[epoch  3][iter   30] loss: 6.2119 RMSElog: 6.2119
[epoch  3][iter   40] loss: 6.6573 RMSElog: 6.6573
[epoch  3][iter   50] loss: 6.1243 RMSElog: 6.1243
[epoch  3][iter   60] loss: 5.6492 RMSElog: 5.6492
[epoch  3][iter   70] loss: 5.8806 RMSElog: 5.8806
[epoch  3][iter   80] loss: 6.2899 RMSElog: 6.2899
[epoch  3][iter   90] loss: 6.0462 RMSElog: 6.0462
[epoch  3][iter  100] loss: 6.1553 RMSElog: 6.1553
[epoch  3][iter  110] loss: 5.9717 RMSElog: 5.9717
[epoch  3][iter  120] loss: 6.1104 RMSElog: 6.1104
[epoch  3][iter  130] loss: 5.2113 RMSElog: 5.2113
[epoch  3][iter  140] loss: 5.6889 RMSElog: 5.6889
[epoch  3][iter  150] loss: 5.4275 RMSElog: 5.4275
[epoch  3][iter  160] loss: 6.1723 RMSElog: 6.1723
[epoch  3][iter  170] loss: 6.1166 RMSElog: 6.1166
[epoch  3][iter  180] loss: 5.7171 RMSElog: 5.7171
[epoch  3][iter  190] loss: 5.3952 RMSElog: 5.3952
[epoch  3][iter  200] loss: 5.9171 RMSElog: 5.9171
[epoch  3][iter  210] loss: 6.6464 RMSElog: 6.6464
[epoch  3][iter  220] loss: 6.8228 RMSElog: 6.8228
[epoch  3][iter  230] loss: 6.2652 RMSElog: 6.2652
[epoch  3][iter  240] loss: 6.9190 RMSElog: 6.9190
[epoch  3][iter  250] loss: 5.1716 RMSElog: 5.1716
[epoch  3][iter  260] loss: 5.8270 RMSElog: 5.8270
[epoch  3][iter  270] loss: 5.9408 RMSElog: 5.9408
[epoch  3][iter  280] loss: 5.9905 RMSElog: 5.9905
[epoch  3][iter  290] loss: 5.0539 RMSElog: 5.0539
[epoch  3][iter  300] loss: 5.1585 RMSElog: 5.1585
[epoch  3][iter  310] loss: 5.5144 RMSElog: 5.5144
[epoch  3][iter  320] loss: 7.1147 RMSElog: 7.1147
[epoch  3][iter  330] loss: 5.4715 RMSElog: 5.4715
[epoch  3][iter  340] loss: 6.4493 RMSElog: 6.4493
[epoch  3][iter  350] loss: 5.7186 RMSElog: 5.7186
[epoch  3][iter  360] loss: 6.2953 RMSElog: 6.2953
[epoch  3][iter  370] loss: 6.0744 RMSElog: 6.0744
[epoch  3][iter  380] loss: 7.1121 RMSElog: 7.1121
[epoch  3][iter  390] loss: 6.0913 RMSElog: 6.0913
[epoch  3][iter  400] loss: 6.6306 RMSElog: 6.6306
[epoch  3][iter  410] loss: 6.7964 RMSElog: 6.7964
[epoch  3][iter  420] loss: 5.7948 RMSElog: 5.7948
[epoch  3][iter  430] loss: 6.2882 RMSElog: 6.2882
[epoch  3][iter  440] loss: 5.5131 RMSElog: 5.5131
[epoch  3][iter  450] loss: 6.9347 RMSElog: 6.9347
[epoch  3][iter  460] loss: 6.1463 RMSElog: 6.1463
[epoch  3][iter  470] loss: 6.4575 RMSElog: 6.4575
[epoch  3][iter  480] loss: 5.9582 RMSElog: 5.9582
[epoch  3][iter  490] loss: 6.8396 RMSElog: 6.8396
[epoch  3][iter  500] loss: 6.0059 RMSElog: 6.0059
[epoch  3][iter  510] loss: 5.1268 RMSElog: 5.1268
[epoch  3][iter  520] loss: 6.8372 RMSElog: 6.8372
[epoch  3][iter  530] loss: 5.4893 RMSElog: 5.4893
[epoch  3][iter  540] loss: 5.5412 RMSElog: 5.5412
[epoch  3][iter  550] loss: 5.1395 RMSElog: 5.1395
[epoch  3][iter  560] loss: 5.9439 RMSElog: 5.9439
[epoch  3][iter  570] loss: 5.7445 RMSElog: 5.7445
[epoch  3][iter  580] loss: 5.7615 RMSElog: 5.7615
[epoch  3][iter  590] loss: 5.4210 RMSElog: 5.4210
[epoch  4][iter    0] loss: 6.6414 RMSElog: 6.6414
[epoch  4][iter   10] loss: 5.1636 RMSElog: 5.1636
[epoch  4][iter   20] loss: 5.3981 RMSElog: 5.3981
[epoch  4][iter   30] loss: 5.2679 RMSElog: 5.2679
[epoch  4][iter   40] loss: 6.0154 RMSElog: 6.0154
[epoch  4][iter   50] loss: 6.1305 RMSElog: 6.1305
[epoch  4][iter   60] loss: 6.5648 RMSElog: 6.5648
[epoch  4][iter   70] loss: 6.1077 RMSElog: 6.1077
[epoch  4][iter   80] loss: 5.4509 RMSElog: 5.4509
[epoch  4][iter   90] loss: 5.4115 RMSElog: 5.4115
[epoch  4][iter  100] loss: 6.1082 RMSElog: 6.1082
[epoch  4][iter  110] loss: 5.3696 RMSElog: 5.3696
[epoch  4][iter  120] loss: 5.7240 RMSElog: 5.7240
[epoch  4][iter  130] loss: 5.8511 RMSElog: 5.8511
[epoch  4][iter  140] loss: 6.1628 RMSElog: 6.1628
[epoch  4][iter  150] loss: 6.0477 RMSElog: 6.0477
[epoch  4][iter  160] loss: 5.7071 RMSElog: 5.7071
[epoch  4][iter  170] loss: 5.7842 RMSElog: 5.7842
[epoch  4][iter  180] loss: 5.9395 RMSElog: 5.9395
[epoch  4][iter  190] loss: 5.0805 RMSElog: 5.0805
[epoch  4][iter  200] loss: 5.7484 RMSElog: 5.7484
[epoch  4][iter  210] loss: 6.1010 RMSElog: 6.1010
[epoch  4][iter  220] loss: 6.1074 RMSElog: 6.1074
[epoch  4][iter  230] loss: 5.8011 RMSElog: 5.8011
[epoch  4][iter  240] loss: 5.3772 RMSElog: 5.3772
[epoch  4][iter  250] loss: 5.4910 RMSElog: 5.4910
[epoch  4][iter  260] loss: 7.0075 RMSElog: 7.0075
[epoch  4][iter  270] loss: 6.8663 RMSElog: 6.8663
[epoch  4][iter  280] loss: 6.2385 RMSElog: 6.2385
[epoch  4][iter  290] loss: 5.4339 RMSElog: 5.4339
[epoch  4][iter  300] loss: 5.5182 RMSElog: 5.5182
[epoch  4][iter  310] loss: 5.2589 RMSElog: 5.2589
[epoch  4][iter  320] loss: 5.1099 RMSElog: 5.1099
[epoch  4][iter  330] loss: 5.6094 RMSElog: 5.6094
[epoch  4][iter  340] loss: 5.4759 RMSElog: 5.4759
[epoch  4][iter  350] loss: 5.6739 RMSElog: 5.6739
[epoch  4][iter  360] loss: 5.8449 RMSElog: 5.8449
[epoch  4][iter  370] loss: 5.4638 RMSElog: 5.4638
[epoch  4][iter  380] loss: 6.1392 RMSElog: 6.1392
[epoch  4][iter  390] loss: 5.8367 RMSElog: 5.8367
[epoch  4][iter  400] loss: 6.1967 RMSElog: 6.1967
[epoch  4][iter  410] loss: 5.0104 RMSElog: 5.0104
[epoch  4][iter  420] loss: 6.2811 RMSElog: 6.2811
[epoch  4][iter  430] loss: 6.0872 RMSElog: 6.0872
[epoch  4][iter  440] loss: 5.6767 RMSElog: 5.6767
[epoch  4][iter  450] loss: 6.0812 RMSElog: 6.0812
[epoch  4][iter  460] loss: 5.5090 RMSElog: 5.5090
[epoch  4][iter  470] loss: 6.9991 RMSElog: 6.9991
[epoch  4][iter  480] loss: 6.1866 RMSElog: 6.1866
[epoch  4][iter  490] loss: 5.6333 RMSElog: 5.6333
[epoch  4][iter  500] loss: 6.0609 RMSElog: 6.0609
[epoch  4][iter  510] loss: 5.6544 RMSElog: 5.6544
[epoch  4][iter  520] loss: 5.8123 RMSElog: 5.8123
[epoch  4][iter  530] loss: 5.5600 RMSElog: 5.5600
[epoch  4][iter  540] loss: 5.5026 RMSElog: 5.5026
[epoch  4][iter  550] loss: 6.3512 RMSElog: 6.3512
[epoch  4][iter  560] loss: 5.5387 RMSElog: 5.5387
[epoch  4][iter  570] loss: 6.0458 RMSElog: 6.0458
[epoch  4][iter  580] loss: 5.5963 RMSElog: 5.5963
[epoch  4][iter  590] loss: 5.7454 RMSElog: 5.7454
[epoch  5][iter    0] loss: 4.8002 RMSElog: 4.8002
[epoch  5][iter   10] loss: 5.2709 RMSElog: 5.2709
[epoch  5][iter   20] loss: 5.6596 RMSElog: 5.6596
[epoch  5][iter   30] loss: 4.8475 RMSElog: 4.8475
[epoch  5][iter   40] loss: 6.5543 RMSElog: 6.5543
[epoch  5][iter   50] loss: 5.5289 RMSElog: 5.5289
[epoch  5][iter   60] loss: 6.2086 RMSElog: 6.2086
[epoch  5][iter   70] loss: 5.2048 RMSElog: 5.2048
[epoch  5][iter   80] loss: 5.5348 RMSElog: 5.5348
[epoch  5][iter   90] loss: 5.6463 RMSElog: 5.6463
[epoch  5][iter  100] loss: 5.6092 RMSElog: 5.6092
[epoch  5][iter  110] loss: 5.7252 RMSElog: 5.7252
[epoch  5][iter  120] loss: 5.7271 RMSElog: 5.7271
[epoch  5][iter  130] loss: 5.6613 RMSElog: 5.6613
[epoch  5][iter  140] loss: 4.8070 RMSElog: 4.8070
[epoch  5][iter  150] loss: 6.2420 RMSElog: 6.2420
[epoch  5][iter  160] loss: 5.5160 RMSElog: 5.5160
[epoch  5][iter  170] loss: 5.9160 RMSElog: 5.9160
[epoch  5][iter  180] loss: 5.1446 RMSElog: 5.1446
[epoch  5][iter  190] loss: 4.7007 RMSElog: 4.7007
[epoch  5][iter  200] loss: 5.7174 RMSElog: 5.7174
[epoch  5][iter  210] loss: 6.2362 RMSElog: 6.2362
[epoch  5][iter  220] loss: 5.4199 RMSElog: 5.4199
[epoch  5][iter  230] loss: 4.9257 RMSElog: 4.9257
[epoch  5][iter  240] loss: 5.0152 RMSElog: 5.0152
[epoch  5][iter  250] loss: 5.0542 RMSElog: 5.0542
[epoch  5][iter  260] loss: 6.2601 RMSElog: 6.2601
[epoch  5][iter  270] loss: 6.1462 RMSElog: 6.1462
[epoch  5][iter  280] loss: 5.9504 RMSElog: 5.9504
[epoch  5][iter  290] loss: 5.6144 RMSElog: 5.6144
[epoch  5][iter  300] loss: 5.0645 RMSElog: 5.0645
[epoch  5][iter  310] loss: 5.3860 RMSElog: 5.3860
[epoch  5][iter  320] loss: 5.1694 RMSElog: 5.1694
[epoch  5][iter  330] loss: 5.2036 RMSElog: 5.2036
[epoch  5][iter  340] loss: 5.1200 RMSElog: 5.1200
[epoch  5][iter  350] loss: 5.7989 RMSElog: 5.7989
[epoch  5][iter  360] loss: 5.6585 RMSElog: 5.6585
[epoch  5][iter  370] loss: 5.1410 RMSElog: 5.1410
[epoch  5][iter  380] loss: 5.8158 RMSElog: 5.8158
[epoch  5][iter  390] loss: 4.5841 RMSElog: 4.5841
[epoch  5][iter  400] loss: 5.9675 RMSElog: 5.9675
[epoch  5][iter  410] loss: 5.8208 RMSElog: 5.8208
[epoch  5][iter  420] loss: 5.5196 RMSElog: 5.5196
[epoch  5][iter  430] loss: 6.0567 RMSElog: 6.0567
[epoch  5][iter  440] loss: 5.0318 RMSElog: 5.0318
[epoch  5][iter  450] loss: 5.2930 RMSElog: 5.2930
[epoch  5][iter  460] loss: 5.2188 RMSElog: 5.2188
[epoch  5][iter  470] loss: 5.8413 RMSElog: 5.8413
[epoch  5][iter  480] loss: 5.2054 RMSElog: 5.2054
[epoch  5][iter  490] loss: 5.8031 RMSElog: 5.8031
[epoch  5][iter  500] loss: 5.0903 RMSElog: 5.0903
[epoch  5][iter  510] loss: 5.2837 RMSElog: 5.2837
[epoch  5][iter  520] loss: 4.9141 RMSElog: 4.9141
[epoch  5][iter  530] loss: 5.2677 RMSElog: 5.2677
[epoch  5][iter  540] loss: 5.7726 RMSElog: 5.7726
[epoch  5][iter  550] loss: 5.2475 RMSElog: 5.2475
[epoch  5][iter  560] loss: 5.8168 RMSElog: 5.8168
[epoch  5][iter  570] loss: 5.7852 RMSElog: 5.7852
[epoch  5][iter  580] loss: 5.0446 RMSElog: 5.0446
[epoch  5][iter  590] loss: 5.4802 RMSElog: 5.4802
[epoch  6][iter    0] loss: 5.0873 RMSElog: 5.0873
[epoch  6][iter   10] loss: 4.9962 RMSElog: 4.9962
[epoch  6][iter   20] loss: 5.7707 RMSElog: 5.7707
[epoch  6][iter   30] loss: 5.5003 RMSElog: 5.5003
[epoch  6][iter   40] loss: 4.7464 RMSElog: 4.7464
[epoch  6][iter   50] loss: 6.3937 RMSElog: 6.3937
[epoch  6][iter   60] loss: 4.5379 RMSElog: 4.5379
[epoch  6][iter   70] loss: 5.1598 RMSElog: 5.1598
[epoch  6][iter   80] loss: 6.0112 RMSElog: 6.0112
[epoch  6][iter   90] loss: 5.2009 RMSElog: 5.2009
[epoch  6][iter  100] loss: 4.3320 RMSElog: 4.3320
[epoch  6][iter  110] loss: 5.6569 RMSElog: 5.6569
[epoch  6][iter  120] loss: 5.2183 RMSElog: 5.2183
[epoch  6][iter  130] loss: 6.0907 RMSElog: 6.0907
[epoch  6][iter  140] loss: 4.6990 RMSElog: 4.6990
[epoch  6][iter  150] loss: 5.4308 RMSElog: 5.4308
[epoch  6][iter  160] loss: 4.2072 RMSElog: 4.2072
[epoch  6][iter  170] loss: 5.2965 RMSElog: 5.2965
[epoch  6][iter  180] loss: 4.9923 RMSElog: 4.9923
[epoch  6][iter  190] loss: 5.2633 RMSElog: 5.2633
[epoch  6][iter  200] loss: 5.9130 RMSElog: 5.9130
[epoch  6][iter  210] loss: 4.5716 RMSElog: 4.5716
[epoch  6][iter  220] loss: 5.5293 RMSElog: 5.5293
[epoch  6][iter  230] loss: 5.1250 RMSElog: 5.1250
[epoch  6][iter  240] loss: 4.4891 RMSElog: 4.4891
[epoch  6][iter  250] loss: 5.8056 RMSElog: 5.8056
[epoch  6][iter  260] loss: 5.6890 RMSElog: 5.6890
[epoch  6][iter  270] loss: 4.8817 RMSElog: 4.8817
[epoch  6][iter  280] loss: 4.6909 RMSElog: 4.6909
[epoch  6][iter  290] loss: 4.9495 RMSElog: 4.9495
[epoch  6][iter  300] loss: 5.0833 RMSElog: 5.0833
[epoch  6][iter  310] loss: 4.9346 RMSElog: 4.9346
[epoch  6][iter  320] loss: 4.9945 RMSElog: 4.9945
[epoch  6][iter  330] loss: 6.2338 RMSElog: 6.2338
[epoch  6][iter  340] loss: 5.8129 RMSElog: 5.8129
[epoch  6][iter  350] loss: 5.5655 RMSElog: 5.5655
[epoch  6][iter  360] loss: 5.8499 RMSElog: 5.8499
[epoch  6][iter  370] loss: 4.6450 RMSElog: 4.6450
[epoch  6][iter  380] loss: 5.4341 RMSElog: 5.4341
[epoch  6][iter  390] loss: 5.0572 RMSElog: 5.0572
[epoch  6][iter  400] loss: 5.8759 RMSElog: 5.8759
[epoch  6][iter  410] loss: 4.7861 RMSElog: 4.7861
[epoch  6][iter  420] loss: 5.6501 RMSElog: 5.6501
[epoch  6][iter  430] loss: 5.1958 RMSElog: 5.1958
[epoch  6][iter  440] loss: 5.5384 RMSElog: 5.5384
[epoch  6][iter  450] loss: 5.2626 RMSElog: 5.2626
[epoch  6][iter  460] loss: 5.4737 RMSElog: 5.4737
[epoch  6][iter  470] loss: 5.2024 RMSElog: 5.2024
[epoch  6][iter  480] loss: 5.0459 RMSElog: 5.0459
[epoch  6][iter  490] loss: 6.1842 RMSElog: 6.1842
[epoch  6][iter  500] loss: 5.2020 RMSElog: 5.2020
[epoch  6][iter  510] loss: 5.6652 RMSElog: 5.6652
[epoch  6][iter  520] loss: 5.2515 RMSElog: 5.2515
[epoch  6][iter  530] loss: 5.2627 RMSElog: 5.2627
[epoch  6][iter  540] loss: 6.3255 RMSElog: 6.3255
[epoch  6][iter  550] loss: 5.7310 RMSElog: 5.7310
[epoch  6][iter  560] loss: 6.2334 RMSElog: 6.2334
[epoch  6][iter  570] loss: 5.2664 RMSElog: 5.2664
[epoch  6][iter  580] loss: 5.4916 RMSElog: 5.4916
[epoch  6][iter  590] loss: 4.9486 RMSElog: 4.9486
[epoch  7][iter    0] loss: 5.2978 RMSElog: 5.2978
[epoch  7][iter   10] loss: 4.5241 RMSElog: 4.5241
[epoch  7][iter   20] loss: 5.5602 RMSElog: 5.5602
[epoch  7][iter   30] loss: 5.7203 RMSElog: 5.7203
[epoch  7][iter   40] loss: 5.2281 RMSElog: 5.2281
[epoch  7][iter   50] loss: 5.1690 RMSElog: 5.1690
[epoch  7][iter   60] loss: 5.2082 RMSElog: 5.2082
[epoch  7][iter   70] loss: 5.1259 RMSElog: 5.1259
[epoch  7][iter   80] loss: 5.1605 RMSElog: 5.1605
[epoch  7][iter   90] loss: 5.9491 RMSElog: 5.9491
[epoch  7][iter  100] loss: 4.9436 RMSElog: 4.9436
[epoch  7][iter  110] loss: 4.8377 RMSElog: 4.8377
[epoch  7][iter  120] loss: 5.8071 RMSElog: 5.8071
[epoch  7][iter  130] loss: 5.3612 RMSElog: 5.3612
[epoch  7][iter  140] loss: 4.8820 RMSElog: 4.8820
[epoch  7][iter  150] loss: 6.2866 RMSElog: 6.2866
[epoch  7][iter  160] loss: 5.2186 RMSElog: 5.2186
[epoch  7][iter  170] loss: 4.9592 RMSElog: 4.9592
[epoch  7][iter  180] loss: 5.0654 RMSElog: 5.0654
[epoch  7][iter  190] loss: 5.1919 RMSElog: 5.1919
[epoch  7][iter  200] loss: 5.4938 RMSElog: 5.4938
[epoch  7][iter  210] loss: 5.3367 RMSElog: 5.3367
[epoch  7][iter  220] loss: 4.8507 RMSElog: 4.8507
[epoch  7][iter  230] loss: 6.1804 RMSElog: 6.1804
[epoch  7][iter  240] loss: 5.7960 RMSElog: 5.7960
[epoch  7][iter  250] loss: 5.0672 RMSElog: 5.0672
[epoch  7][iter  260] loss: 5.4199 RMSElog: 5.4199
[epoch  7][iter  270] loss: 5.3407 RMSElog: 5.3407
[epoch  7][iter  280] loss: 5.7538 RMSElog: 5.7538
[epoch  7][iter  290] loss: 5.1394 RMSElog: 5.1394
[epoch  7][iter  300] loss: 5.5067 RMSElog: 5.5067
[epoch  7][iter  310] loss: 5.4716 RMSElog: 5.4716
[epoch  7][iter  320] loss: 5.7997 RMSElog: 5.7997
[epoch  7][iter  330] loss: 5.8848 RMSElog: 5.8848
[epoch  7][iter  340] loss: 5.3071 RMSElog: 5.3071
[epoch  7][iter  350] loss: 5.2495 RMSElog: 5.2495
[epoch  7][iter  360] loss: 4.8227 RMSElog: 4.8227
[epoch  7][iter  370] loss: 5.4544 RMSElog: 5.4544
[epoch  7][iter  380] loss: 5.4856 RMSElog: 5.4856
[epoch  7][iter  390] loss: 4.6008 RMSElog: 4.6008
[epoch  7][iter  400] loss: 5.2874 RMSElog: 5.2874
[epoch  7][iter  410] loss: 6.0944 RMSElog: 6.0944
[epoch  7][iter  420] loss: 5.3417 RMSElog: 5.3417
[epoch  7][iter  430] loss: 5.4090 RMSElog: 5.4090
[epoch  7][iter  440] loss: 5.5888 RMSElog: 5.5888
[epoch  7][iter  450] loss: 5.8728 RMSElog: 5.8728
[epoch  7][iter  460] loss: 6.1368 RMSElog: 6.1368
[epoch  7][iter  470] loss: 4.8368 RMSElog: 4.8368
[epoch  7][iter  480] loss: 5.0374 RMSElog: 5.0374
[epoch  7][iter  490] loss: 5.7336 RMSElog: 5.7336
[epoch  7][iter  500] loss: 5.5434 RMSElog: 5.5434
[epoch  7][iter  510] loss: 5.6546 RMSElog: 5.6546
[epoch  7][iter  520] loss: 5.4722 RMSElog: 5.4722
[epoch  7][iter  530] loss: 6.2276 RMSElog: 6.2276
[epoch  7][iter  540] loss: 5.8680 RMSElog: 5.8680
[epoch  7][iter  550] loss: 5.5427 RMSElog: 5.5427
[epoch  7][iter  560] loss: 5.4842 RMSElog: 5.4842
[epoch  7][iter  570] loss: 6.0466 RMSElog: 6.0466
[epoch  7][iter  580] loss: 4.9770 RMSElog: 4.9770
[epoch  7][iter  590] loss: 6.1451 RMSElog: 6.1451
[epoch  8][iter    0] loss: 6.2123 RMSElog: 6.2123
[epoch  8][iter   10] loss: 5.1346 RMSElog: 5.1346
[epoch  8][iter   20] loss: 5.7314 RMSElog: 5.7314
[epoch  8][iter   30] loss: 4.7348 RMSElog: 4.7348
[epoch  8][iter   40] loss: 5.3963 RMSElog: 5.3963
[epoch  8][iter   50] loss: 5.1033 RMSElog: 5.1033
[epoch  8][iter   60] loss: 5.5438 RMSElog: 5.5438
[epoch  8][iter   70] loss: 5.6445 RMSElog: 5.6445
[epoch  8][iter   80] loss: 5.3606 RMSElog: 5.3606
[epoch  8][iter   90] loss: 5.2243 RMSElog: 5.2243
[epoch  8][iter  100] loss: 4.6182 RMSElog: 4.6182
[epoch  8][iter  110] loss: 5.0513 RMSElog: 5.0513
[epoch  8][iter  120] loss: 5.2239 RMSElog: 5.2239
[epoch  8][iter  130] loss: 6.0249 RMSElog: 6.0249
[epoch  8][iter  140] loss: 5.3349 RMSElog: 5.3349
[epoch  8][iter  150] loss: 6.1006 RMSElog: 6.1006
[epoch  8][iter  160] loss: 4.9996 RMSElog: 4.9996
[epoch  8][iter  170] loss: 5.1650 RMSElog: 5.1650
[epoch  8][iter  180] loss: 5.2393 RMSElog: 5.2393
[epoch  8][iter  190] loss: 5.9446 RMSElog: 5.9446
[epoch  8][iter  200] loss: 5.1060 RMSElog: 5.1060
[epoch  8][iter  210] loss: 4.4147 RMSElog: 4.4147
[epoch  8][iter  220] loss: 5.2890 RMSElog: 5.2890
[epoch  8][iter  230] loss: 5.0345 RMSElog: 5.0345
[epoch  8][iter  240] loss: 5.2361 RMSElog: 5.2361
[epoch  8][iter  250] loss: 4.4309 RMSElog: 4.4309
[epoch  8][iter  260] loss: 5.2967 RMSElog: 5.2967
[epoch  8][iter  270] loss: 4.8284 RMSElog: 4.8284
[epoch  8][iter  280] loss: 5.4880 RMSElog: 5.4880
[epoch  8][iter  290] loss: 5.7615 RMSElog: 5.7615
[epoch  8][iter  300] loss: 6.1538 RMSElog: 6.1538
[epoch  8][iter  310] loss: 4.9350 RMSElog: 4.9350
[epoch  8][iter  320] loss: 5.6208 RMSElog: 5.6208
[epoch  8][iter  330] loss: 5.8076 RMSElog: 5.8076
[epoch  8][iter  340] loss: 5.5031 RMSElog: 5.5031
[epoch  8][iter  350] loss: 5.9266 RMSElog: 5.9266
[epoch  8][iter  360] loss: 5.9273 RMSElog: 5.9273
[epoch  8][iter  370] loss: 5.0374 RMSElog: 5.0374
[epoch  8][iter  380] loss: 5.1140 RMSElog: 5.1140
[epoch  8][iter  390] loss: 4.8735 RMSElog: 4.8735
[epoch  8][iter  400] loss: 5.6963 RMSElog: 5.6963
[epoch  8][iter  410] loss: 5.5852 RMSElog: 5.5852
[epoch  8][iter  420] loss: 5.0969 RMSElog: 5.0969
[epoch  8][iter  430] loss: 5.0675 RMSElog: 5.0675
[epoch  8][iter  440] loss: 5.4745 RMSElog: 5.4745
[epoch  8][iter  450] loss: 5.2474 RMSElog: 5.2474
[epoch  8][iter  460] loss: 5.8765 RMSElog: 5.8765
[epoch  8][iter  470] loss: 5.3567 RMSElog: 5.3567
[epoch  8][iter  480] loss: 5.5808 RMSElog: 5.5808
[epoch  8][iter  490] loss: 4.8113 RMSElog: 4.8113
[epoch  8][iter  500] loss: 5.7638 RMSElog: 5.7638
[epoch  8][iter  510] loss: 5.6225 RMSElog: 5.6225
[epoch  8][iter  520] loss: 5.4963 RMSElog: 5.4963
[epoch  8][iter  530] loss: 4.8931 RMSElog: 4.8931
[epoch  8][iter  540] loss: 5.3852 RMSElog: 5.3852
[epoch  8][iter  550] loss: 5.8276 RMSElog: 5.8276
[epoch  8][iter  560] loss: 5.3978 RMSElog: 5.3978
[epoch  8][iter  570] loss: 5.4551 RMSElog: 5.4551
[epoch  8][iter  580] loss: 5.2500 RMSElog: 5.2500
[epoch  8][iter  590] loss: 6.0952 RMSElog: 6.0952
[epoch  9][iter    0] loss: 5.2608 RMSElog: 5.2608
[epoch  9][iter   10] loss: 5.2012 RMSElog: 5.2012
[epoch  9][iter   20] loss: 5.7290 RMSElog: 5.7290
[epoch  9][iter   30] loss: 5.5106 RMSElog: 5.5106
[epoch  9][iter   40] loss: 4.9621 RMSElog: 4.9621
[epoch  9][iter   50] loss: 5.8247 RMSElog: 5.8247
[epoch  9][iter   60] loss: 5.3721 RMSElog: 5.3721
[epoch  9][iter   70] loss: 4.6701 RMSElog: 4.6701
[epoch  9][iter   80] loss: 5.9224 RMSElog: 5.9224
[epoch  9][iter   90] loss: 5.1172 RMSElog: 5.1172
[epoch  9][iter  100] loss: 5.3472 RMSElog: 5.3472
[epoch  9][iter  110] loss: 5.7000 RMSElog: 5.7000
[epoch  9][iter  120] loss: 5.5184 RMSElog: 5.5184
[epoch  9][iter  130] loss: 5.0179 RMSElog: 5.0179
[epoch  9][iter  140] loss: 5.9258 RMSElog: 5.9258
[epoch  9][iter  150] loss: 4.5670 RMSElog: 4.5670
[epoch  9][iter  160] loss: 4.8798 RMSElog: 4.8798
[epoch  9][iter  170] loss: 6.1504 RMSElog: 6.1504
[epoch  9][iter  180] loss: 5.7018 RMSElog: 5.7018
[epoch  9][iter  190] loss: 5.4350 RMSElog: 5.4350
[epoch  9][iter  200] loss: 6.1941 RMSElog: 6.1941
[epoch  9][iter  210] loss: 5.6169 RMSElog: 5.6169
[epoch  9][iter  220] loss: 5.4006 RMSElog: 5.4006
[epoch  9][iter  230] loss: 5.6909 RMSElog: 5.6909
[epoch  9][iter  240] loss: 4.6623 RMSElog: 4.6623
[epoch  9][iter  250] loss: 5.1319 RMSElog: 5.1319
[epoch  9][iter  260] loss: 4.6724 RMSElog: 4.6724
[epoch  9][iter  270] loss: 5.1386 RMSElog: 5.1386
[epoch  9][iter  280] loss: 5.4721 RMSElog: 5.4721
[epoch  9][iter  290] loss: 5.6441 RMSElog: 5.6441
[epoch  9][iter  300] loss: 5.3907 RMSElog: 5.3907
[epoch  9][iter  310] loss: 4.9222 RMSElog: 4.9222
[epoch  9][iter  320] loss: 4.4514 RMSElog: 4.4514
[epoch  9][iter  330] loss: 5.1193 RMSElog: 5.1193
[epoch  9][iter  340] loss: 4.5006 RMSElog: 4.5006
[epoch  9][iter  350] loss: 5.2038 RMSElog: 5.2038
[epoch  9][iter  360] loss: 5.6627 RMSElog: 5.6627
[epoch  9][iter  370] loss: 5.5789 RMSElog: 5.5789
[epoch  9][iter  380] loss: 4.9686 RMSElog: 4.9686
[epoch  9][iter  390] loss: 4.8054 RMSElog: 4.8054
[epoch  9][iter  400] loss: 5.0700 RMSElog: 5.0700
[epoch  9][iter  410] loss: 5.3897 RMSElog: 5.3897
[epoch  9][iter  420] loss: 4.7193 RMSElog: 4.7193
[epoch  9][iter  430] loss: 5.0106 RMSElog: 5.0106
[epoch  9][iter  440] loss: 5.7118 RMSElog: 5.7118
[epoch  9][iter  450] loss: 5.2686 RMSElog: 5.2686
[epoch  9][iter  460] loss: 5.6871 RMSElog: 5.6871
[epoch  9][iter  470] loss: 5.7542 RMSElog: 5.7542
[epoch  9][iter  480] loss: 5.5190 RMSElog: 5.5190
[epoch  9][iter  490] loss: 4.9680 RMSElog: 4.9680
[epoch  9][iter  500] loss: 4.9195 RMSElog: 4.9195
[epoch  9][iter  510] loss: 6.1657 RMSElog: 6.1657
[epoch  9][iter  520] loss: 5.0399 RMSElog: 5.0399
[epoch  9][iter  530] loss: 5.8317 RMSElog: 5.8317
[epoch  9][iter  540] loss: 5.3630 RMSElog: 5.3630
[epoch  9][iter  550] loss: 4.6785 RMSElog: 4.6785
[epoch  9][iter  560] loss: 4.5793 RMSElog: 4.5793
[epoch  9][iter  570] loss: 5.4561 RMSElog: 5.4561
[epoch  9][iter  580] loss: 5.1316 RMSElog: 5.1316
[epoch  9][iter  590] loss: 4.8236 RMSElog: 4.8236
[epoch 10][iter    0] loss: 5.1770 RMSElog: 5.1770
[epoch 10][iter   10] loss: 4.8869 RMSElog: 4.8869
[epoch 10][iter   20] loss: 5.1083 RMSElog: 5.1083
[epoch 10][iter   30] loss: 5.0273 RMSElog: 5.0273
[epoch 10][iter   40] loss: 5.3808 RMSElog: 5.3808
[epoch 10][iter   50] loss: 6.2824 RMSElog: 6.2824
[epoch 10][iter   60] loss: 5.6535 RMSElog: 5.6535
[epoch 10][iter   70] loss: 5.1987 RMSElog: 5.1987
[epoch 10][iter   80] loss: 5.6864 RMSElog: 5.6864
[epoch 10][iter   90] loss: 5.1694 RMSElog: 5.1694
[epoch 10][iter  100] loss: 4.4456 RMSElog: 4.4456
[epoch 10][iter  110] loss: 5.4358 RMSElog: 5.4358
[epoch 10][iter  120] loss: 5.3549 RMSElog: 5.3549
[epoch 10][iter  130] loss: 5.3039 RMSElog: 5.3039
[epoch 10][iter  140] loss: 5.5721 RMSElog: 5.5721
[epoch 10][iter  150] loss: 5.5175 RMSElog: 5.5175
[epoch 10][iter  160] loss: 5.0729 RMSElog: 5.0729
[epoch 10][iter  170] loss: 5.3275 RMSElog: 5.3275
[epoch 10][iter  180] loss: 4.4939 RMSElog: 4.4939
[epoch 10][iter  190] loss: 5.5858 RMSElog: 5.5858
[epoch 10][iter  200] loss: 5.6108 RMSElog: 5.6108
[epoch 10][iter  210] loss: 4.4569 RMSElog: 4.4569
[epoch 10][iter  220] loss: 5.9736 RMSElog: 5.9736
[epoch 10][iter  230] loss: 5.6771 RMSElog: 5.6771
[epoch 10][iter  240] loss: 5.0988 RMSElog: 5.0988
[epoch 10][iter  250] loss: 5.1977 RMSElog: 5.1977
[epoch 10][iter  260] loss: 5.3677 RMSElog: 5.3677
[epoch 10][iter  270] loss: 5.4463 RMSElog: 5.4463
[epoch 10][iter  280] loss: 4.7318 RMSElog: 4.7318
[epoch 10][iter  290] loss: 5.9675 RMSElog: 5.9675
[epoch 10][iter  300] loss: 5.0597 RMSElog: 5.0597
[epoch 10][iter  310] loss: 5.1321 RMSElog: 5.1321
[epoch 10][iter  320] loss: 4.4818 RMSElog: 4.4818
[epoch 10][iter  330] loss: 4.9530 RMSElog: 4.9530
[epoch 10][iter  340] loss: 4.8022 RMSElog: 4.8022
[epoch 10][iter  350] loss: 5.3760 RMSElog: 5.3760
[epoch 10][iter  360] loss: 5.8353 RMSElog: 5.8353
[epoch 10][iter  370] loss: 5.1508 RMSElog: 5.1508
[epoch 10][iter  380] loss: 5.1025 RMSElog: 5.1025
[epoch 10][iter  390] loss: 5.4598 RMSElog: 5.4598
[epoch 10][iter  400] loss: 5.2767 RMSElog: 5.2767
[epoch 10][iter  410] loss: 4.5046 RMSElog: 4.5046
[epoch 10][iter  420] loss: 4.7920 RMSElog: 4.7920
[epoch 10][iter  430] loss: 5.6700 RMSElog: 5.6700
[epoch 10][iter  440] loss: 5.0194 RMSElog: 5.0194
[epoch 10][iter  450] loss: 6.0469 RMSElog: 6.0469
[epoch 10][iter  460] loss: 5.7837 RMSElog: 5.7837
[epoch 10][iter  470] loss: 4.4930 RMSElog: 4.4930
[epoch 10][iter  480] loss: 4.8281 RMSElog: 4.8281
[epoch 10][iter  490] loss: 4.9783 RMSElog: 4.9783
[epoch 10][iter  500] loss: 5.3361 RMSElog: 5.3361
[epoch 10][iter  510] loss: 5.8681 RMSElog: 5.8681
[epoch 10][iter  520] loss: 5.6684 RMSElog: 5.6684
[epoch 10][iter  530] loss: 5.9909 RMSElog: 5.9909
[epoch 10][iter  540] loss: 5.0043 RMSElog: 5.0043
[epoch 10][iter  550] loss: 4.3956 RMSElog: 4.3956
[epoch 10][iter  560] loss: 5.3790 RMSElog: 5.3790
[epoch 10][iter  570] loss: 5.0467 RMSElog: 5.0467
[epoch 10][iter  580] loss: 4.9928 RMSElog: 4.9928
[epoch 10][iter  590] loss: 4.8024 RMSElog: 4.8024
[epoch 11][iter    0] loss: 5.7233 RMSElog: 5.7233
[epoch 11][iter   10] loss: 4.7057 RMSElog: 4.7057
[epoch 11][iter   20] loss: 5.4114 RMSElog: 5.4114
[epoch 11][iter   30] loss: 4.6650 RMSElog: 4.6650
[epoch 11][iter   40] loss: 4.2553 RMSElog: 4.2553
[epoch 11][iter   50] loss: 6.3173 RMSElog: 6.3173
[epoch 11][iter   60] loss: 4.6124 RMSElog: 4.6124
[epoch 11][iter   70] loss: 4.8537 RMSElog: 4.8537
[epoch 11][iter   80] loss: 6.2085 RMSElog: 6.2085
[epoch 11][iter   90] loss: 5.0211 RMSElog: 5.0211
[epoch 11][iter  100] loss: 4.7715 RMSElog: 4.7715
[epoch 11][iter  110] loss: 5.5169 RMSElog: 5.5169
[epoch 11][iter  120] loss: 5.5876 RMSElog: 5.5876
[epoch 11][iter  130] loss: 5.2360 RMSElog: 5.2360
[epoch 11][iter  140] loss: 5.2137 RMSElog: 5.2137
[epoch 11][iter  150] loss: 4.7477 RMSElog: 4.7477
[epoch 11][iter  160] loss: 5.5499 RMSElog: 5.5499
[epoch 11][iter  170] loss: 5.2494 RMSElog: 5.2494
[epoch 11][iter  180] loss: 5.0349 RMSElog: 5.0349
[epoch 11][iter  190] loss: 5.6955 RMSElog: 5.6955
[epoch 11][iter  200] loss: 5.8007 RMSElog: 5.8007
[epoch 11][iter  210] loss: 5.3904 RMSElog: 5.3904
[epoch 11][iter  220] loss: 5.2579 RMSElog: 5.2579
[epoch 11][iter  230] loss: 4.9766 RMSElog: 4.9766
[epoch 11][iter  240] loss: 5.0216 RMSElog: 5.0216
[epoch 11][iter  250] loss: 4.8996 RMSElog: 4.8996
[epoch 11][iter  260] loss: 5.6577 RMSElog: 5.6577
[epoch 11][iter  270] loss: 4.8914 RMSElog: 4.8914
[epoch 11][iter  280] loss: 5.6930 RMSElog: 5.6930
[epoch 11][iter  290] loss: 5.1354 RMSElog: 5.1354
[epoch 11][iter  300] loss: 5.6139 RMSElog: 5.6139
[epoch 11][iter  310] loss: 5.5339 RMSElog: 5.5339
[epoch 11][iter  320] loss: 5.2356 RMSElog: 5.2356
[epoch 11][iter  330] loss: 4.7269 RMSElog: 4.7269
[epoch 11][iter  340] loss: 4.8038 RMSElog: 4.8038
[epoch 11][iter  350] loss: 4.7082 RMSElog: 4.7082
[epoch 11][iter  360] loss: 4.7742 RMSElog: 4.7742
[epoch 11][iter  370] loss: 5.0975 RMSElog: 5.0975
[epoch 11][iter  380] loss: 5.5917 RMSElog: 5.5917
[epoch 11][iter  390] loss: 5.6697 RMSElog: 5.6697
[epoch 11][iter  400] loss: 5.4880 RMSElog: 5.4880
[epoch 11][iter  410] loss: 4.8170 RMSElog: 4.8170
[epoch 11][iter  420] loss: 4.9222 RMSElog: 4.9222
[epoch 11][iter  430] loss: 5.6667 RMSElog: 5.6667
[epoch 11][iter  440] loss: 5.2899 RMSElog: 5.2899
[epoch 11][iter  450] loss: 4.2832 RMSElog: 4.2832
[epoch 11][iter  460] loss: 5.4937 RMSElog: 5.4937
[epoch 11][iter  470] loss: 5.2625 RMSElog: 5.2625
[epoch 11][iter  480] loss: 5.0013 RMSElog: 5.0013
[epoch 11][iter  490] loss: 4.8930 RMSElog: 4.8930
[epoch 11][iter  500] loss: 4.9209 RMSElog: 4.9209
[epoch 11][iter  510] loss: 5.6604 RMSElog: 5.6604
[epoch 11][iter  520] loss: 6.1977 RMSElog: 6.1977
[epoch 11][iter  530] loss: 4.9095 RMSElog: 4.9095
[epoch 11][iter  540] loss: 4.8871 RMSElog: 4.8871
[epoch 11][iter  550] loss: 5.0090 RMSElog: 5.0090
[epoch 11][iter  560] loss: 6.3703 RMSElog: 6.3703
[epoch 11][iter  570] loss: 5.6234 RMSElog: 5.6234
[epoch 11][iter  580] loss: 5.7073 RMSElog: 5.7073
[epoch 11][iter  590] loss: 5.3737 RMSElog: 5.3737
[epoch 12][iter    0] loss: 5.2229 RMSElog: 5.2229
[epoch 12][iter   10] loss: 5.2898 RMSElog: 5.2898
[epoch 12][iter   20] loss: 5.3199 RMSElog: 5.3199
[epoch 12][iter   30] loss: 5.3848 RMSElog: 5.3848
[epoch 12][iter   40] loss: 5.2812 RMSElog: 5.2812
[epoch 12][iter   50] loss: 5.0215 RMSElog: 5.0215
[epoch 12][iter   60] loss: 5.7341 RMSElog: 5.7341
[epoch 12][iter   70] loss: 4.8494 RMSElog: 4.8494
[epoch 12][iter   80] loss: 5.3711 RMSElog: 5.3711
[epoch 12][iter   90] loss: 5.1324 RMSElog: 5.1324
[epoch 12][iter  100] loss: 6.0239 RMSElog: 6.0239
[epoch 12][iter  110] loss: 4.9687 RMSElog: 4.9687
[epoch 12][iter  120] loss: 5.4618 RMSElog: 5.4618
[epoch 12][iter  130] loss: 6.2024 RMSElog: 6.2024
[epoch 12][iter  140] loss: 5.4678 RMSElog: 5.4678
[epoch 12][iter  150] loss: 6.1355 RMSElog: 6.1355
[epoch 12][iter  160] loss: 5.8534 RMSElog: 5.8534
[epoch 12][iter  170] loss: 5.8939 RMSElog: 5.8939
[epoch 12][iter  180] loss: 4.6107 RMSElog: 4.6107
[epoch 12][iter  190] loss: 5.1048 RMSElog: 5.1048
[epoch 12][iter  200] loss: 5.8482 RMSElog: 5.8482
[epoch 12][iter  210] loss: 5.5650 RMSElog: 5.5650
[epoch 12][iter  220] loss: 4.4230 RMSElog: 4.4230
[epoch 12][iter  230] loss: 5.2951 RMSElog: 5.2951
[epoch 12][iter  240] loss: 4.5931 RMSElog: 4.5931
[epoch 12][iter  250] loss: 5.0609 RMSElog: 5.0609
[epoch 12][iter  260] loss: 5.2239 RMSElog: 5.2239
[epoch 12][iter  270] loss: 4.4863 RMSElog: 4.4863
[epoch 12][iter  280] loss: 5.1414 RMSElog: 5.1414
[epoch 12][iter  290] loss: 4.8527 RMSElog: 4.8527
[epoch 12][iter  300] loss: 4.8747 RMSElog: 4.8747
[epoch 12][iter  310] loss: 5.3208 RMSElog: 5.3208
[epoch 12][iter  320] loss: 5.0035 RMSElog: 5.0035
[epoch 12][iter  330] loss: 5.5009 RMSElog: 5.5009
[epoch 12][iter  340] loss: 4.5848 RMSElog: 4.5848
[epoch 12][iter  350] loss: 4.3947 RMSElog: 4.3947
[epoch 12][iter  360] loss: 6.1339 RMSElog: 6.1339
[epoch 12][iter  370] loss: 4.3873 RMSElog: 4.3873
[epoch 12][iter  380] loss: 5.7149 RMSElog: 5.7149
[epoch 12][iter  390] loss: 5.6497 RMSElog: 5.6497
[epoch 12][iter  400] loss: 5.0231 RMSElog: 5.0231
[epoch 12][iter  410] loss: 4.5344 RMSElog: 4.5344
[epoch 12][iter  420] loss: 4.7228 RMSElog: 4.7228
[epoch 12][iter  430] loss: 6.2573 RMSElog: 6.2573
[epoch 12][iter  440] loss: 5.4107 RMSElog: 5.4107
[epoch 12][iter  450] loss: 5.6378 RMSElog: 5.6378
[epoch 12][iter  460] loss: 5.5507 RMSElog: 5.5507
[epoch 12][iter  470] loss: 5.4442 RMSElog: 5.4442
[epoch 12][iter  480] loss: 5.2076 RMSElog: 5.2076
[epoch 12][iter  490] loss: 4.9192 RMSElog: 4.9192
[epoch 12][iter  500] loss: 5.1703 RMSElog: 5.1703
[epoch 12][iter  510] loss: 5.3492 RMSElog: 5.3492
[epoch 12][iter  520] loss: 5.1085 RMSElog: 5.1085
[epoch 12][iter  530] loss: 4.9770 RMSElog: 4.9770
[epoch 12][iter  540] loss: 5.7065 RMSElog: 5.7065
[epoch 12][iter  550] loss: 5.4364 RMSElog: 5.4364
[epoch 12][iter  560] loss: 5.4563 RMSElog: 5.4563
[epoch 12][iter  570] loss: 5.7892 RMSElog: 5.7892
[epoch 12][iter  580] loss: 5.0171 RMSElog: 5.0171
[epoch 12][iter  590] loss: 5.3475 RMSElog: 5.3475
[epoch 13][iter    0] loss: 5.6902 RMSElog: 5.6902
[epoch 13][iter   10] loss: 5.6578 RMSElog: 5.6578
[epoch 13][iter   20] loss: 4.8489 RMSElog: 4.8489
[epoch 13][iter   30] loss: 4.5705 RMSElog: 4.5705
[epoch 13][iter   40] loss: 5.1517 RMSElog: 5.1517
[epoch 13][iter   50] loss: 4.7169 RMSElog: 4.7169
[epoch 13][iter   60] loss: 4.8370 RMSElog: 4.8370
[epoch 13][iter   70] loss: 5.6132 RMSElog: 5.6132
[epoch 13][iter   80] loss: 5.8692 RMSElog: 5.8692
[epoch 13][iter   90] loss: 4.9988 RMSElog: 4.9988
[epoch 13][iter  100] loss: 6.1844 RMSElog: 6.1844
[epoch 13][iter  110] loss: 5.3085 RMSElog: 5.3085
[epoch 13][iter  120] loss: 5.3728 RMSElog: 5.3728
[epoch 13][iter  130] loss: 4.8112 RMSElog: 4.8112
[epoch 13][iter  140] loss: 5.1708 RMSElog: 5.1708
[epoch 13][iter  150] loss: 4.9399 RMSElog: 4.9399
[epoch 13][iter  160] loss: 4.3606 RMSElog: 4.3606
[epoch 13][iter  170] loss: 5.4238 RMSElog: 5.4238
[epoch 13][iter  180] loss: 5.2518 RMSElog: 5.2518
[epoch 13][iter  190] loss: 5.0300 RMSElog: 5.0300
[epoch 13][iter  200] loss: 5.8638 RMSElog: 5.8638
[epoch 13][iter  210] loss: 5.8568 RMSElog: 5.8568
[epoch 13][iter  220] loss: 6.1258 RMSElog: 6.1258
[epoch 13][iter  230] loss: 4.4181 RMSElog: 4.4181
[epoch 13][iter  240] loss: 6.2755 RMSElog: 6.2755
[epoch 13][iter  250] loss: 4.4987 RMSElog: 4.4987
[epoch 13][iter  260] loss: 5.2454 RMSElog: 5.2454
[epoch 13][iter  270] loss: 4.8604 RMSElog: 4.8604
[epoch 13][iter  280] loss: 5.8668 RMSElog: 5.8668
[epoch 13][iter  290] loss: 4.2139 RMSElog: 4.2139
[epoch 13][iter  300] loss: 5.2283 RMSElog: 5.2283
[epoch 13][iter  310] loss: 4.7929 RMSElog: 4.7929
[epoch 13][iter  320] loss: 5.7959 RMSElog: 5.7959
[epoch 13][iter  330] loss: 5.8067 RMSElog: 5.8067
[epoch 13][iter  340] loss: 5.3555 RMSElog: 5.3555
[epoch 13][iter  350] loss: 5.6229 RMSElog: 5.6229
[epoch 13][iter  360] loss: 4.8421 RMSElog: 4.8421
[epoch 13][iter  370] loss: 5.6189 RMSElog: 5.6189
[epoch 13][iter  380] loss: 4.9754 RMSElog: 4.9754
[epoch 13][iter  390] loss: 6.3565 RMSElog: 6.3565
[epoch 13][iter  400] loss: 5.2483 RMSElog: 5.2483
[epoch 13][iter  410] loss: 4.8606 RMSElog: 4.8606
[epoch 13][iter  420] loss: 5.5845 RMSElog: 5.5845
[epoch 13][iter  430] loss: 5.0082 RMSElog: 5.0082
[epoch 13][iter  440] loss: 4.9571 RMSElog: 4.9571
[epoch 13][iter  450] loss: 5.1555 RMSElog: 5.1555
[epoch 13][iter  460] loss: 4.8384 RMSElog: 4.8384
[epoch 13][iter  470] loss: 4.3991 RMSElog: 4.3991
[epoch 13][iter  480] loss: 5.3204 RMSElog: 5.3204
[epoch 13][iter  490] loss: 4.7485 RMSElog: 4.7485
[epoch 13][iter  500] loss: 5.6631 RMSElog: 5.6631
################################################
#T2epochs=15 multiplying the output with 65535#
##############################################
[epoch  0][iter    0] loss: 10.5763 RMSElog: 10.5763
[epoch  0][iter   10] loss: 11.4880 RMSElog: 11.4880
[epoch  0][iter   20] loss: 8.9845 RMSElog: 8.9845
[epoch  0][iter   30] loss: 12.1791 RMSElog: 12.1791
[epoch  0][iter   40] loss: 11.4366 RMSElog: 11.4366
[epoch  0][iter   50] loss: 11.9287 RMSElog: 11.9287
[epoch  0][iter   60] loss: 10.5444 RMSElog: 10.5444
[epoch  0][iter   70] loss: 11.3988 RMSElog: 11.3988
[epoch  0][iter   80] loss: 10.3716 RMSElog: 10.3716
[epoch  0][iter   90] loss: 10.4483 RMSElog: 10.4483
[epoch  0][iter  100] loss: 10.8058 RMSElog: 10.8058
[epoch  0][iter  110] loss: 10.6049 RMSElog: 10.6049
[epoch  0][iter  120] loss: 11.0109 RMSElog: 11.0109
[epoch  0][iter  130] loss: 11.6225 RMSElog: 11.6225
[epoch  0][iter  140] loss: 8.5003 RMSElog: 8.5003
[epoch  0][iter  150] loss: 12.0838 RMSElog: 12.0838
[epoch  0][iter  160] loss: 10.2905 RMSElog: 10.2905
[epoch  0][iter  170] loss: 10.3088 RMSElog: 10.3088
[epoch  0][iter  180] loss: 12.2826 RMSElog: 12.2826
[epoch  0][iter  190] loss: 11.9524 RMSElog: 11.9524
[epoch  0][iter  200] loss: 8.5793 RMSElog: 8.5793
[epoch  0][iter  210] loss: 11.1213 RMSElog: 11.1213
[epoch  0][iter  220] loss: 12.0957 RMSElog: 12.0957
[epoch  0][iter  230] loss: 10.8733 RMSElog: 10.8733
[epoch  0][iter  240] loss: 11.4787 RMSElog: 11.4787
[epoch  0][iter  250] loss: 10.0985 RMSElog: 10.0985
[epoch  0][iter  260] loss: 10.8118 RMSElog: 10.8118
[epoch  0][iter  270] loss: 11.9178 RMSElog: 11.9178
[epoch  0][iter  280] loss: 11.6494 RMSElog: 11.6494
[epoch  0][iter  290] loss: 11.0725 RMSElog: 11.0725
[epoch  0][iter  300] loss: 11.0142 RMSElog: 11.0142
[epoch  0][iter  310] loss: 10.8084 RMSElog: 10.8084
[epoch  0][iter  320] loss: 11.6456 RMSElog: 11.6456
[epoch  0][iter  330] loss: 9.2362 RMSElog: 9.2362
[epoch  0][iter  340] loss: 11.2240 RMSElog: 11.2240
[epoch  0][iter  350] loss: 10.6133 RMSElog: 10.6133
[epoch  0][iter  360] loss: 12.3368 RMSElog: 12.3368
[epoch  0][iter  370] loss: 12.3025 RMSElog: 12.3025
[epoch  0][iter  380] loss: 10.6928 RMSElog: 10.6928
[epoch  0][iter  390] loss: 11.1478 RMSElog: 11.1478
[epoch  0][iter  400] loss: 11.7735 RMSElog: 11.7735
[epoch  0][iter  410] loss: 12.2080 RMSElog: 12.2080
[epoch  0][iter  420] loss: 10.6796 RMSElog: 10.6796
[epoch  0][iter  430] loss: 10.0322 RMSElog: 10.0322
[epoch  0][iter  440] loss: 11.6824 RMSElog: 11.6824
[epoch  0][iter  450] loss: 11.3343 RMSElog: 11.3343
[epoch  0][iter  460] loss: 10.2311 RMSElog: 10.2311
[epoch  0][iter  470] loss: 12.4094 RMSElog: 12.4094
[epoch  0][iter  480] loss: 10.0732 RMSElog: 10.0732
[epoch  0][iter  490] loss: 10.6914 RMSElog: 10.6914
[epoch  0][iter  500] loss: 12.2948 RMSElog: 12.2948
[epoch  0][iter  510] loss: 10.7743 RMSElog: 10.7743
[epoch  0][iter  520] loss: 10.2156 RMSElog: 10.2156
[epoch  0][iter  530] loss: 11.8868 RMSElog: 11.8868
[epoch  0][iter  540] loss: 10.8598 RMSElog: 10.8598
[epoch  0][iter  550] loss: 10.8933 RMSElog: 10.8933
[epoch  0][iter  560] loss: 10.7970 RMSElog: 10.7970
[epoch  0][iter  570] loss: 10.6733 RMSElog: 10.6733
[epoch  0][iter  580] loss: 10.9196 RMSElog: 10.9196
[epoch  0][iter  590] loss: 10.2844 RMSElog: 10.2844
[epoch  1][iter    0] loss: 12.0385 RMSElog: 12.0385
[epoch  1][iter   10] loss: 11.5744 RMSElog: 11.5744
[epoch  1][iter   20] loss: 11.0128 RMSElog: 11.0128
[epoch  1][iter   30] loss: 10.9368 RMSElog: 10.9368
[epoch  1][iter   40] loss: 10.6489 RMSElog: 10.6489
[epoch  1][iter   50] loss: 10.7490 RMSElog: 10.7490
[epoch  1][iter   60] loss: 10.7015 RMSElog: 10.7015
[epoch  1][iter   70] loss: 10.8690 RMSElog: 10.8690
[epoch  1][iter   80] loss: 10.5268 RMSElog: 10.5268
[epoch  1][iter   90] loss: 10.7279 RMSElog: 10.7279
[epoch  1][iter  100] loss: 11.2663 RMSElog: 11.2663
[epoch  1][iter  110] loss: 10.7204 RMSElog: 10.7204
[epoch  1][iter  120] loss: 10.7534 RMSElog: 10.7534
[epoch  1][iter  130] loss: 10.3279 RMSElog: 10.3279
[epoch  1][iter  140] loss: 10.5291 RMSElog: 10.5291
[epoch  1][iter  150] loss: 11.0773 RMSElog: 11.0773
[epoch  1][iter  160] loss: 9.7775 RMSElog: 9.7775
[epoch  1][iter  170] loss: 11.6119 RMSElog: 11.6119
[epoch  1][iter  180] loss: 9.2662 RMSElog: 9.2662
[epoch  1][iter  190] loss: 8.8793 RMSElog: 8.8793
[epoch  1][iter  200] loss: 9.1510 RMSElog: 9.1510
[epoch  1][iter  210] loss: 8.5450 RMSElog: 8.5450
[epoch  1][iter  220] loss: 8.0100 RMSElog: 8.0100
[epoch  1][iter  230] loss: 9.0941 RMSElog: 9.0941
[epoch  1][iter  240] loss: 8.4244 RMSElog: 8.4244
[epoch  1][iter  250] loss: 9.0713 RMSElog: 9.0713
[epoch  1][iter  260] loss: 8.7335 RMSElog: 8.7335
[epoch  1][iter  270] loss: 8.2964 RMSElog: 8.2964
[epoch  1][iter  280] loss: 8.0162 RMSElog: 8.0162
[epoch  1][iter  290] loss: 8.0103 RMSElog: 8.0103
[epoch  1][iter  300] loss: 8.7033 RMSElog: 8.7033
[epoch  1][iter  310] loss: 9.6498 RMSElog: 9.6498
[epoch  1][iter  320] loss: 8.1976 RMSElog: 8.1976
[epoch  1][iter  330] loss: 8.0611 RMSElog: 8.0611
[epoch  1][iter  340] loss: 9.4720 RMSElog: 9.4720
[epoch  1][iter  350] loss: 9.1566 RMSElog: 9.1566
[epoch  1][iter  360] loss: 9.0366 RMSElog: 9.0366
[epoch  1][iter  370] loss: 9.5563 RMSElog: 9.5563
[epoch  1][iter  380] loss: 9.5925 RMSElog: 9.5925
[epoch  1][iter  390] loss: 8.0699 RMSElog: 8.0699
[epoch  1][iter  400] loss: 9.3199 RMSElog: 9.3199
[epoch  1][iter  410] loss: 7.5935 RMSElog: 7.5935
[epoch  1][iter  420] loss: 7.5780 RMSElog: 7.5780
[epoch  1][iter  430] loss: 7.6586 RMSElog: 7.6586
[epoch  1][iter  440] loss: 7.9553 RMSElog: 7.9553
[epoch  1][iter  450] loss: 8.3420 RMSElog: 8.3420
[epoch  1][iter  460] loss: 8.2079 RMSElog: 8.2079
[epoch  1][iter  470] loss: 9.7518 RMSElog: 9.7518
[epoch  1][iter  480] loss: 8.3354 RMSElog: 8.3354
[epoch  1][iter  490] loss: 7.9932 RMSElog: 7.9932
[epoch  1][iter  500] loss: 8.8873 RMSElog: 8.8873
[epoch  1][iter  510] loss: 8.1777 RMSElog: 8.1777
[epoch  1][iter  520] loss: 8.2426 RMSElog: 8.2426
[epoch  1][iter  530] loss: 8.6637 RMSElog: 8.6637
[epoch  1][iter  540] loss: 7.9337 RMSElog: 7.9337
[epoch  1][iter  550] loss: 8.8347 RMSElog: 8.8347
[epoch  1][iter  560] loss: 7.5218 RMSElog: 7.5218
[epoch  1][iter  570] loss: 8.6085 RMSElog: 8.6085
[epoch  1][iter  580] loss: 7.7257 RMSElog: 7.7257
[epoch  1][iter  590] loss: 8.1050 RMSElog: 8.1050
[epoch  2][iter    0] loss: 7.7549 RMSElog: 7.7549
[epoch  2][iter   10] loss: 7.6440 RMSElog: 7.6440
[epoch  2][iter   20] loss: 8.2049 RMSElog: 8.2049
[epoch  2][iter   30] loss: 7.4397 RMSElog: 7.4397
[epoch  2][iter   40] loss: 7.5948 RMSElog: 7.5948
[epoch  2][iter   50] loss: 7.5899 RMSElog: 7.5899
[epoch  2][iter   60] loss: 7.3166 RMSElog: 7.3166
[epoch  2][iter   70] loss: 7.7971 RMSElog: 7.7971
[epoch  2][iter   80] loss: 6.8646 RMSElog: 6.8646
[epoch  2][iter   90] loss: 7.2871 RMSElog: 7.2871
[epoch  2][iter  100] loss: 7.7171 RMSElog: 7.7171
[epoch  2][iter  110] loss: 8.0240 RMSElog: 8.0240
[epoch  2][iter  120] loss: 7.6780 RMSElog: 7.6780
[epoch  2][iter  130] loss: 6.6380 RMSElog: 6.6380
[epoch  2][iter  140] loss: 7.6373 RMSElog: 7.6373
[epoch  2][iter  150] loss: 6.9190 RMSElog: 6.9190
[epoch  2][iter  160] loss: 7.0731 RMSElog: 7.0731
[epoch  2][iter  170] loss: 7.4451 RMSElog: 7.4451
[epoch  2][iter  180] loss: 7.4584 RMSElog: 7.4584
[epoch  2][iter  190] loss: 7.9817 RMSElog: 7.9817
[epoch  2][iter  200] loss: 6.4010 RMSElog: 6.4010
[epoch  2][iter  210] loss: 6.8279 RMSElog: 6.8279
[epoch  2][iter  220] loss: 6.4458 RMSElog: 6.4458
[epoch  2][iter  230] loss: 7.4172 RMSElog: 7.4172
[epoch  2][iter  240] loss: 6.6130 RMSElog: 6.6130
[epoch  2][iter  250] loss: 7.4592 RMSElog: 7.4592
[epoch  2][iter  260] loss: 6.0020 RMSElog: 6.0020
[epoch  2][iter  270] loss: 6.8923 RMSElog: 6.8923
[epoch  2][iter  280] loss: 7.1100 RMSElog: 7.1100
[epoch  2][iter  290] loss: 7.7765 RMSElog: 7.7765
[epoch  2][iter  300] loss: 6.5381 RMSElog: 6.5381
[epoch  2][iter  310] loss: 6.0192 RMSElog: 6.0192
[epoch  2][iter  320] loss: 6.9760 RMSElog: 6.9760
[epoch  2][iter  330] loss: 7.1307 RMSElog: 7.1307
[epoch  2][iter  340] loss: 6.4349 RMSElog: 6.4349
[epoch  2][iter  350] loss: 6.2598 RMSElog: 6.2598
[epoch  2][iter  360] loss: 6.1303 RMSElog: 6.1303
[epoch  2][iter  370] loss: 7.1345 RMSElog: 7.1345
[epoch  2][iter  380] loss: 6.4555 RMSElog: 6.4555
[epoch  2][iter  390] loss: 6.8690 RMSElog: 6.8690
[epoch  2][iter  400] loss: 7.8469 RMSElog: 7.8469
[epoch  2][iter  410] loss: 6.3352 RMSElog: 6.3352
[epoch  2][iter  420] loss: 6.9856 RMSElog: 6.9856
[epoch  2][iter  430] loss: 6.5653 RMSElog: 6.5653
[epoch  2][iter  440] loss: 6.4496 RMSElog: 6.4496
[epoch  2][iter  450] loss: 6.4009 RMSElog: 6.4009
[epoch  2][iter  460] loss: 6.4613 RMSElog: 6.4613
[epoch  2][iter  470] loss: 6.3657 RMSElog: 6.3657
[epoch  2][iter  480] loss: 6.4533 RMSElog: 6.4533
[epoch  2][iter  490] loss: 7.0705 RMSElog: 7.0705
[epoch  2][iter  500] loss: 5.6810 RMSElog: 5.6810
[epoch  2][iter  510] loss: 6.1707 RMSElog: 6.1707
[epoch  2][iter  520] loss: 5.8095 RMSElog: 5.8095
[epoch  2][iter  530] loss: 6.3105 RMSElog: 6.3105
[epoch  2][iter  540] loss: 6.8124 RMSElog: 6.8124
[epoch  2][iter  550] loss: 6.6085 RMSElog: 6.6085
[epoch  2][iter  560] loss: 6.6983 RMSElog: 6.6983
[epoch  2][iter  570] loss: 7.0198 RMSElog: 7.0198
[epoch  2][iter  580] loss: 6.5727 RMSElog: 6.5727
[epoch  2][iter  590] loss: 6.4449 RMSElog: 6.4449
[epoch  3][iter    0] loss: 5.8509 RMSElog: 5.8509
[epoch  3][iter   10] loss: 6.2176 RMSElog: 6.2176
[epoch  3][iter   20] loss: 5.9773 RMSElog: 5.9773
[epoch  3][iter   30] loss: 5.6812 RMSElog: 5.6812
[epoch  3][iter   40] loss: 6.0876 RMSElog: 6.0876
[epoch  3][iter   50] loss: 6.5299 RMSElog: 6.5299
[epoch  3][iter   60] loss: 6.1900 RMSElog: 6.1900
[epoch  3][iter   70] loss: 6.6523 RMSElog: 6.6523
[epoch  3][iter   80] loss: 5.2330 RMSElog: 5.2330
[epoch  3][iter   90] loss: 6.7671 RMSElog: 6.7671
[epoch  3][iter  100] loss: 6.6756 RMSElog: 6.6756
[epoch  3][iter  110] loss: 5.8417 RMSElog: 5.8417
[epoch  3][iter  120] loss: 6.6442 RMSElog: 6.6442
[epoch  3][iter  130] loss: 6.4503 RMSElog: 6.4503
[epoch  3][iter  140] loss: 6.5526 RMSElog: 6.5526
[epoch  3][iter  150] loss: 6.4071 RMSElog: 6.4071
[epoch  3][iter  160] loss: 5.3702 RMSElog: 5.3702
[epoch  3][iter  170] loss: 6.2424 RMSElog: 6.2424
[epoch  3][iter  180] loss: 6.4505 RMSElog: 6.4505
[epoch  3][iter  190] loss: 5.9396 RMSElog: 5.9396
[epoch  3][iter  200] loss: 6.3795 RMSElog: 6.3795
[epoch  3][iter  210] loss: 6.0349 RMSElog: 6.0349
[epoch  3][iter  220] loss: 6.7051 RMSElog: 6.7051
[epoch  3][iter  230] loss: 7.2036 RMSElog: 7.2036
[epoch  3][iter  240] loss: 6.3392 RMSElog: 6.3392
[epoch  3][iter  250] loss: 5.9701 RMSElog: 5.9701
[epoch  3][iter  260] loss: 5.8554 RMSElog: 5.8554
[epoch  3][iter  270] loss: 5.8658 RMSElog: 5.8658
[epoch  3][iter  280] loss: 6.0115 RMSElog: 6.0115
[epoch  3][iter  290] loss: 6.3392 RMSElog: 6.3392
[epoch  3][iter  300] loss: 6.1679 RMSElog: 6.1679
[epoch  3][iter  310] loss: 6.2996 RMSElog: 6.2996
[epoch  3][iter  320] loss: 6.6005 RMSElog: 6.6005
[epoch  3][iter  330] loss: 5.9336 RMSElog: 5.9336
[epoch  3][iter  340] loss: 6.0783 RMSElog: 6.0783
[epoch  3][iter  350] loss: 5.3547 RMSElog: 5.3547
[epoch  3][iter  360] loss: 6.5709 RMSElog: 6.5709
[epoch  3][iter  370] loss: 6.3922 RMSElog: 6.3922
[epoch  3][iter  380] loss: 6.0977 RMSElog: 6.0977
[epoch  3][iter  390] loss: 5.9527 RMSElog: 5.9527
[epoch  3][iter  400] loss: 5.5383 RMSElog: 5.5383
[epoch  3][iter  410] loss: 5.5845 RMSElog: 5.5845
[epoch  3][iter  420] loss: 6.2679 RMSElog: 6.2679
[epoch  3][iter  430] loss: 6.0146 RMSElog: 6.0146
[epoch  3][iter  440] loss: 6.5798 RMSElog: 6.5798
[epoch  3][iter  450] loss: 6.7771 RMSElog: 6.7771
[epoch  3][iter  460] loss: 6.4798 RMSElog: 6.4798
[epoch  3][iter  470] loss: 6.3647 RMSElog: 6.3647
[epoch  3][iter  480] loss: 6.4645 RMSElog: 6.4645
[epoch  3][iter  490] loss: 5.3346 RMSElog: 5.3346
[epoch  3][iter  500] loss: 5.7934 RMSElog: 5.7934
[epoch  3][iter  510] loss: 5.7560 RMSElog: 5.7560
[epoch  3][iter  520] loss: 5.4515 RMSElog: 5.4515
[epoch  3][iter  530] loss: 5.0626 RMSElog: 5.0626
[epoch  3][iter  540] loss: 5.3785 RMSElog: 5.3785
[epoch  3][iter  550] loss: 6.0467 RMSElog: 6.0467
[epoch  3][iter  560] loss: 6.0285 RMSElog: 6.0285
[epoch  3][iter  570] loss: 5.9869 RMSElog: 5.9869
[epoch  3][iter  580] loss: 5.7773 RMSElog: 5.7773
[epoch  3][iter  590] loss: 6.3359 RMSElog: 6.3359
[epoch  4][iter    0] loss: 5.8889 RMSElog: 5.8889
[epoch  4][iter   10] loss: 5.8654 RMSElog: 5.8654
[epoch  4][iter   20] loss: 4.9301 RMSElog: 4.9301
[epoch  4][iter   30] loss: 5.8983 RMSElog: 5.8983
[epoch  4][iter   40] loss: 5.0428 RMSElog: 5.0428
[epoch  4][iter   50] loss: 6.1129 RMSElog: 6.1129
[epoch  4][iter   60] loss: 5.5035 RMSElog: 5.5035
[epoch  4][iter   70] loss: 5.5684 RMSElog: 5.5684
[epoch  4][iter   80] loss: 5.0846 RMSElog: 5.0846
[epoch  4][iter   90] loss: 5.2444 RMSElog: 5.2444
[epoch  4][iter  100] loss: 5.9584 RMSElog: 5.9584
[epoch  4][iter  110] loss: 5.7273 RMSElog: 5.7273
[epoch  4][iter  120] loss: 5.3180 RMSElog: 5.3180
[epoch  4][iter  130] loss: 5.3687 RMSElog: 5.3687
[epoch  4][iter  140] loss: 5.7086 RMSElog: 5.7086
[epoch  4][iter  150] loss: 5.0586 RMSElog: 5.0586
[epoch  4][iter  160] loss: 6.2611 RMSElog: 6.2611
[epoch  4][iter  170] loss: 6.8299 RMSElog: 6.8299
[epoch  4][iter  180] loss: 6.4603 RMSElog: 6.4603
[epoch  4][iter  190] loss: 5.7881 RMSElog: 5.7881
[epoch  4][iter  200] loss: 5.5819 RMSElog: 5.5819
[epoch  4][iter  210] loss: 5.5846 RMSElog: 5.5846
[epoch  4][iter  220] loss: 5.8344 RMSElog: 5.8344
[epoch  4][iter  230] loss: 5.4310 RMSElog: 5.4310
[epoch  4][iter  240] loss: 5.8137 RMSElog: 5.8137
[epoch  4][iter  250] loss: 5.9839 RMSElog: 5.9839
[epoch  4][iter  260] loss: 5.3343 RMSElog: 5.3343
[epoch  4][iter  270] loss: 6.3482 RMSElog: 6.3482
[epoch  4][iter  280] loss: 6.4058 RMSElog: 6.4058
[epoch  4][iter  290] loss: 5.4841 RMSElog: 5.4841
[epoch  4][iter  300] loss: 5.2337 RMSElog: 5.2337
[epoch  4][iter  310] loss: 4.9118 RMSElog: 4.9118
[epoch  4][iter  320] loss: 5.7645 RMSElog: 5.7645
[epoch  4][iter  330] loss: 5.5266 RMSElog: 5.5266
[epoch  4][iter  340] loss: 5.3344 RMSElog: 5.3344
[epoch  4][iter  350] loss: 5.4837 RMSElog: 5.4837
[epoch  4][iter  360] loss: 4.9006 RMSElog: 4.9006
[epoch  4][iter  370] loss: 5.8876 RMSElog: 5.8876
[epoch  4][iter  380] loss: 5.1427 RMSElog: 5.1427
[epoch  4][iter  390] loss: 6.1341 RMSElog: 6.1341
[epoch  4][iter  400] loss: 6.2055 RMSElog: 6.2055
[epoch  4][iter  410] loss: 5.1847 RMSElog: 5.1847
[epoch  4][iter  420] loss: 5.4564 RMSElog: 5.4564
[epoch  4][iter  430] loss: 6.5207 RMSElog: 6.5207
[epoch  4][iter  440] loss: 5.2891 RMSElog: 5.2891
[epoch  4][iter  450] loss: 5.4950 RMSElog: 5.4950
[epoch  4][iter  460] loss: 5.9453 RMSElog: 5.9453
[epoch  4][iter  470] loss: 5.2715 RMSElog: 5.2715
[epoch  4][iter  480] loss: 5.2591 RMSElog: 5.2591
[epoch  4][iter  490] loss: 5.7533 RMSElog: 5.7533
[epoch  4][iter  500] loss: 5.0157 RMSElog: 5.0157
[epoch  4][iter  510] loss: 6.1139 RMSElog: 6.1139
[epoch  4][iter  520] loss: 5.2542 RMSElog: 5.2542
[epoch  4][iter  530] loss: 5.1317 RMSElog: 5.1317
[epoch  4][iter  540] loss: 6.6447 RMSElog: 6.6447
[epoch  4][iter  550] loss: 6.1690 RMSElog: 6.1690
[epoch  4][iter  560] loss: 6.0823 RMSElog: 6.0823
[epoch  4][iter  570] loss: 5.6717 RMSElog: 5.6717
[epoch  4][iter  580] loss: 5.1353 RMSElog: 5.1353
[epoch  4][iter  590] loss: 5.4790 RMSElog: 5.4790
[epoch  5][iter    0] loss: 5.8845 RMSElog: 5.8845
[epoch  5][iter   10] loss: 6.6788 RMSElog: 6.6788
[epoch  5][iter   20] loss: 5.6769 RMSElog: 5.6769
[epoch  5][iter   30] loss: 5.5570 RMSElog: 5.5570
[epoch  5][iter   40] loss: 6.5364 RMSElog: 6.5364
[epoch  5][iter   50] loss: 5.9311 RMSElog: 5.9311
[epoch  5][iter   60] loss: 5.8388 RMSElog: 5.8388
[epoch  5][iter   70] loss: 4.7880 RMSElog: 4.7880
[epoch  5][iter   80] loss: 5.3578 RMSElog: 5.3578
[epoch  5][iter   90] loss: 4.7750 RMSElog: 4.7750
[epoch  5][iter  100] loss: 5.3033 RMSElog: 5.3033
[epoch  5][iter  110] loss: 5.6372 RMSElog: 5.6372
[epoch  5][iter  120] loss: 4.8735 RMSElog: 4.8735
[epoch  5][iter  130] loss: 4.9960 RMSElog: 4.9960
[epoch  5][iter  140] loss: 6.1246 RMSElog: 6.1246
[epoch  5][iter  150] loss: 4.8597 RMSElog: 4.8597
[epoch  5][iter  160] loss: 6.2863 RMSElog: 6.2863
[epoch  5][iter  170] loss: 6.0011 RMSElog: 6.0011
[epoch  5][iter  180] loss: 4.7306 RMSElog: 4.7306
[epoch  5][iter  190] loss: 5.2717 RMSElog: 5.2717
[epoch  5][iter  200] loss: 4.8812 RMSElog: 4.8812
[epoch  5][iter  210] loss: 4.9765 RMSElog: 4.9765
[epoch  5][iter  220] loss: 6.0685 RMSElog: 6.0685
[epoch  5][iter  230] loss: 4.7499 RMSElog: 4.7499
[epoch  5][iter  240] loss: 5.4870 RMSElog: 5.4870
[epoch  5][iter  250] loss: 5.3619 RMSElog: 5.3619
[epoch  5][iter  260] loss: 5.6187 RMSElog: 5.6187
[epoch  5][iter  270] loss: 5.4170 RMSElog: 5.4170
[epoch  5][iter  280] loss: 5.2419 RMSElog: 5.2419
[epoch  5][iter  290] loss: 5.5632 RMSElog: 5.5632
[epoch  5][iter  300] loss: 6.1310 RMSElog: 6.1310
[epoch  5][iter  310] loss: 5.3224 RMSElog: 5.3224
[epoch  5][iter  320] loss: 4.9865 RMSElog: 4.9865
[epoch  5][iter  330] loss: 5.0078 RMSElog: 5.0078
[epoch  5][iter  340] loss: 6.4980 RMSElog: 6.4980
[epoch  5][iter  350] loss: 5.1852 RMSElog: 5.1852
[epoch  5][iter  360] loss: 4.9522 RMSElog: 4.9522
[epoch  5][iter  370] loss: 5.3302 RMSElog: 5.3302
[epoch  5][iter  380] loss: 5.5639 RMSElog: 5.5639
[epoch  5][iter  390] loss: 5.2653 RMSElog: 5.2653
[epoch  5][iter  400] loss: 5.0896 RMSElog: 5.0896
[epoch  5][iter  410] loss: 5.7764 RMSElog: 5.7764
[epoch  5][iter  420] loss: 5.5240 RMSElog: 5.5240
[epoch  5][iter  430] loss: 5.5732 RMSElog: 5.5732
[epoch  5][iter  440] loss: 5.4083 RMSElog: 5.4083
[epoch  5][iter  450] loss: 5.2641 RMSElog: 5.2641
[epoch  5][iter  460] loss: 4.9951 RMSElog: 4.9951
[epoch  5][iter  470] loss: 4.5465 RMSElog: 4.5465
[epoch  5][iter  480] loss: 4.8984 RMSElog: 4.8984
[epoch  5][iter  490] loss: 5.4183 RMSElog: 5.4183
[epoch  5][iter  500] loss: 4.9182 RMSElog: 4.9182
[epoch  5][iter  510] loss: 5.5544 RMSElog: 5.5544
[epoch  5][iter  520] loss: 5.4797 RMSElog: 5.4797
[epoch  5][iter  530] loss: 5.3166 RMSElog: 5.3166
[epoch  5][iter  540] loss: 5.7691 RMSElog: 5.7691
[epoch  5][iter  550] loss: 5.3662 RMSElog: 5.3662
[epoch  5][iter  560] loss: 5.3721 RMSElog: 5.3721
[epoch  5][iter  570] loss: 5.4676 RMSElog: 5.4676
[epoch  5][iter  580] loss: 6.0825 RMSElog: 6.0825
[epoch  5][iter  590] loss: 5.2124 RMSElog: 5.2124
[epoch  6][iter    0] loss: 5.8748 RMSElog: 5.8748
[epoch  6][iter   10] loss: 5.6863 RMSElog: 5.6863
[epoch  6][iter   20] loss: 5.3929 RMSElog: 5.3929
[epoch  6][iter   30] loss: 4.7016 RMSElog: 4.7016
[epoch  6][iter   40] loss: 5.1751 RMSElog: 5.1751
[epoch  6][iter   50] loss: 6.0179 RMSElog: 6.0179
[epoch  6][iter   60] loss: 5.1298 RMSElog: 5.1298
[epoch  6][iter   70] loss: 6.2282 RMSElog: 6.2282
[epoch  6][iter   80] loss: 5.2371 RMSElog: 5.2371
[epoch  6][iter   90] loss: 5.1916 RMSElog: 5.1916
[epoch  6][iter  100] loss: 5.3158 RMSElog: 5.3158
[epoch  6][iter  110] loss: 5.4808 RMSElog: 5.4808
[epoch  6][iter  120] loss: 5.3553 RMSElog: 5.3553
[epoch  6][iter  130] loss: 5.2600 RMSElog: 5.2600
[epoch  6][iter  140] loss: 4.5350 RMSElog: 4.5350
[epoch  6][iter  150] loss: 5.9625 RMSElog: 5.9625
[epoch  6][iter  160] loss: 5.9075 RMSElog: 5.9075
[epoch  6][iter  170] loss: 6.4994 RMSElog: 6.4994
[epoch  6][iter  180] loss: 6.4885 RMSElog: 6.4885
[epoch  6][iter  190] loss: 4.6669 RMSElog: 4.6669
[epoch  6][iter  200] loss: 5.0386 RMSElog: 5.0386
[epoch  6][iter  210] loss: 5.6783 RMSElog: 5.6783
[epoch  6][iter  220] loss: 5.4454 RMSElog: 5.4454
[epoch  6][iter  230] loss: 4.9777 RMSElog: 4.9777
[epoch  6][iter  240] loss: 5.5341 RMSElog: 5.5341
[epoch  6][iter  250] loss: 5.0448 RMSElog: 5.0448
[epoch  6][iter  260] loss: 5.5033 RMSElog: 5.5033
[epoch  6][iter  270] loss: 4.8217 RMSElog: 4.8217
[epoch  6][iter  280] loss: 5.4522 RMSElog: 5.4522
[epoch  6][iter  290] loss: 5.3656 RMSElog: 5.3656
[epoch  6][iter  300] loss: 5.3307 RMSElog: 5.3307
[epoch  6][iter  310] loss: 5.3421 RMSElog: 5.3421
[epoch  6][iter  320] loss: 6.0762 RMSElog: 6.0762
[epoch  6][iter  330] loss: 5.7088 RMSElog: 5.7088
[epoch  6][iter  340] loss: 5.9564 RMSElog: 5.9564
[epoch  6][iter  350] loss: 5.5293 RMSElog: 5.5293
[epoch  6][iter  360] loss: 4.6295 RMSElog: 4.6295
[epoch  6][iter  370] loss: 5.3841 RMSElog: 5.3841
[epoch  6][iter  380] loss: 5.0979 RMSElog: 5.0979
[epoch  6][iter  390] loss: 5.7361 RMSElog: 5.7361
[epoch  6][iter  400] loss: 5.3000 RMSElog: 5.3000
[epoch  6][iter  410] loss: 4.8435 RMSElog: 4.8435
[epoch  6][iter  420] loss: 5.1110 RMSElog: 5.1110
[epoch  6][iter  430] loss: 5.0368 RMSElog: 5.0368
[epoch  6][iter  440] loss: 4.9258 RMSElog: 4.9258
[epoch  6][iter  450] loss: 5.7526 RMSElog: 5.7526
[epoch  6][iter  460] loss: 4.9877 RMSElog: 4.9877
[epoch  6][iter  470] loss: 5.4423 RMSElog: 5.4423
[epoch  6][iter  480] loss: 4.7560 RMSElog: 4.7560
[epoch  6][iter  490] loss: 5.5858 RMSElog: 5.5858
[epoch  6][iter  500] loss: 5.5299 RMSElog: 5.5299
[epoch  6][iter  510] loss: 6.1480 RMSElog: 6.1480
[epoch  6][iter  520] loss: 5.3896 RMSElog: 5.3896
[epoch  6][iter  530] loss: 5.8066 RMSElog: 5.8066
[epoch  6][iter  540] loss: 5.7164 RMSElog: 5.7164
[epoch  6][iter  550] loss: 5.3479 RMSElog: 5.3479
[epoch  6][iter  560] loss: 5.0701 RMSElog: 5.0701
[epoch  6][iter  570] loss: 4.9389 RMSElog: 4.9389
[epoch  6][iter  580] loss: 6.2704 RMSElog: 6.2704
[epoch  6][iter  590] loss: 5.3301 RMSElog: 5.3301
[epoch  7][iter    0] loss: 5.6575 RMSElog: 5.6575
[epoch  7][iter   10] loss: 5.0908 RMSElog: 5.0908
[epoch  7][iter   20] loss: 4.2036 RMSElog: 4.2036
[epoch  7][iter   30] loss: 5.6406 RMSElog: 5.6406
[epoch  7][iter   40] loss: 6.0489 RMSElog: 6.0489
[epoch  7][iter   50] loss: 5.4176 RMSElog: 5.4176
[epoch  7][iter   60] loss: 4.8554 RMSElog: 4.8554
[epoch  7][iter   70] loss: 4.6558 RMSElog: 4.6558
[epoch  7][iter   80] loss: 5.1131 RMSElog: 5.1131
[epoch  7][iter   90] loss: 5.0718 RMSElog: 5.0718
[epoch  7][iter  100] loss: 5.0841 RMSElog: 5.0841
[epoch  7][iter  110] loss: 4.3355 RMSElog: 4.3355
[epoch  7][iter  120] loss: 6.1312 RMSElog: 6.1312
[epoch  7][iter  130] loss: 5.4750 RMSElog: 5.4750
[epoch  7][iter  140] loss: 6.0941 RMSElog: 6.0941
[epoch  7][iter  150] loss: 5.0057 RMSElog: 5.0057
[epoch  7][iter  160] loss: 5.1610 RMSElog: 5.1610
[epoch  7][iter  170] loss: 5.6168 RMSElog: 5.6168
[epoch  7][iter  180] loss: 5.7580 RMSElog: 5.7580
[epoch  7][iter  190] loss: 5.5077 RMSElog: 5.5077
[epoch  7][iter  200] loss: 4.7154 RMSElog: 4.7154
[epoch  7][iter  210] loss: 5.8834 RMSElog: 5.8834
[epoch  7][iter  220] loss: 6.4899 RMSElog: 6.4899
[epoch  7][iter  230] loss: 5.1442 RMSElog: 5.1442
[epoch  7][iter  240] loss: 4.7788 RMSElog: 4.7788
[epoch  7][iter  250] loss: 5.5818 RMSElog: 5.5818
[epoch  7][iter  260] loss: 5.5872 RMSElog: 5.5872
[epoch  7][iter  270] loss: 4.7382 RMSElog: 4.7382
[epoch  7][iter  280] loss: 5.1918 RMSElog: 5.1918
[epoch  7][iter  290] loss: 5.2852 RMSElog: 5.2852
[epoch  7][iter  300] loss: 4.7715 RMSElog: 4.7715
[epoch  7][iter  310] loss: 5.6441 RMSElog: 5.6441
[epoch  7][iter  320] loss: 5.1338 RMSElog: 5.1338
[epoch  7][iter  330] loss: 5.3362 RMSElog: 5.3362
[epoch  7][iter  340] loss: 5.3647 RMSElog: 5.3647
[epoch  7][iter  350] loss: 5.0406 RMSElog: 5.0406
[epoch  7][iter  360] loss: 4.7140 RMSElog: 4.7140
[epoch  7][iter  370] loss: 4.9758 RMSElog: 4.9758
[epoch  7][iter  380] loss: 4.2747 RMSElog: 4.2747
[epoch  7][iter  390] loss: 4.9507 RMSElog: 4.9507
[epoch  7][iter  400] loss: 5.3671 RMSElog: 5.3671
[epoch  7][iter  410] loss: 4.9548 RMSElog: 4.9548
[epoch  7][iter  420] loss: 5.0664 RMSElog: 5.0664
[epoch  7][iter  430] loss: 5.2880 RMSElog: 5.2880
[epoch  7][iter  440] loss: 5.4258 RMSElog: 5.4258
[epoch  7][iter  450] loss: 5.6100 RMSElog: 5.6100
[epoch  7][iter  460] loss: 4.7523 RMSElog: 4.7523
[epoch  7][iter  470] loss: 5.3382 RMSElog: 5.3382
[epoch  7][iter  480] loss: 5.2306 RMSElog: 5.2306
[epoch  7][iter  490] loss: 4.7905 RMSElog: 4.7905
[epoch  7][iter  500] loss: 5.9175 RMSElog: 5.9175
[epoch  7][iter  510] loss: 5.1149 RMSElog: 5.1149
[epoch  7][iter  520] loss: 5.4274 RMSElog: 5.4274
[epoch  7][iter  530] loss: 5.1522 RMSElog: 5.1522
[epoch  7][iter  540] loss: 4.9262 RMSElog: 4.9262
[epoch  7][iter  550] loss: 4.5554 RMSElog: 4.5554
[epoch  7][iter  560] loss: 5.6224 RMSElog: 5.6224
[epoch  7][iter  570] loss: 5.9148 RMSElog: 5.9148
[epoch  7][iter  580] loss: 5.1409 RMSElog: 5.1409
[epoch  7][iter  590] loss: 5.7731 RMSElog: 5.7731
[epoch  8][iter    0] loss: 5.6512 RMSElog: 5.6512
[epoch  8][iter   10] loss: 4.6651 RMSElog: 4.6651
[epoch  8][iter   20] loss: 4.7231 RMSElog: 4.7231
[epoch  8][iter   30] loss: 4.7742 RMSElog: 4.7742
[epoch  8][iter   40] loss: 5.8329 RMSElog: 5.8329
[epoch  8][iter   50] loss: 5.2630 RMSElog: 5.2630
[epoch  8][iter   60] loss: 5.3914 RMSElog: 5.3914
[epoch  8][iter   70] loss: 5.0876 RMSElog: 5.0876
[epoch  8][iter   80] loss: 4.6864 RMSElog: 4.6864
[epoch  8][iter   90] loss: 4.5498 RMSElog: 4.5498
[epoch  8][iter  100] loss: 5.2033 RMSElog: 5.2033
[epoch  8][iter  110] loss: 5.3710 RMSElog: 5.3710
[epoch  8][iter  120] loss: 5.1057 RMSElog: 5.1057
[epoch  8][iter  130] loss: 6.0728 RMSElog: 6.0728
[epoch  8][iter  140] loss: 5.3689 RMSElog: 5.3689
[epoch  8][iter  150] loss: 5.3372 RMSElog: 5.3372
[epoch  8][iter  160] loss: 5.7135 RMSElog: 5.7135
[epoch  8][iter  170] loss: 4.8723 RMSElog: 4.8723
[epoch  8][iter  180] loss: 5.4649 RMSElog: 5.4649
[epoch  8][iter  190] loss: 4.4146 RMSElog: 4.4146
[epoch  8][iter  200] loss: 5.4429 RMSElog: 5.4429
[epoch  8][iter  210] loss: 5.0296 RMSElog: 5.0296
[epoch  8][iter  220] loss: 5.3545 RMSElog: 5.3545
[epoch  8][iter  230] loss: 5.0205 RMSElog: 5.0205
[epoch  8][iter  240] loss: 5.0953 RMSElog: 5.0953
[epoch  8][iter  250] loss: 5.3784 RMSElog: 5.3784
[epoch  8][iter  260] loss: 5.4324 RMSElog: 5.4324
[epoch  8][iter  270] loss: 5.0481 RMSElog: 5.0481
[epoch  8][iter  280] loss: 4.3923 RMSElog: 4.3923
[epoch  8][iter  290] loss: 5.8776 RMSElog: 5.8776
[epoch  8][iter  300] loss: 5.3745 RMSElog: 5.3745
[epoch  8][iter  310] loss: 5.0626 RMSElog: 5.0626
[epoch  8][iter  320] loss: 5.6017 RMSElog: 5.6017
[epoch  8][iter  330] loss: 5.6109 RMSElog: 5.6109
[epoch  8][iter  340] loss: 5.5878 RMSElog: 5.5878
[epoch  8][iter  350] loss: 5.4496 RMSElog: 5.4496
[epoch  8][iter  360] loss: 6.0625 RMSElog: 6.0625
[epoch  8][iter  370] loss: 5.4569 RMSElog: 5.4569
[epoch  8][iter  380] loss: 5.1265 RMSElog: 5.1265
[epoch  8][iter  390] loss: 5.0329 RMSElog: 5.0329
[epoch  8][iter  400] loss: 5.6624 RMSElog: 5.6624
[epoch  8][iter  410] loss: 4.9670 RMSElog: 4.9670
[epoch  8][iter  420] loss: 4.9673 RMSElog: 4.9673
[epoch  8][iter  430] loss: 5.2470 RMSElog: 5.2470
[epoch  8][iter  440] loss: 5.6622 RMSElog: 5.6622
[epoch  8][iter  450] loss: 4.4547 RMSElog: 4.4547
[epoch  8][iter  460] loss: 5.2736 RMSElog: 5.2736
[epoch  8][iter  470] loss: 4.9167 RMSElog: 4.9167
[epoch  8][iter  480] loss: 4.3655 RMSElog: 4.3655
[epoch  8][iter  490] loss: 5.7793 RMSElog: 5.7793
[epoch  8][iter  500] loss: 4.5864 RMSElog: 4.5864
[epoch  8][iter  510] loss: 5.6121 RMSElog: 5.6121
[epoch  8][iter  520] loss: 5.1803 RMSElog: 5.1803
[epoch  8][iter  530] loss: 4.9452 RMSElog: 4.9452
[epoch  8][iter  540] loss: 6.0648 RMSElog: 6.0648
[epoch  8][iter  550] loss: 5.2249 RMSElog: 5.2249
[epoch  8][iter  560] loss: 5.2249 RMSElog: 5.2249
[epoch  8][iter  570] loss: 5.4230 RMSElog: 5.4230
[epoch  8][iter  580] loss: 5.3365 RMSElog: 5.3365
[epoch  8][iter  590] loss: 6.2497 RMSElog: 6.2497
[epoch  9][iter    0] loss: 4.4624 RMSElog: 4.4624
[epoch  9][iter   10] loss: 5.3744 RMSElog: 5.3744
[epoch  9][iter   20] loss: 4.6820 RMSElog: 4.6820
[epoch  9][iter   30] loss: 4.7981 RMSElog: 4.7981
[epoch  9][iter   40] loss: 4.7261 RMSElog: 4.7261
[epoch  9][iter   50] loss: 6.2810 RMSElog: 6.2810
[epoch  9][iter   60] loss: 5.7345 RMSElog: 5.7345
[epoch  9][iter   70] loss: 5.1302 RMSElog: 5.1302
[epoch  9][iter   80] loss: 5.3267 RMSElog: 5.3267
[epoch  9][iter   90] loss: 6.0183 RMSElog: 6.0183
[epoch  9][iter  100] loss: 5.3862 RMSElog: 5.3862
[epoch  9][iter  110] loss: 6.2201 RMSElog: 6.2201
[epoch  9][iter  120] loss: 5.5102 RMSElog: 5.5102
[epoch  9][iter  130] loss: 4.9450 RMSElog: 4.9450
[epoch  9][iter  140] loss: 5.7732 RMSElog: 5.7732
[epoch  9][iter  150] loss: 5.4239 RMSElog: 5.4239
[epoch  9][iter  160] loss: 6.4592 RMSElog: 6.4592
[epoch  9][iter  170] loss: 5.3002 RMSElog: 5.3002
[epoch  9][iter  180] loss: 5.9972 RMSElog: 5.9972
[epoch  9][iter  190] loss: 6.1450 RMSElog: 6.1450
[epoch  9][iter  200] loss: 4.6276 RMSElog: 4.6276
[epoch  9][iter  210] loss: 5.8103 RMSElog: 5.8103
[epoch  9][iter  220] loss: 4.6828 RMSElog: 4.6828
[epoch  9][iter  230] loss: 5.4419 RMSElog: 5.4419
[epoch  9][iter  240] loss: 4.8405 RMSElog: 4.8405
[epoch  9][iter  250] loss: 6.1039 RMSElog: 6.1039
[epoch  9][iter  260] loss: 5.3057 RMSElog: 5.3057
[epoch  9][iter  270] loss: 5.6389 RMSElog: 5.6389
[epoch  9][iter  280] loss: 5.4018 RMSElog: 5.4018
[epoch  9][iter  290] loss: 5.9369 RMSElog: 5.9369
[epoch  9][iter  300] loss: 4.5183 RMSElog: 4.5183
[epoch  9][iter  310] loss: 4.8941 RMSElog: 4.8941
[epoch  9][iter  320] loss: 5.7548 RMSElog: 5.7548
[epoch  9][iter  330] loss: 4.5071 RMSElog: 4.5071
[epoch  9][iter  340] loss: 5.2936 RMSElog: 5.2936
[epoch  9][iter  350] loss: 5.9429 RMSElog: 5.9429
[epoch  9][iter  360] loss: 5.6341 RMSElog: 5.6341
[epoch  9][iter  370] loss: 5.1658 RMSElog: 5.1658
[epoch  9][iter  380] loss: 4.9711 RMSElog: 4.9711
[epoch  9][iter  390] loss: 4.6710 RMSElog: 4.6710
[epoch  9][iter  400] loss: 5.0793 RMSElog: 5.0793
[epoch  9][iter  410] loss: 5.3448 RMSElog: 5.3448
[epoch  9][iter  420] loss: 5.9426 RMSElog: 5.9426
[epoch  9][iter  430] loss: 5.5274 RMSElog: 5.5274
[epoch  9][iter  440] loss: 5.4922 RMSElog: 5.4922
[epoch  9][iter  450] loss: 5.9247 RMSElog: 5.9247
[epoch  9][iter  460] loss: 5.1645 RMSElog: 5.1645
[epoch  9][iter  470] loss: 5.0401 RMSElog: 5.0401
[epoch  9][iter  480] loss: 5.5552 RMSElog: 5.5552
[epoch  9][iter  490] loss: 4.3601 RMSElog: 4.3601
[epoch  9][iter  500] loss: 5.7358 RMSElog: 5.7358
[epoch  9][iter  510] loss: 5.3235 RMSElog: 5.3235
[epoch  9][iter  520] loss: 4.8049 RMSElog: 4.8049
[epoch  9][iter  530] loss: 4.5980 RMSElog: 4.5980
[epoch  9][iter  540] loss: 4.8902 RMSElog: 4.8902
[epoch  9][iter  550] loss: 5.0954 RMSElog: 5.0954
[epoch  9][iter  560] loss: 4.7635 RMSElog: 4.7635
[epoch  9][iter  570] loss: 5.8342 RMSElog: 5.8342
[epoch  9][iter  580] loss: 4.3816 RMSElog: 4.3816
[epoch  9][iter  590] loss: 5.6537 RMSElog: 5.6537
[epoch 10][iter    0] loss: 4.8801 RMSElog: 4.8801
[epoch 10][iter   10] loss: 5.7710 RMSElog: 5.7710
[epoch 10][iter   20] loss: 5.8264 RMSElog: 5.8264
[epoch 10][iter   30] loss: 5.5909 RMSElog: 5.5909
[epoch 10][iter   40] loss: 5.0416 RMSElog: 5.0416
[epoch 10][iter   50] loss: 4.9843 RMSElog: 4.9843
[epoch 10][iter   60] loss: 5.0778 RMSElog: 5.0778
[epoch 10][iter   70] loss: 6.0978 RMSElog: 6.0978
[epoch 10][iter   80] loss: 5.4350 RMSElog: 5.4350
[epoch 10][iter   90] loss: 5.0897 RMSElog: 5.0897
[epoch 10][iter  100] loss: 5.6425 RMSElog: 5.6425
[epoch 10][iter  110] loss: 4.3870 RMSElog: 4.3870
[epoch 10][iter  120] loss: 4.7652 RMSElog: 4.7652
[epoch 10][iter  130] loss: 5.5159 RMSElog: 5.5159
[epoch 10][iter  140] loss: 6.0702 RMSElog: 6.0702
[epoch 10][iter  150] loss: 5.6984 RMSElog: 5.6984
[epoch 10][iter  160] loss: 6.2937 RMSElog: 6.2937
[epoch 10][iter  170] loss: 5.6352 RMSElog: 5.6352
[epoch 10][iter  180] loss: 5.2330 RMSElog: 5.2330
[epoch 10][iter  190] loss: 4.8127 RMSElog: 4.8127
[epoch 10][iter  200] loss: 5.0450 RMSElog: 5.0450
[epoch 10][iter  210] loss: 5.0417 RMSElog: 5.0417
[epoch 10][iter  220] loss: 5.0406 RMSElog: 5.0406
[epoch 10][iter  230] loss: 5.3328 RMSElog: 5.3328
[epoch 10][iter  240] loss: 5.4301 RMSElog: 5.4301
[epoch 10][iter  250] loss: 4.4969 RMSElog: 4.4969
[epoch 10][iter  260] loss: 5.6904 RMSElog: 5.6904
[epoch 10][iter  270] loss: 5.3276 RMSElog: 5.3276
[epoch 10][iter  280] loss: 5.1250 RMSElog: 5.1250
[epoch 10][iter  290] loss: 5.8211 RMSElog: 5.8211
[epoch 10][iter  300] loss: 4.9708 RMSElog: 4.9708
[epoch 10][iter  310] loss: 5.4375 RMSElog: 5.4375
[epoch 10][iter  320] loss: 4.8375 RMSElog: 4.8375
[epoch 10][iter  330] loss: 5.3852 RMSElog: 5.3852
[epoch 10][iter  340] loss: 5.3185 RMSElog: 5.3185
[epoch 10][iter  350] loss: 5.4363 RMSElog: 5.4363
[epoch 10][iter  360] loss: 4.9883 RMSElog: 4.9883
[epoch 10][iter  370] loss: 4.8686 RMSElog: 4.8686
[epoch 10][iter  380] loss: 6.1794 RMSElog: 6.1794
[epoch 10][iter  390] loss: 5.3674 RMSElog: 5.3674
[epoch 10][iter  400] loss: 4.9396 RMSElog: 4.9396
[epoch 10][iter  410] loss: 4.6538 RMSElog: 4.6538
[epoch 10][iter  420] loss: 4.7357 RMSElog: 4.7357
[epoch 10][iter  430] loss: 5.6440 RMSElog: 5.6440
[epoch 10][iter  440] loss: 5.6270 RMSElog: 5.6270
[epoch 10][iter  450] loss: 4.7464 RMSElog: 4.7464
[epoch 10][iter  460] loss: 5.1695 RMSElog: 5.1695
[epoch 10][iter  470] loss: 5.6529 RMSElog: 5.6529
[epoch 10][iter  480] loss: 5.1069 RMSElog: 5.1069
[epoch 10][iter  490] loss: 5.2475 RMSElog: 5.2475
[epoch 10][iter  500] loss: 5.7482 RMSElog: 5.7482
[epoch 10][iter  510] loss: 4.5773 RMSElog: 4.5773
[epoch 10][iter  520] loss: 5.1271 RMSElog: 5.1271
[epoch 10][iter  530] loss: 5.0791 RMSElog: 5.0791
[epoch 10][iter  540] loss: 5.4934 RMSElog: 5.4934
[epoch 10][iter  550] loss: 5.3222 RMSElog: 5.3222
[epoch 10][iter  560] loss: 6.6811 RMSElog: 6.6811
[epoch 10][iter  570] loss: 5.0656 RMSElog: 5.0656
[epoch 10][iter  580] loss: 4.9511 RMSElog: 4.9511
[epoch 10][iter  590] loss: 5.4751 RMSElog: 5.4751
[epoch 11][iter    0] loss: 4.7611 RMSElog: 4.7611
[epoch 11][iter   10] loss: 5.6155 RMSElog: 5.6155
[epoch 11][iter   20] loss: 4.5044 RMSElog: 4.5044
[epoch 11][iter   30] loss: 5.0132 RMSElog: 5.0132
[epoch 11][iter   40] loss: 5.3972 RMSElog: 5.3972
[epoch 11][iter   50] loss: 4.9467 RMSElog: 4.9467
[epoch 11][iter   60] loss: 5.5143 RMSElog: 5.5143
[epoch 11][iter   70] loss: 5.6337 RMSElog: 5.6337
[epoch 11][iter   80] loss: 5.2419 RMSElog: 5.2419
[epoch 11][iter   90] loss: 4.9361 RMSElog: 4.9361
[epoch 11][iter  100] loss: 5.1421 RMSElog: 5.1421
[epoch 11][iter  110] loss: 5.6960 RMSElog: 5.6960
[epoch 11][iter  120] loss: 5.6304 RMSElog: 5.6304
[epoch 11][iter  130] loss: 6.1048 RMSElog: 6.1048
[epoch 11][iter  140] loss: 5.3895 RMSElog: 5.3895
[epoch 11][iter  150] loss: 6.0012 RMSElog: 6.0012
[epoch 11][iter  160] loss: 4.9990 RMSElog: 4.9990
[epoch 11][iter  170] loss: 5.5299 RMSElog: 5.5299
[epoch 11][iter  180] loss: 5.2955 RMSElog: 5.2955
[epoch 11][iter  190] loss: 4.9291 RMSElog: 4.9291
[epoch 11][iter  200] loss: 5.8733 RMSElog: 5.8733
[epoch 11][iter  210] loss: 5.3578 RMSElog: 5.3578
[epoch 11][iter  220] loss: 5.2384 RMSElog: 5.2384
[epoch 11][iter  230] loss: 4.7014 RMSElog: 4.7014
[epoch 11][iter  240] loss: 5.3247 RMSElog: 5.3247
[epoch 11][iter  250] loss: 6.2524 RMSElog: 6.2524
[epoch 11][iter  260] loss: 4.7302 RMSElog: 4.7302
[epoch 11][iter  270] loss: 5.2226 RMSElog: 5.2226
[epoch 11][iter  280] loss: 5.3677 RMSElog: 5.3677
[epoch 11][iter  290] loss: 5.4159 RMSElog: 5.4159
[epoch 11][iter  300] loss: 5.3113 RMSElog: 5.3113
[epoch 11][iter  310] loss: 6.1765 RMSElog: 6.1765
[epoch 11][iter  320] loss: 5.4642 RMSElog: 5.4642
[epoch 11][iter  330] loss: 4.5553 RMSElog: 4.5553
[epoch 11][iter  340] loss: 6.0555 RMSElog: 6.0555
[epoch 11][iter  350] loss: 4.5685 RMSElog: 4.5685
[epoch 11][iter  360] loss: 5.6135 RMSElog: 5.6135
[epoch 11][iter  370] loss: 5.0450 RMSElog: 5.0450
[epoch 11][iter  380] loss: 5.2640 RMSElog: 5.2640
[epoch 11][iter  390] loss: 5.2885 RMSElog: 5.2885
[epoch 11][iter  400] loss: 5.2621 RMSElog: 5.2621
[epoch 11][iter  410] loss: 5.6915 RMSElog: 5.6915
[epoch 11][iter  420] loss: 5.3152 RMSElog: 5.3152
[epoch 11][iter  430] loss: 5.6669 RMSElog: 5.6669
[epoch 11][iter  440] loss: 4.7494 RMSElog: 4.7494
[epoch 11][iter  450] loss: 5.8521 RMSElog: 5.8521
[epoch 11][iter  460] loss: 4.5569 RMSElog: 4.5569
[epoch 11][iter  470] loss: 4.0979 RMSElog: 4.0979
[epoch 11][iter  480] loss: 6.0479 RMSElog: 6.0479
[epoch 11][iter  490] loss: 5.4894 RMSElog: 5.4894
[epoch 11][iter  500] loss: 5.6450 RMSElog: 5.6450
[epoch 11][iter  510] loss: 5.3679 RMSElog: 5.3679
[epoch 11][iter  520] loss: 4.8184 RMSElog: 4.8184
[epoch 11][iter  530] loss: 6.1960 RMSElog: 6.1960
[epoch 11][iter  540] loss: 5.5735 RMSElog: 5.5735
[epoch 11][iter  550] loss: 5.5131 RMSElog: 5.5131
[epoch 11][iter  560] loss: 5.2493 RMSElog: 5.2493
[epoch 11][iter  570] loss: 5.6854 RMSElog: 5.6854
[epoch 11][iter  580] loss: 5.1575 RMSElog: 5.1575
[epoch 11][iter  590] loss: 4.8681 RMSElog: 4.8681
[epoch 12][iter    0] loss: 5.2888 RMSElog: 5.2888
[epoch 12][iter   10] loss: 6.2737 RMSElog: 6.2737
[epoch 12][iter   20] loss: 5.1946 RMSElog: 5.1946
[epoch 12][iter   30] loss: 5.9814 RMSElog: 5.9814
[epoch 12][iter   40] loss: 4.8953 RMSElog: 4.8953
[epoch 12][iter   50] loss: 5.2523 RMSElog: 5.2523
[epoch 12][iter   60] loss: 5.0580 RMSElog: 5.0580
[epoch 12][iter   70] loss: 4.7310 RMSElog: 4.7310
[epoch 12][iter   80] loss: 5.3414 RMSElog: 5.3414
[epoch 12][iter   90] loss: 5.0983 RMSElog: 5.0983
[epoch 12][iter  100] loss: 5.2445 RMSElog: 5.2445
[epoch 12][iter  110] loss: 5.0090 RMSElog: 5.0090
[epoch 12][iter  120] loss: 4.9252 RMSElog: 4.9252
[epoch 12][iter  130] loss: 4.9334 RMSElog: 4.9334
[epoch 12][iter  140] loss: 5.0656 RMSElog: 5.0656
[epoch 12][iter  150] loss: 4.7628 RMSElog: 4.7628
[epoch 12][iter  160] loss: 4.9628 RMSElog: 4.9628
[epoch 12][iter  170] loss: 5.0892 RMSElog: 5.0892
[epoch 12][iter  180] loss: 4.8797 RMSElog: 4.8797
[epoch 12][iter  190] loss: 5.3587 RMSElog: 5.3587
[epoch 12][iter  200] loss: 5.3405 RMSElog: 5.3405
[epoch 12][iter  210] loss: 4.4624 RMSElog: 4.4624
[epoch 12][iter  220] loss: 6.1511 RMSElog: 6.1511
[epoch 12][iter  230] loss: 4.9624 RMSElog: 4.9624
[epoch 12][iter  240] loss: 5.3031 RMSElog: 5.3031
[epoch 12][iter  250] loss: 5.4587 RMSElog: 5.4587
[epoch 12][iter  260] loss: 5.3009 RMSElog: 5.3009
[epoch 12][iter  270] loss: 5.4409 RMSElog: 5.4409
[epoch 12][iter  280] loss: 5.0247 RMSElog: 5.0247
[epoch 12][iter  290] loss: 4.9236 RMSElog: 4.9236
[epoch 12][iter  300] loss: 5.2286 RMSElog: 5.2286
[epoch 12][iter  310] loss: 4.7436 RMSElog: 4.7436
[epoch 12][iter  320] loss: 4.9550 RMSElog: 4.9550
[epoch 12][iter  330] loss: 4.6728 RMSElog: 4.6728
[epoch 12][iter  340] loss: 5.4386 RMSElog: 5.4386
[epoch 12][iter  350] loss: 5.0266 RMSElog: 5.0266
[epoch 12][iter  360] loss: 5.2548 RMSElog: 5.2548
[epoch 12][iter  370] loss: 5.8835 RMSElog: 5.8835
[epoch 12][iter  380] loss: 6.1214 RMSElog: 6.1214
[epoch 12][iter  390] loss: 4.9371 RMSElog: 4.9371
[epoch 12][iter  400] loss: 4.5521 RMSElog: 4.5521
[epoch 12][iter  410] loss: 5.0145 RMSElog: 5.0145
[epoch 12][iter  420] loss: 5.3107 RMSElog: 5.3107
[epoch 12][iter  430] loss: 5.5212 RMSElog: 5.5212
[epoch 12][iter  440] loss: 5.7621 RMSElog: 5.7621
[epoch 12][iter  450] loss: 5.6235 RMSElog: 5.6235
[epoch 12][iter  460] loss: 5.1035 RMSElog: 5.1035
[epoch 12][iter  470] loss: 5.3051 RMSElog: 5.3051
[epoch 12][iter  480] loss: 4.6775 RMSElog: 4.6775
[epoch 12][iter  490] loss: 5.8294 RMSElog: 5.8294
[epoch 12][iter  500] loss: 5.5409 RMSElog: 5.5409
[epoch 12][iter  510] loss: 5.3821 RMSElog: 5.3821
[epoch 12][iter  520] loss: 5.5083 RMSElog: 5.5083
[epoch 12][iter  530] loss: 4.8260 RMSElog: 4.8260
[epoch 12][iter  540] loss: 4.5045 RMSElog: 4.5045
[epoch 12][iter  550] loss: 5.0243 RMSElog: 5.0243
[epoch 12][iter  560] loss: 6.2049 RMSElog: 6.2049
[epoch 12][iter  570] loss: 5.3858 RMSElog: 5.3858
[epoch 12][iter  580] loss: 4.5410 RMSElog: 4.5410
[epoch 12][iter  590] loss: 5.6742 RMSElog: 5.6742
[epoch 13][iter    0] loss: 4.6164 RMSElog: 4.6164
[epoch 13][iter   10] loss: 4.6831 RMSElog: 4.6831
[epoch 13][iter   20] loss: 5.9863 RMSElog: 5.9863
[epoch 13][iter   30] loss: 4.7693 RMSElog: 4.7693
[epoch 13][iter   40] loss: 5.5582 RMSElog: 5.5582
[epoch 13][iter   50] loss: 5.2624 RMSElog: 5.2624
[epoch 13][iter   60] loss: 5.0556 RMSElog: 5.0556
[epoch 13][iter   70] loss: 4.8709 RMSElog: 4.8709
[epoch 13][iter   80] loss: 5.0543 RMSElog: 5.0543
[epoch 13][iter   90] loss: 6.2234 RMSElog: 6.2234
[epoch 13][iter  100] loss: 4.8421 RMSElog: 4.8421
[epoch 13][iter  110] loss: 5.4651 RMSElog: 5.4651
[epoch 13][iter  120] loss: 5.4065 RMSElog: 5.4065
[epoch 13][iter  130] loss: 4.9955 RMSElog: 4.9955
[epoch 13][iter  140] loss: 5.0959 RMSElog: 5.0959
[epoch 13][iter  150] loss: 5.7970 RMSElog: 5.7970
[epoch 13][iter  160] loss: 5.2968 RMSElog: 5.2968
[epoch 13][iter  170] loss: 5.6234 RMSElog: 5.6234
[epoch 13][iter  180] loss: 5.2404 RMSElog: 5.2404
[epoch 13][iter  190] loss: 5.1020 RMSElog: 5.1020
[epoch 13][iter  200] loss: 4.9391 RMSElog: 4.9391
[epoch 13][iter  210] loss: 4.8758 RMSElog: 4.8758
[epoch 13][iter  220] loss: 5.4398 RMSElog: 5.4398
[epoch 13][iter  230] loss: 6.1957 RMSElog: 6.1957
[epoch 13][iter  240] loss: 5.0306 RMSElog: 5.0306
[epoch 13][iter  250] loss: 5.5955 RMSElog: 5.5955
[epoch 13][iter  260] loss: 5.8570 RMSElog: 5.8570
[epoch 13][iter  270] loss: 5.3840 RMSElog: 5.3840
[epoch 13][iter  280] loss: 5.4672 RMSElog: 5.4672
[epoch 13][iter  290] loss: 5.7603 RMSElog: 5.7603
[epoch 13][iter  300] loss: 4.9940 RMSElog: 4.9940
[epoch 13][iter  310] loss: 4.4560 RMSElog: 4.4560
[epoch 13][iter  320] loss: 5.5002 RMSElog: 5.5002
[epoch 13][iter  330] loss: 4.8489 RMSElog: 4.8489
[epoch 13][iter  340] loss: 4.4589 RMSElog: 4.4589
[epoch 13][iter  350] loss: 4.8257 RMSElog: 4.8257
[epoch 13][iter  360] loss: 5.1703 RMSElog: 5.1703
[epoch 13][iter  370] loss: 5.6682 RMSElog: 5.6682
[epoch 13][iter  380] loss: 4.9018 RMSElog: 4.9018
[epoch 13][iter  390] loss: 4.9831 RMSElog: 4.9831
[epoch 13][iter  400] loss: 5.3138 RMSElog: 5.3138
[epoch 13][iter  410] loss: 4.6853 RMSElog: 4.6853
[epoch 13][iter  420] loss: 5.5348 RMSElog: 5.5348
[epoch 13][iter  430] loss: 6.0907 RMSElog: 6.0907
[epoch 13][iter  440] loss: 4.7458 RMSElog: 4.7458
[epoch 13][iter  450] loss: 5.7401 RMSElog: 5.7401
[epoch 13][iter  460] loss: 4.9993 RMSElog: 4.9993
[epoch 13][iter  470] loss: 4.6555 RMSElog: 4.6555
[epoch 13][iter  480] loss: 5.4084 RMSElog: 5.4084
[epoch 13][iter  490] loss: 4.3585 RMSElog: 4.3585
[epoch 13][iter  500] loss: 5.4886 RMSElog: 5.4886
[epoch 13][iter  510] loss: 5.5798 RMSElog: 5.5798
[epoch 13][iter  520] loss: 6.2616 RMSElog: 6.2616
[epoch 13][iter  530] loss: 4.9137 RMSElog: 4.9137
[epoch 13][iter  540] loss: 5.3534 RMSElog: 5.3534
[epoch 13][iter  550] loss: 5.3692 RMSElog: 5.3692
[epoch 13][iter  560] loss: 4.7913 RMSElog: 4.7913
[epoch 13][iter  570] loss: 5.2594 RMSElog: 5.2594
[epoch 13][iter  580] loss: 5.2758 RMSElog: 5.2758
[epoch 13][iter  590] loss: 4.9131 RMSElog: 4.9131
[epoch 14][iter    0] loss: 5.4003 RMSElog: 5.4003
[epoch 14][iter   10] loss: 5.5118 RMSElog: 5.5118
[epoch 14][iter   20] loss: 5.7569 RMSElog: 5.7569
[epoch 14][iter   30] loss: 4.6088 RMSElog: 4.6088
[epoch 14][iter   40] loss: 5.2910 RMSElog: 5.2910
[epoch 14][iter   50] loss: 5.0185 RMSElog: 5.0185
[epoch 14][iter   60] loss: 4.8342 RMSElog: 4.8342
[epoch 14][iter   70] loss: 5.7129 RMSElog: 5.7129
[epoch 14][iter   80] loss: 4.5874 RMSElog: 4.5874
[epoch 14][iter   90] loss: 5.8976 RMSElog: 5.8976
[epoch 14][iter  100] loss: 5.2949 RMSElog: 5.2949
[epoch 14][iter  110] loss: 4.9990 RMSElog: 4.9990
[epoch 14][iter  120] loss: 4.3734 RMSElog: 4.3734
[epoch 14][iter  130] loss: 6.0985 RMSElog: 6.0985
[epoch 14][iter  140] loss: 5.1114 RMSElog: 5.1114
[epoch 14][iter  150] loss: 5.7185 RMSElog: 5.7185
[epoch 14][iter  160] loss: 6.1198 RMSElog: 6.1198
[epoch 14][iter  170] loss: 5.3107 RMSElog: 5.3107
[epoch 14][iter  180] loss: 5.3570 RMSElog: 5.3570
[epoch 14][iter  190] loss: 5.2430 RMSElog: 5.2430
[epoch 14][iter  200] loss: 4.6860 RMSElog: 4.6860
[epoch 14][iter  210] loss: 5.5810 RMSElog: 5.5810
[epoch 14][iter  220] loss: 5.3627 RMSElog: 5.3627
[epoch 14][iter  230] loss: 5.7383 RMSElog: 5.7383
[epoch 14][iter  240] loss: 4.9772 RMSElog: 4.9772
[epoch 14][iter  250] loss: 4.8748 RMSElog: 4.8748
[epoch 14][iter  260] loss: 5.5998 RMSElog: 5.5998
[epoch 14][iter  270] loss: 4.9928 RMSElog: 4.9928
[epoch 14][iter  280] loss: 4.7140 RMSElog: 4.7140
[epoch 14][iter  290] loss: 4.3800 RMSElog: 4.3800
[epoch 14][iter  300] loss: 4.3271 RMSElog: 4.3271
[epoch 14][iter  310] loss: 6.1536 RMSElog: 6.1536
[epoch 14][iter  320] loss: 5.6211 RMSElog: 5.6211
[epoch 14][iter  330] loss: 5.7651 RMSElog: 5.7651
[epoch 14][iter  340] loss: 4.4552 RMSElog: 4.4552
[epoch 14][iter  350] loss: 6.2038 RMSElog: 6.2038
[epoch 14][iter  360] loss: 5.0059 RMSElog: 5.0059
[epoch 14][iter  370] loss: 4.7063 RMSElog: 4.7063
[epoch 14][iter  380] loss: 5.2939 RMSElog: 5.2939
[epoch 14][iter  390] loss: 5.4037 RMSElog: 5.4037
[epoch 14][iter  400] loss: 5.5216 RMSElog: 5.5216
[epoch 14][iter  410] loss: 5.0963 RMSElog: 5.0963
[epoch 14][iter  420] loss: 5.2753 RMSElog: 5.2753
[epoch 14][iter  430] loss: 5.2630 RMSElog: 5.2630
[epoch 14][iter  440] loss: 5.5418 RMSElog: 5.5418
[epoch 14][iter  450] loss: 6.1543 RMSElog: 6.1543
[epoch 14][iter  460] loss: 5.5551 RMSElog: 5.5551
[epoch 14][iter  470] loss: 5.8858 RMSElog: 5.8858
[epoch 14][iter  480] loss: 5.1664 RMSElog: 5.1664
[epoch 14][iter  490] loss: 5.4656 RMSElog: 5.4656
[epoch 14][iter  500] loss: 5.8648 RMSElog: 5.8648
[epoch 14][iter  510] loss: 5.5836 RMSElog: 5.5836
[epoch 14][iter  520] loss: 4.6611 RMSElog: 4.6611
[epoch 14][iter  530] loss: 5.4348 RMSElog: 5.4348
[epoch 14][iter  540] loss: 4.8436 RMSElog: 4.8436
[epoch 14][iter  550] loss: 5.1251 RMSElog: 5.1251
[epoch 14][iter  560] loss: 5.4194 RMSElog: 5.4194
[epoch 14][iter  570] loss: 5.4280 RMSElog: 5.4280
[epoch 14][iter  580] loss: 5.2094 RMSElog: 5.2094
[epoch 14][iter  590] loss: 4.6971 RMSElog: 4.6971
###########################
#T3 loss= depth_loss *10###
###########################
[epoch  0][iter    0] loss: 116.0597 RMSElog: 11.6060
[epoch  0][iter   10] loss: 121.2523 RMSElog: 12.1252
[epoch  0][iter   20] loss: 103.0154 RMSElog: 10.3015
[epoch  0][iter   30] loss: 92.8222 RMSElog: 9.2822
[epoch  0][iter   40] loss: 83.6760 RMSElog: 8.3676
[epoch  0][iter   50] loss: 112.0782 RMSElog: 11.2078
[epoch  0][iter   60] loss: 106.0549 RMSElog: 10.6055
[epoch  0][iter   70] loss: 109.4344 RMSElog: 10.9434
[epoch  0][iter   80] loss: 100.7210 RMSElog: 10.0721
[epoch  0][iter   90] loss: 105.6845 RMSElog: 10.5685
[epoch  0][iter  100] loss: 116.8905 RMSElog: 11.6890
[epoch  0][iter  110] loss: 88.5186 RMSElog: 8.8519
[epoch  0][iter  120] loss: 83.4787 RMSElog: 8.3479
[epoch  0][iter  130] loss: 87.1212 RMSElog: 8.7121
[epoch  0][iter  140] loss: 83.6427 RMSElog: 8.3643
[epoch  0][iter  150] loss: 76.5632 RMSElog: 7.6563
[epoch  0][iter  160] loss: 80.6945 RMSElog: 8.0695
[epoch  0][iter  170] loss: 77.6917 RMSElog: 7.7692
[epoch  0][iter  180] loss: 66.9399 RMSElog: 6.6940
[epoch  0][iter  190] loss: 61.8208 RMSElog: 6.1821
[epoch  0][iter  200] loss: 76.4314 RMSElog: 7.6431
[epoch  0][iter  210] loss: 60.9491 RMSElog: 6.0949
[epoch  0][iter  220] loss: 62.8676 RMSElog: 6.2868
[epoch  0][iter  230] loss: 68.8931 RMSElog: 6.8893
[epoch  0][iter  240] loss: 76.3611 RMSElog: 7.6361
[epoch  0][iter  250] loss: 66.6849 RMSElog: 6.6685
[epoch  0][iter  260] loss: 62.1965 RMSElog: 6.2197
[epoch  0][iter  270] loss: 58.8007 RMSElog: 5.8801
[epoch  0][iter  280] loss: 59.1503 RMSElog: 5.9150
[epoch  0][iter  290] loss: 57.0402 RMSElog: 5.7040
[epoch  0][iter  300] loss: 65.9703 RMSElog: 6.5970
[epoch  0][iter  310] loss: 67.9081 RMSElog: 6.7908
[epoch  0][iter  320] loss: 59.4144 RMSElog: 5.9414
[epoch  0][iter  330] loss: 67.3834 RMSElog: 6.7383
[epoch  0][iter  340] loss: 57.0112 RMSElog: 5.7011
[epoch  0][iter  350] loss: 54.3572 RMSElog: 5.4357
[epoch  0][iter  360] loss: 62.8185 RMSElog: 6.2819
[epoch  0][iter  370] loss: 60.2751 RMSElog: 6.0275
[epoch  0][iter  380] loss: 57.7686 RMSElog: 5.7769
[epoch  0][iter  390] loss: 53.0249 RMSElog: 5.3025
[epoch  0][iter  400] loss: 56.4265 RMSElog: 5.6426
[epoch  0][iter  410] loss: 54.8039 RMSElog: 5.4804
[epoch  0][iter  420] loss: 47.3724 RMSElog: 4.7372
[epoch  0][iter  430] loss: 56.2613 RMSElog: 5.6261
[epoch  0][iter  440] loss: 54.3174 RMSElog: 5.4317
[epoch  0][iter  450] loss: 60.8248 RMSElog: 6.0825
[epoch  0][iter  460] loss: 60.4879 RMSElog: 6.0488
[epoch  0][iter  470] loss: 55.5154 RMSElog: 5.5515
[epoch  0][iter  480] loss: 53.6623 RMSElog: 5.3662
[epoch  0][iter  490] loss: 57.8416 RMSElog: 5.7842
[epoch  0][iter  500] loss: 63.8784 RMSElog: 6.3878
[epoch  0][iter  510] loss: 59.9724 RMSElog: 5.9972
[epoch  0][iter  520] loss: 50.3107 RMSElog: 5.0311
[epoch  0][iter  530] loss: 47.7503 RMSElog: 4.7750
[epoch  0][iter  540] loss: 57.4602 RMSElog: 5.7460
[epoch  0][iter  550] loss: 56.2048 RMSElog: 5.6205
[epoch  0][iter  560] loss: 50.0837 RMSElog: 5.0084
[epoch  0][iter  570] loss: 61.5885 RMSElog: 6.1588
[epoch  0][iter  580] loss: 61.9441 RMSElog: 6.1944
[epoch  0][iter  590] loss: 49.8301 RMSElog: 4.9830
[epoch  1][iter    0] loss: 54.7249 RMSElog: 5.4725
[epoch  1][iter   10] loss: 58.7182 RMSElog: 5.8718
[epoch  1][iter   20] loss: 53.2888 RMSElog: 5.3289
[epoch  1][iter   30] loss: 51.0032 RMSElog: 5.1003
[epoch  1][iter   40] loss: 54.6395 RMSElog: 5.4639
[epoch  1][iter   50] loss: 52.9267 RMSElog: 5.2927
[epoch  1][iter   60] loss: 53.6919 RMSElog: 5.3692
[epoch  1][iter   70] loss: 48.4244 RMSElog: 4.8424
[epoch  1][iter   80] loss: 51.8888 RMSElog: 5.1889
[epoch  1][iter   90] loss: 57.5809 RMSElog: 5.7581
[epoch  1][iter  100] loss: 47.0307 RMSElog: 4.7031
[epoch  1][iter  110] loss: 46.5912 RMSElog: 4.6591
[epoch  1][iter  120] loss: 52.0257 RMSElog: 5.2026
[epoch  1][iter  130] loss: 62.3254 RMSElog: 6.2325
[epoch  1][iter  140] loss: 57.7120 RMSElog: 5.7712
[epoch  1][iter  150] loss: 56.3175 RMSElog: 5.6317
[epoch  1][iter  160] loss: 56.6297 RMSElog: 5.6630
[epoch  1][iter  170] loss: 55.4332 RMSElog: 5.5433
[epoch  1][iter  180] loss: 57.4818 RMSElog: 5.7482
[epoch  1][iter  190] loss: 51.0918 RMSElog: 5.1092
[epoch  1][iter  200] loss: 51.1172 RMSElog: 5.1117
[epoch  1][iter  210] loss: 64.7301 RMSElog: 6.4730
[epoch  1][iter  220] loss: 53.0766 RMSElog: 5.3077
[epoch  1][iter  230] loss: 49.0825 RMSElog: 4.9082
[epoch  1][iter  240] loss: 59.0900 RMSElog: 5.9090
[epoch  1][iter  250] loss: 47.3285 RMSElog: 4.7329
[epoch  1][iter  260] loss: 54.9024 RMSElog: 5.4902
[epoch  1][iter  270] loss: 45.0248 RMSElog: 4.5025
[epoch  1][iter  280] loss: 49.2265 RMSElog: 4.9226
[epoch  1][iter  290] loss: 54.1184 RMSElog: 5.4118
[epoch  1][iter  300] loss: 59.5709 RMSElog: 5.9571
[epoch  1][iter  310] loss: 52.3860 RMSElog: 5.2386
[epoch  1][iter  320] loss: 47.3972 RMSElog: 4.7397
[epoch  1][iter  330] loss: 49.1582 RMSElog: 4.9158
[epoch  1][iter  340] loss: 59.1094 RMSElog: 5.9109
[epoch  1][iter  350] loss: 54.2531 RMSElog: 5.4253
[epoch  1][iter  360] loss: 49.4819 RMSElog: 4.9482
[epoch  1][iter  370] loss: 58.0235 RMSElog: 5.8023
[epoch  1][iter  380] loss: 60.4894 RMSElog: 6.0489
[epoch  1][iter  390] loss: 52.4887 RMSElog: 5.2489
[epoch  1][iter  400] loss: 49.0970 RMSElog: 4.9097
[epoch  1][iter  410] loss: 60.7057 RMSElog: 6.0706
[epoch  1][iter  420] loss: 55.3637 RMSElog: 5.5364
[epoch  1][iter  430] loss: 55.3784 RMSElog: 5.5378
[epoch  1][iter  440] loss: 54.0637 RMSElog: 5.4064
[epoch  1][iter  450] loss: 54.1703 RMSElog: 5.4170
[epoch  1][iter  460] loss: 49.7294 RMSElog: 4.9729
[epoch  1][iter  470] loss: 48.2783 RMSElog: 4.8278
[epoch  1][iter  480] loss: 44.8794 RMSElog: 4.4879
[epoch  1][iter  490] loss: 53.1643 RMSElog: 5.3164
[epoch  1][iter  500] loss: 54.1963 RMSElog: 5.4196
[epoch  1][iter  510] loss: 55.5696 RMSElog: 5.5570
[epoch  1][iter  520] loss: 58.2985 RMSElog: 5.8298
[epoch  1][iter  530] loss: 50.5873 RMSElog: 5.0587
[epoch  1][iter  540] loss: 45.0779 RMSElog: 4.5078
[epoch  1][iter  550] loss: 47.5580 RMSElog: 4.7558
[epoch  1][iter  560] loss: 43.0201 RMSElog: 4.3020
[epoch  1][iter  570] loss: 49.9383 RMSElog: 4.9938
[epoch  1][iter  580] loss: 51.0788 RMSElog: 5.1079
[epoch  1][iter  590] loss: 43.6861 RMSElog: 4.3686
[epoch  2][iter    0] loss: 49.6893 RMSElog: 4.9689
[epoch  2][iter   10] loss: 54.2787 RMSElog: 5.4279
[epoch  2][iter   20] loss: 46.3558 RMSElog: 4.6356
[epoch  2][iter   30] loss: 55.3231 RMSElog: 5.5323
[epoch  2][iter   40] loss: 49.9148 RMSElog: 4.9915
[epoch  2][iter   50] loss: 48.7849 RMSElog: 4.8785
[epoch  2][iter   60] loss: 53.1776 RMSElog: 5.3178
[epoch  2][iter   70] loss: 51.6386 RMSElog: 5.1639
[epoch  2][iter   80] loss: 54.6422 RMSElog: 5.4642
[epoch  2][iter   90] loss: 60.0297 RMSElog: 6.0030
[epoch  2][iter  100] loss: 53.1880 RMSElog: 5.3188
[epoch  2][iter  110] loss: 51.6239 RMSElog: 5.1624
[epoch  2][iter  120] loss: 49.8963 RMSElog: 4.9896
[epoch  2][iter  130] loss: 46.0148 RMSElog: 4.6015
[epoch  2][iter  140] loss: 47.6848 RMSElog: 4.7685
[epoch  2][iter  150] loss: 56.1049 RMSElog: 5.6105
[epoch  2][iter  160] loss: 49.2106 RMSElog: 4.9211
[epoch  2][iter  170] loss: 53.3256 RMSElog: 5.3326
[epoch  2][iter  180] loss: 58.5703 RMSElog: 5.8570
[epoch  2][iter  190] loss: 52.3334 RMSElog: 5.2333
[epoch  2][iter  200] loss: 44.6674 RMSElog: 4.4667
[epoch  2][iter  210] loss: 47.6235 RMSElog: 4.7623
[epoch  2][iter  220] loss: 54.4536 RMSElog: 5.4454
[epoch  2][iter  230] loss: 46.5202 RMSElog: 4.6520
[epoch  2][iter  240] loss: 48.2435 RMSElog: 4.8244
[epoch  2][iter  250] loss: 51.5899 RMSElog: 5.1590
[epoch  2][iter  260] loss: 65.3410 RMSElog: 6.5341
[epoch  2][iter  270] loss: 57.8160 RMSElog: 5.7816
[epoch  2][iter  280] loss: 50.0714 RMSElog: 5.0071
[epoch  2][iter  290] loss: 48.9891 RMSElog: 4.8989
[epoch  2][iter  300] loss: 66.0617 RMSElog: 6.6062
[epoch  2][iter  310] loss: 53.0001 RMSElog: 5.3000
[epoch  2][iter  320] loss: 46.8115 RMSElog: 4.6811
[epoch  2][iter  330] loss: 51.1004 RMSElog: 5.1100
[epoch  2][iter  340] loss: 55.0817 RMSElog: 5.5082
[epoch  2][iter  350] loss: 53.1902 RMSElog: 5.3190
[epoch  2][iter  360] loss: 52.7874 RMSElog: 5.2787
[epoch  2][iter  370] loss: 55.0808 RMSElog: 5.5081
[epoch  2][iter  380] loss: 51.6337 RMSElog: 5.1634
[epoch  2][iter  390] loss: 56.8615 RMSElog: 5.6862
[epoch  2][iter  400] loss: 60.7158 RMSElog: 6.0716
[epoch  2][iter  410] loss: 52.6943 RMSElog: 5.2694
[epoch  2][iter  420] loss: 44.2000 RMSElog: 4.4200
[epoch  2][iter  430] loss: 54.2284 RMSElog: 5.4228
[epoch  2][iter  440] loss: 62.1299 RMSElog: 6.2130
[epoch  2][iter  450] loss: 39.2324 RMSElog: 3.9232
[epoch  2][iter  460] loss: 46.8584 RMSElog: 4.6858
[epoch  2][iter  470] loss: 49.6879 RMSElog: 4.9688
[epoch  2][iter  480] loss: 52.4448 RMSElog: 5.2445
[epoch  2][iter  490] loss: 49.2190 RMSElog: 4.9219
[epoch  2][iter  500] loss: 42.5108 RMSElog: 4.2511
[epoch  2][iter  510] loss: 49.7820 RMSElog: 4.9782
[epoch  2][iter  520] loss: 48.7828 RMSElog: 4.8783
[epoch  2][iter  530] loss: 50.9410 RMSElog: 5.0941
[epoch  2][iter  540] loss: 57.5115 RMSElog: 5.7511
[epoch  2][iter  550] loss: 43.1781 RMSElog: 4.3178
[epoch  2][iter  560] loss: 48.2678 RMSElog: 4.8268
[epoch  2][iter  570] loss: 45.4217 RMSElog: 4.5422
[epoch  2][iter  580] loss: 54.5630 RMSElog: 5.4563
[epoch  2][iter  590] loss: 46.8921 RMSElog: 4.6892
[epoch  3][iter    0] loss: 56.8053 RMSElog: 5.6805
[epoch  3][iter   10] loss: 50.2044 RMSElog: 5.0204
[epoch  3][iter   20] loss: 48.4351 RMSElog: 4.8435
[epoch  3][iter   30] loss: 47.2037 RMSElog: 4.7204
[epoch  3][iter   40] loss: 50.0631 RMSElog: 5.0063
[epoch  3][iter   50] loss: 40.9274 RMSElog: 4.0927
[epoch  3][iter   60] loss: 50.1960 RMSElog: 5.0196
[epoch  3][iter   70] loss: 54.1942 RMSElog: 5.4194
[epoch  3][iter   80] loss: 53.0592 RMSElog: 5.3059
[epoch  3][iter   90] loss: 47.7230 RMSElog: 4.7723
[epoch  3][iter  100] loss: 48.1062 RMSElog: 4.8106
[epoch  3][iter  110] loss: 49.5683 RMSElog: 4.9568
[epoch  3][iter  120] loss: 56.4539 RMSElog: 5.6454
[epoch  3][iter  130] loss: 49.9136 RMSElog: 4.9914
[epoch  3][iter  140] loss: 51.4512 RMSElog: 5.1451
[epoch  3][iter  150] loss: 52.8864 RMSElog: 5.2886
[epoch  3][iter  160] loss: 42.4708 RMSElog: 4.2471
[epoch  3][iter  170] loss: 51.0750 RMSElog: 5.1075
[epoch  3][iter  180] loss: 45.2511 RMSElog: 4.5251
[epoch  3][iter  190] loss: 54.6136 RMSElog: 5.4614
[epoch  3][iter  200] loss: 51.0781 RMSElog: 5.1078
[epoch  3][iter  210] loss: 46.8151 RMSElog: 4.6815
[epoch  3][iter  220] loss: 39.8865 RMSElog: 3.9887
[epoch  3][iter  230] loss: 52.1972 RMSElog: 5.2197
[epoch  3][iter  240] loss: 48.5027 RMSElog: 4.8503
[epoch  3][iter  250] loss: 54.0702 RMSElog: 5.4070
[epoch  3][iter  260] loss: 61.9556 RMSElog: 6.1956
[epoch  3][iter  270] loss: 46.2775 RMSElog: 4.6278
[epoch  3][iter  280] loss: 54.8445 RMSElog: 5.4845
[epoch  3][iter  290] loss: 47.1068 RMSElog: 4.7107
[epoch  3][iter  300] loss: 62.5457 RMSElog: 6.2546
[epoch  3][iter  310] loss: 56.6627 RMSElog: 5.6663
[epoch  3][iter  320] loss: 52.1653 RMSElog: 5.2165
[epoch  3][iter  330] loss: 49.5786 RMSElog: 4.9579
[epoch  3][iter  340] loss: 46.6453 RMSElog: 4.6645
[epoch  3][iter  350] loss: 49.7981 RMSElog: 4.9798
[epoch  3][iter  360] loss: 52.8493 RMSElog: 5.2849
[epoch  3][iter  370] loss: 45.8149 RMSElog: 4.5815
[epoch  3][iter  380] loss: 43.2695 RMSElog: 4.3269
[epoch  3][iter  390] loss: 44.2975 RMSElog: 4.4297
[epoch  3][iter  400] loss: 49.6021 RMSElog: 4.9602
[epoch  3][iter  410] loss: 45.4824 RMSElog: 4.5482
[epoch  3][iter  420] loss: 43.8351 RMSElog: 4.3835
[epoch  3][iter  430] loss: 47.9031 RMSElog: 4.7903
[epoch  3][iter  440] loss: 45.2448 RMSElog: 4.5245
[epoch  3][iter  450] loss: 49.2126 RMSElog: 4.9213
[epoch  3][iter  460] loss: 48.7696 RMSElog: 4.8770
[epoch  3][iter  470] loss: 52.5556 RMSElog: 5.2556
[epoch  3][iter  480] loss: 50.1541 RMSElog: 5.0154
[epoch  3][iter  490] loss: 49.5229 RMSElog: 4.9523
[epoch  3][iter  500] loss: 42.0996 RMSElog: 4.2100
[epoch  3][iter  510] loss: 48.9603 RMSElog: 4.8960
[epoch  3][iter  520] loss: 48.7611 RMSElog: 4.8761
[epoch  3][iter  530] loss: 49.1740 RMSElog: 4.9174
[epoch  3][iter  540] loss: 45.5583 RMSElog: 4.5558
[epoch  3][iter  550] loss: 43.4997 RMSElog: 4.3500
[epoch  3][iter  560] loss: 48.9700 RMSElog: 4.8970
[epoch  3][iter  570] loss: 48.4243 RMSElog: 4.8424
[epoch  3][iter  580] loss: 55.0915 RMSElog: 5.5092
[epoch  3][iter  590] loss: 50.9575 RMSElog: 5.0958
[epoch  4][iter    0] loss: 47.3977 RMSElog: 4.7398
[epoch  4][iter   10] loss: 48.2399 RMSElog: 4.8240
[epoch  4][iter   20] loss: 47.3181 RMSElog: 4.7318
[epoch  4][iter   30] loss: 41.4452 RMSElog: 4.1445
[epoch  4][iter   40] loss: 44.8935 RMSElog: 4.4893
[epoch  4][iter   50] loss: 42.6115 RMSElog: 4.2611
[epoch  4][iter   60] loss: 57.1017 RMSElog: 5.7102
[epoch  4][iter   70] loss: 45.9388 RMSElog: 4.5939
[epoch  4][iter   80] loss: 36.1949 RMSElog: 3.6195
[epoch  4][iter   90] loss: 46.4924 RMSElog: 4.6492
[epoch  4][iter  100] loss: 44.0860 RMSElog: 4.4086
[epoch  4][iter  110] loss: 51.2416 RMSElog: 5.1242
[epoch  4][iter  120] loss: 53.9664 RMSElog: 5.3966
[epoch  4][iter  130] loss: 49.0345 RMSElog: 4.9034
[epoch  4][iter  140] loss: 45.7933 RMSElog: 4.5793
[epoch  4][iter  150] loss: 52.7084 RMSElog: 5.2708
[epoch  4][iter  160] loss: 46.4745 RMSElog: 4.6474
[epoch  4][iter  170] loss: 51.0819 RMSElog: 5.1082
[epoch  4][iter  180] loss: 45.7182 RMSElog: 4.5718
[epoch  4][iter  190] loss: 46.9180 RMSElog: 4.6918
[epoch  4][iter  200] loss: 50.5468 RMSElog: 5.0547
[epoch  4][iter  210] loss: 46.8143 RMSElog: 4.6814
[epoch  4][iter  220] loss: 50.2504 RMSElog: 5.0250
[epoch  4][iter  230] loss: 51.9089 RMSElog: 5.1909
[epoch  4][iter  240] loss: 45.0198 RMSElog: 4.5020
[epoch  4][iter  250] loss: 54.0576 RMSElog: 5.4058
[epoch  4][iter  260] loss: 47.9660 RMSElog: 4.7966
[epoch  4][iter  270] loss: 50.0242 RMSElog: 5.0024
[epoch  4][iter  280] loss: 43.5596 RMSElog: 4.3560
[epoch  4][iter  290] loss: 49.2467 RMSElog: 4.9247
[epoch  4][iter  300] loss: 55.1200 RMSElog: 5.5120
[epoch  4][iter  310] loss: 49.6938 RMSElog: 4.9694
[epoch  4][iter  320] loss: 45.8043 RMSElog: 4.5804
[epoch  4][iter  330] loss: 49.2129 RMSElog: 4.9213
[epoch  4][iter  340] loss: 58.5530 RMSElog: 5.8553
[epoch  4][iter  350] loss: 42.9485 RMSElog: 4.2949
[epoch  4][iter  360] loss: 45.0106 RMSElog: 4.5011
[epoch  4][iter  370] loss: 44.2926 RMSElog: 4.4293
[epoch  4][iter  380] loss: 44.7211 RMSElog: 4.4721
[epoch  4][iter  390] loss: 51.5800 RMSElog: 5.1580
[epoch  4][iter  400] loss: 46.9825 RMSElog: 4.6983
[epoch  4][iter  410] loss: 47.4788 RMSElog: 4.7479
[epoch  4][iter  420] loss: 51.0547 RMSElog: 5.1055
[epoch  4][iter  430] loss: 46.9188 RMSElog: 4.6919
[epoch  4][iter  440] loss: 40.6963 RMSElog: 4.0696
[epoch  4][iter  450] loss: 46.8679 RMSElog: 4.6868
[epoch  4][iter  460] loss: 47.4406 RMSElog: 4.7441
[epoch  4][iter  470] loss: 55.0922 RMSElog: 5.5092
[epoch  4][iter  480] loss: 47.1282 RMSElog: 4.7128
[epoch  4][iter  490] loss: 52.0881 RMSElog: 5.2088
[epoch  4][iter  500] loss: 45.8286 RMSElog: 4.5829
[epoch  4][iter  510] loss: 53.4258 RMSElog: 5.3426
[epoch  4][iter  520] loss: 42.0836 RMSElog: 4.2084
[epoch  4][iter  530] loss: 52.1814 RMSElog: 5.2181
[epoch  4][iter  540] loss: 50.8441 RMSElog: 5.0844
[epoch  4][iter  550] loss: 50.5119 RMSElog: 5.0512
[epoch  4][iter  560] loss: 48.5708 RMSElog: 4.8571
[epoch  4][iter  570] loss: 42.4500 RMSElog: 4.2450
[epoch  4][iter  580] loss: 53.6925 RMSElog: 5.3693
[epoch  4][iter  590] loss: 51.3104 RMSElog: 5.1310
[epoch  5][iter    0] loss: 49.2244 RMSElog: 4.9224
[epoch  5][iter   10] loss: 48.6030 RMSElog: 4.8603
[epoch  5][iter   20] loss: 42.2027 RMSElog: 4.2203
[epoch  5][iter   30] loss: 46.8300 RMSElog: 4.6830
[epoch  5][iter   40] loss: 44.5264 RMSElog: 4.4526
[epoch  5][iter   50] loss: 56.5104 RMSElog: 5.6510
[epoch  5][iter   60] loss: 52.4388 RMSElog: 5.2439
[epoch  5][iter   70] loss: 50.3794 RMSElog: 5.0379
[epoch  5][iter   80] loss: 55.3329 RMSElog: 5.5333
[epoch  5][iter   90] loss: 43.1602 RMSElog: 4.3160
[epoch  5][iter  100] loss: 42.3082 RMSElog: 4.2308
[epoch  5][iter  110] loss: 46.4767 RMSElog: 4.6477
[epoch  5][iter  120] loss: 49.7074 RMSElog: 4.9707
[epoch  5][iter  130] loss: 45.9708 RMSElog: 4.5971
[epoch  5][iter  140] loss: 44.7126 RMSElog: 4.4713
[epoch  5][iter  150] loss: 53.7235 RMSElog: 5.3723
[epoch  5][iter  160] loss: 51.0968 RMSElog: 5.1097
[epoch  5][iter  170] loss: 47.3562 RMSElog: 4.7356
[epoch  5][iter  180] loss: 45.8228 RMSElog: 4.5823
[epoch  5][iter  190] loss: 52.7332 RMSElog: 5.2733
[epoch  5][iter  200] loss: 47.7347 RMSElog: 4.7735
[epoch  5][iter  210] loss: 48.4074 RMSElog: 4.8407
[epoch  5][iter  220] loss: 46.6446 RMSElog: 4.6645
[epoch  5][iter  230] loss: 45.7287 RMSElog: 4.5729
[epoch  5][iter  240] loss: 49.2902 RMSElog: 4.9290
[epoch  5][iter  250] loss: 51.4560 RMSElog: 5.1456
[epoch  5][iter  260] loss: 60.2507 RMSElog: 6.0251
[epoch  5][iter  270] loss: 52.0002 RMSElog: 5.2000
[epoch  5][iter  280] loss: 52.1957 RMSElog: 5.2196
[epoch  5][iter  290] loss: 36.8147 RMSElog: 3.6815
[epoch  5][iter  300] loss: 53.1778 RMSElog: 5.3178
[epoch  5][iter  310] loss: 48.4868 RMSElog: 4.8487
[epoch  5][iter  320] loss: 39.7003 RMSElog: 3.9700
[epoch  5][iter  330] loss: 42.6496 RMSElog: 4.2650
[epoch  5][iter  340] loss: 52.0173 RMSElog: 5.2017
[epoch  5][iter  350] loss: 50.1398 RMSElog: 5.0140
[epoch  5][iter  360] loss: 37.3669 RMSElog: 3.7367
[epoch  5][iter  370] loss: 41.7177 RMSElog: 4.1718
[epoch  5][iter  380] loss: 38.2152 RMSElog: 3.8215
[epoch  5][iter  390] loss: 41.7816 RMSElog: 4.1782
[epoch  5][iter  400] loss: 37.1581 RMSElog: 3.7158
[epoch  5][iter  410] loss: 51.1798 RMSElog: 5.1180
[epoch  5][iter  420] loss: 44.4674 RMSElog: 4.4467
[epoch  5][iter  430] loss: 42.6330 RMSElog: 4.2633
[epoch  5][iter  440] loss: 48.2985 RMSElog: 4.8298
[epoch  5][iter  450] loss: 40.4329 RMSElog: 4.0433
[epoch  5][iter  460] loss: 44.8167 RMSElog: 4.4817
[epoch  5][iter  470] loss: 49.5234 RMSElog: 4.9523
[epoch  5][iter  480] loss: 46.3108 RMSElog: 4.6311
[epoch  5][iter  490] loss: 51.3056 RMSElog: 5.1306
[epoch  5][iter  500] loss: 46.5751 RMSElog: 4.6575
[epoch  5][iter  510] loss: 48.9480 RMSElog: 4.8948
[epoch  5][iter  520] loss: 47.9323 RMSElog: 4.7932
[epoch  5][iter  530] loss: 49.9908 RMSElog: 4.9991
[epoch  5][iter  540] loss: 50.7658 RMSElog: 5.0766
[epoch  5][iter  550] loss: 40.2522 RMSElog: 4.0252
[epoch  5][iter  560] loss: 45.4796 RMSElog: 4.5480
[epoch  5][iter  570] loss: 40.9058 RMSElog: 4.0906
[epoch  5][iter  580] loss: 42.3667 RMSElog: 4.2367
[epoch  5][iter  590] loss: 43.7203 RMSElog: 4.3720
[epoch  6][iter    0] loss: 47.6346 RMSElog: 4.7635
[epoch  6][iter   10] loss: 42.6846 RMSElog: 4.2685
[epoch  6][iter   20] loss: 42.6509 RMSElog: 4.2651
[epoch  6][iter   30] loss: 51.0319 RMSElog: 5.1032
[epoch  6][iter   40] loss: 40.7622 RMSElog: 4.0762
[epoch  6][iter   50] loss: 45.5575 RMSElog: 4.5557
[epoch  6][iter   60] loss: 45.7945 RMSElog: 4.5795
[epoch  6][iter   70] loss: 42.8646 RMSElog: 4.2865
[epoch  6][iter   80] loss: 47.6451 RMSElog: 4.7645
[epoch  6][iter   90] loss: 55.8486 RMSElog: 5.5849
[epoch  6][iter  100] loss: 44.0653 RMSElog: 4.4065
[epoch  6][iter  110] loss: 39.2248 RMSElog: 3.9225
[epoch  6][iter  120] loss: 46.9563 RMSElog: 4.6956
[epoch  6][iter  130] loss: 43.7759 RMSElog: 4.3776
[epoch  6][iter  140] loss: 41.5312 RMSElog: 4.1531
[epoch  6][iter  150] loss: 38.1032 RMSElog: 3.8103
[epoch  6][iter  160] loss: 48.4790 RMSElog: 4.8479
[epoch  6][iter  170] loss: 37.8250 RMSElog: 3.7825
[epoch  6][iter  180] loss: 38.5910 RMSElog: 3.8591
[epoch  6][iter  190] loss: 42.4682 RMSElog: 4.2468
[epoch  6][iter  200] loss: 48.4782 RMSElog: 4.8478
[epoch  6][iter  210] loss: 48.7523 RMSElog: 4.8752
[epoch  6][iter  220] loss: 54.6993 RMSElog: 5.4699
[epoch  6][iter  230] loss: 46.5786 RMSElog: 4.6579
[epoch  6][iter  240] loss: 44.0201 RMSElog: 4.4020
[epoch  6][iter  250] loss: 46.0560 RMSElog: 4.6056
[epoch  6][iter  260] loss: 50.0503 RMSElog: 5.0050
[epoch  6][iter  270] loss: 41.4599 RMSElog: 4.1460
[epoch  6][iter  280] loss: 45.5081 RMSElog: 4.5508
[epoch  6][iter  290] loss: 36.2749 RMSElog: 3.6275
[epoch  6][iter  300] loss: 40.3993 RMSElog: 4.0399
[epoch  6][iter  310] loss: 39.3593 RMSElog: 3.9359
[epoch  6][iter  320] loss: 43.9553 RMSElog: 4.3955
[epoch  6][iter  330] loss: 43.5403 RMSElog: 4.3540
[epoch  6][iter  340] loss: 46.4841 RMSElog: 4.6484
[epoch  6][iter  350] loss: 43.0337 RMSElog: 4.3034
[epoch  6][iter  360] loss: 50.0427 RMSElog: 5.0043
[epoch  6][iter  370] loss: 50.1806 RMSElog: 5.0181
[epoch  6][iter  380] loss: 36.0269 RMSElog: 3.6027
[epoch  6][iter  390] loss: 49.4291 RMSElog: 4.9429
[epoch  6][iter  400] loss: 50.0583 RMSElog: 5.0058
[epoch  6][iter  410] loss: 42.4854 RMSElog: 4.2485
[epoch  6][iter  420] loss: 49.4035 RMSElog: 4.9403
[epoch  6][iter  430] loss: 48.7976 RMSElog: 4.8798
[epoch  6][iter  440] loss: 41.9589 RMSElog: 4.1959
[epoch  6][iter  450] loss: 42.5567 RMSElog: 4.2557
[epoch  6][iter  460] loss: 39.1073 RMSElog: 3.9107
[epoch  6][iter  470] loss: 44.2666 RMSElog: 4.4267
[epoch  6][iter  480] loss: 47.8445 RMSElog: 4.7845
[epoch  6][iter  490] loss: 39.8810 RMSElog: 3.9881
[epoch  6][iter  500] loss: 44.2912 RMSElog: 4.4291
[epoch  6][iter  510] loss: 39.3702 RMSElog: 3.9370
[epoch  6][iter  520] loss: 39.0012 RMSElog: 3.9001
[epoch  6][iter  530] loss: 60.3616 RMSElog: 6.0362
[epoch  6][iter  540] loss: 48.7200 RMSElog: 4.8720
[epoch  6][iter  550] loss: 46.4763 RMSElog: 4.6476
[epoch  6][iter  560] loss: 40.7753 RMSElog: 4.0775
[epoch  6][iter  570] loss: 44.8442 RMSElog: 4.4844
[epoch  6][iter  580] loss: 53.5154 RMSElog: 5.3515
[epoch  6][iter  590] loss: 45.7066 RMSElog: 4.5707
[epoch  7][iter    0] loss: 41.5976 RMSElog: 4.1598
[epoch  7][iter   10] loss: 40.7383 RMSElog: 4.0738
[epoch  7][iter   20] loss: 46.0741 RMSElog: 4.6074
[epoch  7][iter   30] loss: 36.3769 RMSElog: 3.6377
[epoch  7][iter   40] loss: 45.8301 RMSElog: 4.5830
[epoch  7][iter   50] loss: 38.7556 RMSElog: 3.8756
[epoch  7][iter   60] loss: 47.7870 RMSElog: 4.7787
[epoch  7][iter   70] loss: 42.2036 RMSElog: 4.2204
[epoch  7][iter   80] loss: 49.8693 RMSElog: 4.9869
[epoch  7][iter   90] loss: 46.3532 RMSElog: 4.6353
[epoch  7][iter  100] loss: 42.8941 RMSElog: 4.2894
[epoch  7][iter  110] loss: 43.5422 RMSElog: 4.3542
[epoch  7][iter  120] loss: 50.9885 RMSElog: 5.0988
[epoch  7][iter  130] loss: 33.3300 RMSElog: 3.3330
[epoch  7][iter  140] loss: 40.2508 RMSElog: 4.0251
[epoch  7][iter  150] loss: 45.7446 RMSElog: 4.5745
[epoch  7][iter  160] loss: 44.5699 RMSElog: 4.4570
[epoch  7][iter  170] loss: 43.0216 RMSElog: 4.3022
[epoch  7][iter  180] loss: 52.2445 RMSElog: 5.2244
[epoch  7][iter  190] loss: 43.3109 RMSElog: 4.3311
[epoch  7][iter  200] loss: 39.1772 RMSElog: 3.9177
[epoch  7][iter  210] loss: 44.2969 RMSElog: 4.4297
[epoch  7][iter  220] loss: 56.5116 RMSElog: 5.6512
[epoch  7][iter  230] loss: 47.6594 RMSElog: 4.7659
[epoch  7][iter  240] loss: 42.1875 RMSElog: 4.2187
[epoch  7][iter  250] loss: 40.7582 RMSElog: 4.0758
[epoch  7][iter  260] loss: 44.8979 RMSElog: 4.4898
[epoch  7][iter  270] loss: 41.0260 RMSElog: 4.1026
[epoch  7][iter  280] loss: 54.0765 RMSElog: 5.4076
[epoch  7][iter  290] loss: 46.2419 RMSElog: 4.6242
[epoch  7][iter  300] loss: 39.3133 RMSElog: 3.9313
[epoch  7][iter  310] loss: 43.5465 RMSElog: 4.3546
[epoch  7][iter  320] loss: 40.1628 RMSElog: 4.0163
[epoch  7][iter  330] loss: 49.3999 RMSElog: 4.9400
[epoch  7][iter  340] loss: 50.3105 RMSElog: 5.0311
[epoch  7][iter  350] loss: 49.1581 RMSElog: 4.9158
[epoch  7][iter  360] loss: 52.3904 RMSElog: 5.2390
[epoch  7][iter  370] loss: 52.1811 RMSElog: 5.2181
[epoch  7][iter  380] loss: 39.6042 RMSElog: 3.9604
[epoch  7][iter  390] loss: 46.0903 RMSElog: 4.6090
[epoch  7][iter  400] loss: 42.1528 RMSElog: 4.2153
[epoch  7][iter  410] loss: 40.0573 RMSElog: 4.0057
[epoch  7][iter  420] loss: 48.8759 RMSElog: 4.8876
[epoch  7][iter  430] loss: 51.7820 RMSElog: 5.1782
[epoch  7][iter  440] loss: 46.8741 RMSElog: 4.6874
[epoch  7][iter  450] loss: 49.0726 RMSElog: 4.9073
[epoch  7][iter  460] loss: 48.1758 RMSElog: 4.8176
[epoch  7][iter  470] loss: 43.1606 RMSElog: 4.3161
[epoch  7][iter  480] loss: 41.5633 RMSElog: 4.1563
[epoch  7][iter  490] loss: 44.4292 RMSElog: 4.4429
[epoch  7][iter  500] loss: 40.8672 RMSElog: 4.0867
[epoch  7][iter  510] loss: 37.1919 RMSElog: 3.7192
[epoch  7][iter  520] loss: 45.4454 RMSElog: 4.5445
[epoch  7][iter  530] loss: 46.1167 RMSElog: 4.6117
[epoch  7][iter  540] loss: 47.1135 RMSElog: 4.7113
[epoch  7][iter  550] loss: 43.8964 RMSElog: 4.3896
[epoch  7][iter  560] loss: 45.7379 RMSElog: 4.5738
[epoch  7][iter  570] loss: 51.9935 RMSElog: 5.1993
[epoch  7][iter  580] loss: 45.2127 RMSElog: 4.5213
[epoch  7][iter  590] loss: 45.7459 RMSElog: 4.5746
[epoch  8][iter    0] loss: 38.9794 RMSElog: 3.8979
[epoch  8][iter   10] loss: 42.9891 RMSElog: 4.2989
[epoch  8][iter   20] loss: 40.6483 RMSElog: 4.0648
[epoch  8][iter   30] loss: 45.1674 RMSElog: 4.5167
[epoch  8][iter   40] loss: 40.9431 RMSElog: 4.0943
[epoch  8][iter   50] loss: 32.7025 RMSElog: 3.2703
[epoch  8][iter   60] loss: 41.9961 RMSElog: 4.1996
[epoch  8][iter   70] loss: 40.7660 RMSElog: 4.0766
[epoch  8][iter   80] loss: 48.4728 RMSElog: 4.8473
[epoch  8][iter   90] loss: 45.5313 RMSElog: 4.5531
[epoch  8][iter  100] loss: 42.0039 RMSElog: 4.2004
[epoch  8][iter  110] loss: 52.6719 RMSElog: 5.2672
[epoch  8][iter  120] loss: 39.9674 RMSElog: 3.9967
[epoch  8][iter  130] loss: 45.0254 RMSElog: 4.5025
[epoch  8][iter  140] loss: 44.8803 RMSElog: 4.4880
[epoch  8][iter  150] loss: 48.2431 RMSElog: 4.8243
[epoch  8][iter  160] loss: 49.1757 RMSElog: 4.9176
[epoch  8][iter  170] loss: 41.6087 RMSElog: 4.1609
[epoch  8][iter  180] loss: 37.4228 RMSElog: 3.7423
[epoch  8][iter  190] loss: 51.5548 RMSElog: 5.1555
[epoch  8][iter  200] loss: 38.4748 RMSElog: 3.8475
[epoch  8][iter  210] loss: 48.8691 RMSElog: 4.8869
[epoch  8][iter  220] loss: 47.8171 RMSElog: 4.7817
[epoch  8][iter  230] loss: 43.7632 RMSElog: 4.3763
[epoch  8][iter  240] loss: 49.7068 RMSElog: 4.9707
[epoch  8][iter  250] loss: 40.0681 RMSElog: 4.0068
[epoch  8][iter  260] loss: 40.5783 RMSElog: 4.0578
[epoch  8][iter  270] loss: 47.7508 RMSElog: 4.7751
[epoch  8][iter  280] loss: 49.5228 RMSElog: 4.9523
[epoch  8][iter  290] loss: 42.6929 RMSElog: 4.2693
[epoch  8][iter  300] loss: 40.5240 RMSElog: 4.0524
[epoch  8][iter  310] loss: 42.6416 RMSElog: 4.2642
[epoch  8][iter  320] loss: 42.5060 RMSElog: 4.2506
[epoch  8][iter  330] loss: 42.3031 RMSElog: 4.2303
[epoch  8][iter  340] loss: 45.6913 RMSElog: 4.5691
[epoch  8][iter  350] loss: 50.4227 RMSElog: 5.0423
[epoch  8][iter  360] loss: 48.5842 RMSElog: 4.8584
[epoch  8][iter  370] loss: 39.8132 RMSElog: 3.9813
[epoch  8][iter  380] loss: 44.1302 RMSElog: 4.4130
[epoch  8][iter  390] loss: 43.8753 RMSElog: 4.3875
[epoch  8][iter  400] loss: 35.4714 RMSElog: 3.5471
[epoch  8][iter  410] loss: 45.7195 RMSElog: 4.5719
[epoch  8][iter  420] loss: 50.6629 RMSElog: 5.0663
[epoch  8][iter  430] loss: 45.5386 RMSElog: 4.5539
[epoch  8][iter  440] loss: 53.9292 RMSElog: 5.3929
[epoch  8][iter  450] loss: 37.6253 RMSElog: 3.7625
[epoch  8][iter  460] loss: 42.7713 RMSElog: 4.2771
[epoch  8][iter  470] loss: 45.5461 RMSElog: 4.5546
[epoch  8][iter  480] loss: 47.5989 RMSElog: 4.7599
[epoch  8][iter  490] loss: 37.5256 RMSElog: 3.7526
[epoch  8][iter  500] loss: 48.4510 RMSElog: 4.8451
[epoch  8][iter  510] loss: 50.8490 RMSElog: 5.0849
[epoch  8][iter  520] loss: 45.1452 RMSElog: 4.5145
[epoch  8][iter  530] loss: 44.4112 RMSElog: 4.4411
[epoch  8][iter  540] loss: 49.2303 RMSElog: 4.9230
[epoch  8][iter  550] loss: 41.8641 RMSElog: 4.1864
[epoch  8][iter  560] loss: 48.4526 RMSElog: 4.8453
[epoch  8][iter  570] loss: 49.9937 RMSElog: 4.9994
[epoch  8][iter  580] loss: 43.7867 RMSElog: 4.3787
[epoch  8][iter  590] loss: 44.2180 RMSElog: 4.4218
[epoch  9][iter    0] loss: 47.5494 RMSElog: 4.7549
[epoch  9][iter   10] loss: 47.3396 RMSElog: 4.7340
[epoch  9][iter   20] loss: 45.2269 RMSElog: 4.5227
[epoch  9][iter   30] loss: 42.2786 RMSElog: 4.2279
[epoch  9][iter   40] loss: 37.8436 RMSElog: 3.7844
[epoch  9][iter   50] loss: 49.2168 RMSElog: 4.9217
[epoch  9][iter   60] loss: 47.7231 RMSElog: 4.7723
[epoch  9][iter   70] loss: 39.6802 RMSElog: 3.9680
[epoch  9][iter   80] loss: 52.5012 RMSElog: 5.2501
[epoch  9][iter   90] loss: 37.6262 RMSElog: 3.7626
[epoch  9][iter  100] loss: 44.1371 RMSElog: 4.4137
[epoch  9][iter  110] loss: 43.2089 RMSElog: 4.3209
[epoch  9][iter  120] loss: 49.5711 RMSElog: 4.9571
[epoch  9][iter  130] loss: 51.9577 RMSElog: 5.1958
[epoch  9][iter  140] loss: 35.8139 RMSElog: 3.5814
[epoch  9][iter  150] loss: 44.3910 RMSElog: 4.4391
[epoch  9][iter  160] loss: 42.5827 RMSElog: 4.2583
[epoch  9][iter  170] loss: 44.8834 RMSElog: 4.4883
[epoch  9][iter  180] loss: 45.9706 RMSElog: 4.5971
[epoch  9][iter  190] loss: 35.5191 RMSElog: 3.5519
[epoch  9][iter  200] loss: 50.1526 RMSElog: 5.0153
[epoch  9][iter  210] loss: 32.3216 RMSElog: 3.2322
[epoch  9][iter  220] loss: 47.4699 RMSElog: 4.7470
[epoch  9][iter  230] loss: 51.4590 RMSElog: 5.1459
[epoch  9][iter  240] loss: 42.8174 RMSElog: 4.2817
[epoch  9][iter  250] loss: 36.4496 RMSElog: 3.6450
[epoch  9][iter  260] loss: 39.4747 RMSElog: 3.9475
[epoch  9][iter  270] loss: 37.1891 RMSElog: 3.7189
[epoch  9][iter  280] loss: 48.8332 RMSElog: 4.8833
[epoch  9][iter  290] loss: 49.5102 RMSElog: 4.9510
[epoch  9][iter  300] loss: 41.7703 RMSElog: 4.1770
[epoch  9][iter  310] loss: 47.1148 RMSElog: 4.7115
[epoch  9][iter  320] loss: 45.4806 RMSElog: 4.5481
[epoch  9][iter  330] loss: 43.3634 RMSElog: 4.3363
[epoch  9][iter  340] loss: 42.1037 RMSElog: 4.2104
[epoch  9][iter  350] loss: 49.5743 RMSElog: 4.9574
[epoch  9][iter  360] loss: 41.5144 RMSElog: 4.1514
[epoch  9][iter  370] loss: 38.9882 RMSElog: 3.8988
[epoch  9][iter  380] loss: 45.0156 RMSElog: 4.5016
[epoch  9][iter  390] loss: 38.9068 RMSElog: 3.8907
[epoch  9][iter  400] loss: 46.0089 RMSElog: 4.6009
[epoch  9][iter  410] loss: 40.2353 RMSElog: 4.0235
[epoch  9][iter  420] loss: 39.2083 RMSElog: 3.9208
[epoch  9][iter  430] loss: 38.8418 RMSElog: 3.8842
[epoch  9][iter  440] loss: 53.4135 RMSElog: 5.3414
[epoch  9][iter  450] loss: 50.1238 RMSElog: 5.0124
[epoch  9][iter  460] loss: 43.6651 RMSElog: 4.3665
[epoch  9][iter  470] loss: 43.9437 RMSElog: 4.3944
[epoch  9][iter  480] loss: 42.8422 RMSElog: 4.2842
[epoch  9][iter  490] loss: 38.3766 RMSElog: 3.8377
[epoch  9][iter  500] loss: 50.8424 RMSElog: 5.0842
[epoch  9][iter  510] loss: 44.3817 RMSElog: 4.4382
[epoch  9][iter  520] loss: 48.1093 RMSElog: 4.8109
[epoch  9][iter  530] loss: 51.9794 RMSElog: 5.1979
[epoch  9][iter  540] loss: 40.9075 RMSElog: 4.0908
[epoch  9][iter  550] loss: 47.2014 RMSElog: 4.7201
[epoch  9][iter  560] loss: 44.8828 RMSElog: 4.4883
[epoch  9][iter  570] loss: 46.2143 RMSElog: 4.6214
[epoch  9][iter  580] loss: 42.6653 RMSElog: 4.2665
[epoch  9][iter  590] loss: 50.1614 RMSElog: 5.0161
[epoch 10][iter    0] loss: 43.3303 RMSElog: 4.3330
[epoch 10][iter   10] loss: 41.9209 RMSElog: 4.1921
[epoch 10][iter   20] loss: 37.9008 RMSElog: 3.7901
[epoch 10][iter   30] loss: 41.7186 RMSElog: 4.1719
[epoch 10][iter   40] loss: 37.8232 RMSElog: 3.7823
[epoch 10][iter   50] loss: 40.2369 RMSElog: 4.0237
[epoch 10][iter   60] loss: 50.0845 RMSElog: 5.0084
[epoch 10][iter   70] loss: 40.2166 RMSElog: 4.0217
[epoch 10][iter   80] loss: 44.2518 RMSElog: 4.4252
[epoch 10][iter   90] loss: 46.0256 RMSElog: 4.6026
[epoch 10][iter  100] loss: 45.7691 RMSElog: 4.5769
[epoch 10][iter  110] loss: 52.1288 RMSElog: 5.2129
[epoch 10][iter  120] loss: 40.3500 RMSElog: 4.0350
[epoch 10][iter  130] loss: 43.6189 RMSElog: 4.3619
[epoch 10][iter  140] loss: 46.8994 RMSElog: 4.6899
[epoch 10][iter  150] loss: 39.5131 RMSElog: 3.9513
[epoch 10][iter  160] loss: 33.8446 RMSElog: 3.3845
[epoch 10][iter  170] loss: 46.5965 RMSElog: 4.6597
[epoch 10][iter  180] loss: 42.4422 RMSElog: 4.2442
[epoch 10][iter  190] loss: 44.9027 RMSElog: 4.4903
[epoch 10][iter  200] loss: 41.9289 RMSElog: 4.1929
[epoch 10][iter  210] loss: 47.9313 RMSElog: 4.7931
[epoch 10][iter  220] loss: 42.3428 RMSElog: 4.2343
[epoch 10][iter  230] loss: 38.7452 RMSElog: 3.8745
[epoch 10][iter  240] loss: 39.7096 RMSElog: 3.9710
[epoch 10][iter  250] loss: 46.2504 RMSElog: 4.6250
[epoch 10][iter  260] loss: 44.7527 RMSElog: 4.4753
[epoch 10][iter  270] loss: 50.5986 RMSElog: 5.0599
[epoch 10][iter  280] loss: 41.3005 RMSElog: 4.1300
[epoch 10][iter  290] loss: 49.0332 RMSElog: 4.9033
[epoch 10][iter  300] loss: 39.1006 RMSElog: 3.9101
[epoch 10][iter  310] loss: 40.3688 RMSElog: 4.0369
[epoch 10][iter  320] loss: 47.8232 RMSElog: 4.7823
[epoch 10][iter  330] loss: 44.0845 RMSElog: 4.4084
[epoch 10][iter  340] loss: 44.7269 RMSElog: 4.4727
[epoch 10][iter  350] loss: 51.0083 RMSElog: 5.1008
[epoch 10][iter  360] loss: 47.6272 RMSElog: 4.7627
[epoch 10][iter  370] loss: 40.2794 RMSElog: 4.0279
[epoch 10][iter  380] loss: 49.9460 RMSElog: 4.9946
[epoch 10][iter  390] loss: 40.7609 RMSElog: 4.0761
[epoch 10][iter  400] loss: 45.7170 RMSElog: 4.5717
[epoch 10][iter  410] loss: 43.8538 RMSElog: 4.3854
[epoch 10][iter  420] loss: 51.5398 RMSElog: 5.1540
[epoch 10][iter  430] loss: 52.5315 RMSElog: 5.2531
[epoch 10][iter  440] loss: 34.8193 RMSElog: 3.4819
[epoch 10][iter  450] loss: 45.5715 RMSElog: 4.5571
[epoch 10][iter  460] loss: 41.7582 RMSElog: 4.1758
[epoch 10][iter  470] loss: 38.3003 RMSElog: 3.8300
[epoch 10][iter  480] loss: 35.5843 RMSElog: 3.5584
[epoch 10][iter  490] loss: 40.0555 RMSElog: 4.0055
[epoch 10][iter  500] loss: 37.2690 RMSElog: 3.7269
[epoch 10][iter  510] loss: 42.2682 RMSElog: 4.2268
[epoch 10][iter  520] loss: 43.9214 RMSElog: 4.3921
[epoch 10][iter  530] loss: 49.3185 RMSElog: 4.9319
[epoch 10][iter  540] loss: 45.1551 RMSElog: 4.5155
[epoch 10][iter  550] loss: 50.7537 RMSElog: 5.0754
[epoch 10][iter  560] loss: 36.0108 RMSElog: 3.6011
[epoch 10][iter  570] loss: 32.2041 RMSElog: 3.2204
[epoch 10][iter  580] loss: 39.5094 RMSElog: 3.9509
[epoch 10][iter  590] loss: 44.1412 RMSElog: 4.4141
[epoch 11][iter    0] loss: 43.5619 RMSElog: 4.3562
[epoch 11][iter   10] loss: 43.0093 RMSElog: 4.3009
[epoch 11][iter   20] loss: 43.4285 RMSElog: 4.3429
[epoch 11][iter   30] loss: 43.4662 RMSElog: 4.3466
[epoch 11][iter   40] loss: 42.4017 RMSElog: 4.2402
[epoch 11][iter   50] loss: 34.1247 RMSElog: 3.4125
[epoch 11][iter   60] loss: 52.6076 RMSElog: 5.2608
[epoch 11][iter   70] loss: 40.3511 RMSElog: 4.0351
[epoch 11][iter   80] loss: 48.8962 RMSElog: 4.8896
[epoch 11][iter   90] loss: 43.1943 RMSElog: 4.3194
[epoch 11][iter  100] loss: 40.1989 RMSElog: 4.0199
[epoch 11][iter  110] loss: 46.8090 RMSElog: 4.6809
[epoch 11][iter  120] loss: 37.0708 RMSElog: 3.7071
[epoch 11][iter  130] loss: 35.8350 RMSElog: 3.5835
[epoch 11][iter  140] loss: 48.0472 RMSElog: 4.8047
[epoch 11][iter  150] loss: 39.2926 RMSElog: 3.9293
[epoch 11][iter  160] loss: 42.2718 RMSElog: 4.2272
[epoch 11][iter  170] loss: 41.4110 RMSElog: 4.1411
[epoch 11][iter  180] loss: 44.4932 RMSElog: 4.4493
[epoch 11][iter  190] loss: 39.9095 RMSElog: 3.9909
[epoch 11][iter  200] loss: 47.7292 RMSElog: 4.7729
[epoch 11][iter  210] loss: 45.0156 RMSElog: 4.5016
[epoch 11][iter  220] loss: 55.3288 RMSElog: 5.5329
[epoch 11][iter  230] loss: 42.2153 RMSElog: 4.2215
[epoch 11][iter  240] loss: 50.9870 RMSElog: 5.0987
[epoch 11][iter  250] loss: 38.6941 RMSElog: 3.8694
[epoch 11][iter  260] loss: 46.5842 RMSElog: 4.6584
[epoch 11][iter  270] loss: 45.0526 RMSElog: 4.5053
[epoch 11][iter  280] loss: 41.9258 RMSElog: 4.1926
[epoch 11][iter  290] loss: 40.4185 RMSElog: 4.0419
[epoch 11][iter  300] loss: 44.7424 RMSElog: 4.4742
[epoch 11][iter  310] loss: 62.7644 RMSElog: 6.2764
[epoch 11][iter  320] loss: 38.7122 RMSElog: 3.8712
[epoch 11][iter  330] loss: 44.4610 RMSElog: 4.4461
[epoch 11][iter  340] loss: 39.1534 RMSElog: 3.9153
[epoch 11][iter  350] loss: 44.1933 RMSElog: 4.4193
[epoch 11][iter  360] loss: 46.2456 RMSElog: 4.6246
[epoch 11][iter  370] loss: 56.7097 RMSElog: 5.6710
[epoch 11][iter  380] loss: 40.1063 RMSElog: 4.0106
[epoch 11][iter  390] loss: 40.0113 RMSElog: 4.0011
[epoch 11][iter  400] loss: 49.4538 RMSElog: 4.9454
[epoch 11][iter  410] loss: 37.4352 RMSElog: 3.7435
[epoch 11][iter  420] loss: 48.8146 RMSElog: 4.8815
[epoch 11][iter  430] loss: 44.1736 RMSElog: 4.4174
[epoch 11][iter  440] loss: 43.5535 RMSElog: 4.3553
[epoch 11][iter  450] loss: 39.0168 RMSElog: 3.9017
[epoch 11][iter  460] loss: 39.4598 RMSElog: 3.9460
[epoch 11][iter  470] loss: 33.7215 RMSElog: 3.3722
[epoch 11][iter  480] loss: 38.7849 RMSElog: 3.8785
[epoch 11][iter  490] loss: 43.3658 RMSElog: 4.3366
[epoch 11][iter  500] loss: 46.4095 RMSElog: 4.6410
[epoch 11][iter  510] loss: 43.5689 RMSElog: 4.3569
[epoch 11][iter  520] loss: 44.0591 RMSElog: 4.4059
[epoch 11][iter  530] loss: 41.8842 RMSElog: 4.1884
[epoch 11][iter  540] loss: 38.9905 RMSElog: 3.8990
[epoch 11][iter  550] loss: 43.8665 RMSElog: 4.3866
[epoch 11][iter  560] loss: 40.0188 RMSElog: 4.0019
[epoch 11][iter  570] loss: 39.7555 RMSElog: 3.9755
[epoch 11][iter  580] loss: 42.2820 RMSElog: 4.2282
[epoch 11][iter  590] loss: 45.0923 RMSElog: 4.5092
[epoch 12][iter    0] loss: 48.8379 RMSElog: 4.8838
[epoch 12][iter   10] loss: 41.8696 RMSElog: 4.1870
[epoch 12][iter   20] loss: 44.5270 RMSElog: 4.4527
[epoch 12][iter   30] loss: 46.2841 RMSElog: 4.6284
[epoch 12][iter   40] loss: 37.2023 RMSElog: 3.7202
[epoch 12][iter   50] loss: 41.6826 RMSElog: 4.1683
[epoch 12][iter   60] loss: 45.2721 RMSElog: 4.5272
[epoch 12][iter   70] loss: 42.0523 RMSElog: 4.2052
[epoch 12][iter   80] loss: 48.2653 RMSElog: 4.8265
[epoch 12][iter   90] loss: 40.5554 RMSElog: 4.0555
[epoch 12][iter  100] loss: 41.4967 RMSElog: 4.1497
[epoch 12][iter  110] loss: 43.7578 RMSElog: 4.3758
[epoch 12][iter  120] loss: 43.8843 RMSElog: 4.3884
[epoch 12][iter  130] loss: 48.0120 RMSElog: 4.8012
[epoch 12][iter  140] loss: 42.4703 RMSElog: 4.2470
[epoch 12][iter  150] loss: 45.9629 RMSElog: 4.5963
[epoch 12][iter  160] loss: 42.2607 RMSElog: 4.2261
[epoch 12][iter  170] loss: 47.6781 RMSElog: 4.7678
[epoch 12][iter  180] loss: 42.7051 RMSElog: 4.2705
[epoch 12][iter  190] loss: 38.8423 RMSElog: 3.8842
[epoch 12][iter  200] loss: 41.9015 RMSElog: 4.1901
[epoch 12][iter  210] loss: 44.9244 RMSElog: 4.4924
[epoch 12][iter  220] loss: 42.8242 RMSElog: 4.2824
[epoch 12][iter  230] loss: 45.9406 RMSElog: 4.5941
[epoch 12][iter  240] loss: 41.7583 RMSElog: 4.1758
[epoch 12][iter  250] loss: 41.9505 RMSElog: 4.1951
[epoch 12][iter  260] loss: 36.9484 RMSElog: 3.6948
[epoch 12][iter  270] loss: 40.7030 RMSElog: 4.0703
[epoch 12][iter  280] loss: 35.9162 RMSElog: 3.5916
[epoch 12][iter  290] loss: 48.7498 RMSElog: 4.8750
[epoch 12][iter  300] loss: 41.0769 RMSElog: 4.1077
[epoch 12][iter  310] loss: 50.2604 RMSElog: 5.0260
[epoch 12][iter  320] loss: 44.6112 RMSElog: 4.4611
[epoch 12][iter  330] loss: 47.3318 RMSElog: 4.7332
[epoch 12][iter  340] loss: 45.7263 RMSElog: 4.5726
[epoch 12][iter  350] loss: 38.4767 RMSElog: 3.8477
[epoch 12][iter  360] loss: 41.2764 RMSElog: 4.1276
[epoch 12][iter  370] loss: 46.4597 RMSElog: 4.6460
[epoch 12][iter  380] loss: 36.1386 RMSElog: 3.6139
[epoch 12][iter  390] loss: 41.3268 RMSElog: 4.1327
[epoch 12][iter  400] loss: 44.7513 RMSElog: 4.4751
[epoch 12][iter  410] loss: 46.6819 RMSElog: 4.6682
[epoch 12][iter  420] loss: 44.2633 RMSElog: 4.4263
[epoch 12][iter  430] loss: 37.5799 RMSElog: 3.7580
[epoch 12][iter  440] loss: 42.8175 RMSElog: 4.2817
[epoch 12][iter  450] loss: 46.3811 RMSElog: 4.6381
[epoch 12][iter  460] loss: 47.1222 RMSElog: 4.7122
[epoch 12][iter  470] loss: 55.4094 RMSElog: 5.5409
[epoch 12][iter  480] loss: 44.7699 RMSElog: 4.4770
[epoch 12][iter  490] loss: 50.3064 RMSElog: 5.0306
[epoch 12][iter  500] loss: 38.8488 RMSElog: 3.8849
[epoch 12][iter  510] loss: 41.7352 RMSElog: 4.1735
[epoch 12][iter  520] loss: 43.6919 RMSElog: 4.3692
[epoch 12][iter  530] loss: 44.7691 RMSElog: 4.4769
[epoch 12][iter  540] loss: 40.8960 RMSElog: 4.0896
[epoch 12][iter  550] loss: 47.7755 RMSElog: 4.7776
[epoch 12][iter  560] loss: 46.4380 RMSElog: 4.6438
[epoch 12][iter  570] loss: 40.8538 RMSElog: 4.0854
[epoch 12][iter  580] loss: 44.2813 RMSElog: 4.4281
[epoch 12][iter  590] loss: 43.4335 RMSElog: 4.3433
[epoch 13][iter    0] loss: 39.1585 RMSElog: 3.9158
[epoch 13][iter   10] loss: 48.5788 RMSElog: 4.8579
[epoch 13][iter   20] loss: 44.7486 RMSElog: 4.4749
[epoch 13][iter   30] loss: 38.3427 RMSElog: 3.8343
[epoch 13][iter   40] loss: 41.1639 RMSElog: 4.1164
[epoch 13][iter   50] loss: 46.7031 RMSElog: 4.6703
[epoch 13][iter   60] loss: 48.4795 RMSElog: 4.8480
[epoch 13][iter   70] loss: 39.2812 RMSElog: 3.9281
[epoch 13][iter   80] loss: 34.6629 RMSElog: 3.4663
[epoch 13][iter   90] loss: 54.7640 RMSElog: 5.4764
[epoch 13][iter  100] loss: 46.9989 RMSElog: 4.6999
[epoch 13][iter  110] loss: 42.3077 RMSElog: 4.2308
[epoch 13][iter  120] loss: 47.6160 RMSElog: 4.7616
[epoch 13][iter  130] loss: 40.1425 RMSElog: 4.0143
[epoch 13][iter  140] loss: 40.6752 RMSElog: 4.0675
[epoch 13][iter  150] loss: 50.8995 RMSElog: 5.0900
[epoch 13][iter  160] loss: 36.8982 RMSElog: 3.6898
[epoch 13][iter  170] loss: 41.2668 RMSElog: 4.1267
[epoch 13][iter  180] loss: 41.5198 RMSElog: 4.1520
[epoch 13][iter  190] loss: 38.6997 RMSElog: 3.8700
[epoch 13][iter  200] loss: 42.2913 RMSElog: 4.2291
[epoch 13][iter  210] loss: 37.0923 RMSElog: 3.7092
[epoch 13][iter  220] loss: 51.8219 RMSElog: 5.1822
[epoch 13][iter  230] loss: 36.6369 RMSElog: 3.6637
[epoch 13][iter  240] loss: 40.8072 RMSElog: 4.0807
[epoch 13][iter  250] loss: 36.9383 RMSElog: 3.6938
[epoch 13][iter  260] loss: 41.4389 RMSElog: 4.1439
[epoch 13][iter  270] loss: 42.0605 RMSElog: 4.2060
[epoch 13][iter  280] loss: 45.6868 RMSElog: 4.5687
[epoch 13][iter  290] loss: 38.0609 RMSElog: 3.8061
[epoch 13][iter  300] loss: 47.5578 RMSElog: 4.7558
[epoch 13][iter  310] loss: 45.7805 RMSElog: 4.5780
[epoch 13][iter  320] loss: 36.8781 RMSElog: 3.6878
[epoch 13][iter  330] loss: 43.2583 RMSElog: 4.3258
[epoch 13][iter  340] loss: 38.7526 RMSElog: 3.8753
[epoch 13][iter  350] loss: 36.8188 RMSElog: 3.6819
[epoch 13][iter  360] loss: 43.7982 RMSElog: 4.3798
[epoch 13][iter  370] loss: 42.4374 RMSElog: 4.2437
[epoch 13][iter  380] loss: 48.4770 RMSElog: 4.8477
[epoch 13][iter  390] loss: 49.9910 RMSElog: 4.9991
[epoch 13][iter  400] loss: 47.9911 RMSElog: 4.7991
[epoch 13][iter  410] loss: 31.9398 RMSElog: 3.1940
[epoch 13][iter  420] loss: 47.8765 RMSElog: 4.7877
[epoch 13][iter  430] loss: 50.5584 RMSElog: 5.0558
[epoch 13][iter  440] loss: 53.6382 RMSElog: 5.3638
[epoch 13][iter  450] loss: 44.4104 RMSElog: 4.4410
[epoch 13][iter  460] loss: 42.1663 RMSElog: 4.2166
[epoch 13][iter  470] loss: 36.5601 RMSElog: 3.6560
[epoch 13][iter  480] loss: 48.8253 RMSElog: 4.8825
[epoch 13][iter  490] loss: 44.4776 RMSElog: 4.4478
[epoch 13][iter  500] loss: 45.9990 RMSElog: 4.5999
[epoch 13][iter  510] loss: 43.5563 RMSElog: 4.3556
[epoch 13][iter  520] loss: 37.4727 RMSElog: 3.7473
[epoch 13][iter  530] loss: 41.4563 RMSElog: 4.1456
[epoch 13][iter  540] loss: 48.5234 RMSElog: 4.8523
[epoch 13][iter  550] loss: 41.7153 RMSElog: 4.1715
[epoch 13][iter  560] loss: 43.0812 RMSElog: 4.3081
[epoch 13][iter  570] loss: 47.7008 RMSElog: 4.7701
[epoch 13][iter  580] loss: 42.1334 RMSElog: 4.2133
[epoch 13][iter  590] loss: 42.0699 RMSElog: 4.2070
[epoch 14][iter    0] loss: 40.9191 RMSElog: 4.0919
[epoch 14][iter   10] loss: 49.4808 RMSElog: 4.9481
[epoch 14][iter   20] loss: 44.1100 RMSElog: 4.4110
[epoch 14][iter   30] loss: 43.1531 RMSElog: 4.3153
[epoch 14][iter   40] loss: 32.6766 RMSElog: 3.2677
[epoch 14][iter   50] loss: 44.7052 RMSElog: 4.4705
[epoch 14][iter   60] loss: 44.2507 RMSElog: 4.4251
[epoch 14][iter   70] loss: 35.9622 RMSElog: 3.5962
[epoch 14][iter   80] loss: 35.8170 RMSElog: 3.5817
[epoch 14][iter   90] loss: 38.1694 RMSElog: 3.8169
[epoch 14][iter  100] loss: 40.7206 RMSElog: 4.0721
[epoch 14][iter  110] loss: 51.0594 RMSElog: 5.1059
[epoch 14][iter  120] loss: 37.8451 RMSElog: 3.7845
[epoch 14][iter  130] loss: 41.9315 RMSElog: 4.1931
[epoch 14][iter  140] loss: 43.7785 RMSElog: 4.3779
[epoch 14][iter  150] loss: 42.8228 RMSElog: 4.2823
[epoch 14][iter  160] loss: 48.0162 RMSElog: 4.8016
[epoch 14][iter  170] loss: 45.2838 RMSElog: 4.5284
[epoch 14][iter  180] loss: 43.1807 RMSElog: 4.3181
[epoch 14][iter  190] loss: 37.2149 RMSElog: 3.7215
[epoch 14][iter  200] loss: 42.6851 RMSElog: 4.2685
[epoch 14][iter  210] loss: 41.1425 RMSElog: 4.1142
[epoch 14][iter  220] loss: 44.3178 RMSElog: 4.4318
[epoch 14][iter  230] loss: 44.4677 RMSElog: 4.4468
[epoch 14][iter  240] loss: 52.5657 RMSElog: 5.2566
[epoch 14][iter  250] loss: 40.5968 RMSElog: 4.0597
[epoch 14][iter  260] loss: 39.4319 RMSElog: 3.9432
[epoch 14][iter  270] loss: 45.4287 RMSElog: 4.5429
[epoch 14][iter  280] loss: 42.4658 RMSElog: 4.2466
[epoch 14][iter  290] loss: 40.5962 RMSElog: 4.0596
[epoch 14][iter  300] loss: 40.1195 RMSElog: 4.0119
[epoch 14][iter  310] loss: 43.3706 RMSElog: 4.3371
[epoch 14][iter  320] loss: 44.0804 RMSElog: 4.4080
[epoch 14][iter  330] loss: 47.0287 RMSElog: 4.7029
[epoch 14][iter  340] loss: 44.0274 RMSElog: 4.4027
[epoch 14][iter  350] loss: 41.3454 RMSElog: 4.1345
[epoch 14][iter  360] loss: 43.9683 RMSElog: 4.3968
[epoch 14][iter  370] loss: 35.8468 RMSElog: 3.5847
[epoch 14][iter  380] loss: 47.0721 RMSElog: 4.7072
[epoch 14][iter  390] loss: 42.0531 RMSElog: 4.2053
[epoch 14][iter  400] loss: 43.3480 RMSElog: 4.3348
[epoch 14][iter  410] loss: 45.8996 RMSElog: 4.5900
[epoch 14][iter  420] loss: 45.5060 RMSElog: 4.5506
[epoch 14][iter  430] loss: 39.7073 RMSElog: 3.9707
[epoch 14][iter  440] loss: 38.9018 RMSElog: 3.8902
[epoch 14][iter  450] loss: 47.7568 RMSElog: 4.7757
[epoch 14][iter  460] loss: 40.0030 RMSElog: 4.0003
[epoch 14][iter  470] loss: 45.0929 RMSElog: 4.5093
[epoch 14][iter  480] loss: 36.7738 RMSElog: 3.6774
[epoch 14][iter  490] loss: 35.8767 RMSElog: 3.5877
[epoch 14][iter  500] loss: 38.3778 RMSElog: 3.8378
[epoch 14][iter  510] loss: 37.0839 RMSElog: 3.7084
[epoch 14][iter  520] loss: 37.9054 RMSElog: 3.7905
[epoch 14][iter  530] loss: 44.4613 RMSElog: 4.4461
[epoch 14][iter  540] loss: 40.4312 RMSElog: 4.0431
[epoch 14][iter  550] loss: 40.1045 RMSElog: 4.0104
[epoch 14][iter  560] loss: 38.2300 RMSElog: 3.8230
[epoch 14][iter  570] loss: 38.8909 RMSElog: 3.8891
[epoch 14][iter  580] loss: 43.9568 RMSElog: 4.3957
[epoch 14][iter  590] loss: 50.3931 RMSElog: 5.0393
#############################################################
#T4 loss= depth_loss *10, scaling output with max from gt###
###########################################################
[epoch  0][iter    0] loss: 107.4776 RMSElog: 10.7478
[epoch  0][iter   10] loss: 97.2066 RMSElog: 9.7207
[epoch  0][iter   20] loss: 98.0998 RMSElog: 9.8100
[epoch  0][iter   30] loss: 105.4607 RMSElog: 10.5461
[epoch  0][iter   40] loss: 81.6557 RMSElog: 8.1656
[epoch  0][iter   50] loss: 94.1273 RMSElog: 9.4127
[epoch  0][iter   60] loss: 100.9995 RMSElog: 10.0999
[epoch  0][iter   70] loss: 98.4189 RMSElog: 9.8419
[epoch  0][iter   80] loss: 80.1099 RMSElog: 8.0110
[epoch  0][iter   90] loss: 91.3062 RMSElog: 9.1306
[epoch  0][iter  100] loss: 103.1093 RMSElog: 10.3109
[epoch  0][iter  110] loss: 89.6239 RMSElog: 8.9624
[epoch  0][iter  120] loss: 90.1303 RMSElog: 9.0130
[epoch  0][iter  130] loss: 88.7931 RMSElog: 8.8793
[epoch  0][iter  140] loss: 81.2774 RMSElog: 8.1277
[epoch  0][iter  150] loss: 85.0508 RMSElog: 8.5051
[epoch  0][iter  160] loss: 76.1519 RMSElog: 7.6152
[epoch  0][iter  170] loss: 76.2598 RMSElog: 7.6260
[epoch  0][iter  180] loss: 76.2693 RMSElog: 7.6269
[epoch  0][iter  190] loss: 77.5311 RMSElog: 7.7531
[epoch  0][iter  200] loss: 79.5432 RMSElog: 7.9543
[epoch  0][iter  210] loss: 74.2075 RMSElog: 7.4207
[epoch  0][iter  220] loss: 78.0465 RMSElog: 7.8047
[epoch  0][iter  230] loss: 65.9737 RMSElog: 6.5974
[epoch  0][iter  240] loss: 65.2891 RMSElog: 6.5289
[epoch  0][iter  250] loss: 71.9620 RMSElog: 7.1962
[epoch  0][iter  260] loss: 71.6289 RMSElog: 7.1629
[epoch  0][iter  270] loss: 63.1770 RMSElog: 6.3177
[epoch  0][iter  280] loss: 60.5174 RMSElog: 6.0517
[epoch  0][iter  290] loss: 69.8205 RMSElog: 6.9821
[epoch  0][iter  300] loss: 62.7728 RMSElog: 6.2773
[epoch  0][iter  310] loss: 79.0256 RMSElog: 7.9026
[epoch  0][iter  320] loss: 69.3971 RMSElog: 6.9397
[epoch  0][iter  330] loss: 59.3283 RMSElog: 5.9328
[epoch  0][iter  340] loss: 59.8497 RMSElog: 5.9850
[epoch  0][iter  350] loss: 57.7515 RMSElog: 5.7751
[epoch  0][iter  360] loss: 63.1812 RMSElog: 6.3181
[epoch  0][iter  370] loss: 58.1041 RMSElog: 5.8104
[epoch  0][iter  380] loss: 59.2254 RMSElog: 5.9225
[epoch  0][iter  390] loss: 69.5195 RMSElog: 6.9519
[epoch  0][iter  400] loss: 54.7414 RMSElog: 5.4741
[epoch  0][iter  410] loss: 58.2400 RMSElog: 5.8240
[epoch  0][iter  420] loss: 52.1792 RMSElog: 5.2179
[epoch  0][iter  430] loss: 64.4705 RMSElog: 6.4470
[epoch  0][iter  440] loss: 56.4807 RMSElog: 5.6481
[epoch  0][iter  450] loss: 51.1182 RMSElog: 5.1118
[epoch  0][iter  460] loss: 53.6719 RMSElog: 5.3672
[epoch  0][iter  470] loss: 52.4303 RMSElog: 5.2430
[epoch  0][iter  480] loss: 51.7111 RMSElog: 5.1711
[epoch  0][iter  490] loss: 57.5316 RMSElog: 5.7532
[epoch  0][iter  500] loss: 56.4488 RMSElog: 5.6449
[epoch  0][iter  510] loss: 57.0340 RMSElog: 5.7034
[epoch  0][iter  520] loss: 69.5758 RMSElog: 6.9576
[epoch  0][iter  530] loss: 52.9799 RMSElog: 5.2980
[epoch  0][iter  540] loss: 58.0127 RMSElog: 5.8013
[epoch  0][iter  550] loss: 50.1325 RMSElog: 5.0132
[epoch  0][iter  560] loss: 58.1760 RMSElog: 5.8176
[epoch  0][iter  570] loss: 60.2198 RMSElog: 6.0220
[epoch  0][iter  580] loss: 63.0919 RMSElog: 6.3092
[epoch  0][iter  590] loss: 56.7608 RMSElog: 5.6761
[epoch  1][iter    0] loss: 55.9244 RMSElog: 5.5924
[epoch  1][iter   10] loss: 51.9860 RMSElog: 5.1986
[epoch  1][iter   20] loss: 59.7472 RMSElog: 5.9747
[epoch  1][iter   30] loss: 50.6594 RMSElog: 5.0659
[epoch  1][iter   40] loss: 50.0059 RMSElog: 5.0006
[epoch  1][iter   50] loss: 54.0913 RMSElog: 5.4091
[epoch  1][iter   60] loss: 52.5697 RMSElog: 5.2570
[epoch  1][iter   70] loss: 50.9085 RMSElog: 5.0908
[epoch  1][iter   80] loss: 51.3437 RMSElog: 5.1344
[epoch  1][iter   90] loss: 54.9602 RMSElog: 5.4960
[epoch  1][iter  100] loss: 65.5310 RMSElog: 6.5531
[epoch  1][iter  110] loss: 48.9056 RMSElog: 4.8906
[epoch  1][iter  120] loss: 59.1203 RMSElog: 5.9120
[epoch  1][iter  130] loss: 61.1216 RMSElog: 6.1122
[epoch  1][iter  140] loss: 66.0832 RMSElog: 6.6083
[epoch  1][iter  150] loss: 55.6394 RMSElog: 5.5639
[epoch  1][iter  160] loss: 50.2878 RMSElog: 5.0288
[epoch  1][iter  170] loss: 44.0467 RMSElog: 4.4047
[epoch  1][iter  180] loss: 56.8026 RMSElog: 5.6803
[epoch  1][iter  190] loss: 61.0607 RMSElog: 6.1061
[epoch  1][iter  200] loss: 50.9806 RMSElog: 5.0981
[epoch  1][iter  210] loss: 64.0045 RMSElog: 6.4004
[epoch  1][iter  220] loss: 55.8401 RMSElog: 5.5840
[epoch  1][iter  230] loss: 52.0112 RMSElog: 5.2011
[epoch  1][iter  240] loss: 58.2043 RMSElog: 5.8204
[epoch  1][iter  250] loss: 54.8660 RMSElog: 5.4866
[epoch  1][iter  260] loss: 62.5686 RMSElog: 6.2569
[epoch  1][iter  270] loss: 64.6267 RMSElog: 6.4627
[epoch  1][iter  280] loss: 42.5698 RMSElog: 4.2570
[epoch  1][iter  290] loss: 49.7199 RMSElog: 4.9720
[epoch  1][iter  300] loss: 51.3284 RMSElog: 5.1328
[epoch  1][iter  310] loss: 53.3092 RMSElog: 5.3309
[epoch  1][iter  320] loss: 55.7847 RMSElog: 5.5785
[epoch  1][iter  330] loss: 59.0027 RMSElog: 5.9003
[epoch  1][iter  340] loss: 48.0152 RMSElog: 4.8015
[epoch  1][iter  350] loss: 57.0116 RMSElog: 5.7012
[epoch  1][iter  360] loss: 52.2042 RMSElog: 5.2204
[epoch  1][iter  370] loss: 58.5980 RMSElog: 5.8598
[epoch  1][iter  380] loss: 44.1256 RMSElog: 4.4126
[epoch  1][iter  390] loss: 52.8545 RMSElog: 5.2855
[epoch  1][iter  400] loss: 54.8483 RMSElog: 5.4848
[epoch  1][iter  410] loss: 47.8462 RMSElog: 4.7846
[epoch  1][iter  420] loss: 52.3170 RMSElog: 5.2317
[epoch  1][iter  430] loss: 50.5468 RMSElog: 5.0547
[epoch  1][iter  440] loss: 40.7816 RMSElog: 4.0782
[epoch  1][iter  450] loss: 49.5087 RMSElog: 4.9509
[epoch  1][iter  460] loss: 53.1408 RMSElog: 5.3141
[epoch  1][iter  470] loss: 55.8069 RMSElog: 5.5807
[epoch  1][iter  480] loss: 49.5844 RMSElog: 4.9584
[epoch  1][iter  490] loss: 47.8066 RMSElog: 4.7807
[epoch  1][iter  500] loss: 54.8883 RMSElog: 5.4888
[epoch  1][iter  510] loss: 62.6167 RMSElog: 6.2617
[epoch  1][iter  520] loss: 49.6500 RMSElog: 4.9650
[epoch  1][iter  530] loss: 54.0554 RMSElog: 5.4055
[epoch  1][iter  540] loss: 55.8948 RMSElog: 5.5895
[epoch  1][iter  550] loss: 54.1275 RMSElog: 5.4127
[epoch  1][iter  560] loss: 58.0040 RMSElog: 5.8004
[epoch  1][iter  570] loss: 50.3914 RMSElog: 5.0391
[epoch  1][iter  580] loss: 63.7977 RMSElog: 6.3798
[epoch  1][iter  590] loss: 47.3516 RMSElog: 4.7352
[epoch  2][iter    0] loss: 57.9942 RMSElog: 5.7994
[epoch  2][iter   10] loss: 57.2652 RMSElog: 5.7265
[epoch  2][iter   20] loss: 54.2474 RMSElog: 5.4247
[epoch  2][iter   30] loss: 49.0642 RMSElog: 4.9064
[epoch  2][iter   40] loss: 55.4264 RMSElog: 5.5426
[epoch  2][iter   50] loss: 52.6821 RMSElog: 5.2682
[epoch  2][iter   60] loss: 48.0816 RMSElog: 4.8082
[epoch  2][iter   70] loss: 45.1778 RMSElog: 4.5178
[epoch  2][iter   80] loss: 45.9362 RMSElog: 4.5936
[epoch  2][iter   90] loss: 47.4289 RMSElog: 4.7429
[epoch  2][iter  100] loss: 44.9087 RMSElog: 4.4909
[epoch  2][iter  110] loss: 50.8570 RMSElog: 5.0857
[epoch  2][iter  120] loss: 65.2775 RMSElog: 6.5277
[epoch  2][iter  130] loss: 41.9494 RMSElog: 4.1949
[epoch  2][iter  140] loss: 53.0456 RMSElog: 5.3046
[epoch  2][iter  150] loss: 59.2609 RMSElog: 5.9261
[epoch  2][iter  160] loss: 48.0522 RMSElog: 4.8052
[epoch  2][iter  170] loss: 54.4620 RMSElog: 5.4462
[epoch  2][iter  180] loss: 51.7067 RMSElog: 5.1707
[epoch  2][iter  190] loss: 46.6924 RMSElog: 4.6692
[epoch  2][iter  200] loss: 53.5196 RMSElog: 5.3520
[epoch  2][iter  210] loss: 50.1698 RMSElog: 5.0170
[epoch  2][iter  220] loss: 49.8442 RMSElog: 4.9844
[epoch  2][iter  230] loss: 46.2077 RMSElog: 4.6208
[epoch  2][iter  240] loss: 43.6189 RMSElog: 4.3619
[epoch  2][iter  250] loss: 55.2105 RMSElog: 5.5210
[epoch  2][iter  260] loss: 57.9629 RMSElog: 5.7963
[epoch  2][iter  270] loss: 50.2788 RMSElog: 5.0279
[epoch  2][iter  280] loss: 56.8173 RMSElog: 5.6817
[epoch  2][iter  290] loss: 55.1453 RMSElog: 5.5145
[epoch  2][iter  300] loss: 52.0066 RMSElog: 5.2007
[epoch  2][iter  310] loss: 53.4651 RMSElog: 5.3465
[epoch  2][iter  320] loss: 48.4409 RMSElog: 4.8441
[epoch  2][iter  330] loss: 52.3401 RMSElog: 5.2340
[epoch  2][iter  340] loss: 54.1877 RMSElog: 5.4188
[epoch  2][iter  350] loss: 53.4182 RMSElog: 5.3418
[epoch  2][iter  360] loss: 62.3481 RMSElog: 6.2348
[epoch  2][iter  370] loss: 45.7060 RMSElog: 4.5706
[epoch  2][iter  380] loss: 49.6110 RMSElog: 4.9611
[epoch  2][iter  390] loss: 53.7162 RMSElog: 5.3716
[epoch  2][iter  400] loss: 48.9325 RMSElog: 4.8933
[epoch  2][iter  410] loss: 52.4758 RMSElog: 5.2476
[epoch  2][iter  420] loss: 56.0799 RMSElog: 5.6080
[epoch  2][iter  430] loss: 43.0204 RMSElog: 4.3020
[epoch  2][iter  440] loss: 47.5370 RMSElog: 4.7537
[epoch  2][iter  450] loss: 52.3566 RMSElog: 5.2357
[epoch  2][iter  460] loss: 48.0943 RMSElog: 4.8094
[epoch  2][iter  470] loss: 53.4626 RMSElog: 5.3463
[epoch  2][iter  480] loss: 37.9100 RMSElog: 3.7910
[epoch  2][iter  490] loss: 56.7104 RMSElog: 5.6710
[epoch  2][iter  500] loss: 51.8953 RMSElog: 5.1895
[epoch  2][iter  510] loss: 48.6366 RMSElog: 4.8637
[epoch  2][iter  520] loss: 55.2280 RMSElog: 5.5228
[epoch  2][iter  530] loss: 52.0746 RMSElog: 5.2075
[epoch  2][iter  540] loss: 50.9266 RMSElog: 5.0927
[epoch  2][iter  550] loss: 51.7282 RMSElog: 5.1728
[epoch  2][iter  560] loss: 48.1487 RMSElog: 4.8149
[epoch  2][iter  570] loss: 44.5080 RMSElog: 4.4508
[epoch  2][iter  580] loss: 49.3286 RMSElog: 4.9329
[epoch  2][iter  590] loss: 45.9285 RMSElog: 4.5928
[epoch  3][iter    0] loss: 49.7686 RMSElog: 4.9769
[epoch  3][iter   10] loss: 55.0352 RMSElog: 5.5035
[epoch  3][iter   20] loss: 59.3704 RMSElog: 5.9370
[epoch  3][iter   30] loss: 51.9439 RMSElog: 5.1944
[epoch  3][iter   40] loss: 48.4656 RMSElog: 4.8466
[epoch  3][iter   50] loss: 48.1265 RMSElog: 4.8127
[epoch  3][iter   60] loss: 49.7724 RMSElog: 4.9772
[epoch  3][iter   70] loss: 53.1687 RMSElog: 5.3169
[epoch  3][iter   80] loss: 47.1632 RMSElog: 4.7163
[epoch  3][iter   90] loss: 49.2245 RMSElog: 4.9224
[epoch  3][iter  100] loss: 53.6638 RMSElog: 5.3664
[epoch  3][iter  110] loss: 46.7288 RMSElog: 4.6729
[epoch  3][iter  120] loss: 54.8772 RMSElog: 5.4877
[epoch  3][iter  130] loss: 43.9740 RMSElog: 4.3974
[epoch  3][iter  140] loss: 45.1582 RMSElog: 4.5158
[epoch  3][iter  150] loss: 57.7126 RMSElog: 5.7713
[epoch  3][iter  160] loss: 48.5297 RMSElog: 4.8530
[epoch  3][iter  170] loss: 49.0414 RMSElog: 4.9041
[epoch  3][iter  180] loss: 48.8850 RMSElog: 4.8885
[epoch  3][iter  190] loss: 46.1175 RMSElog: 4.6117
[epoch  3][iter  200] loss: 47.8231 RMSElog: 4.7823
[epoch  3][iter  210] loss: 48.7707 RMSElog: 4.8771
[epoch  3][iter  220] loss: 61.6445 RMSElog: 6.1644
[epoch  3][iter  230] loss: 47.5121 RMSElog: 4.7512
[epoch  3][iter  240] loss: 50.7192 RMSElog: 5.0719
[epoch  3][iter  250] loss: 54.3895 RMSElog: 5.4389
[epoch  3][iter  260] loss: 42.2856 RMSElog: 4.2286
[epoch  3][iter  270] loss: 49.7549 RMSElog: 4.9755
[epoch  3][iter  280] loss: 49.9788 RMSElog: 4.9979
[epoch  3][iter  290] loss: 48.5878 RMSElog: 4.8588
[epoch  3][iter  300] loss: 60.1439 RMSElog: 6.0144
[epoch  3][iter  310] loss: 51.4711 RMSElog: 5.1471
[epoch  3][iter  320] loss: 45.2835 RMSElog: 4.5284
[epoch  3][iter  330] loss: 48.6626 RMSElog: 4.8663
[epoch  3][iter  340] loss: 50.4026 RMSElog: 5.0403
[epoch  3][iter  350] loss: 48.4152 RMSElog: 4.8415
[epoch  3][iter  360] loss: 49.6466 RMSElog: 4.9647
[epoch  3][iter  370] loss: 54.2997 RMSElog: 5.4300
[epoch  3][iter  380] loss: 47.1761 RMSElog: 4.7176
[epoch  3][iter  390] loss: 41.0303 RMSElog: 4.1030
[epoch  3][iter  400] loss: 55.6719 RMSElog: 5.5672
[epoch  3][iter  410] loss: 48.3372 RMSElog: 4.8337
[epoch  3][iter  420] loss: 44.1099 RMSElog: 4.4110
[epoch  3][iter  430] loss: 38.8226 RMSElog: 3.8823
[epoch  3][iter  440] loss: 45.8595 RMSElog: 4.5860
[epoch  3][iter  450] loss: 46.3099 RMSElog: 4.6310
[epoch  3][iter  460] loss: 49.7842 RMSElog: 4.9784
[epoch  3][iter  470] loss: 37.0399 RMSElog: 3.7040
[epoch  3][iter  480] loss: 40.4301 RMSElog: 4.0430
[epoch  3][iter  490] loss: 46.7313 RMSElog: 4.6731
[epoch  3][iter  500] loss: 54.4605 RMSElog: 5.4460
[epoch  3][iter  510] loss: 51.8408 RMSElog: 5.1841
[epoch  3][iter  520] loss: 56.5881 RMSElog: 5.6588
[epoch  3][iter  530] loss: 47.1901 RMSElog: 4.7190
[epoch  3][iter  540] loss: 50.3143 RMSElog: 5.0314
[epoch  3][iter  550] loss: 55.7882 RMSElog: 5.5788
[epoch  3][iter  560] loss: 51.2492 RMSElog: 5.1249
[epoch  3][iter  570] loss: 43.9380 RMSElog: 4.3938
[epoch  3][iter  580] loss: 55.7646 RMSElog: 5.5765
[epoch  3][iter  590] loss: 41.2936 RMSElog: 4.1294
[epoch  4][iter    0] loss: 49.8563 RMSElog: 4.9856
[epoch  4][iter   10] loss: 54.9517 RMSElog: 5.4952
[epoch  4][iter   20] loss: 41.0124 RMSElog: 4.1012
[epoch  4][iter   30] loss: 49.6706 RMSElog: 4.9671
[epoch  4][iter   40] loss: 44.3517 RMSElog: 4.4352
[epoch  4][iter   50] loss: 49.3814 RMSElog: 4.9381
[epoch  4][iter   60] loss: 46.0500 RMSElog: 4.6050
[epoch  4][iter   70] loss: 47.8994 RMSElog: 4.7899
[epoch  4][iter   80] loss: 46.3808 RMSElog: 4.6381
[epoch  4][iter   90] loss: 53.6277 RMSElog: 5.3628
[epoch  4][iter  100] loss: 46.4063 RMSElog: 4.6406
[epoch  4][iter  110] loss: 45.5986 RMSElog: 4.5599
[epoch  4][iter  120] loss: 43.3728 RMSElog: 4.3373
[epoch  4][iter  130] loss: 54.8368 RMSElog: 5.4837
[epoch  4][iter  140] loss: 49.2429 RMSElog: 4.9243
[epoch  4][iter  150] loss: 47.5828 RMSElog: 4.7583
[epoch  4][iter  160] loss: 39.3945 RMSElog: 3.9395
[epoch  4][iter  170] loss: 43.3063 RMSElog: 4.3306
[epoch  4][iter  180] loss: 48.2086 RMSElog: 4.8209
[epoch  4][iter  190] loss: 52.4291 RMSElog: 5.2429
[epoch  4][iter  200] loss: 50.5756 RMSElog: 5.0576
[epoch  4][iter  210] loss: 48.3330 RMSElog: 4.8333
[epoch  4][iter  220] loss: 52.2468 RMSElog: 5.2247
[epoch  4][iter  230] loss: 39.2575 RMSElog: 3.9258
[epoch  4][iter  240] loss: 44.7030 RMSElog: 4.4703
[epoch  4][iter  250] loss: 41.5658 RMSElog: 4.1566
[epoch  4][iter  260] loss: 50.2943 RMSElog: 5.0294
[epoch  4][iter  270] loss: 59.1456 RMSElog: 5.9146
[epoch  4][iter  280] loss: 45.9151 RMSElog: 4.5915
[epoch  4][iter  290] loss: 47.0677 RMSElog: 4.7068
[epoch  4][iter  300] loss: 45.2707 RMSElog: 4.5271
[epoch  4][iter  310] loss: 48.5527 RMSElog: 4.8553
[epoch  4][iter  320] loss: 51.8428 RMSElog: 5.1843
[epoch  4][iter  330] loss: 53.6780 RMSElog: 5.3678
[epoch  4][iter  340] loss: 47.0930 RMSElog: 4.7093
[epoch  4][iter  350] loss: 40.5433 RMSElog: 4.0543
[epoch  4][iter  360] loss: 47.3733 RMSElog: 4.7373
[epoch  4][iter  370] loss: 50.5826 RMSElog: 5.0583
[epoch  4][iter  380] loss: 45.1082 RMSElog: 4.5108
[epoch  4][iter  390] loss: 49.8299 RMSElog: 4.9830
[epoch  4][iter  400] loss: 47.0839 RMSElog: 4.7084
[epoch  4][iter  410] loss: 50.2507 RMSElog: 5.0251
[epoch  4][iter  420] loss: 47.2446 RMSElog: 4.7245
[epoch  4][iter  430] loss: 47.5577 RMSElog: 4.7558
[epoch  4][iter  440] loss: 54.0541 RMSElog: 5.4054
[epoch  4][iter  450] loss: 49.5839 RMSElog: 4.9584
[epoch  4][iter  460] loss: 46.2259 RMSElog: 4.6226
[epoch  4][iter  470] loss: 46.4728 RMSElog: 4.6473
[epoch  4][iter  480] loss: 44.8343 RMSElog: 4.4834
[epoch  4][iter  490] loss: 46.1320 RMSElog: 4.6132
[epoch  4][iter  500] loss: 38.6512 RMSElog: 3.8651
[epoch  4][iter  510] loss: 50.4981 RMSElog: 5.0498
[epoch  4][iter  520] loss: 51.2053 RMSElog: 5.1205
[epoch  4][iter  530] loss: 46.9179 RMSElog: 4.6918
[epoch  4][iter  540] loss: 44.2696 RMSElog: 4.4270
[epoch  4][iter  550] loss: 54.4037 RMSElog: 5.4404
[epoch  4][iter  560] loss: 55.2848 RMSElog: 5.5285
[epoch  4][iter  570] loss: 55.8071 RMSElog: 5.5807
[epoch  4][iter  580] loss: 43.8424 RMSElog: 4.3842
[epoch  4][iter  590] loss: 52.9048 RMSElog: 5.2905
[epoch  5][iter    0] loss: 47.2194 RMSElog: 4.7219
[epoch  5][iter   10] loss: 44.7170 RMSElog: 4.4717
[epoch  5][iter   20] loss: 51.1548 RMSElog: 5.1155
[epoch  5][iter   30] loss: 53.3132 RMSElog: 5.3313
[epoch  5][iter   40] loss: 40.4145 RMSElog: 4.0414
[epoch  5][iter   50] loss: 54.8907 RMSElog: 5.4891
[epoch  5][iter   60] loss: 49.0454 RMSElog: 4.9045
[epoch  5][iter   70] loss: 45.1903 RMSElog: 4.5190
[epoch  5][iter   80] loss: 50.1403 RMSElog: 5.0140
[epoch  5][iter   90] loss: 50.3763 RMSElog: 5.0376
[epoch  5][iter  100] loss: 40.1033 RMSElog: 4.0103
[epoch  5][iter  110] loss: 52.0924 RMSElog: 5.2092
[epoch  5][iter  120] loss: 55.5212 RMSElog: 5.5521
[epoch  5][iter  130] loss: 47.6798 RMSElog: 4.7680
[epoch  5][iter  140] loss: 40.1265 RMSElog: 4.0127
[epoch  5][iter  150] loss: 51.8155 RMSElog: 5.1816
[epoch  5][iter  160] loss: 51.6777 RMSElog: 5.1678
[epoch  5][iter  170] loss: 47.6326 RMSElog: 4.7633
[epoch  5][iter  180] loss: 43.7451 RMSElog: 4.3745
[epoch  5][iter  190] loss: 51.9416 RMSElog: 5.1942
[epoch  5][iter  200] loss: 50.7536 RMSElog: 5.0754
[epoch  5][iter  210] loss: 43.0404 RMSElog: 4.3040
[epoch  5][iter  220] loss: 42.3728 RMSElog: 4.2373
[epoch  5][iter  230] loss: 52.1009 RMSElog: 5.2101
[epoch  5][iter  240] loss: 50.9060 RMSElog: 5.0906
[epoch  5][iter  250] loss: 43.8904 RMSElog: 4.3890
[epoch  5][iter  260] loss: 49.2516 RMSElog: 4.9252
[epoch  5][iter  270] loss: 40.0638 RMSElog: 4.0064
[epoch  5][iter  280] loss: 46.6345 RMSElog: 4.6634
[epoch  5][iter  290] loss: 53.5690 RMSElog: 5.3569
[epoch  5][iter  300] loss: 46.2380 RMSElog: 4.6238
[epoch  5][iter  310] loss: 50.6640 RMSElog: 5.0664
[epoch  5][iter  320] loss: 43.6289 RMSElog: 4.3629
[epoch  5][iter  330] loss: 53.6800 RMSElog: 5.3680
[epoch  5][iter  340] loss: 48.7725 RMSElog: 4.8772
[epoch  5][iter  350] loss: 47.7070 RMSElog: 4.7707
[epoch  5][iter  360] loss: 56.2723 RMSElog: 5.6272
[epoch  5][iter  370] loss: 54.3317 RMSElog: 5.4332
[epoch  5][iter  380] loss: 45.6008 RMSElog: 4.5601
[epoch  5][iter  390] loss: 47.2660 RMSElog: 4.7266
[epoch  5][iter  400] loss: 53.3205 RMSElog: 5.3321
[epoch  5][iter  410] loss: 43.8357 RMSElog: 4.3836
[epoch  5][iter  420] loss: 43.7034 RMSElog: 4.3703
[epoch  5][iter  430] loss: 44.2613 RMSElog: 4.4261
[epoch  5][iter  440] loss: 42.8296 RMSElog: 4.2830
[epoch  5][iter  450] loss: 42.9724 RMSElog: 4.2972
[epoch  5][iter  460] loss: 43.3652 RMSElog: 4.3365
[epoch  5][iter  470] loss: 52.0049 RMSElog: 5.2005
[epoch  5][iter  480] loss: 39.7852 RMSElog: 3.9785
[epoch  5][iter  490] loss: 40.8951 RMSElog: 4.0895
[epoch  5][iter  500] loss: 54.3289 RMSElog: 5.4329
[epoch  5][iter  510] loss: 45.1626 RMSElog: 4.5163
[epoch  5][iter  520] loss: 49.0224 RMSElog: 4.9022
[epoch  5][iter  530] loss: 53.4768 RMSElog: 5.3477
[epoch  5][iter  540] loss: 50.9144 RMSElog: 5.0914
[epoch  5][iter  550] loss: 38.0665 RMSElog: 3.8066
[epoch  5][iter  560] loss: 49.4042 RMSElog: 4.9404
[epoch  5][iter  570] loss: 42.9078 RMSElog: 4.2908
[epoch  5][iter  580] loss: 47.6474 RMSElog: 4.7647
[epoch  5][iter  590] loss: 45.3084 RMSElog: 4.5308
[epoch  6][iter    0] loss: 42.3588 RMSElog: 4.2359
[epoch  6][iter   10] loss: 49.5942 RMSElog: 4.9594
[epoch  6][iter   20] loss: 44.4865 RMSElog: 4.4487
[epoch  6][iter   30] loss: 46.0306 RMSElog: 4.6031
[epoch  6][iter   40] loss: 53.4999 RMSElog: 5.3500
[epoch  6][iter   50] loss: 45.6343 RMSElog: 4.5634
[epoch  6][iter   60] loss: 36.2241 RMSElog: 3.6224
[epoch  6][iter   70] loss: 46.3494 RMSElog: 4.6349
[epoch  6][iter   80] loss: 47.4033 RMSElog: 4.7403
[epoch  6][iter   90] loss: 37.9769 RMSElog: 3.7977
[epoch  6][iter  100] loss: 37.8239 RMSElog: 3.7824
[epoch  6][iter  110] loss: 34.2355 RMSElog: 3.4236
[epoch  6][iter  120] loss: 41.4603 RMSElog: 4.1460
[epoch  6][iter  130] loss: 34.3575 RMSElog: 3.4358
[epoch  6][iter  140] loss: 36.6652 RMSElog: 3.6665
[epoch  6][iter  150] loss: 41.4960 RMSElog: 4.1496
[epoch  6][iter  160] loss: 45.9345 RMSElog: 4.5934
[epoch  6][iter  170] loss: 46.1108 RMSElog: 4.6111
[epoch  6][iter  180] loss: 49.8482 RMSElog: 4.9848
[epoch  6][iter  190] loss: 44.2391 RMSElog: 4.4239
[epoch  6][iter  200] loss: 38.7402 RMSElog: 3.8740
[epoch  6][iter  210] loss: 49.0866 RMSElog: 4.9087
[epoch  6][iter  220] loss: 44.2443 RMSElog: 4.4244
[epoch  6][iter  230] loss: 46.1602 RMSElog: 4.6160
[epoch  6][iter  240] loss: 40.5125 RMSElog: 4.0513
[epoch  6][iter  250] loss: 51.8363 RMSElog: 5.1836
[epoch  6][iter  260] loss: 43.3468 RMSElog: 4.3347
[epoch  6][iter  270] loss: 44.3449 RMSElog: 4.4345
[epoch  6][iter  280] loss: 41.8464 RMSElog: 4.1846
[epoch  6][iter  290] loss: 40.2569 RMSElog: 4.0257
[epoch  6][iter  300] loss: 44.3891 RMSElog: 4.4389
[epoch  6][iter  310] loss: 38.9018 RMSElog: 3.8902
[epoch  6][iter  320] loss: 44.3404 RMSElog: 4.4340
[epoch  6][iter  330] loss: 47.5415 RMSElog: 4.7542
[epoch  6][iter  340] loss: 47.8555 RMSElog: 4.7856
[epoch  6][iter  350] loss: 46.2639 RMSElog: 4.6264
[epoch  6][iter  360] loss: 42.7639 RMSElog: 4.2764
[epoch  6][iter  370] loss: 51.4850 RMSElog: 5.1485
[epoch  6][iter  380] loss: 44.9474 RMSElog: 4.4947
[epoch  6][iter  390] loss: 47.9544 RMSElog: 4.7954
[epoch  6][iter  400] loss: 44.8469 RMSElog: 4.4847
[epoch  6][iter  410] loss: 49.3027 RMSElog: 4.9303
[epoch  6][iter  420] loss: 42.2130 RMSElog: 4.2213
[epoch  6][iter  430] loss: 41.7886 RMSElog: 4.1789
[epoch  6][iter  440] loss: 42.2689 RMSElog: 4.2269
[epoch  6][iter  450] loss: 45.5664 RMSElog: 4.5566
[epoch  6][iter  460] loss: 47.5094 RMSElog: 4.7509
[epoch  6][iter  470] loss: 51.4334 RMSElog: 5.1433
[epoch  6][iter  480] loss: 32.6856 RMSElog: 3.2686
[epoch  6][iter  490] loss: 43.1195 RMSElog: 4.3120
[epoch  6][iter  500] loss: 39.1094 RMSElog: 3.9109
[epoch  6][iter  510] loss: 59.0434 RMSElog: 5.9043
[epoch  6][iter  520] loss: 43.0517 RMSElog: 4.3052
[epoch  6][iter  530] loss: 46.5111 RMSElog: 4.6511
[epoch  6][iter  540] loss: 42.4600 RMSElog: 4.2460
[epoch  6][iter  550] loss: 39.0981 RMSElog: 3.9098
[epoch  6][iter  560] loss: 62.5442 RMSElog: 6.2544
[epoch  6][iter  570] loss: 39.7852 RMSElog: 3.9785
[epoch  6][iter  580] loss: 43.2921 RMSElog: 4.3292
[epoch  6][iter  590] loss: 45.7001 RMSElog: 4.5700
[epoch  7][iter    0] loss: 56.1994 RMSElog: 5.6199
[epoch  7][iter   10] loss: 45.8580 RMSElog: 4.5858
[epoch  7][iter   20] loss: 42.9908 RMSElog: 4.2991
[epoch  7][iter   30] loss: 48.5771 RMSElog: 4.8577
[epoch  7][iter   40] loss: 38.2988 RMSElog: 3.8299
[epoch  7][iter   50] loss: 45.2809 RMSElog: 4.5281
[epoch  7][iter   60] loss: 51.7475 RMSElog: 5.1748
[epoch  7][iter   70] loss: 37.5552 RMSElog: 3.7555
[epoch  7][iter   80] loss: 37.7135 RMSElog: 3.7713
[epoch  7][iter   90] loss: 39.8827 RMSElog: 3.9883
[epoch  7][iter  100] loss: 37.5781 RMSElog: 3.7578
[epoch  7][iter  110] loss: 52.2628 RMSElog: 5.2263
[epoch  7][iter  120] loss: 45.9137 RMSElog: 4.5914
[epoch  7][iter  130] loss: 37.2266 RMSElog: 3.7227
[epoch  7][iter  140] loss: 50.4210 RMSElog: 5.0421
[epoch  7][iter  150] loss: 46.2351 RMSElog: 4.6235
[epoch  7][iter  160] loss: 48.4205 RMSElog: 4.8420
[epoch  7][iter  170] loss: 42.7478 RMSElog: 4.2748
[epoch  7][iter  180] loss: 49.0679 RMSElog: 4.9068
[epoch  7][iter  190] loss: 42.4929 RMSElog: 4.2493
[epoch  7][iter  200] loss: 45.3344 RMSElog: 4.5334
[epoch  7][iter  210] loss: 45.3967 RMSElog: 4.5397
[epoch  7][iter  220] loss: 43.0852 RMSElog: 4.3085
[epoch  7][iter  230] loss: 48.3814 RMSElog: 4.8381
[epoch  7][iter  240] loss: 43.6859 RMSElog: 4.3686
[epoch  7][iter  250] loss: 38.1850 RMSElog: 3.8185
[epoch  7][iter  260] loss: 46.1639 RMSElog: 4.6164
[epoch  7][iter  270] loss: 42.5094 RMSElog: 4.2509
[epoch  7][iter  280] loss: 48.8219 RMSElog: 4.8822
[epoch  7][iter  290] loss: 48.4588 RMSElog: 4.8459
[epoch  7][iter  300] loss: 44.1137 RMSElog: 4.4114
[epoch  7][iter  310] loss: 49.7147 RMSElog: 4.9715
[epoch  7][iter  320] loss: 45.7259 RMSElog: 4.5726
[epoch  7][iter  330] loss: 38.5163 RMSElog: 3.8516
[epoch  7][iter  340] loss: 45.6730 RMSElog: 4.5673
[epoch  7][iter  350] loss: 36.1601 RMSElog: 3.6160
[epoch  7][iter  360] loss: 41.1375 RMSElog: 4.1138
[epoch  7][iter  370] loss: 46.0405 RMSElog: 4.6041
[epoch  7][iter  380] loss: 38.3986 RMSElog: 3.8399
[epoch  7][iter  390] loss: 38.7612 RMSElog: 3.8761
[epoch  7][iter  400] loss: 38.9351 RMSElog: 3.8935
[epoch  7][iter  410] loss: 47.2026 RMSElog: 4.7203
[epoch  7][iter  420] loss: 43.5234 RMSElog: 4.3523
[epoch  7][iter  430] loss: 39.4165 RMSElog: 3.9416
[epoch  7][iter  440] loss: 36.3140 RMSElog: 3.6314
[epoch  7][iter  450] loss: 45.7280 RMSElog: 4.5728
[epoch  7][iter  460] loss: 39.8930 RMSElog: 3.9893
[epoch  7][iter  470] loss: 43.9603 RMSElog: 4.3960
[epoch  7][iter  480] loss: 48.2339 RMSElog: 4.8234
[epoch  7][iter  490] loss: 51.7218 RMSElog: 5.1722
[epoch  7][iter  500] loss: 48.4489 RMSElog: 4.8449
[epoch  7][iter  510] loss: 44.5908 RMSElog: 4.4591
[epoch  7][iter  520] loss: 48.9356 RMSElog: 4.8936
[epoch  7][iter  530] loss: 37.4460 RMSElog: 3.7446
[epoch  7][iter  540] loss: 45.9017 RMSElog: 4.5902
[epoch  7][iter  550] loss: 37.3321 RMSElog: 3.7332
[epoch  7][iter  560] loss: 43.0358 RMSElog: 4.3036
[epoch  7][iter  570] loss: 49.3983 RMSElog: 4.9398
[epoch  7][iter  580] loss: 51.5444 RMSElog: 5.1544
[epoch  7][iter  590] loss: 46.7666 RMSElog: 4.6767
[epoch  8][iter    0] loss: 45.4146 RMSElog: 4.5415
[epoch  8][iter   10] loss: 45.3757 RMSElog: 4.5376
[epoch  8][iter   20] loss: 53.8864 RMSElog: 5.3886
[epoch  8][iter   30] loss: 46.1583 RMSElog: 4.6158
[epoch  8][iter   40] loss: 52.5306 RMSElog: 5.2531
[epoch  8][iter   50] loss: 37.3647 RMSElog: 3.7365
[epoch  8][iter   60] loss: 52.0659 RMSElog: 5.2066
[epoch  8][iter   70] loss: 41.6536 RMSElog: 4.1654
[epoch  8][iter   80] loss: 45.4258 RMSElog: 4.5426
[epoch  8][iter   90] loss: 43.8019 RMSElog: 4.3802
[epoch  8][iter  100] loss: 41.8083 RMSElog: 4.1808
[epoch  8][iter  110] loss: 55.3853 RMSElog: 5.5385
[epoch  8][iter  120] loss: 49.2904 RMSElog: 4.9290
[epoch  8][iter  130] loss: 45.9144 RMSElog: 4.5914
[epoch  8][iter  140] loss: 38.8700 RMSElog: 3.8870
[epoch  8][iter  150] loss: 48.7157 RMSElog: 4.8716
[epoch  8][iter  160] loss: 44.9538 RMSElog: 4.4954
[epoch  8][iter  170] loss: 46.9459 RMSElog: 4.6946
[epoch  8][iter  180] loss: 48.6520 RMSElog: 4.8652
[epoch  8][iter  190] loss: 39.1473 RMSElog: 3.9147
[epoch  8][iter  200] loss: 46.8322 RMSElog: 4.6832
[epoch  8][iter  210] loss: 49.8840 RMSElog: 4.9884
[epoch  8][iter  220] loss: 49.9569 RMSElog: 4.9957
[epoch  8][iter  230] loss: 33.3779 RMSElog: 3.3378
[epoch  8][iter  240] loss: 43.2304 RMSElog: 4.3230
[epoch  8][iter  250] loss: 53.0991 RMSElog: 5.3099
[epoch  8][iter  260] loss: 40.1472 RMSElog: 4.0147
[epoch  8][iter  270] loss: 39.0913 RMSElog: 3.9091
[epoch  8][iter  280] loss: 46.7456 RMSElog: 4.6746
[epoch  8][iter  290] loss: 47.5444 RMSElog: 4.7544
[epoch  8][iter  300] loss: 44.2952 RMSElog: 4.4295
[epoch  8][iter  310] loss: 45.0479 RMSElog: 4.5048
[epoch  8][iter  320] loss: 44.7559 RMSElog: 4.4756
[epoch  8][iter  330] loss: 48.0414 RMSElog: 4.8041
[epoch  8][iter  340] loss: 52.0436 RMSElog: 5.2044
[epoch  8][iter  350] loss: 50.9818 RMSElog: 5.0982
[epoch  8][iter  360] loss: 49.1281 RMSElog: 4.9128
[epoch  8][iter  370] loss: 47.4816 RMSElog: 4.7482
[epoch  8][iter  380] loss: 45.7794 RMSElog: 4.5779
[epoch  8][iter  390] loss: 50.6293 RMSElog: 5.0629
[epoch  8][iter  400] loss: 39.8047 RMSElog: 3.9805
[epoch  8][iter  410] loss: 38.2166 RMSElog: 3.8217
[epoch  8][iter  420] loss: 45.0623 RMSElog: 4.5062
[epoch  8][iter  430] loss: 39.1646 RMSElog: 3.9165
[epoch  8][iter  440] loss: 36.6777 RMSElog: 3.6678
[epoch  8][iter  450] loss: 52.1742 RMSElog: 5.2174
[epoch  8][iter  460] loss: 49.5662 RMSElog: 4.9566
[epoch  8][iter  470] loss: 40.9416 RMSElog: 4.0942
[epoch  8][iter  480] loss: 36.6620 RMSElog: 3.6662
[epoch  8][iter  490] loss: 41.6274 RMSElog: 4.1627
[epoch  8][iter  500] loss: 44.1211 RMSElog: 4.4121
[epoch  8][iter  510] loss: 41.9130 RMSElog: 4.1913
[epoch  8][iter  520] loss: 45.3482 RMSElog: 4.5348
[epoch  8][iter  530] loss: 43.4631 RMSElog: 4.3463
[epoch  8][iter  540] loss: 44.2605 RMSElog: 4.4261
[epoch  8][iter  550] loss: 34.6301 RMSElog: 3.4630
[epoch  8][iter  560] loss: 47.2687 RMSElog: 4.7269
[epoch  8][iter  570] loss: 46.9569 RMSElog: 4.6957
[epoch  8][iter  580] loss: 38.7976 RMSElog: 3.8798
[epoch  8][iter  590] loss: 39.1267 RMSElog: 3.9127
[epoch  9][iter    0] loss: 48.2803 RMSElog: 4.8280
[epoch  9][iter   10] loss: 40.2451 RMSElog: 4.0245
[epoch  9][iter   20] loss: 43.3937 RMSElog: 4.3394
[epoch  9][iter   30] loss: 44.4629 RMSElog: 4.4463
[epoch  9][iter   40] loss: 45.4543 RMSElog: 4.5454
[epoch  9][iter   50] loss: 43.2826 RMSElog: 4.3283
[epoch  9][iter   60] loss: 49.1366 RMSElog: 4.9137
[epoch  9][iter   70] loss: 44.4789 RMSElog: 4.4479
[epoch  9][iter   80] loss: 42.6145 RMSElog: 4.2615
[epoch  9][iter   90] loss: 33.2858 RMSElog: 3.3286
[epoch  9][iter  100] loss: 46.9534 RMSElog: 4.6953
[epoch  9][iter  110] loss: 43.9885 RMSElog: 4.3989
[epoch  9][iter  120] loss: 46.1390 RMSElog: 4.6139
[epoch  9][iter  130] loss: 48.0767 RMSElog: 4.8077
[epoch  9][iter  140] loss: 42.4739 RMSElog: 4.2474
[epoch  9][iter  150] loss: 47.0000 RMSElog: 4.7000
[epoch  9][iter  160] loss: 37.9479 RMSElog: 3.7948
[epoch  9][iter  170] loss: 37.8357 RMSElog: 3.7836
[epoch  9][iter  180] loss: 37.0978 RMSElog: 3.7098
[epoch  9][iter  190] loss: 41.2368 RMSElog: 4.1237
[epoch  9][iter  200] loss: 44.0675 RMSElog: 4.4067
[epoch  9][iter  210] loss: 50.6773 RMSElog: 5.0677
[epoch  9][iter  220] loss: 39.5681 RMSElog: 3.9568
[epoch  9][iter  230] loss: 42.5267 RMSElog: 4.2527
[epoch  9][iter  240] loss: 46.4542 RMSElog: 4.6454
[epoch  9][iter  250] loss: 47.2414 RMSElog: 4.7241
[epoch  9][iter  260] loss: 48.5652 RMSElog: 4.8565
[epoch  9][iter  270] loss: 38.5373 RMSElog: 3.8537
[epoch  9][iter  280] loss: 50.7784 RMSElog: 5.0778
[epoch  9][iter  290] loss: 42.9326 RMSElog: 4.2933
[epoch  9][iter  300] loss: 38.1510 RMSElog: 3.8151
[epoch  9][iter  310] loss: 38.2937 RMSElog: 3.8294
[epoch  9][iter  320] loss: 53.6050 RMSElog: 5.3605
[epoch  9][iter  330] loss: 52.2537 RMSElog: 5.2254
[epoch  9][iter  340] loss: 37.5371 RMSElog: 3.7537
[epoch  9][iter  350] loss: 45.6720 RMSElog: 4.5672
[epoch  9][iter  360] loss: 41.3554 RMSElog: 4.1355
[epoch  9][iter  370] loss: 49.6487 RMSElog: 4.9649
[epoch  9][iter  380] loss: 43.6463 RMSElog: 4.3646
[epoch  9][iter  390] loss: 39.8449 RMSElog: 3.9845
[epoch  9][iter  400] loss: 38.3115 RMSElog: 3.8311
[epoch  9][iter  410] loss: 45.7638 RMSElog: 4.5764
[epoch  9][iter  420] loss: 49.4077 RMSElog: 4.9408
[epoch  9][iter  430] loss: 42.5957 RMSElog: 4.2596
[epoch  9][iter  440] loss: 49.2573 RMSElog: 4.9257
[epoch  9][iter  450] loss: 38.8888 RMSElog: 3.8889
[epoch  9][iter  460] loss: 47.2050 RMSElog: 4.7205
[epoch  9][iter  470] loss: 46.3841 RMSElog: 4.6384
[epoch  9][iter  480] loss: 46.6779 RMSElog: 4.6678
[epoch  9][iter  490] loss: 36.7986 RMSElog: 3.6799
[epoch  9][iter  500] loss: 39.7365 RMSElog: 3.9737
[epoch  9][iter  510] loss: 44.8619 RMSElog: 4.4862
[epoch  9][iter  520] loss: 44.5564 RMSElog: 4.4556
[epoch  9][iter  530] loss: 40.7133 RMSElog: 4.0713
[epoch  9][iter  540] loss: 48.2264 RMSElog: 4.8226
[epoch  9][iter  550] loss: 38.5701 RMSElog: 3.8570
[epoch  9][iter  560] loss: 50.5257 RMSElog: 5.0526
[epoch  9][iter  570] loss: 51.3270 RMSElog: 5.1327
[epoch  9][iter  580] loss: 39.2435 RMSElog: 3.9244
[epoch  9][iter  590] loss: 49.6613 RMSElog: 4.9661
[epoch 10][iter    0] loss: 36.9322 RMSElog: 3.6932
[epoch 10][iter   10] loss: 50.0673 RMSElog: 5.0067
[epoch 10][iter   20] loss: 46.0271 RMSElog: 4.6027
[epoch 10][iter   30] loss: 46.9721 RMSElog: 4.6972
[epoch 10][iter   40] loss: 43.2485 RMSElog: 4.3249
[epoch 10][iter   50] loss: 41.5781 RMSElog: 4.1578
[epoch 10][iter   60] loss: 38.7275 RMSElog: 3.8728
[epoch 10][iter   70] loss: 43.7876 RMSElog: 4.3788
[epoch 10][iter   80] loss: 44.1933 RMSElog: 4.4193
[epoch 10][iter   90] loss: 40.7443 RMSElog: 4.0744
[epoch 10][iter  100] loss: 46.7287 RMSElog: 4.6729
[epoch 10][iter  110] loss: 36.2556 RMSElog: 3.6256
[epoch 10][iter  120] loss: 34.7670 RMSElog: 3.4767
[epoch 10][iter  130] loss: 42.8248 RMSElog: 4.2825
[epoch 10][iter  140] loss: 41.2401 RMSElog: 4.1240
[epoch 10][iter  150] loss: 47.8613 RMSElog: 4.7861
[epoch 10][iter  160] loss: 51.9552 RMSElog: 5.1955
[epoch 10][iter  170] loss: 48.7420 RMSElog: 4.8742
[epoch 10][iter  180] loss: 52.0303 RMSElog: 5.2030
[epoch 10][iter  190] loss: 46.6908 RMSElog: 4.6691
[epoch 10][iter  200] loss: 43.1403 RMSElog: 4.3140
[epoch 10][iter  210] loss: 52.3263 RMSElog: 5.2326
[epoch 10][iter  220] loss: 48.3106 RMSElog: 4.8311
[epoch 10][iter  230] loss: 45.6714 RMSElog: 4.5671
[epoch 10][iter  240] loss: 39.3557 RMSElog: 3.9356
[epoch 10][iter  250] loss: 45.6907 RMSElog: 4.5691
[epoch 10][iter  260] loss: 45.5892 RMSElog: 4.5589
[epoch 10][iter  270] loss: 41.6469 RMSElog: 4.1647
[epoch 10][iter  280] loss: 47.9582 RMSElog: 4.7958
[epoch 10][iter  290] loss: 39.8242 RMSElog: 3.9824
[epoch 10][iter  300] loss: 38.8447 RMSElog: 3.8845
[epoch 10][iter  310] loss: 49.4028 RMSElog: 4.9403
[epoch 10][iter  320] loss: 36.1965 RMSElog: 3.6196
[epoch 10][iter  330] loss: 40.1309 RMSElog: 4.0131
[epoch 10][iter  340] loss: 44.7019 RMSElog: 4.4702
[epoch 10][iter  350] loss: 48.5516 RMSElog: 4.8552
[epoch 10][iter  360] loss: 40.3723 RMSElog: 4.0372
[epoch 10][iter  370] loss: 37.8670 RMSElog: 3.7867
[epoch 10][iter  380] loss: 45.5332 RMSElog: 4.5533
[epoch 10][iter  390] loss: 41.4400 RMSElog: 4.1440
[epoch 10][iter  400] loss: 49.8790 RMSElog: 4.9879
[epoch 10][iter  410] loss: 40.2551 RMSElog: 4.0255
[epoch 10][iter  420] loss: 49.5539 RMSElog: 4.9554
[epoch 10][iter  430] loss: 47.0093 RMSElog: 4.7009
[epoch 10][iter  440] loss: 40.3434 RMSElog: 4.0343
[epoch 10][iter  450] loss: 52.1570 RMSElog: 5.2157
[epoch 10][iter  460] loss: 40.0971 RMSElog: 4.0097
[epoch 10][iter  470] loss: 43.0363 RMSElog: 4.3036
[epoch 10][iter  480] loss: 48.4283 RMSElog: 4.8428
[epoch 10][iter  490] loss: 47.1628 RMSElog: 4.7163
[epoch 10][iter  500] loss: 43.2430 RMSElog: 4.3243
[epoch 10][iter  510] loss: 40.3572 RMSElog: 4.0357
[epoch 10][iter  520] loss: 42.7396 RMSElog: 4.2740
[epoch 10][iter  530] loss: 45.6521 RMSElog: 4.5652
[epoch 10][iter  540] loss: 45.5196 RMSElog: 4.5520
[epoch 10][iter  550] loss: 39.7504 RMSElog: 3.9750
[epoch 10][iter  560] loss: 44.9042 RMSElog: 4.4904
[epoch 10][iter  570] loss: 42.9525 RMSElog: 4.2953
[epoch 10][iter  580] loss: 42.4960 RMSElog: 4.2496
[epoch 10][iter  590] loss: 54.5275 RMSElog: 5.4527
[epoch 11][iter    0] loss: 48.0171 RMSElog: 4.8017
[epoch 11][iter   10] loss: 37.0460 RMSElog: 3.7046
[epoch 11][iter   20] loss: 50.3881 RMSElog: 5.0388
[epoch 11][iter   30] loss: 38.2126 RMSElog: 3.8213
[epoch 11][iter   40] loss: 51.8607 RMSElog: 5.1861
[epoch 11][iter   50] loss: 50.1768 RMSElog: 5.0177
[epoch 11][iter   60] loss: 43.9438 RMSElog: 4.3944
[epoch 11][iter   70] loss: 42.8964 RMSElog: 4.2896
[epoch 11][iter   80] loss: 47.6764 RMSElog: 4.7676
[epoch 11][iter   90] loss: 44.6619 RMSElog: 4.4662
[epoch 11][iter  100] loss: 39.6338 RMSElog: 3.9634
[epoch 11][iter  110] loss: 38.8577 RMSElog: 3.8858
[epoch 11][iter  120] loss: 51.3847 RMSElog: 5.1385
[epoch 11][iter  130] loss: 47.4788 RMSElog: 4.7479
[epoch 11][iter  140] loss: 45.0597 RMSElog: 4.5060
[epoch 11][iter  150] loss: 46.0444 RMSElog: 4.6044
[epoch 11][iter  160] loss: 44.1372 RMSElog: 4.4137
[epoch 11][iter  170] loss: 43.7385 RMSElog: 4.3739
[epoch 11][iter  180] loss: 54.7321 RMSElog: 5.4732
[epoch 11][iter  190] loss: 37.9677 RMSElog: 3.7968
[epoch 11][iter  200] loss: 50.4056 RMSElog: 5.0406
[epoch 11][iter  210] loss: 43.0658 RMSElog: 4.3066
[epoch 11][iter  220] loss: 40.2133 RMSElog: 4.0213
[epoch 11][iter  230] loss: 41.7257 RMSElog: 4.1726
[epoch 11][iter  240] loss: 47.5293 RMSElog: 4.7529
[epoch 11][iter  250] loss: 46.4765 RMSElog: 4.6477
[epoch 11][iter  260] loss: 40.1917 RMSElog: 4.0192
[epoch 11][iter  270] loss: 42.2832 RMSElog: 4.2283
[epoch 11][iter  280] loss: 43.0431 RMSElog: 4.3043
[epoch 11][iter  290] loss: 44.3565 RMSElog: 4.4357
[epoch 11][iter  300] loss: 48.3089 RMSElog: 4.8309
[epoch 11][iter  310] loss: 49.3702 RMSElog: 4.9370
[epoch 11][iter  320] loss: 45.1334 RMSElog: 4.5133
[epoch 11][iter  330] loss: 51.7693 RMSElog: 5.1769
[epoch 11][iter  340] loss: 43.9596 RMSElog: 4.3960
[epoch 11][iter  350] loss: 49.4926 RMSElog: 4.9493
[epoch 11][iter  360] loss: 41.6772 RMSElog: 4.1677
[epoch 11][iter  370] loss: 44.4427 RMSElog: 4.4443
[epoch 11][iter  380] loss: 44.5344 RMSElog: 4.4534
[epoch 11][iter  390] loss: 41.9797 RMSElog: 4.1980
[epoch 11][iter  400] loss: 41.8811 RMSElog: 4.1881
[epoch 11][iter  410] loss: 43.3385 RMSElog: 4.3338
[epoch 11][iter  420] loss: 42.1057 RMSElog: 4.2106
[epoch 11][iter  430] loss: 41.9767 RMSElog: 4.1977
[epoch 11][iter  440] loss: 44.0306 RMSElog: 4.4031
[epoch 11][iter  450] loss: 42.1814 RMSElog: 4.2181
[epoch 11][iter  460] loss: 40.4365 RMSElog: 4.0436
[epoch 11][iter  470] loss: 51.5654 RMSElog: 5.1565
[epoch 11][iter  480] loss: 36.5994 RMSElog: 3.6599
[epoch 11][iter  490] loss: 48.1645 RMSElog: 4.8164
[epoch 11][iter  500] loss: 47.8491 RMSElog: 4.7849
[epoch 11][iter  510] loss: 55.0259 RMSElog: 5.5026
[epoch 11][iter  520] loss: 40.8997 RMSElog: 4.0900
[epoch 11][iter  530] loss: 39.7577 RMSElog: 3.9758
[epoch 11][iter  540] loss: 49.6823 RMSElog: 4.9682
[epoch 11][iter  550] loss: 37.8019 RMSElog: 3.7802
[epoch 11][iter  560] loss: 46.7549 RMSElog: 4.6755
[epoch 11][iter  570] loss: 50.8410 RMSElog: 5.0841
[epoch 11][iter  580] loss: 41.0172 RMSElog: 4.1017
[epoch 11][iter  590] loss: 43.1137 RMSElog: 4.3114
[epoch 12][iter    0] loss: 52.4561 RMSElog: 5.2456
[epoch 12][iter   10] loss: 42.6531 RMSElog: 4.2653
[epoch 12][iter   20] loss: 46.8649 RMSElog: 4.6865
[epoch 12][iter   30] loss: 38.5707 RMSElog: 3.8571
[epoch 12][iter   40] loss: 43.8253 RMSElog: 4.3825
[epoch 12][iter   50] loss: 44.7010 RMSElog: 4.4701
[epoch 12][iter   60] loss: 52.3588 RMSElog: 5.2359
[epoch 12][iter   70] loss: 45.0284 RMSElog: 4.5028
[epoch 12][iter   80] loss: 39.4321 RMSElog: 3.9432
[epoch 12][iter   90] loss: 45.9708 RMSElog: 4.5971
[epoch 12][iter  100] loss: 45.9721 RMSElog: 4.5972
[epoch 12][iter  110] loss: 53.8683 RMSElog: 5.3868
[epoch 12][iter  120] loss: 53.2863 RMSElog: 5.3286
[epoch 12][iter  130] loss: 44.7903 RMSElog: 4.4790
[epoch 12][iter  140] loss: 41.6061 RMSElog: 4.1606
[epoch 12][iter  150] loss: 41.5128 RMSElog: 4.1513
[epoch 12][iter  160] loss: 41.9175 RMSElog: 4.1917
[epoch 12][iter  170] loss: 38.9922 RMSElog: 3.8992
[epoch 12][iter  180] loss: 46.6521 RMSElog: 4.6652
[epoch 12][iter  190] loss: 46.7518 RMSElog: 4.6752
[epoch 12][iter  200] loss: 42.7828 RMSElog: 4.2783
[epoch 12][iter  210] loss: 36.7688 RMSElog: 3.6769
[epoch 12][iter  220] loss: 42.3991 RMSElog: 4.2399
[epoch 12][iter  230] loss: 48.0386 RMSElog: 4.8039
[epoch 12][iter  240] loss: 41.1742 RMSElog: 4.1174
[epoch 12][iter  250] loss: 49.2379 RMSElog: 4.9238
[epoch 12][iter  260] loss: 36.9899 RMSElog: 3.6990
[epoch 12][iter  270] loss: 43.9121 RMSElog: 4.3912
[epoch 12][iter  280] loss: 40.3402 RMSElog: 4.0340
[epoch 12][iter  290] loss: 50.4040 RMSElog: 5.0404
[epoch 12][iter  300] loss: 50.0511 RMSElog: 5.0051
[epoch 12][iter  310] loss: 39.0163 RMSElog: 3.9016
[epoch 12][iter  320] loss: 48.2524 RMSElog: 4.8252
[epoch 12][iter  330] loss: 44.3226 RMSElog: 4.4323
[epoch 12][iter  340] loss: 46.3919 RMSElog: 4.6392
[epoch 12][iter  350] loss: 49.2199 RMSElog: 4.9220
[epoch 12][iter  360] loss: 37.0605 RMSElog: 3.7061
[epoch 12][iter  370] loss: 31.9920 RMSElog: 3.1992
[epoch 12][iter  380] loss: 38.0189 RMSElog: 3.8019
[epoch 12][iter  390] loss: 49.7202 RMSElog: 4.9720
[epoch 12][iter  400] loss: 37.9657 RMSElog: 3.7966
[epoch 12][iter  410] loss: 44.8253 RMSElog: 4.4825
[epoch 12][iter  420] loss: 42.3376 RMSElog: 4.2338
[epoch 12][iter  430] loss: 41.8963 RMSElog: 4.1896
[epoch 12][iter  440] loss: 40.8205 RMSElog: 4.0820
[epoch 12][iter  450] loss: 48.8301 RMSElog: 4.8830
[epoch 12][iter  460] loss: 48.4119 RMSElog: 4.8412
[epoch 12][iter  470] loss: 42.5033 RMSElog: 4.2503
[epoch 12][iter  480] loss: 40.9666 RMSElog: 4.0967
[epoch 12][iter  490] loss: 37.9127 RMSElog: 3.7913
[epoch 12][iter  500] loss: 40.1013 RMSElog: 4.0101
[epoch 12][iter  510] loss: 43.7054 RMSElog: 4.3705
[epoch 12][iter  520] loss: 43.0755 RMSElog: 4.3076
[epoch 12][iter  530] loss: 41.7644 RMSElog: 4.1764
[epoch 12][iter  540] loss: 46.0936 RMSElog: 4.6094
[epoch 12][iter  550] loss: 41.8539 RMSElog: 4.1854
[epoch 12][iter  560] loss: 51.2599 RMSElog: 5.1260
[epoch 12][iter  570] loss: 43.3326 RMSElog: 4.3333
[epoch 12][iter  580] loss: 41.4786 RMSElog: 4.1479
[epoch 12][iter  590] loss: 41.3379 RMSElog: 4.1338
[epoch 13][iter    0] loss: 46.7952 RMSElog: 4.6795
[epoch 13][iter   10] loss: 42.8277 RMSElog: 4.2828
[epoch 13][iter   20] loss: 49.5636 RMSElog: 4.9564
[epoch 13][iter   30] loss: 48.2874 RMSElog: 4.8287
[epoch 13][iter   40] loss: 34.5350 RMSElog: 3.4535
[epoch 13][iter   50] loss: 32.5867 RMSElog: 3.2587
[epoch 13][iter   60] loss: 46.4541 RMSElog: 4.6454
[epoch 13][iter   70] loss: 51.2534 RMSElog: 5.1253
[epoch 13][iter   80] loss: 39.0001 RMSElog: 3.9000
[epoch 13][iter   90] loss: 43.5346 RMSElog: 4.3535
[epoch 13][iter  100] loss: 45.2501 RMSElog: 4.5250
[epoch 13][iter  110] loss: 38.7011 RMSElog: 3.8701
[epoch 13][iter  120] loss: 45.7773 RMSElog: 4.5777
[epoch 13][iter  130] loss: 41.7159 RMSElog: 4.1716
[epoch 13][iter  140] loss: 40.6867 RMSElog: 4.0687
[epoch 13][iter  150] loss: 47.2098 RMSElog: 4.7210
[epoch 13][iter  160] loss: 43.6494 RMSElog: 4.3649
[epoch 13][iter  170] loss: 39.0615 RMSElog: 3.9061
[epoch 13][iter  180] loss: 46.1529 RMSElog: 4.6153
[epoch 13][iter  190] loss: 35.4372 RMSElog: 3.5437
[epoch 13][iter  200] loss: 35.5486 RMSElog: 3.5549
[epoch 13][iter  210] loss: 50.3341 RMSElog: 5.0334
[epoch 13][iter  220] loss: 45.9408 RMSElog: 4.5941
[epoch 13][iter  230] loss: 43.1681 RMSElog: 4.3168
[epoch 13][iter  240] loss: 42.4587 RMSElog: 4.2459
[epoch 13][iter  250] loss: 48.0450 RMSElog: 4.8045
[epoch 13][iter  260] loss: 43.0238 RMSElog: 4.3024
[epoch 13][iter  270] loss: 41.3038 RMSElog: 4.1304
[epoch 13][iter  280] loss: 51.6042 RMSElog: 5.1604
[epoch 13][iter  290] loss: 45.5120 RMSElog: 4.5512
[epoch 13][iter  300] loss: 46.9804 RMSElog: 4.6980
[epoch 13][iter  310] loss: 42.4694 RMSElog: 4.2469
[epoch 13][iter  320] loss: 38.7036 RMSElog: 3.8704
[epoch 13][iter  330] loss: 46.6004 RMSElog: 4.6600
[epoch 13][iter  340] loss: 37.9795 RMSElog: 3.7980
[epoch 13][iter  350] loss: 35.1068 RMSElog: 3.5107
[epoch 13][iter  360] loss: 40.1132 RMSElog: 4.0113
[epoch 13][iter  370] loss: 39.6225 RMSElog: 3.9622
[epoch 13][iter  380] loss: 37.5527 RMSElog: 3.7553
[epoch 13][iter  390] loss: 46.7085 RMSElog: 4.6708
[epoch 13][iter  400] loss: 42.8733 RMSElog: 4.2873
[epoch 13][iter  410] loss: 47.6634 RMSElog: 4.7663
[epoch 13][iter  420] loss: 42.2560 RMSElog: 4.2256
[epoch 13][iter  430] loss: 44.5457 RMSElog: 4.4546
[epoch 13][iter  440] loss: 42.6293 RMSElog: 4.2629
[epoch 13][iter  450] loss: 51.3064 RMSElog: 5.1306
[epoch 13][iter  460] loss: 49.7712 RMSElog: 4.9771
[epoch 13][iter  470] loss: 45.1069 RMSElog: 4.5107
[epoch 13][iter  480] loss: 44.0638 RMSElog: 4.4064
[epoch 13][iter  490] loss: 52.7617 RMSElog: 5.2762
[epoch 13][iter  500] loss: 49.5819 RMSElog: 4.9582
[epoch 13][iter  510] loss: 43.8453 RMSElog: 4.3845
[epoch 13][iter  520] loss: 42.5980 RMSElog: 4.2598
[epoch 13][iter  530] loss: 45.4116 RMSElog: 4.5412
[epoch 13][iter  540] loss: 42.6215 RMSElog: 4.2622
[epoch 13][iter  550] loss: 46.0449 RMSElog: 4.6045
[epoch 13][iter  560] loss: 42.9898 RMSElog: 4.2990
[epoch 13][iter  570] loss: 43.5527 RMSElog: 4.3553
[epoch 13][iter  580] loss: 38.5143 RMSElog: 3.8514
[epoch 13][iter  590] loss: 37.7260 RMSElog: 3.7726
[epoch 14][iter    0] loss: 37.4193 RMSElog: 3.7419
[epoch 14][iter   10] loss: 47.9833 RMSElog: 4.7983
[epoch 14][iter   20] loss: 44.6884 RMSElog: 4.4688
[epoch 14][iter   30] loss: 45.0265 RMSElog: 4.5027
[epoch 14][iter   40] loss: 48.6310 RMSElog: 4.8631
[epoch 14][iter   50] loss: 43.9794 RMSElog: 4.3979
[epoch 14][iter   60] loss: 51.1944 RMSElog: 5.1194
[epoch 14][iter   70] loss: 36.5579 RMSElog: 3.6558
[epoch 14][iter   80] loss: 53.7767 RMSElog: 5.3777
[epoch 14][iter   90] loss: 40.4430 RMSElog: 4.0443
[epoch 14][iter  100] loss: 38.9982 RMSElog: 3.8998
[epoch 14][iter  110] loss: 47.4006 RMSElog: 4.7401
[epoch 14][iter  120] loss: 43.2084 RMSElog: 4.3208
[epoch 14][iter  130] loss: 39.6668 RMSElog: 3.9667
[epoch 14][iter  140] loss: 41.2993 RMSElog: 4.1299
[epoch 14][iter  150] loss: 43.3623 RMSElog: 4.3362
[epoch 14][iter  160] loss: 48.7026 RMSElog: 4.8703
[epoch 14][iter  170] loss: 45.8351 RMSElog: 4.5835
[epoch 14][iter  180] loss: 46.5709 RMSElog: 4.6571
[epoch 14][iter  190] loss: 43.5898 RMSElog: 4.3590
[epoch 14][iter  200] loss: 46.8653 RMSElog: 4.6865
[epoch 14][iter  210] loss: 36.8455 RMSElog: 3.6846
[epoch 14][iter  220] loss: 47.9531 RMSElog: 4.7953
[epoch 14][iter  230] loss: 41.3246 RMSElog: 4.1325
[epoch 14][iter  240] loss: 41.0125 RMSElog: 4.1012
[epoch 14][iter  250] loss: 42.5286 RMSElog: 4.2529
[epoch 14][iter  260] loss: 36.0605 RMSElog: 3.6061
[epoch 14][iter  270] loss: 40.0006 RMSElog: 4.0001
[epoch 14][iter  280] loss: 43.6395 RMSElog: 4.3639
[epoch 14][iter  290] loss: 48.4283 RMSElog: 4.8428
[epoch 14][iter  300] loss: 43.4257 RMSElog: 4.3426
[epoch 14][iter  310] loss: 46.5703 RMSElog: 4.6570
[epoch 14][iter  320] loss: 49.9151 RMSElog: 4.9915
[epoch 14][iter  330] loss: 46.0386 RMSElog: 4.6039
[epoch 14][iter  340] loss: 50.8836 RMSElog: 5.0884
[epoch 14][iter  350] loss: 35.0250 RMSElog: 3.5025
[epoch 14][iter  360] loss: 42.8000 RMSElog: 4.2800
[epoch 14][iter  370] loss: 46.8181 RMSElog: 4.6818
[epoch 14][iter  380] loss: 38.3037 RMSElog: 3.8304
[epoch 14][iter  390] loss: 38.9370 RMSElog: 3.8937
[epoch 14][iter  400] loss: 50.0456 RMSElog: 5.0046
[epoch 14][iter  410] loss: 42.7095 RMSElog: 4.2710
[epoch 14][iter  420] loss: 41.8304 RMSElog: 4.1830
[epoch 14][iter  430] loss: 39.9729 RMSElog: 3.9973
[epoch 14][iter  440] loss: 41.5349 RMSElog: 4.1535
[epoch 14][iter  450] loss: 31.9751 RMSElog: 3.1975
[epoch 14][iter  460] loss: 36.6210 RMSElog: 3.6621
[epoch 14][iter  470] loss: 51.8759 RMSElog: 5.1876
[epoch 14][iter  480] loss: 52.0904 RMSElog: 5.2090
[epoch 14][iter  490] loss: 41.1142 RMSElog: 4.1114
[epoch 14][iter  500] loss: 46.8143 RMSElog: 4.6814
[epoch 14][iter  510] loss: 47.5595 RMSElog: 4.7560
[epoch 14][iter  520] loss: 30.9949 RMSElog: 3.0995
[epoch 14][iter  530] loss: 42.4894 RMSElog: 4.2489
[epoch 14][iter  540] loss: 41.1672 RMSElog: 4.1167
[epoch 14][iter  550] loss: 43.9040 RMSElog: 4.3904
[epoch 14][iter  560] loss: 36.2504 RMSElog: 3.6250
[epoch 14][iter  570] loss: 39.9556 RMSElog: 3.9956
[epoch 14][iter  580] loss: 37.7501 RMSElog: 3.7750
[epoch 14][iter  590] loss: 43.2417 RMSElog: 4.3242
#########################################################
#T5 new writing format with mean loss values per epoch###
#########################################################
Epoch: 0 	Training Loss: 218.323489 	Validation Loss: 3190.639995
Epoch: 1 	Training Loss: 161.791607 	Validation Loss: 2971.991576
Epoch: 2 	Training Loss: 153.060507 	Validation Loss: 2943.733243
Epoch: 3 	Training Loss: 147.769037 	Validation Loss: 2913.439976
Epoch: 4 	Training Loss: 144.670657 	Validation Loss: 2981.408303
Epoch: 5 	Training Loss: 141.627008 	Validation Loss: 3036.567865
Epoch: 6 	Training Loss: 136.028979 	Validation Loss: 2960.067773
Epoch: 7 	Training Loss: 134.438687 	Validation Loss: 2990.395778
Epoch: 8 	Training Loss: 133.278901 	Validation Loss: 2984.382511
Epoch: 9 	Training Loss: 132.745751 	Validation Loss: 3005.913722
Epoch: 10 	Training Loss: 131.944287 	Validation Loss: 2985.078438
Epoch: 11 	Training Loss: 131.310158 	Validation Loss: 3002.103553
Epoch: 12 	Training Loss: 130.691597 	Validation Loss: 2960.858379
Epoch: 13 	Training Loss: 130.358167 	Validation Loss: 2963.756556
Epoch: 14 	Training Loss: 130.327321 	Validation Loss: 2987.865914
########################################################################################
#T6 OpenCV for reading images and new writing format with mean loss values per epoch###
######################################################################################
Epoch: 0 	Training Loss: 72.505482 	Validation Loss: 647.524093
Epoch: 1 	Training Loss: 52.735931 	Validation Loss: 579.435907
Epoch: 2 	Training Loss: 49.588619 	Validation Loss: 578.332334
Epoch: 3 	Training Loss: 47.645998 	Validation Loss: 570.564801
Epoch: 4 	Training Loss: 46.516390 	Validation Loss: 575.315201
Epoch: 5 	Training Loss: 45.500057 	Validation Loss: 542.247915
Epoch: 6 	Training Loss: 43.742459 	Validation Loss: 568.261544
Epoch: 7 	Training Loss: 43.168033 	Validation Loss: 555.085680
Epoch: 8 	Training Loss: 42.848036 	Validation Loss: 563.829187
Epoch: 9 	Training Loss: 42.592307 	Validation Loss: 564.242237
Epoch: 10 	Training Loss: 42.362520 	Validation Loss: 563.083668
Epoch: 11 	Training Loss: 42.173374 	Validation Loss: 564.704679
Epoch: 12 	Training Loss: 41.961735 	Validation Loss: 565.880260
Epoch: 13 	Training Loss: 41.932512 	Validation Loss: 563.702862
Epoch: 14 	Training Loss: 41.911617 	Validation Loss: 562.930368
#####################################################
#T7 Correct OpenCV + scaled with 6000 max distance###
#####################################################
Epoch: 0 	Training Loss: 71.522216 	Validation Loss: 634.709847
Epoch: 1 	Training Loss: 52.496933 	Validation Loss: 568.594156
Epoch: 0 	Training Loss: 71.441583 	Validation Loss: 582.000842
Epoch: 1 	Training Loss: 52.667783 	Validation Loss: 550.886978
Epoch: 2 	Training Loss: 49.435081 	Validation Loss: 569.665952
Epoch: 3 	Training Loss: 47.854489 	Validation Loss: 546.760939
Epoch: 4 	Training Loss: 46.669482 	Validation Loss: 560.649896
Epoch: 5 	Training Loss: 45.435553 	Validation Loss: 559.654089
Epoch: 6 	Training Loss: 43.736908 	Validation Loss: 562.862632
Epoch: 7 	Training Loss: 43.174027 	Validation Loss: 563.286662
Epoch: 8 	Training Loss: 42.861744 	Validation Loss: 566.057721
Epoch: 9 	Training Loss: 42.615629 	Validation Loss: 566.830226
Epoch: 10 	Training Loss: 42.380751 	Validation Loss: 559.617975
Epoch: 11 	Training Loss: 42.181563 	Validation Loss: 558.789870
Epoch: 12 	Training Loss: 41.964575 	Validation Loss: 562.879307
Epoch: 13 	Training Loss: 41.930627 	Validation Loss: 562.940166
Epoch: 14 	Training Loss: 41.909513 	Validation Loss: 561.895508
############################################################################
#T8 Normalization and output scaled with dataset's max depth value (6571)###
############################################################################
Epoch: 0 	Training Loss: 73.591091 	Validation Loss: 334.585985
Epoch: 1 	Training Loss: 52.881647 	Validation Loss: 300.649156
Epoch: 2 	Training Loss: 49.718020 	Validation Loss: 292.941894
Epoch: 3 	Training Loss: 47.831229 	Validation Loss: 293.486121
Epoch: 4 	Training Loss: 46.425209 	Validation Loss: 302.485963
Epoch: 5 	Training Loss: 45.433531 	Validation Loss: 291.521615
Epoch: 6 	Training Loss: 43.696943 	Validation Loss: 282.382142
Epoch: 7 	Training Loss: 43.127439 	Validation Loss: 281.995301
Epoch: 8 	Training Loss: 42.818761 	Validation Loss: 283.636910
Epoch: 9 	Training Loss: 42.551161 	Validation Loss: 284.511475
Epoch: 10 	Training Loss: 42.330795 	Validation Loss: 285.507601
Epoch: 11 	Training Loss: 42.136648 	Validation Loss: 285.606830
Epoch: 12 	Training Loss: 41.928242 	Validation Loss: 284.750849
Epoch: 13 	Training Loss: 41.903246 	Validation Loss: 285.224225
Epoch: 14 	Training Loss: 41.883721 	Validation Loss: 284.111349
########################################################################
#T9 Replaced the RMSElog with L1 + fixed validation loss calculations###
########################################################################
Epoch: 0 	Training Loss: 348695.798333 	Validation Loss: 196926.170417
Epoch: 1 	Training Loss: 349168.011146 	Validation Loss: 196926.170417
Epoch: 2 	Training Loss: 349168.011146 	Validation Loss: 196926.170417
Epoch: 3 	Training Loss: 349168.011146 	Validation Loss: 196926.170417
Epoch: 4 	Training Loss: 349168.011146 	Validation Loss: 196926.170417
###############################
#T10 RMSElog + Gradient loss###
###############################
Epoch: 0 	Training Loss: 72.817112 	Validation Loss: 31.633706
Epoch: 1 	Training Loss: 52.483049 	Validation Loss: 30.168883
Epoch: 2 	Training Loss: 49.284091 	Validation Loss: 29.629003
Epoch: 3 	Training Loss: 47.537898 	Validation Loss: 28.984106
Epoch: 4 	Training Loss: 16436.115889 	Validation Loss: 9262.647938
Epoch: 5 	Training Loss: 16428.003895 	Validation Loss: 9262.647938
Epoch: 6 	Training Loss: 16428.003895 	Validation Loss: 9262.647938
#without multiplying RMSElog with 10
Epoch: 0 	Training Loss: 9.832156 	Validation Loss: 5.534951
Epoch: 1 	Training Loss: 9.740003 	Validation Loss: 5.087373
Epoch: 2 	Training Loss: 7.958119 	Validation Loss: 3.942562
Epoch: 3 	Training Loss: 6.490294 	Validation Loss: 3.572223
Epoch: 4 	Training Loss: 15508.536245 	Validation Loss: 8808.685171
Epoch: 5 	Training Loss: 15506.100960 	Validation Loss: 8808.685171
Epoch: 6 	Training Loss: 15506.100960 	Validation Loss: 8808.685171
Epoch: 7 	Training Loss: 15506.100960 	Validation Loss: 8808.685171
Epoch: 8 	Training Loss: 15506.100960 	Validation Loss: 8808.685171
#with 10*RMSElog+0.01*Gradloss
Epoch: 0 	Training Loss: 71.032243 	Validation Loss: 31.405424
Epoch: 1 	Training Loss: 52.342564 	Validation Loss: 30.076333
Epoch: 2 	Training Loss: 49.236700 	Validation Loss: 29.189601
Epoch: 3 	Training Loss: 47.607095 	Validation Loss: 28.712669
Epoch: 4 	Training Loss: 207.849284 	Validation Loss: 119.254914
Epoch: 5 	Training Loss: 205.001159 	Validation Loss: 118.963997
Epoch: 6 	Training Loss: 203.225360 	Validation Loss: 118.457929
Epoch: 7 	Training Loss: 202.604656 	Validation Loss: 118.713248
Epoch: 8 	Training Loss: 202.252168 	Validation Loss: 118.581607
Epoch: 9 	Training Loss: 201.974782 	Validation Loss: 118.437636
Epoch: 10 	Training Loss: 201.741446 	Validation Loss: 118.658782
Epoch: 11 	Training Loss: 201.517309 	Validation Loss: 118.785269
Epoch: 12 	Training Loss: 201.293668 	Validation Loss: 118.764189
Epoch: 13 	Training Loss: 201.262652 	Validation Loss: 118.558743
Epoch: 14 	Training Loss: 201.239118 	Validation Loss: 118.499354
#######################################
#T11 10*RMSElog + 0.001Gradient loss###
#######################################
Epoch: 0 	Training Loss: 71.832258 	Validation Loss: 33.474613
Epoch: 1 	Training Loss: 52.359627 	Validation Loss: 30.055750
Epoch: 2 	Training Loss: 49.235497 	Validation Loss: 29.139652
Epoch: 3 	Training Loss: 47.685723 	Validation Loss: 29.724331
Epoch: 4 	Training Loss: 64.230400 	Validation Loss: 122.345865
Epoch: 5 	Training Loss: 62.474140 	Validation Loss: 122.662295
Epoch: 6 	Training Loss: 60.797856 	Validation Loss: 121.472427
Epoch: 7 	Training Loss: 60.225152 	Validation Loss: 122.062222
Epoch: 8 	Training Loss: 59.901332 	Validation Loss: 121.686721
Epoch: 9 	Training Loss: 59.648156 	Validation Loss: 122.240948
Epoch: 10 	Training Loss: 59.427121 	Validation Loss: 121.564282
Epoch: 11 	Training Loss: 59.234733 	Validation Loss: 121.758565
Epoch: 12 	Training Loss: 59.032521 	Validation Loss: 122.386282
Epoch: 13 	Training Loss: 59.006660 	Validation Loss: 121.685634
Epoch: 14 	Training Loss: 58.986532 	Validation Loss: 121.968695
##########################################################
#T12 10*(RMSElog + 0.001Gradient) loss from first epoch###
##########################################################
Epoch: 0 	Training Loss: 234.860366 	Validation Loss: 123.409271
Epoch: 1 	Training Loss: 213.608231 	Validation Loss: 120.461459
Epoch: 2 	Training Loss: 210.068082 	Validation Loss: 119.966006
Epoch: 3 	Training Loss: 207.647689 	Validation Loss: 118.847557
Epoch: 4 	Training Loss: 206.009137 	Validation Loss: 119.613805
Epoch: 5 	Training Loss: 204.814821 	Validation Loss: 118.515004
Epoch: 6 	Training Loss: 203.006403 	Validation Loss: 117.970202
Epoch: 7 	Training Loss: 202.441845 	Validation Loss: 118.178787
Epoch: 8 	Training Loss: 202.105901 	Validation Loss: 118.267506
Epoch: 9 	Training Loss: 201.841144 	Validation Loss: 118.128915
Epoch: 10 	Training Loss: 201.598701 	Validation Loss: 118.181688
Epoch: 11 	Training Loss: 201.381984 	Validation Loss: 118.349451
Epoch: 12 	Training Loss: 201.168103 	Validation Loss: 118.388195
Epoch: 13 	Training Loss: 201.139963 	Validation Loss: 118.110719
Epoch: 14 	Training Loss: 201.117745 	Validation Loss: 118.385841
###########################################################
#T13 10*(RMSElog + 0.001Gradient) loss from fifth epoch###
###########################################################
Epoch: 0 	Training Loss: 72.242223 	Validation Loss: 31.889383
Epoch: 1 	Training Loss: 52.551342 	Validation Loss: 33.087305
Epoch: 2 	Training Loss: 49.390865 	Validation Loss: 29.670458
Epoch: 3 	Training Loss: 47.580885 	Validation Loss: 30.022435
Epoch: 4 	Training Loss: 209.201722 	Validation Loss: 119.522692
Epoch: 5 	Training Loss: 205.865814 	Validation Loss: 119.919068
Epoch: 6 	Training Loss: 204.083051 	Validation Loss: 118.460484
Epoch: 7 	Training Loss: 203.495021 	Validation Loss: 118.269128
Epoch: 8 	Training Loss: 203.177149 	Validation Loss: 118.292333
Epoch: 9 	Training Loss: 202.906779 	Validation Loss: 118.366459
Epoch: 10 	Training Loss: 202.667279 	Validation Loss: 118.741499
Epoch: 11 	Training Loss: 202.461366 	Validation Loss: 118.504895
Epoch: 12 	Training Loss: 202.250685 	Validation Loss: 118.536363
#############################################################
#T14 10*(RMSElog + 0.0001Gradient) loss from fifth epoch###
###########################################################
Epoch: 0 	Training Loss: 71.555313 	Validation Loss: 31.761263
Epoch: 1 	Training Loss: 52.534996 	Validation Loss: 30.018957
Epoch: 2 	Training Loss: 49.329313 	Validation Loss: 30.028566
Epoch: 3 	Training Loss: 47.687178 	Validation Loss: 29.279360
Epoch: 4 	Training Loss: 63.943180 	Validation Loss: 39.075531
Epoch: 5 	Training Loss: 62.471233 	Validation Loss: 39.974888
Epoch: 6 	Training Loss: 60.849002 	Validation Loss: 37.791539
Epoch: 7 	Training Loss: 60.237083 	Validation Loss: 37.898419
Epoch: 8 	Training Loss: 59.920208 	Validation Loss: 38.309986
Epoch: 9 	Training Loss: 59.660524 	Validation Loss: 38.034950
Epoch: 10 	Training Loss: 59.447020 	Validation Loss: 37.896212
Epoch: 11 	Training Loss: 59.247549 	Validation Loss: 37.881258
Epoch: 12 	Training Loss: 59.045494 	Validation Loss: 38.091940
Epoch: 13 	Training Loss: 59.015735 	Validation Loss: 37.977092
Epoch: 14 	Training Loss: 58.996475 	Validation Loss: 38.082026
#############################################################
#T15 10*(RMSElog + 0.0001Gradient) loss from 9th epoch###
###########################################################
Epoch: 0 	Training Loss: 70.149903 	Validation Loss: 32.802570
Epoch: 1 	Training Loss: 52.564960 	Validation Loss: 29.945798
Epoch: 2 	Training Loss: 49.248309 	Validation Loss: 29.441234
Epoch: 3 	Training Loss: 47.595019 	Validation Loss: 28.755488
Epoch: 4 	Training Loss: 46.251358 	Validation Loss: 28.579883
Epoch: 5 	Training Loss: 45.491962 	Validation Loss: 28.726109
Epoch: 6 	Training Loss: 43.749567 	Validation Loss: 28.551801
Epoch: 7 	Training Loss: 43.185884 	Validation Loss: 28.114691
Epoch: 8 	Training Loss: 42.863575 	Validation Loss: 28.386005
Epoch: 9 	Training Loss: 61.542229 	Validation Loss: 38.463959
Epoch: 10 	Training Loss: 60.201215 	Validation Loss: 38.549866
Epoch: 11 	Training Loss: 59.836116 	Validation Loss: 38.524443
Epoch: 12 	Training Loss: 59.567863 	Validation Loss: 38.203795
Epoch: 13 	Training Loss: 59.529789 	Validation Loss: 38.482591
Epoch: 14 	Training Loss: 59.500130 	Validation Loss: 38.195396
#############################################################
#T16 10*(RMSElog + 0.00001Gradient) loss from 6th epoch###
###########################################################
Epoch: 0 	Training Loss: 72.153392 	Validation Loss: 31.264566
Epoch: 1 	Training Loss: 52.314216 	Validation Loss: 29.856190
Epoch: 2 	Training Loss: 49.115639 	Validation Loss: 28.966565
Epoch: 3 	Training Loss: 47.485217 	Validation Loss: 29.830860
Epoch: 4 	Training Loss: 46.351494 	Validation Loss: 29.946467
Epoch: 5 	Training Loss: 45.416158 	Validation Loss: 29.208772
Epoch: 6 	Training Loss: 45.764047 	Validation Loss: 29.254085
Epoch: 7 	Training Loss: 45.144117 	Validation Loss: 29.461738
Epoch: 8 	Training Loss: 44.806133 	Validation Loss: 29.326297
Epoch: 9 	Training Loss: 44.531219 	Validation Loss: 29.466952
Epoch: 10 	Training Loss: 44.310029 	Validation Loss: 29.677233
Epoch: 11 	Training Loss: 44.108825 	Validation Loss: 29.437116
Epoch: 12 	Training Loss: 43.908629 	Validation Loss: 29.490945
Epoch: 13 	Training Loss: 43.879263 	Validation Loss: 29.470908
Epoch: 14 	Training Loss: 43.861073 	Validation Loss: 29.600205
#####################################################################################################
#T17 10*(RMSElog + 0.001Gradient) loss from 6th epoch and rescale only when representing the image###
#####################################################################################################
Epoch: 0 	Training Loss: 46.262334 	Validation Loss: 124.492148
Epoch: 1 	Training Loss: 33.890917 	Validation Loss: 124.665089
Epoch: 2 	Training Loss: 31.661771 	Validation Loss: 124.624641
Epoch: 3 	Training Loss: 30.552040 	Validation Loss: 124.468659
Epoch: 4 	Training Loss: 29.588214 	Validation Loss: 124.987759
Epoch: 5 	Training Loss: 28.949841 	Validation Loss: 124.473632
Epoch: 6 	Training Loss: 27.932352 	Validation Loss: 212.520806
Epoch: 7 	Training Loss: 27.590929 	Validation Loss: 213.084324
Epoch: 8 	Training Loss: 27.392887 	Validation Loss: 212.731555
Epoch: 9 	Training Loss: 27.224437 	Validation Loss: 213.287331
Epoch: 10 	Training Loss: 27.085230 	Validation Loss: 212.823790
Epoch: 11 	Training Loss: 26.965576 	Validation Loss: 213.489338
Epoch: 12 	Training Loss: 26.836066 	Validation Loss: 212.749622
Epoch: 13 	Training Loss: 26.819028 	Validation Loss: 212.579156
Epoch: 14 	Training Loss: 26.806246 	Validation Loss: 213.137326
#######################################################
#T18 10*(RMSElog + Gradient) loss from fourth epoch###
#####################################################
Epoch: 0 	Training Loss: 45.695432 	Validation Loss: 124.565707
Epoch: 1 	Training Loss: 33.789065 	Validation Loss: 124.531511
Epoch: 2 	Training Loss: 31.646521 	Validation Loss: 124.467397
Epoch: 3 	Training Loss: 30.369382 	Validation Loss: 124.488080
Epoch: 4 	Training Loss: 55.872741 	Validation Loss: 88086.851940
Epoch: 5 	Training Loss: 54.573107 	Validation Loss: 88086.852539
Epoch: 6 	Training Loss: 53.508264 	Validation Loss: 88086.853216
Epoch: 7 	Training Loss: 53.153942 	Validation Loss: 88086.858867
Epoch: 8 	Training Loss: 52.954895 	Validation Loss: 88086.867487
Epoch: 9 	Training Loss: 52.791361 	Validation Loss: 88086.866341
Epoch: 10 	Training Loss: 52.656817 	Validation Loss: 88086.864388
##########################################################
#T19 10*(RMSElog + 0.1Gradient) loss from fourth epoch###
########################################################
Epoch: 0 	Training Loss: 47.290279 	Validation Loss: 20.942171
Epoch: 1 	Training Loss: 34.036138 	Validation Loss: 20.024993
Epoch: 2 	Training Loss: 31.726348 	Validation Loss: 19.564738
Epoch: 3 	Training Loss: 30.536148 	Validation Loss: 18.665320
Epoch: 4 	Training Loss: 32.638300 	Validation Loss: 20.142975
Epoch: 5 	Training Loss: 31.876325 	Validation Loss: 20.035331
Epoch: 6 	Training Loss: 30.783780 	Validation Loss: 19.862722
Epoch: 7 	Training Loss: 30.465918 	Validation Loss: 19.631996
Epoch: 8 	Training Loss: 30.272671 	Validation Loss: 19.769627
Epoch: 9 	Training Loss: 30.119601 	Validation Loss: 19.744707
Epoch: 10 	Training Loss: 29.983248 	Validation Loss: 20.013631
Epoch: 11 	Training Loss: 29.863599 	Validation Loss: 20.121653
Epoch: 12 	Training Loss: 29.744425 	Validation Loss: 20.005530
Epoch: 13 	Training Loss: 29.726055 	Validation Loss: 19.808809
Epoch: 14 	Training Loss: 29.714553 	Validation Loss: 19.944820
##########################################################
#T20 10*(RMSElog + 0.01Gradient) loss from fourth epoch###
########################################################
Epoch: 0 	Training Loss: 46.329153 	Validation Loss: 20.795426
Epoch: 1 	Training Loss: 33.980463 	Validation Loss: 20.051017
Epoch: 2 	Training Loss: 31.762273 	Validation Loss: 18.553545
Epoch: 3 	Training Loss: 30.518021 	Validation Loss: 18.597357
Epoch: 4 	Training Loss: 29.915605 	Validation Loss: 18.968144
Epoch: 5 	Training Loss: 29.302504 	Validation Loss: 18.544869
Epoch: 6 	Training Loss: 28.282091 	Validation Loss: 18.484615
Epoch: 7 	Training Loss: 27.914046 	Validation Loss: 18.204546
Epoch: 8 	Training Loss: 27.707277 	Validation Loss: 18.407984
Epoch: 9 	Training Loss: 27.542064 	Validation Loss: 18.395518
Epoch: 10 	Training Loss: 27.403059 	Validation Loss: 18.595136
Epoch: 11 	Training Loss: 27.279787 	Validation Loss: 18.428205
Epoch: 12 	Training Loss: 27.159425 	Validation Loss: 18.514055
Epoch: 13 	Training Loss: 27.141705 	Validation Loss: 18.494223
Epoch: 14 	Training Loss: 27.129397 	Validation Loss: 18.615933