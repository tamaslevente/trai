{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook will help you train a latent Point-Cloud GAN.\n",
    "\n",
    "(Assumes latent_3d_points is in the PYTHONPATH and that a trained AE model exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os.path as osp\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from latent_3d_points.src.point_net_ae import PointNetAutoEncoder\n",
    "from latent_3d_points.src.autoencoder import Configuration as Conf\n",
    "from latent_3d_points.src.neural_net import MODEL_SAVER_ID\n",
    "\n",
    "from latent_3d_points.src.in_out import snc_category_to_synth_id, create_dir, PointCloudDataSet, \\\n",
    "                                        load_all_point_clouds_under_folder\n",
    "\n",
    "from latent_3d_points.src.general_utils import plot_3d_point_cloud\n",
    "from latent_3d_points.src.tf_utils import reset_tf_graph\n",
    "\n",
    "from latent_3d_points.src.vanilla_gan import Vanilla_GAN\n",
    "from latent_3d_points.src.w_gan_gp import W_GAN_GP\n",
    "from latent_3d_points.src.generators_discriminators import latent_code_discriminator_two_layers,\\\n",
    "latent_code_generator_two_layers\n",
    "\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from open3d import JVisualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify where the raw point-clouds and the pre-trained AE are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top-dir of where point-clouds are stored.\n",
    "top_in_dir = '../data/shape_net_core_uniform_samples_2048/'    \n",
    "\n",
    "ae_configuration = '../data/single_class_ae/configuration'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save GANs check-points etc.\n",
    "top_out_dir = '../data/'\n",
    "experiment_name = 'latent_gan_with_chamfer_ae'\n",
    "\n",
    "ae_epoch = 500           # Epoch of AE to load.\n",
    "bneck_size = 128         # Bottleneck-size of the AE\n",
    "n_pc_points = 2048       # Number of points per model.\n",
    "\n",
    "# class_name = raw_input('Give me the class name (e.g. \"chair\"): ').lower()\n",
    "class_name = \"chair\"\n",
    "class_name = class_name.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load point-clouds.\n",
    "syn_id = snc_category_to_synth_id()[class_name]\n",
    "# class_dir = osp.join(top_in_dir , syn_id)\n",
    "class_dir = osp.join(top_in_dir , \"own\")\n",
    "# class_dir = osp.join(top_in_dir , snc_category_to_synth_id()[\"table\"])\n",
    "all_pc_data = load_all_point_clouds_under_folder(class_dir, n_threads=8, file_ending='.ply', verbose=True)\n",
    "print 'Shape of DATA =', all_pc_data.point_clouds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained AE\n",
    "reset_tf_graph()\n",
    "ae_conf = Conf.load(ae_configuration)\n",
    "ae_conf.encoder_args['verbose'] = False\n",
    "ae_conf.decoder_args['verbose'] = False\n",
    "ae = PointNetAutoEncoder(ae_conf.experiment_name, ae_conf)\n",
    "ae.restore_model(ae_conf.train_dir, ae_epoch, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use AE to convert raw pointclouds to latent codes.\n",
    "latent_codes = ae.get_latent_codes(all_pc_data.point_clouds)\n",
    "latent_data = PointCloudDataSet(latent_codes)\n",
    "print 'Shape of DATA =', latent_data.point_clouds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the decoded AE latent-codes look descent.\n",
    "L = ae.decode(latent_codes)\n",
    "i = 0\n",
    "plot_3d_point_cloud(L[i][:, 0], L[i][:, 1], L[i][:, 2], in_u_sphere=True);\n",
    "i = 20\n",
    "plot_3d_point_cloud(L[i][:, 0], L[i][:, 1], L[i][:, 2], in_u_sphere=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "L = ae.decode(latent_codes)\n",
    "for i in range(1):\n",
    "    points = L[i]\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(points)\n",
    "\n",
    "    visualizer = JVisualizer()\n",
    "    visualizer.add_geometry(pcd)\n",
    "    visualizer.show()\n",
    "\n",
    "    o3d.io.write_point_cloud(\"pcd_aereconstruction\"+str(i)+\".pcd\", pcd, write_ascii=False, compressed=False, print_progress=False)\n",
    "# ###############\n",
    "for i in range(1):\n",
    "    pcdin=all_pc_data.point_clouds\n",
    "    pointsin = pcdin[i]\n",
    "    print(type(pointsin))\n",
    "    pcd_in = o3d.geometry.PointCloud()\n",
    "    pcd_in.points = o3d.utility.Vector3dVector(pointsin)\n",
    "    visualizer_in = JVisualizer()\n",
    "    visualizer_in.add_geometry(pcd_in)\n",
    "    visualizer_in.show()\n",
    "    o3d.io.write_point_cloud(\"pcd_input\"+str(i)+\".pcd\", pcd_in, write_ascii=False, compressed=False, print_progress=False)\n",
    "\n",
    "\n",
    "############################\n",
    "latent_codes = ae.get_latent_codes(all_pc_data.point_clouds)\n",
    "print(latent_codes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(o3d.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set GAN parameters.\n",
    "\n",
    "use_wgan = True     # Wasserstein with gradient penalty, or not?\n",
    "n_epochs = 1        # Epochs to train.\n",
    "\n",
    "plot_train_curve = True\n",
    "save_gan_model = False\n",
    "saver_step = np.hstack([np.array([1, 5, 10]), np.arange(50, n_epochs + 1, 50)])\n",
    "\n",
    "# If true, every 'saver_step' epochs we produce & save synthetic pointclouds.\n",
    "save_synthetic_samples = True\n",
    "# How many synthetic samples to produce at each save step.\n",
    "n_syn_samples = latent_data.num_examples\n",
    "\n",
    "# Optimization parameters\n",
    "init_lr = 0.0001\n",
    "batch_size = 50\n",
    "noise_params = {'mu':0, 'sigma': 0.2}\n",
    "noise_dim = bneck_size\n",
    "beta = 0.5 # ADAM's momentum.\n",
    "\n",
    "n_out = [bneck_size] # Dimensionality of generated samples.\n",
    "\n",
    "if save_synthetic_samples:\n",
    "    synthetic_data_out_dir = osp.join(top_out_dir, 'OUT/synthetic_samples/', experiment_name)\n",
    "    create_dir(synthetic_data_out_dir)\n",
    "\n",
    "if save_gan_model:\n",
    "    train_dir = osp.join(top_out_dir, 'OUT/latent_gan', experiment_name)\n",
    "    create_dir(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_tf_graph()\n",
    "\n",
    "if use_wgan:\n",
    "    lam = 10 # lambda of W-GAN-GP\n",
    "    gan = W_GAN_GP(experiment_name, init_lr, lam, n_out, noise_dim, \\\n",
    "                  latent_code_discriminator_two_layers, \n",
    "                  latent_code_generator_two_layers,\\\n",
    "                  beta=beta)\n",
    "else:    \n",
    "    gan = Vanilla_GAN(experiment_name, init_lr, n_out, noise_dim,\n",
    "                     latent_code_discriminator_two_layers, latent_code_generator_two_layers,\n",
    "                     beta=beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accum_syn_data = []\n",
    "train_stats = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hack(self, n_samples, noise_params):\n",
    "        noise = self.generator_noise_distribution_hack(n_samples, self.noise_dim, **noise_params)\n",
    "        feed_dict = {self.noise: noise}\n",
    "        return self.sess.run([self.generator_out], feed_dict=feed_dict)[0]\n",
    "    \n",
    "def generator_noise_distribution_hack(self, n_samples, ndims, mu, sigma):\n",
    "        return np.random.normal(mu, sigma, (n_samples, ndims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train the GAN.\n",
    "\n",
    "for _ in range(n_epochs):\n",
    "    loss, duration = gan._single_epoch_train(latent_data, batch_size, noise_params)\n",
    "    epoch = int(gan.sess.run(gan.increment_epoch))\n",
    "    print epoch, loss\n",
    "\n",
    "    if save_gan_model and epoch in saver_step:\n",
    "        checkpoint_path = osp.join(train_dir, MODEL_SAVER_ID)\n",
    "        gan.saver.save(gan.sess, checkpoint_path, global_step=gan.epoch)\n",
    "\n",
    "#     if save_synthetic_samples and epoch in saver_step:\n",
    "\n",
    "    syn_latent_data = gan.generate(n_syn_samples, noise_params) #original\n",
    "#     syn_latent_data = gan.generate_chair(n_syn_samples, noise_params) #hardcoded chair\n",
    "#     syn_latent_data = gan.generate_chair_ml(n_syn_samples, noise_params) #hardcoded chair with missing leg\n",
    "#     syn_latent_data = gan.generate_table(n_syn_samples, noise_params) #hardcoded table\n",
    "    syn_data = ae.decode(syn_latent_data)\n",
    "    np.savez(osp.join(synthetic_data_out_dir, 'epoch_' + str(epoch)), syn_data)\n",
    "    print(syn_data[0].size)\n",
    "    for k in range(10):  # plot three (syntetic) random examples.\n",
    "#             plot_3d_point_cloud(syn_data[k][:, 0], syn_data[k][:, 1], syn_data[k][:, 2],\n",
    "#                                in_u_sphere=True)\n",
    "        points = syn_data[k]\n",
    "#             print(points)\n",
    "        lines = points.size\n",
    "        colors = [[1, 0, 0] for i in range(lines)]\n",
    "        pcd = o3d.geometry.PointCloud()\n",
    "        pcd.points = o3d.utility.Vector3dVector(points)\n",
    "        pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "\n",
    "        visualizer = JVisualizer()\n",
    "        visualizer.add_geometry(pcd)\n",
    "        visualizer.show()\n",
    "        o3d.io.write_point_cloud(\"pcd_generated\"+str(k)+\".pcd\", pcd, write_ascii=False, compressed=False, print_progress=False)\n",
    "\n",
    "    train_stats.append((epoch, ) + loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if plot_train_curve:\n",
    "    x = range(len(train_stats))\n",
    "    d_loss = [t[1] for t in train_stats]\n",
    "    g_loss = [t[2] for t in train_stats]\n",
    "    plt.plot(x, d_loss, '--')\n",
    "    plt.plot(x, g_loss)\n",
    "    plt.title('Latent GAN training. (%s)' %(class_name))\n",
    "    plt.legend(['Discriminator', 'Generator'], loc=0)\n",
    "    \n",
    "    plt.tick_params(axis='x', which='both', bottom='off', top='off')\n",
    "    plt.tick_params(axis='y', which='both', left='off', right='off')\n",
    "    \n",
    "    plt.xlabel('Epochs.') \n",
    "    plt.ylabel('Loss.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
